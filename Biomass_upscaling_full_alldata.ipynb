{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deac3630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a069d5",
   "metadata": {},
   "source": [
    "## Import Fluxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1496f5e9",
   "metadata": {},
   "source": [
    "## Upscaling of Biomass Growth Time-Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63996f90-20da-4da2-86ec-5fea99139d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time steps and features to extract\n",
    "#file_path = '/mnt/data/high_losing_42_11_1xscale.h5'\n",
    "import h5py\n",
    "import cv2\n",
    "\n",
    "base_dir = '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/'\n",
    "f_dir = os.listdir(base_dir)\n",
    "h5_files = []\n",
    "h5_files_train = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in f_dir:\n",
    "    if (filename.endswith(\".h5\")) & (((\"2x\" in filename))):\n",
    "        h5_files_train.append(base_dir+filename)\n",
    "#     if (filename.endswith(\".h5\")) & ((\"8x\" in filename)):\n",
    "#         if ((\"10_5\" not in filename) & (\"20_20\" not in filename)):\n",
    "#             h5_files_train.append(base_dir+filename)\n",
    "    if (filename.endswith(\".h5\")) & (((\"10x\" in filename))):\n",
    "#         if ((\"10_5\" in filename) | (\"20_20\" in filename)):\n",
    "        h5_files.append(base_dir+filename)\n",
    "# Run the processing in parallel\n",
    "h5_files_train =  sorted(\n",
    "    h5_files_train,\n",
    "    key=lambda x: tuple(map(int, x.split('high_losing_')[1].split('_')[:2]))\n",
    ")\n",
    "h5_files =  sorted(\n",
    "    h5_files,\n",
    "    key=lambda x: tuple(map(int, x.split('high_losing_')[1].split('_')[:2]))\n",
    ")\n",
    "\n",
    "h5_files = h5_files[0:6]\n",
    "h5_files_train = h5_files_train[0:6]\n",
    "hdf5_file = h5py.File(h5_files[0], 'r')\n",
    "# Fetching the first five time steps\n",
    "time_steps = sorted(list(hdf5_file.keys()))\n",
    "\n",
    "# Inspecting the features present in each of these time steps\n",
    "features_per_time_step = {time_step: list(hdf5_file[time_step].keys()) for time_step in time_steps}\n",
    "\n",
    "\n",
    "num_time_steps = 115\n",
    "num_features = 9  # As observed from the dataset\n",
    "\n",
    "# Initialize the final array with the desired shape [5, 100, 200, 19]\n",
    "y = np.zeros((len(h5_files),num_time_steps-1, 100, 2000, num_features))\n",
    "\n",
    "# Extracting and reshaping data from the first five time steps\n",
    "for i in range(len(h5_files)):\n",
    "    hdf5_file = h5py.File(h5_files[i], 'r')\n",
    "    for t_idx, time_step in enumerate(time_steps[2:]):  # Skipping the first two non-time-step groups\n",
    "        count=0\n",
    "        for f_idx, feature in enumerate(features_per_time_step[time_step]):\n",
    "            if ('O2' not in feature)& ('Perm' not in feature)& ('Material' not in feature)& ('Sat' not in feature)& ('Z' not in feature)& ('biocide' not in feature)& ('ethanol' not in feature)& ('Chubbite' not in feature):\n",
    "                dataset = hdf5_file[time_step][feature]\n",
    "                y[i, t_idx, :, :, count] = dataset[:, :, 0]#cv2.resize(dataset[:, :, 0], [500,50])  # Reshape and assign\n",
    "                count = count+1\n",
    "\n",
    "# Checking the shape of the extracted data array\n",
    "y.shape\n",
    "\n",
    "\n",
    "x = np.zeros((len(h5_files_train),num_time_steps-1, 100, 400, num_features))\n",
    "\n",
    "# Extracting and reshaping data from the first five time steps\n",
    "for i in range(len(h5_files_train)):\n",
    "    hdf5_file = h5py.File(h5_files_train[i], 'r')\n",
    "    for t_idx, time_step in enumerate(time_steps[2:]):  # Skipping the first two non-time-step groups\n",
    "        count=0\n",
    "        for f_idx, feature in enumerate(features_per_time_step[time_step]):\n",
    "            if ('O2' not in feature)& ('Perm' not in feature)& ('Material' not in feature)& ('Sat' not in feature)& ('Z' not in feature)& ('biocide' not in feature)& ('ethanol' not in feature)& ('Chubbite' not in feature):\n",
    "                dataset = hdf5_file[time_step][feature]\n",
    "                x[i, t_idx, :, :, count] = dataset[:, :, 0]#cv2.resize(dataset[:, :, 0], [500,50])  # Reshape and assign\n",
    "                count = count+1\n",
    "# Open the HDF5 file and extract data to fit the specified shape [5, 100, 200, n]\n",
    "#extracted_data_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476a0bc2-95a9-49ff-a5dd-5e14bf76634f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'molasses_im [mol_m^3]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### cv2.resize(dataset[:, :, 0], [50,500])\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "275f7f74-9fd8-417c-932e-260a200e0719",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names = [\n",
    "    \"Liquid X-Velocity [m_per_h]\",\n",
    "    \"Liquid Y-Velocity [m_per_h]\",\n",
    "    \"Liquid_Pressure [Pa]\",\n",
    "    #\"Permeability_X [m^2]\",\n",
    "    \"Porosity\",\n",
    "    \"Temperature [C]\",\n",
    "    #\"Total_CO2 [M]\",\n",
    "    \"Total_Cr(VI) [M]\",\n",
    "    #\"Total_O2 [M]\",\n",
    "    \"Total_molasses [M]\",\n",
    "    \"biomass [mol_m^3]\",\n",
    "    \"molasses_im [mol_m^3]\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6efc8b30-b8a8-41b7-b364-ba28b40230ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n=7\n",
    "#import matplotlib.pyplot as plt\n",
    "#from matplotlib.colors import LogNorm\n",
    "#fig=plt.figure(figsize=(24,24), dpi=600)\n",
    "#plt.imshow(y[2,100,:,:,n])#, norm=LogNorm())\n",
    "#field_names[n]\n",
    "#fig.savefig('label.png')  # Replace 'output.png' with your desired file path and format\n",
    "\n",
    "#plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bcf82d7-0337-42cc-854a-ffe3f64a8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DoubleConv_down(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=1, padding=0),\n",
    "            #nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n",
    "            #nn.BatchNorm2d(200),\n",
    "            nn.ReLU(inplace=True),\n",
    "            #nn.Conv2d(200, out_channels, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(out_channels),\n",
    "            #nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class DoubleConv_up(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 600, kernel_size=1, padding=0),\n",
    "            #nn.BatchNorm2d(600),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(600, 800, kernel_size=1, padding=0),\n",
    "            #nn.BatchNorm2d(800),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(800, out_channels, kernel_size=1, padding=0),\n",
    "            #nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        #self.inc = DoubleConv_down(400, 256)\n",
    "        self.up = DoubleConv_up(400, 1200)\n",
    "        # Add more layers and upsampling layers to complete the U-Net structure\n",
    "\n",
    "        self.outc = nn.Conv2d(1200, 2000, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[0,:,:,:,:].permute(0,2,1,3)\n",
    "        #x = self.inc(x)\n",
    "        x = self.up(x)\n",
    "        # Implement forward pass with upsampling and concatenations\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (numpy array): A numpy array of shape [N, 100, 200, 19] where N is the number of samples.\n",
    "        \"\"\"\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.target = torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.x[idx]\n",
    "        target = self.target[idx]\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8680afc-0e78-4530-bc22-3f43da2aa26c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChannelAttention(\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, num_channels, reduction_ratio=2):\n",
    "        \"\"\"\n",
    "        Initializes the ChannelAttention module.\n",
    "        \n",
    "        :param num_channels: Number of input channels.\n",
    "        :param reduction_ratio: Reduction ratio for the hidden layer in MLP.\n",
    "        \"\"\"\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "\n",
    "        # Define the MLP layers\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(num_channels, num_channels // reduction_ratio),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_channels // reduction_ratio, num_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the ChannelAttention module.\n",
    "        \n",
    "        :param x: Input tensor of shape (batch_size, num_channels, height, width).\n",
    "        :return: Output tensor after applying channel attention.\n",
    "        \"\"\"\n",
    "        # Global average pooling\n",
    "        avg_pool = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        avg_pool = avg_pool.view(avg_pool.shape[0], -1)\n",
    "\n",
    "        # Channel attention\n",
    "        attention = self.mlp(avg_pool)\n",
    "        attention = torch.sigmoid(attention).view(attention.shape[0], attention.shape[1], 1, 1)\n",
    "\n",
    "        # Scale the input by attention scores\n",
    "        return x * attention\n",
    "\n",
    "# Example of using the ChannelAttention module\n",
    "# Assuming an input with 64 channels\n",
    "channel_att = ChannelAttention(num_channels=64)\n",
    "channel_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f2592df-d31c-4607-b81c-017084ecd638",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvUp(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "        \n",
    "class UNet1(nn.Module):\n",
    "    def __init__(self, inc, outc):\n",
    "        super(UNet1, self).__init__()\n",
    "        self.up = ConvUp(inc, outc)\n",
    "        self.att1 = ChannelAttention(num_channels=100)\n",
    "        self.att2 = ChannelAttention(num_channels=outc)\n",
    "    def forward(self, x):\n",
    "        x = x[0,:,:,:,:]\n",
    "        x = self.att1(x).permute(0,2,1,3)\n",
    "        x = self.up(x)\n",
    "        x = self.att2(x)\n",
    "        return x\n",
    "\n",
    "class UNet2(nn.Module):\n",
    "    def __init__(self, inc, outc):\n",
    "        super(UNet2, self).__init__()\n",
    "        self.up = ConvUp(inc, outc)\n",
    "        self.att1 = ChannelAttention(num_channels=100)\n",
    "        self.att2 = ChannelAttention(num_channels=outc)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1,3)\n",
    "        x = self.att1(x).permute(0,2,1,3)\n",
    "        x = self.up(x)\n",
    "        x = self.att2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e68ded-2c01-4c03-a76e-326325915f73",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scales' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#n=7\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#plt.figure(figsize=(24,24), dpi=100)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#plt.imshow(x[0,113,:,:,n])#, norm=LogNorm())\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#field_names[n]\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#np.min(x[2,113,:,:,n])\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mscales\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scales' is not defined"
     ]
    }
   ],
   "source": [
    "#n=7\n",
    "#plt.figure(figsize=(24,24), dpi=100)\n",
    "#plt.imshow(x[0,113,:,:,n])#, norm=LogNorm())\n",
    "#field_names[n]\n",
    "#np.min(x[2,113,:,:,n])\n",
    "scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93adeadb-7e30-43f1-8da5-4077c52a1a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-3.8411125549317013, 597.4671440974266],\n",
       " [-615.1042997637863, 357.84820748247256],\n",
       " [0.0, 6822.169871757096],\n",
       " [0.0001, 0.6],\n",
       " [7.788525219135888, 15.024872883573318],\n",
       " [9.998195511764697e-21, 0.0021615209557147423],\n",
       " [9.992418077905442e-21, 0.00019222389277486797],\n",
       " [1e-10, 353.88041919640534],\n",
       " [9.999999999999992e-21, 4.945182939521276]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales=[]\n",
    "for i in range(x.shape[4]):\n",
    "    xmin = np.min(x[:,:,:,:,i])\n",
    "    xmax = np.max(x[:,:,:,:,i])\n",
    "    x[:,:,:,:,i] = (x[:,:,:,:,i] - xmin)/(xmax-xmin)\n",
    "    y[:,:,:,:,i] = (y[:,:,:,:,i] - xmin)/(xmax-xmin)\n",
    "    scales.append([xmin,xmax])\n",
    "\n",
    "scales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c16944c-4862-4d5f-88ca-de48325114c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet1(\n",
       "  (up): ConvUp(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(400, 2000, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (att1): ChannelAttention(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=50, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=50, out_features=100, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (att2): ChannelAttention(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=2000, out_features=1000, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1000, out_features=2000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model1 = UNet1(400, 2000).to(device)\n",
    "\n",
    "#model2 = UNet2(400, 800).to(device)\n",
    "\n",
    "#model3 = UNet2(800, 1200).to(device)\n",
    "\n",
    "#model4 = UNet2(1200, 1600).to(device)\n",
    "\n",
    "#model5 = UNet2(1600, 2000).to(device)\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52073d39-5290-4a06-a5a8-6568c3c70c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40000, Loss: 0.02761196717619896, Learning Rate: 0.001999\n",
      "Epoch 2/40000, Loss: 0.01751270331442356, Learning Rate: 0.001999\n",
      "Epoch 3/40000, Loss: 0.018807901069521904, Learning Rate: 0.001998\n",
      "Epoch 4/40000, Loss: 0.013042382895946503, Learning Rate: 0.001998\n",
      "Epoch 5/40000, Loss: 0.013634243980050087, Learning Rate: 0.001997\n",
      "Epoch 6/40000, Loss: 0.011085296981036663, Learning Rate: 0.001996\n",
      "Epoch 7/40000, Loss: 0.01023821346461773, Learning Rate: 0.001996\n",
      "Epoch 8/40000, Loss: 0.007736549712717533, Learning Rate: 0.001995\n",
      "Epoch 9/40000, Loss: 0.00970153883099556, Learning Rate: 0.001995\n",
      "Epoch 10/40000, Loss: 0.006039892788976431, Learning Rate: 0.001994\n",
      "Epoch 11/40000, Loss: 0.006785817909985781, Learning Rate: 0.001993\n",
      "Epoch 12/40000, Loss: 0.009605549275875092, Learning Rate: 0.001993\n",
      "Epoch 13/40000, Loss: 0.011814183555543423, Learning Rate: 0.001992\n",
      "Epoch 14/40000, Loss: 0.011377447284758091, Learning Rate: 0.001992\n",
      "Epoch 15/40000, Loss: 0.005181838758289814, Learning Rate: 0.001991\n",
      "Epoch 16/40000, Loss: 0.005811644718050957, Learning Rate: 0.001990\n",
      "Epoch 17/40000, Loss: 0.009438606910407543, Learning Rate: 0.001990\n",
      "Epoch 18/40000, Loss: 0.005662665702402592, Learning Rate: 0.001989\n",
      "Epoch 19/40000, Loss: 0.008171954192221165, Learning Rate: 0.001989\n",
      "Epoch 20/40000, Loss: 0.009005799889564514, Learning Rate: 0.001988\n",
      "Epoch 21/40000, Loss: 0.010343579575419426, Learning Rate: 0.001987\n",
      "Epoch 22/40000, Loss: 0.004725898150354624, Learning Rate: 0.001987\n",
      "Epoch 23/40000, Loss: 0.0087698670104146, Learning Rate: 0.001986\n",
      "Epoch 24/40000, Loss: 0.008778927847743034, Learning Rate: 0.001986\n",
      "Epoch 25/40000, Loss: 0.004560423083603382, Learning Rate: 0.001985\n",
      "Epoch 26/40000, Loss: 0.007641672622412443, Learning Rate: 0.001984\n",
      "Epoch 27/40000, Loss: 0.009639604017138481, Learning Rate: 0.001984\n",
      "Epoch 28/40000, Loss: 0.007955520413815975, Learning Rate: 0.001983\n",
      "Epoch 29/40000, Loss: 0.008635958656668663, Learning Rate: 0.001983\n",
      "Epoch 30/40000, Loss: 0.008382429368793964, Learning Rate: 0.001982\n",
      "Epoch 31/40000, Loss: 0.009265239350497723, Learning Rate: 0.001981\n",
      "Epoch 32/40000, Loss: 0.0077790310606360435, Learning Rate: 0.001981\n",
      "Epoch 33/40000, Loss: 0.007729472126811743, Learning Rate: 0.001980\n",
      "Epoch 34/40000, Loss: 0.008169813081622124, Learning Rate: 0.001980\n",
      "Epoch 35/40000, Loss: 0.009132630191743374, Learning Rate: 0.001979\n",
      "Epoch 36/40000, Loss: 0.007088599726557732, Learning Rate: 0.001979\n",
      "Epoch 37/40000, Loss: 0.008144223131239414, Learning Rate: 0.001978\n",
      "Epoch 38/40000, Loss: 0.004605471156537533, Learning Rate: 0.001977\n",
      "Epoch 39/40000, Loss: 0.008862010203301907, Learning Rate: 0.001977\n",
      "Epoch 40/40000, Loss: 0.007501168176531792, Learning Rate: 0.001976\n",
      "Epoch 41/40000, Loss: 0.00417507067322731, Learning Rate: 0.001976\n",
      "Epoch 42/40000, Loss: 0.007371008396148682, Learning Rate: 0.001975\n",
      "Epoch 43/40000, Loss: 0.008579383604228497, Learning Rate: 0.001974\n",
      "Epoch 44/40000, Loss: 0.006847770418971777, Learning Rate: 0.001974\n",
      "Epoch 45/40000, Loss: 0.008523477241396904, Learning Rate: 0.001973\n",
      "Epoch 46/40000, Loss: 0.007374800741672516, Learning Rate: 0.001973\n",
      "Epoch 47/40000, Loss: 0.004497584421187639, Learning Rate: 0.001972\n",
      "Epoch 48/40000, Loss: 0.007662671618163586, Learning Rate: 0.001971\n",
      "Epoch 49/40000, Loss: 0.00646606832742691, Learning Rate: 0.001971\n",
      "Epoch 50/40000, Loss: 0.007794603239744902, Learning Rate: 0.001970\n",
      "Epoch 51/40000, Loss: 0.006377744022756815, Learning Rate: 0.001970\n",
      "Epoch 52/40000, Loss: 0.006566053722053766, Learning Rate: 0.001969\n",
      "Epoch 53/40000, Loss: 0.0039892918430268764, Learning Rate: 0.001968\n",
      "Epoch 54/40000, Loss: 0.008114947006106377, Learning Rate: 0.001968\n",
      "Epoch 55/40000, Loss: 0.003862893907353282, Learning Rate: 0.001967\n",
      "Epoch 56/40000, Loss: 0.004267905373126268, Learning Rate: 0.001967\n",
      "Epoch 57/40000, Loss: 0.007593005429953337, Learning Rate: 0.001966\n",
      "Epoch 58/40000, Loss: 0.008056587539613247, Learning Rate: 0.001966\n",
      "Epoch 59/40000, Loss: 0.006848456338047981, Learning Rate: 0.001965\n",
      "Epoch 60/40000, Loss: 0.0037644340191036463, Learning Rate: 0.001964\n",
      "Epoch 61/40000, Loss: 0.006866157054901123, Learning Rate: 0.001964\n",
      "Epoch 62/40000, Loss: 0.00420014513656497, Learning Rate: 0.001963\n",
      "Epoch 63/40000, Loss: 0.004287736490368843, Learning Rate: 0.001963\n",
      "Epoch 64/40000, Loss: 0.00704152463003993, Learning Rate: 0.001962\n",
      "Epoch 65/40000, Loss: 0.007187321782112122, Learning Rate: 0.001961\n",
      "Epoch 66/40000, Loss: 0.007989497855305672, Learning Rate: 0.001961\n",
      "Epoch 67/40000, Loss: 0.0060967495664954185, Learning Rate: 0.001960\n",
      "Epoch 68/40000, Loss: 0.004193316213786602, Learning Rate: 0.001960\n",
      "Epoch 69/40000, Loss: 0.007664169650524855, Learning Rate: 0.001959\n",
      "Epoch 70/40000, Loss: 0.007646939251571894, Learning Rate: 0.001958\n",
      "Epoch 71/40000, Loss: 0.007305393926799297, Learning Rate: 0.001958\n",
      "Epoch 72/40000, Loss: 0.004016557242721319, Learning Rate: 0.001957\n",
      "Epoch 73/40000, Loss: 0.00676764827221632, Learning Rate: 0.001957\n",
      "Epoch 74/40000, Loss: 0.005816797260195017, Learning Rate: 0.001956\n",
      "Epoch 75/40000, Loss: 0.007293743547052145, Learning Rate: 0.001956\n",
      "Epoch 76/40000, Loss: 0.005830247886478901, Learning Rate: 0.001955\n",
      "Epoch 77/40000, Loss: 0.007250691764056683, Learning Rate: 0.001954\n",
      "Epoch 78/40000, Loss: 0.007347593549638987, Learning Rate: 0.001954\n",
      "Epoch 79/40000, Loss: 0.007520733401179314, Learning Rate: 0.001953\n",
      "Epoch 80/40000, Loss: 0.0038809715770184994, Learning Rate: 0.001953\n",
      "Epoch 81/40000, Loss: 0.0057408735156059265, Learning Rate: 0.001952\n",
      "Epoch 82/40000, Loss: 0.0058547998778522015, Learning Rate: 0.001951\n",
      "Epoch 83/40000, Loss: 0.0076432013884186745, Learning Rate: 0.001951\n",
      "Epoch 84/40000, Loss: 0.007488093804568052, Learning Rate: 0.001950\n",
      "Epoch 85/40000, Loss: 0.003931486047804356, Learning Rate: 0.001950\n",
      "Epoch 86/40000, Loss: 0.007341896649450064, Learning Rate: 0.001949\n",
      "Epoch 87/40000, Loss: 0.00710205128416419, Learning Rate: 0.001948\n",
      "Epoch 88/40000, Loss: 0.007319057825952768, Learning Rate: 0.001948\n",
      "Epoch 89/40000, Loss: 0.005443507805466652, Learning Rate: 0.001947\n",
      "Epoch 90/40000, Loss: 0.007401956245303154, Learning Rate: 0.001947\n",
      "Epoch 91/40000, Loss: 0.006625890266150236, Learning Rate: 0.001946\n",
      "Epoch 92/40000, Loss: 0.005571154877543449, Learning Rate: 0.001946\n",
      "Epoch 93/40000, Loss: 0.0073373098857700825, Learning Rate: 0.001945\n",
      "Epoch 94/40000, Loss: 0.007253285497426987, Learning Rate: 0.001944\n",
      "Epoch 95/40000, Loss: 0.005478937644511461, Learning Rate: 0.001944\n",
      "Epoch 96/40000, Loss: 0.0072217839770019054, Learning Rate: 0.001943\n",
      "Epoch 97/40000, Loss: 0.003618544666096568, Learning Rate: 0.001943\n",
      "Epoch 98/40000, Loss: 0.0037320484407246113, Learning Rate: 0.001942\n",
      "Epoch 99/40000, Loss: 0.003893071785569191, Learning Rate: 0.001941\n",
      "Epoch 100/40000, Loss: 0.005488286726176739, Learning Rate: 0.001941\n",
      "Epoch 101/40000, Loss: 0.005638111382722855, Learning Rate: 0.001940\n",
      "Epoch 102/40000, Loss: 0.007034914568066597, Learning Rate: 0.001940\n",
      "Epoch 103/40000, Loss: 0.006274597719311714, Learning Rate: 0.001939\n",
      "Epoch 104/40000, Loss: 0.0037927161902189255, Learning Rate: 0.001939\n",
      "Epoch 105/40000, Loss: 0.003502130741253495, Learning Rate: 0.001938\n",
      "Epoch 106/40000, Loss: 0.0036916641984134912, Learning Rate: 0.001937\n",
      "Epoch 107/40000, Loss: 0.006816461216658354, Learning Rate: 0.001937\n",
      "Epoch 108/40000, Loss: 0.005323679652065039, Learning Rate: 0.001936\n",
      "Epoch 109/40000, Loss: 0.0037454592529684305, Learning Rate: 0.001936\n",
      "Epoch 110/40000, Loss: 0.0037679062224924564, Learning Rate: 0.001935\n",
      "Epoch 111/40000, Loss: 0.0035468237474560738, Learning Rate: 0.001934\n",
      "Epoch 112/40000, Loss: 0.00375158223323524, Learning Rate: 0.001934\n",
      "Epoch 113/40000, Loss: 0.0034854519180953503, Learning Rate: 0.001933\n",
      "Epoch 114/40000, Loss: 0.0069463783875107765, Learning Rate: 0.001933\n",
      "Epoch 115/40000, Loss: 0.0034023812040686607, Learning Rate: 0.001932\n",
      "Epoch 116/40000, Loss: 0.0035894163884222507, Learning Rate: 0.001932\n",
      "Epoch 117/40000, Loss: 0.00666062580421567, Learning Rate: 0.001931\n",
      "Epoch 118/40000, Loss: 0.0034773219376802444, Learning Rate: 0.001930\n",
      "Epoch 119/40000, Loss: 0.006707558408379555, Learning Rate: 0.001930\n",
      "Epoch 120/40000, Loss: 0.006300368811935186, Learning Rate: 0.001929\n",
      "Epoch 121/40000, Loss: 0.006661819294095039, Learning Rate: 0.001929\n",
      "Epoch 122/40000, Loss: 0.0034820984583348036, Learning Rate: 0.001928\n",
      "Epoch 123/40000, Loss: 0.006786729209125042, Learning Rate: 0.001928\n",
      "Epoch 124/40000, Loss: 0.0038075633347034454, Learning Rate: 0.001927\n",
      "Epoch 125/40000, Loss: 0.006551841273903847, Learning Rate: 0.001926\n",
      "Epoch 126/40000, Loss: 0.006739463657140732, Learning Rate: 0.001926\n",
      "Epoch 127/40000, Loss: 0.003414734033867717, Learning Rate: 0.001925\n",
      "Epoch 128/40000, Loss: 0.006584536284208298, Learning Rate: 0.001925\n",
      "Epoch 129/40000, Loss: 0.0068371049128472805, Learning Rate: 0.001924\n",
      "Epoch 130/40000, Loss: 0.003357360139489174, Learning Rate: 0.001923\n",
      "Epoch 131/40000, Loss: 0.006471049971878529, Learning Rate: 0.001923\n",
      "Epoch 132/40000, Loss: 0.006291591562330723, Learning Rate: 0.001922\n",
      "Epoch 133/40000, Loss: 0.006452267058193684, Learning Rate: 0.001922\n",
      "Epoch 134/40000, Loss: 0.0050461385399103165, Learning Rate: 0.001921\n",
      "Epoch 135/40000, Loss: 0.006356220692396164, Learning Rate: 0.001921\n",
      "Epoch 136/40000, Loss: 0.0034153505694121122, Learning Rate: 0.001920\n",
      "Epoch 137/40000, Loss: 0.007156676147133112, Learning Rate: 0.001919\n",
      "Epoch 138/40000, Loss: 0.00684141693636775, Learning Rate: 0.001919\n",
      "Epoch 139/40000, Loss: 0.0066332221031188965, Learning Rate: 0.001918\n",
      "Epoch 140/40000, Loss: 0.004887019749730825, Learning Rate: 0.001918\n",
      "Epoch 141/40000, Loss: 0.0035285945050418377, Learning Rate: 0.001917\n",
      "Epoch 142/40000, Loss: 0.003481046063825488, Learning Rate: 0.001917\n",
      "Epoch 143/40000, Loss: 0.0065414500422775745, Learning Rate: 0.001916\n",
      "Epoch 144/40000, Loss: 0.006427284330129623, Learning Rate: 0.001915\n",
      "Epoch 145/40000, Loss: 0.006338639184832573, Learning Rate: 0.001915\n",
      "Epoch 146/40000, Loss: 0.003645167453214526, Learning Rate: 0.001914\n",
      "Epoch 147/40000, Loss: 0.0064436085522174835, Learning Rate: 0.001914\n",
      "Epoch 148/40000, Loss: 0.003464951179921627, Learning Rate: 0.001913\n",
      "Epoch 149/40000, Loss: 0.006344204302877188, Learning Rate: 0.001913\n",
      "Epoch 150/40000, Loss: 0.0063665397465229034, Learning Rate: 0.001912\n",
      "Epoch 151/40000, Loss: 0.003246224019676447, Learning Rate: 0.001911\n",
      "Epoch 152/40000, Loss: 0.004793522879481316, Learning Rate: 0.001911\n",
      "Epoch 153/40000, Loss: 0.0062413522973656654, Learning Rate: 0.001910\n",
      "Epoch 154/40000, Loss: 0.005834484472870827, Learning Rate: 0.001910\n",
      "Epoch 155/40000, Loss: 0.00649194186553359, Learning Rate: 0.001909\n",
      "Epoch 156/40000, Loss: 0.003296453272923827, Learning Rate: 0.001909\n",
      "Epoch 157/40000, Loss: 0.003462004940956831, Learning Rate: 0.001908\n",
      "Epoch 158/40000, Loss: 0.0033255768939852715, Learning Rate: 0.001907\n",
      "Epoch 159/40000, Loss: 0.006238496862351894, Learning Rate: 0.001907\n",
      "Epoch 160/40000, Loss: 0.0031799469143152237, Learning Rate: 0.001906\n",
      "Epoch 161/40000, Loss: 0.00469683762639761, Learning Rate: 0.001906\n",
      "Epoch 162/40000, Loss: 0.004767094738781452, Learning Rate: 0.001905\n",
      "Epoch 163/40000, Loss: 0.00604840274900198, Learning Rate: 0.001905\n",
      "Epoch 164/40000, Loss: 0.006224673707038164, Learning Rate: 0.001904\n",
      "Epoch 165/40000, Loss: 0.006377754267305136, Learning Rate: 0.001903\n",
      "Epoch 166/40000, Loss: 0.0034297623205929995, Learning Rate: 0.001903\n",
      "Epoch 167/40000, Loss: 0.004788858816027641, Learning Rate: 0.001902\n",
      "Epoch 168/40000, Loss: 0.00460925605148077, Learning Rate: 0.001902\n",
      "Epoch 169/40000, Loss: 0.003321166383102536, Learning Rate: 0.001901\n",
      "Epoch 170/40000, Loss: 0.003327828599140048, Learning Rate: 0.001901\n",
      "Epoch 171/40000, Loss: 0.0033291641157120466, Learning Rate: 0.001900\n",
      "Epoch 172/40000, Loss: 0.006011333782225847, Learning Rate: 0.001899\n",
      "Epoch 173/40000, Loss: 0.006157359573990107, Learning Rate: 0.001899\n",
      "Epoch 174/40000, Loss: 0.003240045625716448, Learning Rate: 0.001898\n",
      "Epoch 175/40000, Loss: 0.0057732355780899525, Learning Rate: 0.001898\n",
      "Epoch 176/40000, Loss: 0.006403973791748285, Learning Rate: 0.001897\n",
      "Epoch 177/40000, Loss: 0.00627715140581131, Learning Rate: 0.001897\n",
      "Epoch 178/40000, Loss: 0.006075566168874502, Learning Rate: 0.001896\n",
      "Epoch 179/40000, Loss: 0.005962651688605547, Learning Rate: 0.001895\n",
      "Epoch 180/40000, Loss: 0.0046456377021968365, Learning Rate: 0.001895\n",
      "Epoch 181/40000, Loss: 0.00592143926769495, Learning Rate: 0.001894\n",
      "Epoch 182/40000, Loss: 0.005763046443462372, Learning Rate: 0.001894\n",
      "Epoch 183/40000, Loss: 0.006008474621921778, Learning Rate: 0.001893\n",
      "Epoch 184/40000, Loss: 0.004579710308462381, Learning Rate: 0.001893\n",
      "Epoch 185/40000, Loss: 0.004679819103330374, Learning Rate: 0.001892\n",
      "Epoch 186/40000, Loss: 0.003400923451408744, Learning Rate: 0.001891\n",
      "Epoch 187/40000, Loss: 0.006322745233774185, Learning Rate: 0.001891\n",
      "Epoch 188/40000, Loss: 0.005830212961882353, Learning Rate: 0.001890\n",
      "Epoch 189/40000, Loss: 0.006436913274228573, Learning Rate: 0.001890\n",
      "Epoch 190/40000, Loss: 0.006390167400240898, Learning Rate: 0.001889\n",
      "Epoch 191/40000, Loss: 0.0031986432150006294, Learning Rate: 0.001889\n",
      "Epoch 192/40000, Loss: 0.003371014492586255, Learning Rate: 0.001888\n",
      "Epoch 193/40000, Loss: 0.0031501424964517355, Learning Rate: 0.001887\n",
      "Epoch 194/40000, Loss: 0.004594326950609684, Learning Rate: 0.001887\n",
      "Epoch 195/40000, Loss: 0.006076293531805277, Learning Rate: 0.001886\n",
      "Epoch 196/40000, Loss: 0.005944826640188694, Learning Rate: 0.001886\n",
      "Epoch 197/40000, Loss: 0.005845935549587011, Learning Rate: 0.001885\n",
      "Epoch 198/40000, Loss: 0.00578140327706933, Learning Rate: 0.001885\n",
      "Epoch 199/40000, Loss: 0.005755975842475891, Learning Rate: 0.001884\n",
      "Epoch 200/40000, Loss: 0.00305919349193573, Learning Rate: 0.001884\n",
      "Epoch 201/40000, Loss: 0.005914465058594942, Learning Rate: 0.001883\n",
      "Epoch 202/40000, Loss: 0.006063937675207853, Learning Rate: 0.001882\n",
      "Epoch 203/40000, Loss: 0.0045345239341259, Learning Rate: 0.001882\n",
      "Epoch 204/40000, Loss: 0.005624639801681042, Learning Rate: 0.001881\n",
      "Epoch 205/40000, Loss: 0.005855475552380085, Learning Rate: 0.001881\n",
      "Epoch 206/40000, Loss: 0.005906970705837011, Learning Rate: 0.001880\n",
      "Epoch 207/40000, Loss: 0.00448366766795516, Learning Rate: 0.001880\n",
      "Epoch 208/40000, Loss: 0.00446658581495285, Learning Rate: 0.001879\n",
      "Epoch 209/40000, Loss: 0.0031129729468375444, Learning Rate: 0.001878\n",
      "Epoch 210/40000, Loss: 0.0031389170326292515, Learning Rate: 0.001878\n",
      "Epoch 211/40000, Loss: 0.0031640799716115, Learning Rate: 0.001877\n",
      "Epoch 212/40000, Loss: 0.004304029513150454, Learning Rate: 0.001877\n",
      "Epoch 213/40000, Loss: 0.005838358774781227, Learning Rate: 0.001876\n",
      "Epoch 214/40000, Loss: 0.005675534717738628, Learning Rate: 0.001876\n",
      "Epoch 215/40000, Loss: 0.003121012356132269, Learning Rate: 0.001875\n",
      "Epoch 216/40000, Loss: 0.003064169082790613, Learning Rate: 0.001875\n",
      "Epoch 217/40000, Loss: 0.004349251743406057, Learning Rate: 0.001874\n",
      "Epoch 218/40000, Loss: 0.004329270217567682, Learning Rate: 0.001873\n",
      "Epoch 219/40000, Loss: 0.003083899151533842, Learning Rate: 0.001873\n",
      "Epoch 220/40000, Loss: 0.004284248687326908, Learning Rate: 0.001872\n",
      "Epoch 221/40000, Loss: 0.005690980702638626, Learning Rate: 0.001872\n",
      "Epoch 222/40000, Loss: 0.005823374725878239, Learning Rate: 0.001871\n",
      "Epoch 223/40000, Loss: 0.005747030954807997, Learning Rate: 0.001871\n",
      "Epoch 224/40000, Loss: 0.004393213428556919, Learning Rate: 0.001870\n",
      "Epoch 225/40000, Loss: 0.005659390240907669, Learning Rate: 0.001869\n",
      "Epoch 226/40000, Loss: 0.003040378913283348, Learning Rate: 0.001869\n",
      "Epoch 227/40000, Loss: 0.005558756645768881, Learning Rate: 0.001868\n",
      "Epoch 228/40000, Loss: 0.005767769180238247, Learning Rate: 0.001868\n",
      "Epoch 229/40000, Loss: 0.005637086927890778, Learning Rate: 0.001867\n",
      "Epoch 230/40000, Loss: 0.0030812544282525778, Learning Rate: 0.001867\n",
      "Epoch 231/40000, Loss: 0.005545787047594786, Learning Rate: 0.001866\n",
      "Epoch 232/40000, Loss: 0.0032126165460795164, Learning Rate: 0.001866\n",
      "Epoch 233/40000, Loss: 0.005507404915988445, Learning Rate: 0.001865\n",
      "Epoch 234/40000, Loss: 0.0029567128513008356, Learning Rate: 0.001864\n",
      "Epoch 235/40000, Loss: 0.003076619002968073, Learning Rate: 0.001864\n",
      "Epoch 236/40000, Loss: 0.003197800600901246, Learning Rate: 0.001863\n",
      "Epoch 237/40000, Loss: 0.005594164133071899, Learning Rate: 0.001863\n",
      "Epoch 238/40000, Loss: 0.005500183906406164, Learning Rate: 0.001862\n",
      "Epoch 239/40000, Loss: 0.0030963981989771128, Learning Rate: 0.001862\n",
      "Epoch 240/40000, Loss: 0.005698753520846367, Learning Rate: 0.001861\n",
      "Epoch 241/40000, Loss: 0.003209854941815138, Learning Rate: 0.001861\n",
      "Epoch 242/40000, Loss: 0.005501929670572281, Learning Rate: 0.001860\n",
      "Epoch 243/40000, Loss: 0.005453132092952728, Learning Rate: 0.001859\n",
      "Epoch 244/40000, Loss: 0.0041693029925227165, Learning Rate: 0.001859\n",
      "Epoch 245/40000, Loss: 0.0028916343580931425, Learning Rate: 0.001858\n",
      "Epoch 246/40000, Loss: 0.004261823836714029, Learning Rate: 0.001858\n",
      "Epoch 247/40000, Loss: 0.005587749183177948, Learning Rate: 0.001857\n",
      "Epoch 248/40000, Loss: 0.002974147442728281, Learning Rate: 0.001857\n",
      "Epoch 249/40000, Loss: 0.0030002745334059, Learning Rate: 0.001856\n",
      "Epoch 250/40000, Loss: 0.004178456496447325, Learning Rate: 0.001855\n",
      "Epoch 251/40000, Loss: 0.005592784844338894, Learning Rate: 0.001855\n",
      "Epoch 252/40000, Loss: 0.002951342146843672, Learning Rate: 0.001854\n",
      "Epoch 253/40000, Loss: 0.004143855068832636, Learning Rate: 0.001854\n",
      "Epoch 254/40000, Loss: 0.0029547831509262323, Learning Rate: 0.001853\n",
      "Epoch 255/40000, Loss: 0.0040856096893548965, Learning Rate: 0.001853\n",
      "Epoch 256/40000, Loss: 0.005631405394524336, Learning Rate: 0.001852\n",
      "Epoch 257/40000, Loss: 0.004064128268510103, Learning Rate: 0.001852\n",
      "Epoch 258/40000, Loss: 0.004225507378578186, Learning Rate: 0.001851\n",
      "Epoch 259/40000, Loss: 0.0043266243301332, Learning Rate: 0.001850\n",
      "Epoch 260/40000, Loss: 0.0029657066334038973, Learning Rate: 0.001850\n",
      "Epoch 261/40000, Loss: 0.005574692972004414, Learning Rate: 0.001849\n",
      "Epoch 262/40000, Loss: 0.005751301068812609, Learning Rate: 0.001849\n",
      "Epoch 263/40000, Loss: 0.00438845157623291, Learning Rate: 0.001848\n",
      "Epoch 264/40000, Loss: 0.005495032761245966, Learning Rate: 0.001848\n",
      "Epoch 265/40000, Loss: 0.005314559210091829, Learning Rate: 0.001847\n",
      "Epoch 266/40000, Loss: 0.004083422943949699, Learning Rate: 0.001847\n",
      "Epoch 267/40000, Loss: 0.0053624967113137245, Learning Rate: 0.001846\n",
      "Epoch 268/40000, Loss: 0.0030160897877067327, Learning Rate: 0.001845\n",
      "Epoch 269/40000, Loss: 0.0029689553193747997, Learning Rate: 0.001845\n",
      "Epoch 270/40000, Loss: 0.005822920240461826, Learning Rate: 0.001844\n",
      "Epoch 271/40000, Loss: 0.0030638796743005514, Learning Rate: 0.001844\n",
      "Epoch 272/40000, Loss: 0.004215687979012728, Learning Rate: 0.001843\n",
      "Epoch 273/40000, Loss: 0.003087652148678899, Learning Rate: 0.001843\n",
      "Epoch 274/40000, Loss: 0.0041524614207446575, Learning Rate: 0.001842\n",
      "Epoch 275/40000, Loss: 0.004255938809365034, Learning Rate: 0.001842\n",
      "Epoch 276/40000, Loss: 0.005508929491043091, Learning Rate: 0.001841\n",
      "Epoch 277/40000, Loss: 0.005488904193043709, Learning Rate: 0.001841\n",
      "Epoch 278/40000, Loss: 0.002808305202051997, Learning Rate: 0.001840\n",
      "Epoch 279/40000, Loss: 0.002868599956855178, Learning Rate: 0.001839\n",
      "Epoch 280/40000, Loss: 0.002862214110791683, Learning Rate: 0.001839\n",
      "Epoch 281/40000, Loss: 0.0040552266873419285, Learning Rate: 0.001838\n",
      "Epoch 282/40000, Loss: 0.005474233999848366, Learning Rate: 0.001838\n",
      "Epoch 283/40000, Loss: 0.005323740188032389, Learning Rate: 0.001837\n",
      "Epoch 284/40000, Loss: 0.004151409491896629, Learning Rate: 0.001837\n",
      "Epoch 285/40000, Loss: 0.002996341325342655, Learning Rate: 0.001836\n",
      "Epoch 286/40000, Loss: 0.004047302529215813, Learning Rate: 0.001836\n",
      "Epoch 287/40000, Loss: 0.005233240779489279, Learning Rate: 0.001835\n",
      "Epoch 288/40000, Loss: 0.0029895170591771603, Learning Rate: 0.001834\n",
      "Epoch 289/40000, Loss: 0.00296553666703403, Learning Rate: 0.001834\n",
      "Epoch 290/40000, Loss: 0.005132307764142752, Learning Rate: 0.001833\n",
      "Epoch 291/40000, Loss: 0.002877267776057124, Learning Rate: 0.001833\n",
      "Epoch 292/40000, Loss: 0.0028992414008826017, Learning Rate: 0.001832\n",
      "Epoch 293/40000, Loss: 0.005635656416416168, Learning Rate: 0.001832\n",
      "Epoch 294/40000, Loss: 0.003035519504919648, Learning Rate: 0.001831\n",
      "Epoch 295/40000, Loss: 0.0029118862003087997, Learning Rate: 0.001831\n",
      "Epoch 296/40000, Loss: 0.00301233003847301, Learning Rate: 0.001830\n",
      "Epoch 297/40000, Loss: 0.005139004439115524, Learning Rate: 0.001830\n",
      "Epoch 298/40000, Loss: 0.005266873631626368, Learning Rate: 0.001829\n",
      "Epoch 299/40000, Loss: 0.005490739364176989, Learning Rate: 0.001828\n",
      "Epoch 300/40000, Loss: 0.002783283358439803, Learning Rate: 0.001828\n",
      "Epoch 301/40000, Loss: 0.0028467157389968634, Learning Rate: 0.001827\n",
      "Epoch 302/40000, Loss: 0.005495845340192318, Learning Rate: 0.001827\n",
      "Epoch 303/40000, Loss: 0.005868962965905666, Learning Rate: 0.001826\n",
      "Epoch 304/40000, Loss: 0.005957971327006817, Learning Rate: 0.001826\n",
      "Epoch 305/40000, Loss: 0.004031290300190449, Learning Rate: 0.001825\n",
      "Epoch 306/40000, Loss: 0.004002775996923447, Learning Rate: 0.001825\n",
      "Epoch 307/40000, Loss: 0.005617683753371239, Learning Rate: 0.001824\n",
      "Epoch 308/40000, Loss: 0.005317443981766701, Learning Rate: 0.001823\n",
      "Epoch 309/40000, Loss: 0.005534985102713108, Learning Rate: 0.001823\n",
      "Epoch 310/40000, Loss: 0.005854523275047541, Learning Rate: 0.001822\n",
      "Epoch 311/40000, Loss: 0.003955187741667032, Learning Rate: 0.001822\n",
      "Epoch 312/40000, Loss: 0.003044649725779891, Learning Rate: 0.001821\n",
      "Epoch 313/40000, Loss: 0.00504687987267971, Learning Rate: 0.001821\n",
      "Epoch 314/40000, Loss: 0.0028695922810584307, Learning Rate: 0.001820\n",
      "Epoch 315/40000, Loss: 0.005238065030425787, Learning Rate: 0.001820\n",
      "Epoch 316/40000, Loss: 0.005202936474233866, Learning Rate: 0.001819\n",
      "Epoch 317/40000, Loss: 0.005414800252765417, Learning Rate: 0.001819\n",
      "Epoch 318/40000, Loss: 0.005040514748543501, Learning Rate: 0.001818\n",
      "Epoch 319/40000, Loss: 0.005476795602589846, Learning Rate: 0.001817\n",
      "Epoch 320/40000, Loss: 0.005334790796041489, Learning Rate: 0.001817\n",
      "Epoch 321/40000, Loss: 0.0052684093825519085, Learning Rate: 0.001816\n",
      "Epoch 322/40000, Loss: 0.0038975668139755726, Learning Rate: 0.001816\n",
      "Epoch 323/40000, Loss: 0.004199719522148371, Learning Rate: 0.001815\n",
      "Epoch 324/40000, Loss: 0.0041217743419110775, Learning Rate: 0.001815\n",
      "Epoch 325/40000, Loss: 0.003926144912838936, Learning Rate: 0.001814\n",
      "Epoch 326/40000, Loss: 0.002985041355714202, Learning Rate: 0.001814\n",
      "Epoch 327/40000, Loss: 0.005401975940912962, Learning Rate: 0.001813\n",
      "Epoch 328/40000, Loss: 0.002790262456983328, Learning Rate: 0.001813\n",
      "Epoch 329/40000, Loss: 0.005387722048908472, Learning Rate: 0.001812\n",
      "Epoch 330/40000, Loss: 0.005623341538012028, Learning Rate: 0.001811\n",
      "Epoch 331/40000, Loss: 0.004019278567284346, Learning Rate: 0.001811\n",
      "Epoch 332/40000, Loss: 0.0029824036173522472, Learning Rate: 0.001810\n",
      "Epoch 333/40000, Loss: 0.0028359766583889723, Learning Rate: 0.001810\n",
      "Epoch 334/40000, Loss: 0.0053829895332455635, Learning Rate: 0.001809\n",
      "Epoch 335/40000, Loss: 0.002776011824607849, Learning Rate: 0.001809\n",
      "Epoch 336/40000, Loss: 0.005192314740270376, Learning Rate: 0.001808\n",
      "Epoch 337/40000, Loss: 0.005349092651158571, Learning Rate: 0.001808\n",
      "Epoch 338/40000, Loss: 0.004196430090814829, Learning Rate: 0.001807\n",
      "Epoch 339/40000, Loss: 0.005287245847284794, Learning Rate: 0.001807\n",
      "Epoch 340/40000, Loss: 0.005428562872111797, Learning Rate: 0.001806\n",
      "Epoch 341/40000, Loss: 0.005259849596768618, Learning Rate: 0.001806\n",
      "Epoch 342/40000, Loss: 0.0050920178182423115, Learning Rate: 0.001805\n",
      "Epoch 343/40000, Loss: 0.00390263763256371, Learning Rate: 0.001804\n",
      "Epoch 344/40000, Loss: 0.005660526920109987, Learning Rate: 0.001804\n",
      "Epoch 345/40000, Loss: 0.005307316314429045, Learning Rate: 0.001803\n",
      "Epoch 346/40000, Loss: 0.003958921413868666, Learning Rate: 0.001803\n",
      "Epoch 347/40000, Loss: 0.005806544795632362, Learning Rate: 0.001802\n",
      "Epoch 348/40000, Loss: 0.0027507857885211706, Learning Rate: 0.001802\n",
      "Epoch 349/40000, Loss: 0.003969823941588402, Learning Rate: 0.001801\n",
      "Epoch 350/40000, Loss: 0.0053178840316832066, Learning Rate: 0.001801\n",
      "Epoch 351/40000, Loss: 0.005204214248806238, Learning Rate: 0.001800\n",
      "Epoch 352/40000, Loss: 0.0053682331927120686, Learning Rate: 0.001800\n",
      "Epoch 353/40000, Loss: 0.0027420981787145138, Learning Rate: 0.001799\n",
      "Epoch 354/40000, Loss: 0.004964061547070742, Learning Rate: 0.001798\n",
      "Epoch 355/40000, Loss: 0.00302131287753582, Learning Rate: 0.001798\n",
      "Epoch 356/40000, Loss: 0.003955700900405645, Learning Rate: 0.001797\n",
      "Epoch 357/40000, Loss: 0.0027590845711529255, Learning Rate: 0.001797\n",
      "Epoch 358/40000, Loss: 0.005146129056811333, Learning Rate: 0.001796\n",
      "Epoch 359/40000, Loss: 0.002797348191961646, Learning Rate: 0.001796\n",
      "Epoch 360/40000, Loss: 0.005245888605713844, Learning Rate: 0.001795\n",
      "Epoch 361/40000, Loss: 0.0049616447649896145, Learning Rate: 0.001795\n",
      "Epoch 362/40000, Loss: 0.0050672488287091255, Learning Rate: 0.001794\n",
      "Epoch 363/40000, Loss: 0.005211678799241781, Learning Rate: 0.001794\n",
      "Epoch 364/40000, Loss: 0.005165664944797754, Learning Rate: 0.001793\n",
      "Epoch 365/40000, Loss: 0.005357994697988033, Learning Rate: 0.001793\n",
      "Epoch 366/40000, Loss: 0.0026966696605086327, Learning Rate: 0.001792\n",
      "Epoch 367/40000, Loss: 0.005142930429428816, Learning Rate: 0.001791\n",
      "Epoch 368/40000, Loss: 0.005329999141395092, Learning Rate: 0.001791\n",
      "Epoch 369/40000, Loss: 0.003791571594774723, Learning Rate: 0.001790\n",
      "Epoch 370/40000, Loss: 0.004939203150570393, Learning Rate: 0.001790\n",
      "Epoch 371/40000, Loss: 0.00382182071916759, Learning Rate: 0.001789\n",
      "Epoch 372/40000, Loss: 0.0026887913700193167, Learning Rate: 0.001789\n",
      "Epoch 373/40000, Loss: 0.002800734480842948, Learning Rate: 0.001788\n",
      "Epoch 374/40000, Loss: 0.003679330460727215, Learning Rate: 0.001788\n",
      "Epoch 375/40000, Loss: 0.004855629056692123, Learning Rate: 0.001787\n",
      "Epoch 376/40000, Loss: 0.002708932152017951, Learning Rate: 0.001787\n",
      "Epoch 377/40000, Loss: 0.005194341763854027, Learning Rate: 0.001786\n",
      "Epoch 378/40000, Loss: 0.003786074463278055, Learning Rate: 0.001786\n",
      "Epoch 379/40000, Loss: 0.004842647351324558, Learning Rate: 0.001785\n",
      "Epoch 380/40000, Loss: 0.005094738677144051, Learning Rate: 0.001785\n",
      "Epoch 381/40000, Loss: 0.002661094069480896, Learning Rate: 0.001784\n",
      "Epoch 382/40000, Loss: 0.002738100476562977, Learning Rate: 0.001783\n",
      "Epoch 383/40000, Loss: 0.005238337907940149, Learning Rate: 0.001783\n",
      "Epoch 384/40000, Loss: 0.005596824921667576, Learning Rate: 0.001782\n",
      "Epoch 385/40000, Loss: 0.005281738005578518, Learning Rate: 0.001782\n",
      "Epoch 386/40000, Loss: 0.005107230506837368, Learning Rate: 0.001781\n",
      "Epoch 387/40000, Loss: 0.004837811924517155, Learning Rate: 0.001781\n",
      "Epoch 388/40000, Loss: 0.0049304598942399025, Learning Rate: 0.001780\n",
      "Epoch 389/40000, Loss: 0.0027346971910446882, Learning Rate: 0.001780\n",
      "Epoch 390/40000, Loss: 0.0038636191748082638, Learning Rate: 0.001779\n",
      "Epoch 391/40000, Loss: 0.004848581273108721, Learning Rate: 0.001779\n",
      "Epoch 392/40000, Loss: 0.004896131809800863, Learning Rate: 0.001778\n",
      "Epoch 393/40000, Loss: 0.005028100684285164, Learning Rate: 0.001778\n",
      "Epoch 394/40000, Loss: 0.005212709307670593, Learning Rate: 0.001777\n",
      "Epoch 395/40000, Loss: 0.004846109542995691, Learning Rate: 0.001776\n",
      "Epoch 396/40000, Loss: 0.004849831108003855, Learning Rate: 0.001776\n",
      "Epoch 397/40000, Loss: 0.002787234028801322, Learning Rate: 0.001775\n",
      "Epoch 398/40000, Loss: 0.0027943127788603306, Learning Rate: 0.001775\n",
      "Epoch 399/40000, Loss: 0.005200870800763369, Learning Rate: 0.001774\n",
      "Epoch 400/40000, Loss: 0.0048085288144648075, Learning Rate: 0.001774\n",
      "Epoch 401/40000, Loss: 0.004766966216266155, Learning Rate: 0.001773\n",
      "Epoch 402/40000, Loss: 0.0051134428940713406, Learning Rate: 0.001773\n",
      "Epoch 403/40000, Loss: 0.004954472184181213, Learning Rate: 0.001772\n",
      "Epoch 404/40000, Loss: 0.005239013582468033, Learning Rate: 0.001772\n",
      "Epoch 405/40000, Loss: 0.004073962569236755, Learning Rate: 0.001771\n",
      "Epoch 406/40000, Loss: 0.005281979683786631, Learning Rate: 0.001771\n",
      "Epoch 407/40000, Loss: 0.00315491808578372, Learning Rate: 0.001770\n",
      "Epoch 408/40000, Loss: 0.004403118044137955, Learning Rate: 0.001770\n",
      "Epoch 409/40000, Loss: 0.004450798034667969, Learning Rate: 0.001769\n",
      "Epoch 410/40000, Loss: 0.005109991412609816, Learning Rate: 0.001769\n",
      "Epoch 411/40000, Loss: 0.004974664654582739, Learning Rate: 0.001768\n",
      "Epoch 412/40000, Loss: 0.004875555168837309, Learning Rate: 0.001767\n",
      "Epoch 413/40000, Loss: 0.002772368025034666, Learning Rate: 0.001767\n",
      "Epoch 414/40000, Loss: 0.0027465263847261667, Learning Rate: 0.001766\n",
      "Epoch 415/40000, Loss: 0.005133785307407379, Learning Rate: 0.001766\n",
      "Epoch 416/40000, Loss: 0.0037298183888196945, Learning Rate: 0.001765\n",
      "Epoch 417/40000, Loss: 0.005107109434902668, Learning Rate: 0.001765\n",
      "Epoch 418/40000, Loss: 0.002755468711256981, Learning Rate: 0.001764\n",
      "Epoch 419/40000, Loss: 0.005060235504060984, Learning Rate: 0.001764\n",
      "Epoch 420/40000, Loss: 0.002630888484418392, Learning Rate: 0.001763\n",
      "Epoch 421/40000, Loss: 0.0027411580085754395, Learning Rate: 0.001763\n",
      "Epoch 422/40000, Loss: 0.005180077161639929, Learning Rate: 0.001762\n",
      "Epoch 423/40000, Loss: 0.003603750141337514, Learning Rate: 0.001762\n",
      "Epoch 424/40000, Loss: 0.005175137426704168, Learning Rate: 0.001761\n",
      "Epoch 425/40000, Loss: 0.0027049274649471045, Learning Rate: 0.001761\n",
      "Epoch 426/40000, Loss: 0.002658500801771879, Learning Rate: 0.001760\n",
      "Epoch 427/40000, Loss: 0.005108799319714308, Learning Rate: 0.001760\n",
      "Epoch 428/40000, Loss: 0.0027313674800097942, Learning Rate: 0.001759\n",
      "Epoch 429/40000, Loss: 0.002687115455046296, Learning Rate: 0.001758\n",
      "Epoch 430/40000, Loss: 0.0035675005055963993, Learning Rate: 0.001758\n",
      "Epoch 431/40000, Loss: 0.005015246104449034, Learning Rate: 0.001757\n",
      "Epoch 432/40000, Loss: 0.00504562770947814, Learning Rate: 0.001757\n",
      "Epoch 433/40000, Loss: 0.0026796048041433096, Learning Rate: 0.001756\n",
      "Epoch 434/40000, Loss: 0.005243757274001837, Learning Rate: 0.001756\n",
      "Epoch 435/40000, Loss: 0.00518639525398612, Learning Rate: 0.001755\n",
      "Epoch 436/40000, Loss: 0.004800513852387667, Learning Rate: 0.001755\n",
      "Epoch 437/40000, Loss: 0.0025673178024590015, Learning Rate: 0.001754\n",
      "Epoch 438/40000, Loss: 0.005144435912370682, Learning Rate: 0.001754\n",
      "Epoch 439/40000, Loss: 0.0038706930354237556, Learning Rate: 0.001753\n",
      "Epoch 440/40000, Loss: 0.005148483440279961, Learning Rate: 0.001753\n",
      "Epoch 441/40000, Loss: 0.0037435409612953663, Learning Rate: 0.001752\n",
      "Epoch 442/40000, Loss: 0.0025348763447254896, Learning Rate: 0.001752\n",
      "Epoch 443/40000, Loss: 0.00272012478671968, Learning Rate: 0.001751\n",
      "Epoch 444/40000, Loss: 0.002615126082673669, Learning Rate: 0.001751\n",
      "Epoch 445/40000, Loss: 0.0027459515258669853, Learning Rate: 0.001750\n",
      "Epoch 446/40000, Loss: 0.0028003104962408543, Learning Rate: 0.001750\n",
      "Epoch 447/40000, Loss: 0.0025534729938954115, Learning Rate: 0.001749\n",
      "Epoch 448/40000, Loss: 0.004990538582205772, Learning Rate: 0.001748\n",
      "Epoch 449/40000, Loss: 0.002587588271126151, Learning Rate: 0.001748\n",
      "Epoch 450/40000, Loss: 0.005005341488867998, Learning Rate: 0.001747\n",
      "Epoch 451/40000, Loss: 0.0028223213739693165, Learning Rate: 0.001747\n",
      "Epoch 452/40000, Loss: 0.002633193973451853, Learning Rate: 0.001746\n",
      "Epoch 453/40000, Loss: 0.0025672398041933775, Learning Rate: 0.001746\n",
      "Epoch 454/40000, Loss: 0.0025854140985757113, Learning Rate: 0.001745\n",
      "Epoch 455/40000, Loss: 0.004845455288887024, Learning Rate: 0.001745\n",
      "Epoch 456/40000, Loss: 0.00475280312821269, Learning Rate: 0.001744\n",
      "Epoch 457/40000, Loss: 0.004999380558729172, Learning Rate: 0.001744\n",
      "Epoch 458/40000, Loss: 0.002613469958305359, Learning Rate: 0.001743\n",
      "Epoch 459/40000, Loss: 0.0024999561719596386, Learning Rate: 0.001743\n",
      "Epoch 460/40000, Loss: 0.0049896808341145515, Learning Rate: 0.001742\n",
      "Epoch 461/40000, Loss: 0.00488238362595439, Learning Rate: 0.001742\n",
      "Epoch 462/40000, Loss: 0.005061968229711056, Learning Rate: 0.001741\n",
      "Epoch 463/40000, Loss: 0.004699456971138716, Learning Rate: 0.001741\n",
      "Epoch 464/40000, Loss: 0.004984928760677576, Learning Rate: 0.001740\n",
      "Epoch 465/40000, Loss: 0.0026532751508057117, Learning Rate: 0.001740\n",
      "Epoch 466/40000, Loss: 0.00503157265484333, Learning Rate: 0.001739\n",
      "Epoch 467/40000, Loss: 0.005039675161242485, Learning Rate: 0.001739\n",
      "Epoch 468/40000, Loss: 0.004615569021552801, Learning Rate: 0.001738\n",
      "Epoch 469/40000, Loss: 0.004919759463518858, Learning Rate: 0.001737\n",
      "Epoch 470/40000, Loss: 0.002670715795829892, Learning Rate: 0.001737\n",
      "Epoch 471/40000, Loss: 0.004733087494969368, Learning Rate: 0.001736\n",
      "Epoch 472/40000, Loss: 0.0036187784280627966, Learning Rate: 0.001736\n",
      "Epoch 473/40000, Loss: 0.005073663778603077, Learning Rate: 0.001735\n",
      "Epoch 474/40000, Loss: 0.002581393113359809, Learning Rate: 0.001735\n",
      "Epoch 475/40000, Loss: 0.005308753345161676, Learning Rate: 0.001734\n",
      "Epoch 476/40000, Loss: 0.004954851698130369, Learning Rate: 0.001734\n",
      "Epoch 477/40000, Loss: 0.0035769420210272074, Learning Rate: 0.001733\n",
      "Epoch 478/40000, Loss: 0.004814544692635536, Learning Rate: 0.001733\n",
      "Epoch 479/40000, Loss: 0.004675441887229681, Learning Rate: 0.001732\n",
      "Epoch 480/40000, Loss: 0.005074128042906523, Learning Rate: 0.001732\n",
      "Epoch 481/40000, Loss: 0.0034490106627345085, Learning Rate: 0.001731\n",
      "Epoch 482/40000, Loss: 0.00469448696821928, Learning Rate: 0.001731\n",
      "Epoch 483/40000, Loss: 0.002677024807780981, Learning Rate: 0.001730\n",
      "Epoch 484/40000, Loss: 0.003522380255162716, Learning Rate: 0.001730\n",
      "Epoch 485/40000, Loss: 0.0035726288333535194, Learning Rate: 0.001729\n",
      "Epoch 486/40000, Loss: 0.004916696809232235, Learning Rate: 0.001729\n",
      "Epoch 487/40000, Loss: 0.004597836639732122, Learning Rate: 0.001728\n",
      "Epoch 488/40000, Loss: 0.004985135514289141, Learning Rate: 0.001728\n",
      "Epoch 489/40000, Loss: 0.002575913444161415, Learning Rate: 0.001727\n",
      "Epoch 490/40000, Loss: 0.0027453782968223095, Learning Rate: 0.001727\n",
      "Epoch 491/40000, Loss: 0.005055573303252459, Learning Rate: 0.001726\n",
      "Epoch 492/40000, Loss: 0.0025113581214100122, Learning Rate: 0.001726\n",
      "Epoch 493/40000, Loss: 0.00476742023602128, Learning Rate: 0.001725\n",
      "Epoch 494/40000, Loss: 0.005090708378702402, Learning Rate: 0.001725\n",
      "Epoch 495/40000, Loss: 0.005015530623495579, Learning Rate: 0.001724\n",
      "Epoch 496/40000, Loss: 0.005130913108587265, Learning Rate: 0.001723\n",
      "Epoch 497/40000, Loss: 0.004748271778225899, Learning Rate: 0.001723\n",
      "Epoch 498/40000, Loss: 0.0051291389390826225, Learning Rate: 0.001722\n",
      "Epoch 499/40000, Loss: 0.005031276494264603, Learning Rate: 0.001722\n",
      "Epoch 500/40000, Loss: 0.0026331732515245676, Learning Rate: 0.001721\n",
      "Epoch 501/40000, Loss: 0.002533587394282222, Learning Rate: 0.001721\n",
      "Epoch 502/40000, Loss: 0.00498964125290513, Learning Rate: 0.001720\n",
      "Epoch 503/40000, Loss: 0.0049971165135502815, Learning Rate: 0.001720\n",
      "Epoch 504/40000, Loss: 0.0025962621439248323, Learning Rate: 0.001719\n",
      "Epoch 505/40000, Loss: 0.00455474341288209, Learning Rate: 0.001719\n",
      "Epoch 506/40000, Loss: 0.0034424499608576298, Learning Rate: 0.001718\n",
      "Epoch 507/40000, Loss: 0.00492513133212924, Learning Rate: 0.001718\n",
      "Epoch 508/40000, Loss: 0.005094126798212528, Learning Rate: 0.001717\n",
      "Epoch 509/40000, Loss: 0.004594141151756048, Learning Rate: 0.001717\n",
      "Epoch 510/40000, Loss: 0.004824650939553976, Learning Rate: 0.001716\n",
      "Epoch 511/40000, Loss: 0.0026232011150568724, Learning Rate: 0.001716\n",
      "Epoch 512/40000, Loss: 0.005152782890945673, Learning Rate: 0.001715\n",
      "Epoch 513/40000, Loss: 0.0029689990915358067, Learning Rate: 0.001715\n",
      "Epoch 514/40000, Loss: 0.004615898709744215, Learning Rate: 0.001714\n",
      "Epoch 515/40000, Loss: 0.0035723152104765177, Learning Rate: 0.001714\n",
      "Epoch 516/40000, Loss: 0.002719583222642541, Learning Rate: 0.001713\n",
      "Epoch 517/40000, Loss: 0.004869056865572929, Learning Rate: 0.001713\n",
      "Epoch 518/40000, Loss: 0.004862137604504824, Learning Rate: 0.001712\n",
      "Epoch 519/40000, Loss: 0.0048326896503567696, Learning Rate: 0.001712\n",
      "Epoch 520/40000, Loss: 0.004971392452716827, Learning Rate: 0.001711\n",
      "Epoch 521/40000, Loss: 0.004529271274805069, Learning Rate: 0.001711\n",
      "Epoch 522/40000, Loss: 0.004713582340627909, Learning Rate: 0.001710\n",
      "Epoch 523/40000, Loss: 0.0027467829640954733, Learning Rate: 0.001710\n",
      "Epoch 524/40000, Loss: 0.003617395181208849, Learning Rate: 0.001709\n",
      "Epoch 525/40000, Loss: 0.005374586675316095, Learning Rate: 0.001709\n",
      "Epoch 526/40000, Loss: 0.004889108706265688, Learning Rate: 0.001708\n",
      "Epoch 527/40000, Loss: 0.0025888720992952585, Learning Rate: 0.001708\n",
      "Epoch 528/40000, Loss: 0.0054069748148322105, Learning Rate: 0.001707\n",
      "Epoch 529/40000, Loss: 0.004828932229429483, Learning Rate: 0.001706\n",
      "Epoch 530/40000, Loss: 0.0026681015733629465, Learning Rate: 0.001706\n",
      "Epoch 531/40000, Loss: 0.003581879660487175, Learning Rate: 0.001705\n",
      "Epoch 532/40000, Loss: 0.005003850441426039, Learning Rate: 0.001705\n",
      "Epoch 533/40000, Loss: 0.004810635931789875, Learning Rate: 0.001704\n",
      "Epoch 534/40000, Loss: 0.002761007519438863, Learning Rate: 0.001704\n",
      "Epoch 535/40000, Loss: 0.004794531036168337, Learning Rate: 0.001703\n",
      "Epoch 536/40000, Loss: 0.004486402962356806, Learning Rate: 0.001703\n",
      "Epoch 537/40000, Loss: 0.002450265921652317, Learning Rate: 0.001702\n",
      "Epoch 538/40000, Loss: 0.004768358077853918, Learning Rate: 0.001702\n",
      "Epoch 539/40000, Loss: 0.004532329738140106, Learning Rate: 0.001701\n",
      "Epoch 540/40000, Loss: 0.004949058871716261, Learning Rate: 0.001701\n",
      "Epoch 541/40000, Loss: 0.003408813150599599, Learning Rate: 0.001700\n",
      "Epoch 542/40000, Loss: 0.004473379347473383, Learning Rate: 0.001700\n",
      "Epoch 543/40000, Loss: 0.0034594202879816294, Learning Rate: 0.001699\n",
      "Epoch 544/40000, Loss: 0.004881992004811764, Learning Rate: 0.001699\n",
      "Epoch 545/40000, Loss: 0.0027198183815926313, Learning Rate: 0.001698\n",
      "Epoch 546/40000, Loss: 0.004460506606847048, Learning Rate: 0.001698\n",
      "Epoch 547/40000, Loss: 0.004541665781289339, Learning Rate: 0.001697\n",
      "Epoch 548/40000, Loss: 0.0049454872496426105, Learning Rate: 0.001697\n",
      "Epoch 549/40000, Loss: 0.004682217724621296, Learning Rate: 0.001696\n",
      "Epoch 550/40000, Loss: 0.005198861937969923, Learning Rate: 0.001696\n",
      "Epoch 551/40000, Loss: 0.004831609781831503, Learning Rate: 0.001695\n",
      "Epoch 552/40000, Loss: 0.004968421068042517, Learning Rate: 0.001695\n",
      "Epoch 553/40000, Loss: 0.002492164261639118, Learning Rate: 0.001694\n",
      "Epoch 554/40000, Loss: 0.004729404114186764, Learning Rate: 0.001694\n",
      "Epoch 555/40000, Loss: 0.0025500261690467596, Learning Rate: 0.001693\n",
      "Epoch 556/40000, Loss: 0.004476528149098158, Learning Rate: 0.001693\n",
      "Epoch 557/40000, Loss: 0.00456097861751914, Learning Rate: 0.001692\n",
      "Epoch 558/40000, Loss: 0.0025022884365171194, Learning Rate: 0.001692\n",
      "Epoch 559/40000, Loss: 0.004452992230653763, Learning Rate: 0.001691\n",
      "Epoch 560/40000, Loss: 0.0027117859572172165, Learning Rate: 0.001691\n",
      "Epoch 561/40000, Loss: 0.0025517696049064398, Learning Rate: 0.001690\n",
      "Epoch 562/40000, Loss: 0.0025276311207562685, Learning Rate: 0.001690\n",
      "Epoch 563/40000, Loss: 0.0025319738779217005, Learning Rate: 0.001689\n",
      "Epoch 564/40000, Loss: 0.004887773189693689, Learning Rate: 0.001689\n",
      "Epoch 565/40000, Loss: 0.004470780026167631, Learning Rate: 0.001688\n",
      "Epoch 566/40000, Loss: 0.004809250123798847, Learning Rate: 0.001688\n",
      "Epoch 567/40000, Loss: 0.004601320717483759, Learning Rate: 0.001687\n",
      "Epoch 568/40000, Loss: 0.004767225589603186, Learning Rate: 0.001687\n",
      "Epoch 569/40000, Loss: 0.004712189082056284, Learning Rate: 0.001686\n",
      "Epoch 570/40000, Loss: 0.003431793302297592, Learning Rate: 0.001686\n",
      "Epoch 571/40000, Loss: 0.004972710739821196, Learning Rate: 0.001685\n",
      "Epoch 572/40000, Loss: 0.0026276777498424053, Learning Rate: 0.001685\n",
      "Epoch 573/40000, Loss: 0.00258965278044343, Learning Rate: 0.001684\n",
      "Epoch 574/40000, Loss: 0.00274510495364666, Learning Rate: 0.001684\n",
      "Epoch 575/40000, Loss: 0.003892764914780855, Learning Rate: 0.001683\n",
      "Epoch 576/40000, Loss: 0.00308992643840611, Learning Rate: 0.001683\n",
      "Epoch 577/40000, Loss: 0.0035459825303405523, Learning Rate: 0.001682\n",
      "Epoch 578/40000, Loss: 0.0038789776153862476, Learning Rate: 0.001682\n",
      "Epoch 579/40000, Loss: 0.004574998747557402, Learning Rate: 0.001681\n",
      "Epoch 580/40000, Loss: 0.004982175305485725, Learning Rate: 0.001681\n",
      "Epoch 581/40000, Loss: 0.005223351065069437, Learning Rate: 0.001680\n",
      "Epoch 582/40000, Loss: 0.0026877697091549635, Learning Rate: 0.001680\n",
      "Epoch 583/40000, Loss: 0.00488121435046196, Learning Rate: 0.001679\n",
      "Epoch 584/40000, Loss: 0.0025765285827219486, Learning Rate: 0.001679\n",
      "Epoch 585/40000, Loss: 0.004892988596111536, Learning Rate: 0.001678\n",
      "Epoch 586/40000, Loss: 0.005192556418478489, Learning Rate: 0.001678\n",
      "Epoch 587/40000, Loss: 0.005124965682625771, Learning Rate: 0.001677\n",
      "Epoch 588/40000, Loss: 0.005333936307579279, Learning Rate: 0.001677\n",
      "Epoch 589/40000, Loss: 0.005502073559910059, Learning Rate: 0.001676\n",
      "Epoch 590/40000, Loss: 0.0027343593537807465, Learning Rate: 0.001676\n",
      "Epoch 591/40000, Loss: 0.00340108759701252, Learning Rate: 0.001675\n",
      "Epoch 592/40000, Loss: 0.003423447720706463, Learning Rate: 0.001675\n",
      "Epoch 593/40000, Loss: 0.0028163783717900515, Learning Rate: 0.001674\n",
      "Epoch 594/40000, Loss: 0.004876913037151098, Learning Rate: 0.001674\n",
      "Epoch 595/40000, Loss: 0.002628074260428548, Learning Rate: 0.001673\n",
      "Epoch 596/40000, Loss: 0.004972321912646294, Learning Rate: 0.001673\n",
      "Epoch 597/40000, Loss: 0.004952630493789911, Learning Rate: 0.001672\n",
      "Epoch 598/40000, Loss: 0.00516679510474205, Learning Rate: 0.001672\n",
      "Epoch 599/40000, Loss: 0.0024628734681755304, Learning Rate: 0.001671\n",
      "Epoch 600/40000, Loss: 0.0025663303676992655, Learning Rate: 0.001671\n",
      "Epoch 601/40000, Loss: 0.004848469980061054, Learning Rate: 0.001670\n",
      "Epoch 602/40000, Loss: 0.0024866205640137196, Learning Rate: 0.001670\n",
      "Epoch 603/40000, Loss: 0.004783904179930687, Learning Rate: 0.001669\n",
      "Epoch 604/40000, Loss: 0.004821436945348978, Learning Rate: 0.001669\n",
      "Epoch 605/40000, Loss: 0.004693453665822744, Learning Rate: 0.001668\n",
      "Epoch 606/40000, Loss: 0.003432354424148798, Learning Rate: 0.001668\n",
      "Epoch 607/40000, Loss: 0.002526522148400545, Learning Rate: 0.001667\n",
      "Epoch 608/40000, Loss: 0.004423309583216906, Learning Rate: 0.001667\n",
      "Epoch 609/40000, Loss: 0.0048570455983281136, Learning Rate: 0.001666\n",
      "Epoch 610/40000, Loss: 0.0026083223056048155, Learning Rate: 0.001666\n",
      "Epoch 611/40000, Loss: 0.00458022765815258, Learning Rate: 0.001665\n",
      "Epoch 612/40000, Loss: 0.0025437013246119022, Learning Rate: 0.001665\n",
      "Epoch 613/40000, Loss: 0.0047360556200146675, Learning Rate: 0.001664\n",
      "Epoch 614/40000, Loss: 0.0048319497145712376, Learning Rate: 0.001664\n",
      "Epoch 615/40000, Loss: 0.00505127152428031, Learning Rate: 0.001663\n",
      "Epoch 616/40000, Loss: 0.005013599526137114, Learning Rate: 0.001663\n",
      "Epoch 617/40000, Loss: 0.002461305120959878, Learning Rate: 0.001662\n",
      "Epoch 618/40000, Loss: 0.0043929764069616795, Learning Rate: 0.001662\n",
      "Epoch 619/40000, Loss: 0.0025578702334314585, Learning Rate: 0.001661\n",
      "Epoch 620/40000, Loss: 0.004600187763571739, Learning Rate: 0.001661\n",
      "Epoch 621/40000, Loss: 0.002462917473167181, Learning Rate: 0.001660\n",
      "Epoch 622/40000, Loss: 0.004709291271865368, Learning Rate: 0.001660\n",
      "Epoch 623/40000, Loss: 0.004805919714272022, Learning Rate: 0.001659\n",
      "Epoch 624/40000, Loss: 0.004507918376475573, Learning Rate: 0.001659\n",
      "Epoch 625/40000, Loss: 0.004466183949261904, Learning Rate: 0.001658\n",
      "Epoch 626/40000, Loss: 0.004481521435081959, Learning Rate: 0.001658\n",
      "Epoch 627/40000, Loss: 0.0027359535451978445, Learning Rate: 0.001657\n",
      "Epoch 628/40000, Loss: 0.003638050053268671, Learning Rate: 0.001657\n",
      "Epoch 629/40000, Loss: 0.002422512276098132, Learning Rate: 0.001656\n",
      "Epoch 630/40000, Loss: 0.00494340667501092, Learning Rate: 0.001656\n",
      "Epoch 631/40000, Loss: 0.003697579028084874, Learning Rate: 0.001655\n",
      "Epoch 632/40000, Loss: 0.004814102314412594, Learning Rate: 0.001655\n",
      "Epoch 633/40000, Loss: 0.0025435276329517365, Learning Rate: 0.001654\n",
      "Epoch 634/40000, Loss: 0.00478625763207674, Learning Rate: 0.001654\n",
      "Epoch 635/40000, Loss: 0.004744559992104769, Learning Rate: 0.001653\n",
      "Epoch 636/40000, Loss: 0.002432501409202814, Learning Rate: 0.001653\n",
      "Epoch 637/40000, Loss: 0.004422797821462154, Learning Rate: 0.001652\n",
      "Epoch 638/40000, Loss: 0.004705060739070177, Learning Rate: 0.001652\n",
      "Epoch 639/40000, Loss: 0.0043984996154904366, Learning Rate: 0.001651\n",
      "Epoch 640/40000, Loss: 0.004771609324961901, Learning Rate: 0.001651\n",
      "Epoch 641/40000, Loss: 0.0047323936596512794, Learning Rate: 0.001650\n",
      "Epoch 642/40000, Loss: 0.002412915462628007, Learning Rate: 0.001650\n",
      "Epoch 643/40000, Loss: 0.0044115688651800156, Learning Rate: 0.001649\n",
      "Epoch 644/40000, Loss: 0.004705141764134169, Learning Rate: 0.001649\n",
      "Epoch 645/40000, Loss: 0.004867152776569128, Learning Rate: 0.001648\n",
      "Epoch 646/40000, Loss: 0.003395788837224245, Learning Rate: 0.001648\n",
      "Epoch 647/40000, Loss: 0.002521831775084138, Learning Rate: 0.001647\n",
      "Epoch 648/40000, Loss: 0.005080892704427242, Learning Rate: 0.001647\n",
      "Epoch 649/40000, Loss: 0.0024414847139269114, Learning Rate: 0.001646\n",
      "Epoch 650/40000, Loss: 0.002435040194541216, Learning Rate: 0.001646\n",
      "Epoch 651/40000, Loss: 0.0024372879415750504, Learning Rate: 0.001645\n",
      "Epoch 652/40000, Loss: 0.004827221389859915, Learning Rate: 0.001645\n",
      "Epoch 653/40000, Loss: 0.004605488386005163, Learning Rate: 0.001644\n",
      "Epoch 654/40000, Loss: 0.002446358557790518, Learning Rate: 0.001644\n",
      "Epoch 655/40000, Loss: 0.0043169669806957245, Learning Rate: 0.001643\n",
      "Epoch 656/40000, Loss: 0.003291700268164277, Learning Rate: 0.001643\n",
      "Epoch 657/40000, Loss: 0.002362574217841029, Learning Rate: 0.001642\n",
      "Epoch 658/40000, Loss: 0.00473320996388793, Learning Rate: 0.001642\n",
      "Epoch 659/40000, Loss: 0.002498591784387827, Learning Rate: 0.001641\n",
      "Epoch 660/40000, Loss: 0.002549590077251196, Learning Rate: 0.001641\n",
      "Epoch 661/40000, Loss: 0.0029804196674376726, Learning Rate: 0.001640\n",
      "Epoch 662/40000, Loss: 0.0025180112570524216, Learning Rate: 0.001640\n",
      "Epoch 663/40000, Loss: 0.003368053352460265, Learning Rate: 0.001639\n",
      "Epoch 664/40000, Loss: 0.0026247522328048944, Learning Rate: 0.001639\n",
      "Epoch 665/40000, Loss: 0.004719231743365526, Learning Rate: 0.001638\n",
      "Epoch 666/40000, Loss: 0.0024915244430303574, Learning Rate: 0.001638\n",
      "Epoch 667/40000, Loss: 0.00475468672811985, Learning Rate: 0.001637\n",
      "Epoch 668/40000, Loss: 0.004664934705942869, Learning Rate: 0.001637\n",
      "Epoch 669/40000, Loss: 0.004400523845106363, Learning Rate: 0.001636\n",
      "Epoch 670/40000, Loss: 0.00473199225962162, Learning Rate: 0.001636\n",
      "Epoch 671/40000, Loss: 0.004544589202851057, Learning Rate: 0.001635\n",
      "Epoch 672/40000, Loss: 0.0024725322145968676, Learning Rate: 0.001635\n",
      "Epoch 673/40000, Loss: 0.0035636955872178078, Learning Rate: 0.001634\n",
      "Epoch 674/40000, Loss: 0.004738905001431704, Learning Rate: 0.001634\n",
      "Epoch 675/40000, Loss: 0.004866716917604208, Learning Rate: 0.001633\n",
      "Epoch 676/40000, Loss: 0.0024311658926308155, Learning Rate: 0.001633\n",
      "Epoch 677/40000, Loss: 0.004741258919239044, Learning Rate: 0.001632\n",
      "Epoch 678/40000, Loss: 0.004321745131164789, Learning Rate: 0.001632\n",
      "Epoch 679/40000, Loss: 0.0043776389211416245, Learning Rate: 0.001631\n",
      "Epoch 680/40000, Loss: 0.004753336776047945, Learning Rate: 0.001631\n",
      "Epoch 681/40000, Loss: 0.004717243369668722, Learning Rate: 0.001630\n",
      "Epoch 682/40000, Loss: 0.0026002756785601377, Learning Rate: 0.001630\n",
      "Epoch 683/40000, Loss: 0.0034479391761124134, Learning Rate: 0.001629\n",
      "Epoch 684/40000, Loss: 0.002402520040050149, Learning Rate: 0.001629\n",
      "Epoch 685/40000, Loss: 0.002487148391082883, Learning Rate: 0.001628\n",
      "Epoch 686/40000, Loss: 0.004498528782278299, Learning Rate: 0.001628\n",
      "Epoch 687/40000, Loss: 0.003277943702414632, Learning Rate: 0.001627\n",
      "Epoch 688/40000, Loss: 0.00481757614761591, Learning Rate: 0.001627\n",
      "Epoch 689/40000, Loss: 0.0026155970990657806, Learning Rate: 0.001627\n",
      "Epoch 690/40000, Loss: 0.0034802588634192944, Learning Rate: 0.001626\n",
      "Epoch 691/40000, Loss: 0.003449175739660859, Learning Rate: 0.001626\n",
      "Epoch 692/40000, Loss: 0.0024925845209509134, Learning Rate: 0.001625\n",
      "Epoch 693/40000, Loss: 0.0048113237135112286, Learning Rate: 0.001625\n",
      "Epoch 694/40000, Loss: 0.002446134574711323, Learning Rate: 0.001624\n",
      "Epoch 695/40000, Loss: 0.004798587877303362, Learning Rate: 0.001624\n",
      "Epoch 696/40000, Loss: 0.004410463385283947, Learning Rate: 0.001623\n",
      "Epoch 697/40000, Loss: 0.004930333234369755, Learning Rate: 0.001623\n",
      "Epoch 698/40000, Loss: 0.005484304390847683, Learning Rate: 0.001622\n",
      "Epoch 699/40000, Loss: 0.004981609061360359, Learning Rate: 0.001622\n",
      "Epoch 700/40000, Loss: 0.0049585141241550446, Learning Rate: 0.001621\n",
      "Epoch 701/40000, Loss: 0.004540753550827503, Learning Rate: 0.001621\n",
      "Epoch 702/40000, Loss: 0.002457894617691636, Learning Rate: 0.001620\n",
      "Epoch 703/40000, Loss: 0.0032533558551222086, Learning Rate: 0.001620\n",
      "Epoch 704/40000, Loss: 0.003318140981718898, Learning Rate: 0.001619\n",
      "Epoch 705/40000, Loss: 0.0024781618267297745, Learning Rate: 0.001619\n",
      "Epoch 706/40000, Loss: 0.003421847242861986, Learning Rate: 0.001618\n",
      "Epoch 707/40000, Loss: 0.0024910022038966417, Learning Rate: 0.001618\n",
      "Epoch 708/40000, Loss: 0.0024513211101293564, Learning Rate: 0.001617\n",
      "Epoch 709/40000, Loss: 0.0032765588257461786, Learning Rate: 0.001617\n",
      "Epoch 710/40000, Loss: 0.0037748217582702637, Learning Rate: 0.001616\n",
      "Epoch 711/40000, Loss: 0.0032785770017653704, Learning Rate: 0.001616\n",
      "Epoch 712/40000, Loss: 0.005454584490507841, Learning Rate: 0.001615\n",
      "Epoch 713/40000, Loss: 0.0028119601774960756, Learning Rate: 0.001615\n",
      "Epoch 714/40000, Loss: 0.002702383790165186, Learning Rate: 0.001614\n",
      "Epoch 715/40000, Loss: 0.004731697496026754, Learning Rate: 0.001614\n",
      "Epoch 716/40000, Loss: 0.002456996124237776, Learning Rate: 0.001613\n",
      "Epoch 717/40000, Loss: 0.005345277022570372, Learning Rate: 0.001613\n",
      "Epoch 718/40000, Loss: 0.005227506626397371, Learning Rate: 0.001612\n",
      "Epoch 719/40000, Loss: 0.00501749012619257, Learning Rate: 0.001612\n",
      "Epoch 720/40000, Loss: 0.005108063109219074, Learning Rate: 0.001611\n",
      "Epoch 721/40000, Loss: 0.004693769384175539, Learning Rate: 0.001611\n",
      "Epoch 722/40000, Loss: 0.0034386722836643457, Learning Rate: 0.001610\n",
      "Epoch 723/40000, Loss: 0.0024960467126220465, Learning Rate: 0.001610\n",
      "Epoch 724/40000, Loss: 0.0032354954164475203, Learning Rate: 0.001610\n",
      "Epoch 725/40000, Loss: 0.0025207556318491697, Learning Rate: 0.001609\n",
      "Epoch 726/40000, Loss: 0.004722357727587223, Learning Rate: 0.001609\n",
      "Epoch 727/40000, Loss: 0.004683300387114286, Learning Rate: 0.001608\n",
      "Epoch 728/40000, Loss: 0.004638098180294037, Learning Rate: 0.001608\n",
      "Epoch 729/40000, Loss: 0.004716266877949238, Learning Rate: 0.001607\n",
      "Epoch 730/40000, Loss: 0.002380675869062543, Learning Rate: 0.001607\n",
      "Epoch 731/40000, Loss: 0.0024576555006206036, Learning Rate: 0.001606\n",
      "Epoch 732/40000, Loss: 0.0026089143939316273, Learning Rate: 0.001606\n",
      "Epoch 733/40000, Loss: 0.0027990764938294888, Learning Rate: 0.001605\n",
      "Epoch 734/40000, Loss: 0.0025666141882538795, Learning Rate: 0.001605\n",
      "Epoch 735/40000, Loss: 0.004760914947837591, Learning Rate: 0.001604\n",
      "Epoch 736/40000, Loss: 0.005212804302573204, Learning Rate: 0.001604\n",
      "Epoch 737/40000, Loss: 0.002959930570796132, Learning Rate: 0.001603\n",
      "Epoch 738/40000, Loss: 0.00252751586958766, Learning Rate: 0.001603\n",
      "Epoch 739/40000, Loss: 0.004731395281851292, Learning Rate: 0.001602\n",
      "Epoch 740/40000, Loss: 0.0024623398203402758, Learning Rate: 0.001602\n",
      "Epoch 741/40000, Loss: 0.002533035119995475, Learning Rate: 0.001601\n",
      "Epoch 742/40000, Loss: 0.0033027969766408205, Learning Rate: 0.001601\n",
      "Epoch 743/40000, Loss: 0.0033980663865804672, Learning Rate: 0.001600\n",
      "Epoch 744/40000, Loss: 0.004633475560694933, Learning Rate: 0.001600\n",
      "Epoch 745/40000, Loss: 0.0033833952620625496, Learning Rate: 0.001599\n",
      "Epoch 746/40000, Loss: 0.002507114317268133, Learning Rate: 0.001599\n",
      "Epoch 747/40000, Loss: 0.0023461286909878254, Learning Rate: 0.001598\n",
      "Epoch 748/40000, Loss: 0.004744622856378555, Learning Rate: 0.001598\n",
      "Epoch 749/40000, Loss: 0.005065169185400009, Learning Rate: 0.001598\n",
      "Epoch 750/40000, Loss: 0.0043602329678833485, Learning Rate: 0.001597\n",
      "Epoch 751/40000, Loss: 0.004556368105113506, Learning Rate: 0.001597\n",
      "Epoch 752/40000, Loss: 0.0033001962583512068, Learning Rate: 0.001596\n",
      "Epoch 753/40000, Loss: 0.0042960927821695805, Learning Rate: 0.001596\n",
      "Epoch 754/40000, Loss: 0.004349584225565195, Learning Rate: 0.001595\n",
      "Epoch 755/40000, Loss: 0.0023657099809497595, Learning Rate: 0.001595\n",
      "Epoch 756/40000, Loss: 0.0042694308795034885, Learning Rate: 0.001594\n",
      "Epoch 757/40000, Loss: 0.004686303902417421, Learning Rate: 0.001594\n",
      "Epoch 758/40000, Loss: 0.0024360879324376583, Learning Rate: 0.001593\n",
      "Epoch 759/40000, Loss: 0.004562898073345423, Learning Rate: 0.001593\n",
      "Epoch 760/40000, Loss: 0.004271981306374073, Learning Rate: 0.001592\n",
      "Epoch 761/40000, Loss: 0.0023564579896628857, Learning Rate: 0.001592\n",
      "Epoch 762/40000, Loss: 0.004606988281011581, Learning Rate: 0.001591\n",
      "Epoch 763/40000, Loss: 0.004545524250715971, Learning Rate: 0.001591\n",
      "Epoch 764/40000, Loss: 0.0031860379967838526, Learning Rate: 0.001590\n",
      "Epoch 765/40000, Loss: 0.004576508887112141, Learning Rate: 0.001590\n",
      "Epoch 766/40000, Loss: 0.004790809936821461, Learning Rate: 0.001589\n",
      "Epoch 767/40000, Loss: 0.002570288721472025, Learning Rate: 0.001589\n",
      "Epoch 768/40000, Loss: 0.004602124448865652, Learning Rate: 0.001588\n",
      "Epoch 769/40000, Loss: 0.004482128191739321, Learning Rate: 0.001588\n",
      "Epoch 770/40000, Loss: 0.00246142758987844, Learning Rate: 0.001587\n",
      "Epoch 771/40000, Loss: 0.004243606701493263, Learning Rate: 0.001587\n",
      "Epoch 772/40000, Loss: 0.00243682274594903, Learning Rate: 0.001587\n",
      "Epoch 773/40000, Loss: 0.004546620417386293, Learning Rate: 0.001586\n",
      "Epoch 774/40000, Loss: 0.0022904144134372473, Learning Rate: 0.001586\n",
      "Epoch 775/40000, Loss: 0.004627999849617481, Learning Rate: 0.001585\n",
      "Epoch 776/40000, Loss: 0.002457221271470189, Learning Rate: 0.001585\n",
      "Epoch 777/40000, Loss: 0.004674356430768967, Learning Rate: 0.001584\n",
      "Epoch 778/40000, Loss: 0.004545289091765881, Learning Rate: 0.001584\n",
      "Epoch 779/40000, Loss: 0.0032798019237816334, Learning Rate: 0.001583\n",
      "Epoch 780/40000, Loss: 0.0045627872459590435, Learning Rate: 0.001583\n",
      "Epoch 781/40000, Loss: 0.002383636776357889, Learning Rate: 0.001582\n",
      "Epoch 782/40000, Loss: 0.003142366884276271, Learning Rate: 0.001582\n",
      "Epoch 783/40000, Loss: 0.004586254246532917, Learning Rate: 0.001581\n",
      "Epoch 784/40000, Loss: 0.002400971483439207, Learning Rate: 0.001581\n",
      "Epoch 785/40000, Loss: 0.0033319462090730667, Learning Rate: 0.001580\n",
      "Epoch 786/40000, Loss: 0.004498315043747425, Learning Rate: 0.001580\n",
      "Epoch 787/40000, Loss: 0.003225571010261774, Learning Rate: 0.001579\n",
      "Epoch 788/40000, Loss: 0.004237367305904627, Learning Rate: 0.001579\n",
      "Epoch 789/40000, Loss: 0.004196161404252052, Learning Rate: 0.001578\n",
      "Epoch 790/40000, Loss: 0.004479455295950174, Learning Rate: 0.001578\n",
      "Epoch 791/40000, Loss: 0.0023135675583034754, Learning Rate: 0.001577\n",
      "Epoch 792/40000, Loss: 0.004501300863921642, Learning Rate: 0.001577\n",
      "Epoch 793/40000, Loss: 0.0024605123326182365, Learning Rate: 0.001577\n",
      "Epoch 794/40000, Loss: 0.0024055566173046827, Learning Rate: 0.001576\n",
      "Epoch 795/40000, Loss: 0.002319764345884323, Learning Rate: 0.001576\n",
      "Epoch 796/40000, Loss: 0.004451844375580549, Learning Rate: 0.001575\n",
      "Epoch 797/40000, Loss: 0.004493550397455692, Learning Rate: 0.001575\n",
      "Epoch 798/40000, Loss: 0.0032887703273445368, Learning Rate: 0.001574\n",
      "Epoch 799/40000, Loss: 0.004725469741970301, Learning Rate: 0.001574\n",
      "Epoch 800/40000, Loss: 0.004601897671818733, Learning Rate: 0.001573\n",
      "Epoch 801/40000, Loss: 0.0023736576549708843, Learning Rate: 0.001573\n",
      "Epoch 802/40000, Loss: 0.004603966139256954, Learning Rate: 0.001572\n",
      "Epoch 803/40000, Loss: 0.004608897492289543, Learning Rate: 0.001572\n",
      "Epoch 804/40000, Loss: 0.003157907398417592, Learning Rate: 0.001571\n",
      "Epoch 805/40000, Loss: 0.004387011751532555, Learning Rate: 0.001571\n",
      "Epoch 806/40000, Loss: 0.004391442518681288, Learning Rate: 0.001570\n",
      "Epoch 807/40000, Loss: 0.004361188504844904, Learning Rate: 0.001570\n",
      "Epoch 808/40000, Loss: 0.0045494213700294495, Learning Rate: 0.001569\n",
      "Epoch 809/40000, Loss: 0.0023000205401331186, Learning Rate: 0.001569\n",
      "Epoch 810/40000, Loss: 0.0023486565332859755, Learning Rate: 0.001569\n",
      "Epoch 811/40000, Loss: 0.0032152971252799034, Learning Rate: 0.001568\n",
      "Epoch 812/40000, Loss: 0.003187366295605898, Learning Rate: 0.001568\n",
      "Epoch 813/40000, Loss: 0.0023345628287643194, Learning Rate: 0.001567\n",
      "Epoch 814/40000, Loss: 0.004462132696062326, Learning Rate: 0.001567\n",
      "Epoch 815/40000, Loss: 0.004603341221809387, Learning Rate: 0.001566\n",
      "Epoch 816/40000, Loss: 0.003404962131753564, Learning Rate: 0.001566\n",
      "Epoch 817/40000, Loss: 0.004181657452136278, Learning Rate: 0.001565\n",
      "Epoch 818/40000, Loss: 0.004358167294412851, Learning Rate: 0.001565\n",
      "Epoch 819/40000, Loss: 0.002406446961686015, Learning Rate: 0.001564\n",
      "Epoch 820/40000, Loss: 0.0031239173840731382, Learning Rate: 0.001564\n",
      "Epoch 821/40000, Loss: 0.0032415271271020174, Learning Rate: 0.001563\n",
      "Epoch 822/40000, Loss: 0.0025241239927709103, Learning Rate: 0.001563\n",
      "Epoch 823/40000, Loss: 0.0033880819100886583, Learning Rate: 0.001562\n",
      "Epoch 824/40000, Loss: 0.004465942271053791, Learning Rate: 0.001562\n",
      "Epoch 825/40000, Loss: 0.004675442818552256, Learning Rate: 0.001561\n",
      "Epoch 826/40000, Loss: 0.003117063781246543, Learning Rate: 0.001561\n",
      "Epoch 827/40000, Loss: 0.0044977967627346516, Learning Rate: 0.001561\n",
      "Epoch 828/40000, Loss: 0.0044891685247421265, Learning Rate: 0.001560\n",
      "Epoch 829/40000, Loss: 0.0031986061949282885, Learning Rate: 0.001560\n",
      "Epoch 830/40000, Loss: 0.004252760671079159, Learning Rate: 0.001559\n",
      "Epoch 831/40000, Loss: 0.0044444468803703785, Learning Rate: 0.001559\n",
      "Epoch 832/40000, Loss: 0.002298119477927685, Learning Rate: 0.001558\n",
      "Epoch 833/40000, Loss: 0.003122898517176509, Learning Rate: 0.001558\n",
      "Epoch 834/40000, Loss: 0.0033744736574590206, Learning Rate: 0.001557\n",
      "Epoch 835/40000, Loss: 0.0023578961845487356, Learning Rate: 0.001557\n",
      "Epoch 836/40000, Loss: 0.0022705928422510624, Learning Rate: 0.001556\n",
      "Epoch 837/40000, Loss: 0.004661542363464832, Learning Rate: 0.001556\n",
      "Epoch 838/40000, Loss: 0.0023729263339191675, Learning Rate: 0.001555\n",
      "Epoch 839/40000, Loss: 0.004548001103103161, Learning Rate: 0.001555\n",
      "Epoch 840/40000, Loss: 0.004191923420876265, Learning Rate: 0.001554\n",
      "Epoch 841/40000, Loss: 0.004426413681358099, Learning Rate: 0.001554\n",
      "Epoch 842/40000, Loss: 0.004213272128254175, Learning Rate: 0.001554\n",
      "Epoch 843/40000, Loss: 0.004522884730249643, Learning Rate: 0.001553\n",
      "Epoch 844/40000, Loss: 0.004296669736504555, Learning Rate: 0.001553\n",
      "Epoch 845/40000, Loss: 0.002382861217483878, Learning Rate: 0.001552\n",
      "Epoch 846/40000, Loss: 0.004454068373888731, Learning Rate: 0.001552\n",
      "Epoch 847/40000, Loss: 0.004648952279239893, Learning Rate: 0.001551\n",
      "Epoch 848/40000, Loss: 0.003306225873529911, Learning Rate: 0.001551\n",
      "Epoch 849/40000, Loss: 0.0033594064880162477, Learning Rate: 0.001550\n",
      "Epoch 850/40000, Loss: 0.004369182046502829, Learning Rate: 0.001550\n",
      "Epoch 851/40000, Loss: 0.0026061481330543756, Learning Rate: 0.001549\n",
      "Epoch 852/40000, Loss: 0.00439341738820076, Learning Rate: 0.001549\n",
      "Epoch 853/40000, Loss: 0.002685499843209982, Learning Rate: 0.001548\n",
      "Epoch 854/40000, Loss: 0.002361078280955553, Learning Rate: 0.001548\n",
      "Epoch 855/40000, Loss: 0.003279325785115361, Learning Rate: 0.001548\n",
      "Epoch 856/40000, Loss: 0.003114456543698907, Learning Rate: 0.001547\n",
      "Epoch 857/40000, Loss: 0.004410934168845415, Learning Rate: 0.001547\n",
      "Epoch 858/40000, Loss: 0.002551177516579628, Learning Rate: 0.001546\n",
      "Epoch 859/40000, Loss: 0.0025610884185880423, Learning Rate: 0.001546\n",
      "Epoch 860/40000, Loss: 0.00429094722494483, Learning Rate: 0.001545\n",
      "Epoch 861/40000, Loss: 0.00439900578930974, Learning Rate: 0.001545\n",
      "Epoch 862/40000, Loss: 0.002275960985571146, Learning Rate: 0.001544\n",
      "Epoch 863/40000, Loss: 0.004381735809147358, Learning Rate: 0.001544\n",
      "Epoch 864/40000, Loss: 0.004687652923166752, Learning Rate: 0.001543\n",
      "Epoch 865/40000, Loss: 0.004624018445611, Learning Rate: 0.001543\n",
      "Epoch 866/40000, Loss: 0.002374493284150958, Learning Rate: 0.001542\n",
      "Epoch 867/40000, Loss: 0.0031949514523148537, Learning Rate: 0.001542\n",
      "Epoch 868/40000, Loss: 0.0044918847270309925, Learning Rate: 0.001541\n",
      "Epoch 869/40000, Loss: 0.004443368408828974, Learning Rate: 0.001541\n",
      "Epoch 870/40000, Loss: 0.004571007564663887, Learning Rate: 0.001541\n",
      "Epoch 871/40000, Loss: 0.004544486757367849, Learning Rate: 0.001540\n",
      "Epoch 872/40000, Loss: 0.004087365232408047, Learning Rate: 0.001540\n",
      "Epoch 873/40000, Loss: 0.003120832610875368, Learning Rate: 0.001539\n",
      "Epoch 874/40000, Loss: 0.004505643621087074, Learning Rate: 0.001539\n",
      "Epoch 875/40000, Loss: 0.004125164356082678, Learning Rate: 0.001538\n",
      "Epoch 876/40000, Loss: 0.004424375481903553, Learning Rate: 0.001538\n",
      "Epoch 877/40000, Loss: 0.0023238782305270433, Learning Rate: 0.001537\n",
      "Epoch 878/40000, Loss: 0.0023356815800070763, Learning Rate: 0.001537\n",
      "Epoch 879/40000, Loss: 0.0024387529119849205, Learning Rate: 0.001536\n",
      "Epoch 880/40000, Loss: 0.0045038494281470776, Learning Rate: 0.001536\n",
      "Epoch 881/40000, Loss: 0.002296595135703683, Learning Rate: 0.001535\n",
      "Epoch 882/40000, Loss: 0.002218682086095214, Learning Rate: 0.001535\n",
      "Epoch 883/40000, Loss: 0.004470569547265768, Learning Rate: 0.001535\n",
      "Epoch 884/40000, Loss: 0.004589685704559088, Learning Rate: 0.001534\n",
      "Epoch 885/40000, Loss: 0.002485709497705102, Learning Rate: 0.001534\n",
      "Epoch 886/40000, Loss: 0.0022631450556218624, Learning Rate: 0.001533\n",
      "Epoch 887/40000, Loss: 0.0023775636218488216, Learning Rate: 0.001533\n",
      "Epoch 888/40000, Loss: 0.004165272228419781, Learning Rate: 0.001532\n",
      "Epoch 889/40000, Loss: 0.004422619007527828, Learning Rate: 0.001532\n",
      "Epoch 890/40000, Loss: 0.002374267438426614, Learning Rate: 0.001531\n",
      "Epoch 891/40000, Loss: 0.002257803687825799, Learning Rate: 0.001531\n",
      "Epoch 892/40000, Loss: 0.003269264241680503, Learning Rate: 0.001530\n",
      "Epoch 893/40000, Loss: 0.004553345963358879, Learning Rate: 0.001530\n",
      "Epoch 894/40000, Loss: 0.00487858708947897, Learning Rate: 0.001529\n",
      "Epoch 895/40000, Loss: 0.004880442284047604, Learning Rate: 0.001529\n",
      "Epoch 896/40000, Loss: 0.0028524219524115324, Learning Rate: 0.001529\n",
      "Epoch 897/40000, Loss: 0.002573024481534958, Learning Rate: 0.001528\n",
      "Epoch 898/40000, Loss: 0.0023886344861239195, Learning Rate: 0.001528\n",
      "Epoch 899/40000, Loss: 0.00314346537925303, Learning Rate: 0.001527\n",
      "Epoch 900/40000, Loss: 0.004619567655026913, Learning Rate: 0.001527\n",
      "Epoch 901/40000, Loss: 0.004112727474421263, Learning Rate: 0.001526\n",
      "Epoch 902/40000, Loss: 0.004350358620285988, Learning Rate: 0.001526\n",
      "Epoch 903/40000, Loss: 0.0025770377833396196, Learning Rate: 0.001525\n",
      "Epoch 904/40000, Loss: 0.004353975411504507, Learning Rate: 0.001525\n",
      "Epoch 905/40000, Loss: 0.004177233669906855, Learning Rate: 0.001524\n",
      "Epoch 906/40000, Loss: 0.002369324676692486, Learning Rate: 0.001524\n",
      "Epoch 907/40000, Loss: 0.004124579951167107, Learning Rate: 0.001524\n",
      "Epoch 908/40000, Loss: 0.004218489862978458, Learning Rate: 0.001523\n",
      "Epoch 909/40000, Loss: 0.00453291367739439, Learning Rate: 0.001523\n",
      "Epoch 910/40000, Loss: 0.0030155908316373825, Learning Rate: 0.001522\n",
      "Epoch 911/40000, Loss: 0.00444715516641736, Learning Rate: 0.001522\n",
      "Epoch 912/40000, Loss: 0.002302011474967003, Learning Rate: 0.001521\n",
      "Epoch 913/40000, Loss: 0.0030475053936243057, Learning Rate: 0.001521\n",
      "Epoch 914/40000, Loss: 0.003131503239274025, Learning Rate: 0.001520\n",
      "Epoch 915/40000, Loss: 0.0022307017352432013, Learning Rate: 0.001520\n",
      "Epoch 916/40000, Loss: 0.004436966963112354, Learning Rate: 0.001519\n",
      "Epoch 917/40000, Loss: 0.0023430984001606703, Learning Rate: 0.001519\n",
      "Epoch 918/40000, Loss: 0.002451003761962056, Learning Rate: 0.001519\n",
      "Epoch 919/40000, Loss: 0.00439175870269537, Learning Rate: 0.001518\n",
      "Epoch 920/40000, Loss: 0.004204754251986742, Learning Rate: 0.001518\n",
      "Epoch 921/40000, Loss: 0.0031573865562677383, Learning Rate: 0.001517\n",
      "Epoch 922/40000, Loss: 0.00223237625323236, Learning Rate: 0.001517\n",
      "Epoch 923/40000, Loss: 0.004390797112137079, Learning Rate: 0.001516\n",
      "Epoch 924/40000, Loss: 0.003044653916731477, Learning Rate: 0.001516\n",
      "Epoch 925/40000, Loss: 0.004149565473198891, Learning Rate: 0.001515\n",
      "Epoch 926/40000, Loss: 0.00232643517665565, Learning Rate: 0.001515\n",
      "Epoch 927/40000, Loss: 0.0031677046790719032, Learning Rate: 0.001514\n",
      "Epoch 928/40000, Loss: 0.0044629271142184734, Learning Rate: 0.001514\n",
      "Epoch 929/40000, Loss: 0.0023920827079564333, Learning Rate: 0.001514\n",
      "Epoch 930/40000, Loss: 0.004482564516365528, Learning Rate: 0.001513\n",
      "Epoch 931/40000, Loss: 0.003022915916517377, Learning Rate: 0.001513\n",
      "Epoch 932/40000, Loss: 0.00440542446449399, Learning Rate: 0.001512\n",
      "Epoch 933/40000, Loss: 0.0023160947021096945, Learning Rate: 0.001512\n",
      "Epoch 934/40000, Loss: 0.0022415800485759974, Learning Rate: 0.001511\n",
      "Epoch 935/40000, Loss: 0.004448195453733206, Learning Rate: 0.001511\n",
      "Epoch 936/40000, Loss: 0.003107778960838914, Learning Rate: 0.001510\n",
      "Epoch 937/40000, Loss: 0.0031496654264628887, Learning Rate: 0.001510\n",
      "Epoch 938/40000, Loss: 0.004450386855751276, Learning Rate: 0.001509\n",
      "Epoch 939/40000, Loss: 0.004576530773192644, Learning Rate: 0.001509\n",
      "Epoch 940/40000, Loss: 0.004078399855643511, Learning Rate: 0.001509\n",
      "Epoch 941/40000, Loss: 0.002293161116540432, Learning Rate: 0.001508\n",
      "Epoch 942/40000, Loss: 0.004126862622797489, Learning Rate: 0.001508\n",
      "Epoch 943/40000, Loss: 0.004165567457675934, Learning Rate: 0.001507\n",
      "Epoch 944/40000, Loss: 0.002263555536046624, Learning Rate: 0.001507\n",
      "Epoch 945/40000, Loss: 0.0041056848131120205, Learning Rate: 0.001506\n",
      "Epoch 946/40000, Loss: 0.0023218716960400343, Learning Rate: 0.001506\n",
      "Epoch 947/40000, Loss: 0.0022496082819998264, Learning Rate: 0.001505\n",
      "Epoch 948/40000, Loss: 0.004571613855659962, Learning Rate: 0.001505\n",
      "Epoch 949/40000, Loss: 0.0022939497139304876, Learning Rate: 0.001504\n",
      "Epoch 950/40000, Loss: 0.004642060957849026, Learning Rate: 0.001504\n",
      "Epoch 951/40000, Loss: 0.0024318210780620575, Learning Rate: 0.001504\n",
      "Epoch 952/40000, Loss: 0.004495387431234121, Learning Rate: 0.001503\n",
      "Epoch 953/40000, Loss: 0.003084499156102538, Learning Rate: 0.001503\n",
      "Epoch 954/40000, Loss: 0.002377523109316826, Learning Rate: 0.001502\n",
      "Epoch 955/40000, Loss: 0.002357146702706814, Learning Rate: 0.001502\n",
      "Epoch 956/40000, Loss: 0.00446917861700058, Learning Rate: 0.001501\n",
      "Epoch 957/40000, Loss: 0.004737571347504854, Learning Rate: 0.001501\n",
      "Epoch 958/40000, Loss: 0.004626980051398277, Learning Rate: 0.001500\n",
      "Epoch 959/40000, Loss: 0.0030116226989775896, Learning Rate: 0.001500\n",
      "Epoch 960/40000, Loss: 0.004095758777111769, Learning Rate: 0.001500\n",
      "Epoch 961/40000, Loss: 0.004311523400247097, Learning Rate: 0.001499\n",
      "Epoch 962/40000, Loss: 0.004375018645077944, Learning Rate: 0.001499\n",
      "Epoch 963/40000, Loss: 0.002400232246145606, Learning Rate: 0.001498\n",
      "Epoch 964/40000, Loss: 0.004448141902685165, Learning Rate: 0.001498\n",
      "Epoch 965/40000, Loss: 0.004535011947154999, Learning Rate: 0.001497\n",
      "Epoch 966/40000, Loss: 0.003056713379919529, Learning Rate: 0.001497\n",
      "Epoch 967/40000, Loss: 0.004403918050229549, Learning Rate: 0.001496\n",
      "Epoch 968/40000, Loss: 0.0022526446264237165, Learning Rate: 0.001496\n",
      "Epoch 969/40000, Loss: 0.004362056963145733, Learning Rate: 0.001495\n",
      "Epoch 970/40000, Loss: 0.003100466448813677, Learning Rate: 0.001495\n",
      "Epoch 971/40000, Loss: 0.002249179407954216, Learning Rate: 0.001495\n",
      "Epoch 972/40000, Loss: 0.0041144187562167645, Learning Rate: 0.001494\n",
      "Epoch 973/40000, Loss: 0.0022506362292915583, Learning Rate: 0.001494\n",
      "Epoch 974/40000, Loss: 0.00442461995407939, Learning Rate: 0.001493\n",
      "Epoch 975/40000, Loss: 0.0046227662824094296, Learning Rate: 0.001493\n",
      "Epoch 976/40000, Loss: 0.002350602764636278, Learning Rate: 0.001492\n",
      "Epoch 977/40000, Loss: 0.0024921975564211607, Learning Rate: 0.001492\n",
      "Epoch 978/40000, Loss: 0.0024566752836108208, Learning Rate: 0.001491\n",
      "Epoch 979/40000, Loss: 0.004387400113046169, Learning Rate: 0.001491\n",
      "Epoch 980/40000, Loss: 0.004550743382424116, Learning Rate: 0.001491\n",
      "Epoch 981/40000, Loss: 0.0024951687082648277, Learning Rate: 0.001490\n",
      "Epoch 982/40000, Loss: 0.004447383340448141, Learning Rate: 0.001490\n",
      "Epoch 983/40000, Loss: 0.004205544013530016, Learning Rate: 0.001489\n",
      "Epoch 984/40000, Loss: 0.00232892669737339, Learning Rate: 0.001489\n",
      "Epoch 985/40000, Loss: 0.0024788000155240297, Learning Rate: 0.001488\n",
      "Epoch 986/40000, Loss: 0.004501889925450087, Learning Rate: 0.001488\n",
      "Epoch 987/40000, Loss: 0.00243099476210773, Learning Rate: 0.001487\n",
      "Epoch 988/40000, Loss: 0.003238705452531576, Learning Rate: 0.001487\n",
      "Epoch 989/40000, Loss: 0.002250959165394306, Learning Rate: 0.001487\n",
      "Epoch 990/40000, Loss: 0.0023155049420893192, Learning Rate: 0.001486\n",
      "Epoch 991/40000, Loss: 0.004356554243713617, Learning Rate: 0.001486\n",
      "Epoch 992/40000, Loss: 0.0023556083906441927, Learning Rate: 0.001485\n",
      "Epoch 993/40000, Loss: 0.004360880237072706, Learning Rate: 0.001485\n",
      "Epoch 994/40000, Loss: 0.0030276766046881676, Learning Rate: 0.001484\n",
      "Epoch 995/40000, Loss: 0.002169289393350482, Learning Rate: 0.001484\n",
      "Epoch 996/40000, Loss: 0.0022866087965667248, Learning Rate: 0.001483\n",
      "Epoch 997/40000, Loss: 0.004408927168697119, Learning Rate: 0.001483\n",
      "Epoch 998/40000, Loss: 0.00399648305028677, Learning Rate: 0.001483\n",
      "Epoch 999/40000, Loss: 0.004469005856662989, Learning Rate: 0.001482\n",
      "Epoch 1000/40000, Loss: 0.004571674857288599, Learning Rate: 0.001482\n",
      "Epoch 1001/40000, Loss: 0.004652475006878376, Learning Rate: 0.001481\n",
      "Epoch 1002/40000, Loss: 0.004751294385641813, Learning Rate: 0.001481\n",
      "Epoch 1003/40000, Loss: 0.002282551722601056, Learning Rate: 0.001480\n",
      "Epoch 1004/40000, Loss: 0.004217314533889294, Learning Rate: 0.001480\n",
      "Epoch 1005/40000, Loss: 0.004744292702525854, Learning Rate: 0.001479\n",
      "Epoch 1006/40000, Loss: 0.004389433655887842, Learning Rate: 0.001479\n",
      "Epoch 1007/40000, Loss: 0.002265376038849354, Learning Rate: 0.001479\n",
      "Epoch 1008/40000, Loss: 0.0043565137311816216, Learning Rate: 0.001478\n",
      "Epoch 1009/40000, Loss: 0.004407876171171665, Learning Rate: 0.001478\n",
      "Epoch 1010/40000, Loss: 0.0022758375853300095, Learning Rate: 0.001477\n",
      "Epoch 1011/40000, Loss: 0.0029258199501782656, Learning Rate: 0.001477\n",
      "Epoch 1012/40000, Loss: 0.0029754710849374533, Learning Rate: 0.001476\n",
      "Epoch 1013/40000, Loss: 0.0040878052823245525, Learning Rate: 0.001476\n",
      "Epoch 1014/40000, Loss: 0.004504903685301542, Learning Rate: 0.001475\n",
      "Epoch 1015/40000, Loss: 0.0022572947200387716, Learning Rate: 0.001475\n",
      "Epoch 1016/40000, Loss: 0.002274532802402973, Learning Rate: 0.001475\n",
      "Epoch 1017/40000, Loss: 0.004556157160550356, Learning Rate: 0.001474\n",
      "Epoch 1018/40000, Loss: 0.004439190961420536, Learning Rate: 0.001474\n",
      "Epoch 1019/40000, Loss: 0.00218711094930768, Learning Rate: 0.001473\n",
      "Epoch 1020/40000, Loss: 0.002283442532643676, Learning Rate: 0.001473\n",
      "Epoch 1021/40000, Loss: 0.002352252369746566, Learning Rate: 0.001472\n",
      "Epoch 1022/40000, Loss: 0.004426294472068548, Learning Rate: 0.001472\n",
      "Epoch 1023/40000, Loss: 0.0022313501685857773, Learning Rate: 0.001471\n",
      "Epoch 1024/40000, Loss: 0.004348445683717728, Learning Rate: 0.001471\n",
      "Epoch 1025/40000, Loss: 0.004426450934261084, Learning Rate: 0.001471\n",
      "Epoch 1026/40000, Loss: 0.003988263662904501, Learning Rate: 0.001470\n",
      "Epoch 1027/40000, Loss: 0.0023110422771424055, Learning Rate: 0.001470\n",
      "Epoch 1028/40000, Loss: 0.0023320838809013367, Learning Rate: 0.001469\n",
      "Epoch 1029/40000, Loss: 0.0022700827103108168, Learning Rate: 0.001469\n",
      "Epoch 1030/40000, Loss: 0.0023136253003031015, Learning Rate: 0.001468\n",
      "Epoch 1031/40000, Loss: 0.004397749435156584, Learning Rate: 0.001468\n",
      "Epoch 1032/40000, Loss: 0.002225155709311366, Learning Rate: 0.001467\n",
      "Epoch 1033/40000, Loss: 0.0029782576020807028, Learning Rate: 0.001467\n",
      "Epoch 1034/40000, Loss: 0.0023026498965919018, Learning Rate: 0.001467\n",
      "Epoch 1035/40000, Loss: 0.0022082997020334005, Learning Rate: 0.001466\n",
      "Epoch 1036/40000, Loss: 0.0023147149477154016, Learning Rate: 0.001466\n",
      "Epoch 1037/40000, Loss: 0.0023710792884230614, Learning Rate: 0.001465\n",
      "Epoch 1038/40000, Loss: 0.00231571844778955, Learning Rate: 0.001465\n",
      "Epoch 1039/40000, Loss: 0.002227806719020009, Learning Rate: 0.001464\n",
      "Epoch 1040/40000, Loss: 0.004363586660474539, Learning Rate: 0.001464\n",
      "Epoch 1041/40000, Loss: 0.004040286876261234, Learning Rate: 0.001464\n",
      "Epoch 1042/40000, Loss: 0.003086086129769683, Learning Rate: 0.001463\n",
      "Epoch 1043/40000, Loss: 0.004704778548330069, Learning Rate: 0.001463\n",
      "Epoch 1044/40000, Loss: 0.0044236681424081326, Learning Rate: 0.001462\n",
      "Epoch 1045/40000, Loss: 0.0022828257642686367, Learning Rate: 0.001462\n",
      "Epoch 1046/40000, Loss: 0.002185693010687828, Learning Rate: 0.001461\n",
      "Epoch 1047/40000, Loss: 0.0030198348686099052, Learning Rate: 0.001461\n",
      "Epoch 1048/40000, Loss: 0.004308921750634909, Learning Rate: 0.001460\n",
      "Epoch 1049/40000, Loss: 0.004296305123716593, Learning Rate: 0.001460\n",
      "Epoch 1050/40000, Loss: 0.003995475824922323, Learning Rate: 0.001460\n",
      "Epoch 1051/40000, Loss: 0.004310184624046087, Learning Rate: 0.001459\n",
      "Epoch 1052/40000, Loss: 0.0043940674513578415, Learning Rate: 0.001459\n",
      "Epoch 1053/40000, Loss: 0.002387589542195201, Learning Rate: 0.001458\n",
      "Epoch 1054/40000, Loss: 0.004155277274549007, Learning Rate: 0.001458\n",
      "Epoch 1055/40000, Loss: 0.00447388831526041, Learning Rate: 0.001457\n",
      "Epoch 1056/40000, Loss: 0.003969426266849041, Learning Rate: 0.001457\n",
      "Epoch 1057/40000, Loss: 0.0022411192767322063, Learning Rate: 0.001457\n",
      "Epoch 1058/40000, Loss: 0.004307204391807318, Learning Rate: 0.001456\n",
      "Epoch 1059/40000, Loss: 0.002195524051785469, Learning Rate: 0.001456\n",
      "Epoch 1060/40000, Loss: 0.004303308669477701, Learning Rate: 0.001455\n",
      "Epoch 1061/40000, Loss: 0.004489151760935783, Learning Rate: 0.001455\n",
      "Epoch 1062/40000, Loss: 0.00446282047778368, Learning Rate: 0.001454\n",
      "Epoch 1063/40000, Loss: 0.0021862320136278868, Learning Rate: 0.001454\n",
      "Epoch 1064/40000, Loss: 0.0030565971974283457, Learning Rate: 0.001453\n",
      "Epoch 1065/40000, Loss: 0.004380656406283379, Learning Rate: 0.001453\n",
      "Epoch 1066/40000, Loss: 0.002272613113746047, Learning Rate: 0.001453\n",
      "Epoch 1067/40000, Loss: 0.0043534524738788605, Learning Rate: 0.001452\n",
      "Epoch 1068/40000, Loss: 0.0022569403517991304, Learning Rate: 0.001452\n",
      "Epoch 1069/40000, Loss: 0.003079161047935486, Learning Rate: 0.001451\n",
      "Epoch 1070/40000, Loss: 0.0024077557027339935, Learning Rate: 0.001451\n",
      "Epoch 1071/40000, Loss: 0.002424949314445257, Learning Rate: 0.001450\n",
      "Epoch 1072/40000, Loss: 0.0040896497666835785, Learning Rate: 0.001450\n",
      "Epoch 1073/40000, Loss: 0.0029583307914435863, Learning Rate: 0.001450\n",
      "Epoch 1074/40000, Loss: 0.004339916165918112, Learning Rate: 0.001449\n",
      "Epoch 1075/40000, Loss: 0.004514667671173811, Learning Rate: 0.001449\n",
      "Epoch 1076/40000, Loss: 0.0047380272299051285, Learning Rate: 0.001448\n",
      "Epoch 1077/40000, Loss: 0.0031115589663386345, Learning Rate: 0.001448\n",
      "Epoch 1078/40000, Loss: 0.0022534294985234737, Learning Rate: 0.001447\n",
      "Epoch 1079/40000, Loss: 0.004375321324914694, Learning Rate: 0.001447\n",
      "Epoch 1080/40000, Loss: 0.004354291595518589, Learning Rate: 0.001446\n",
      "Epoch 1081/40000, Loss: 0.002243924420326948, Learning Rate: 0.001446\n",
      "Epoch 1082/40000, Loss: 0.004297284875065088, Learning Rate: 0.001446\n",
      "Epoch 1083/40000, Loss: 0.004003509413450956, Learning Rate: 0.001445\n",
      "Epoch 1084/40000, Loss: 0.004496641457080841, Learning Rate: 0.001445\n",
      "Epoch 1085/40000, Loss: 0.004458025097846985, Learning Rate: 0.001444\n",
      "Epoch 1086/40000, Loss: 0.00396746676415205, Learning Rate: 0.001444\n",
      "Epoch 1087/40000, Loss: 0.004373053088784218, Learning Rate: 0.001443\n",
      "Epoch 1088/40000, Loss: 0.003020200878381729, Learning Rate: 0.001443\n",
      "Epoch 1089/40000, Loss: 0.004294734448194504, Learning Rate: 0.001443\n",
      "Epoch 1090/40000, Loss: 0.004318606574088335, Learning Rate: 0.001442\n",
      "Epoch 1091/40000, Loss: 0.002226833486929536, Learning Rate: 0.001442\n",
      "Epoch 1092/40000, Loss: 0.00290164933539927, Learning Rate: 0.001441\n",
      "Epoch 1093/40000, Loss: 0.0029092615004628897, Learning Rate: 0.001441\n",
      "Epoch 1094/40000, Loss: 0.0022730561904609203, Learning Rate: 0.001440\n",
      "Epoch 1095/40000, Loss: 0.004128637257963419, Learning Rate: 0.001440\n",
      "Epoch 1096/40000, Loss: 0.002918232697993517, Learning Rate: 0.001440\n",
      "Epoch 1097/40000, Loss: 0.0029744324274361134, Learning Rate: 0.001439\n",
      "Epoch 1098/40000, Loss: 0.004436267074197531, Learning Rate: 0.001439\n",
      "Epoch 1099/40000, Loss: 0.0031405645422637463, Learning Rate: 0.001438\n",
      "Epoch 1100/40000, Loss: 0.004021650645881891, Learning Rate: 0.001438\n",
      "Epoch 1101/40000, Loss: 0.004372576251626015, Learning Rate: 0.001437\n",
      "Epoch 1102/40000, Loss: 0.00239657168276608, Learning Rate: 0.001437\n",
      "Epoch 1103/40000, Loss: 0.004347540903836489, Learning Rate: 0.001437\n",
      "Epoch 1104/40000, Loss: 0.0023387817200273275, Learning Rate: 0.001436\n",
      "Epoch 1105/40000, Loss: 0.0023443724494427443, Learning Rate: 0.001436\n",
      "Epoch 1106/40000, Loss: 0.002332547213882208, Learning Rate: 0.001435\n",
      "Epoch 1107/40000, Loss: 0.004128552041947842, Learning Rate: 0.001435\n",
      "Epoch 1108/40000, Loss: 0.004256442189216614, Learning Rate: 0.001434\n",
      "Epoch 1109/40000, Loss: 0.004097774624824524, Learning Rate: 0.001434\n",
      "Epoch 1110/40000, Loss: 0.004015183076262474, Learning Rate: 0.001434\n",
      "Epoch 1111/40000, Loss: 0.004085506312549114, Learning Rate: 0.001433\n",
      "Epoch 1112/40000, Loss: 0.0024041440337896347, Learning Rate: 0.001433\n",
      "Epoch 1113/40000, Loss: 0.0029460617806762457, Learning Rate: 0.001432\n",
      "Epoch 1114/40000, Loss: 0.002953951247036457, Learning Rate: 0.001432\n",
      "Epoch 1115/40000, Loss: 0.0022013315465301275, Learning Rate: 0.001431\n",
      "Epoch 1116/40000, Loss: 0.0023819459602236748, Learning Rate: 0.001431\n",
      "Epoch 1117/40000, Loss: 0.004034559242427349, Learning Rate: 0.001431\n",
      "Epoch 1118/40000, Loss: 0.00438861595466733, Learning Rate: 0.001430\n",
      "Epoch 1119/40000, Loss: 0.0045807138085365295, Learning Rate: 0.001430\n",
      "Epoch 1120/40000, Loss: 0.0022314845118671656, Learning Rate: 0.001429\n",
      "Epoch 1121/40000, Loss: 0.0022741304710507393, Learning Rate: 0.001429\n",
      "Epoch 1122/40000, Loss: 0.004316392354667187, Learning Rate: 0.001428\n",
      "Epoch 1123/40000, Loss: 0.004453947767615318, Learning Rate: 0.001428\n",
      "Epoch 1124/40000, Loss: 0.0039518000558018684, Learning Rate: 0.001428\n",
      "Epoch 1125/40000, Loss: 0.002223790157586336, Learning Rate: 0.001427\n",
      "Epoch 1126/40000, Loss: 0.0029805207159370184, Learning Rate: 0.001427\n",
      "Epoch 1127/40000, Loss: 0.002242190996184945, Learning Rate: 0.001426\n",
      "Epoch 1128/40000, Loss: 0.004381396807730198, Learning Rate: 0.001426\n",
      "Epoch 1129/40000, Loss: 0.002244090661406517, Learning Rate: 0.001425\n",
      "Epoch 1130/40000, Loss: 0.002180501352995634, Learning Rate: 0.001425\n",
      "Epoch 1131/40000, Loss: 0.0022882744669914246, Learning Rate: 0.001425\n",
      "Epoch 1132/40000, Loss: 0.004334979224950075, Learning Rate: 0.001424\n",
      "Epoch 1133/40000, Loss: 0.004395610187202692, Learning Rate: 0.001424\n",
      "Epoch 1134/40000, Loss: 0.004463880322873592, Learning Rate: 0.001423\n",
      "Epoch 1135/40000, Loss: 0.003085620701313019, Learning Rate: 0.001423\n",
      "Epoch 1136/40000, Loss: 0.0023198199924081564, Learning Rate: 0.001422\n",
      "Epoch 1137/40000, Loss: 0.004546232987195253, Learning Rate: 0.001422\n",
      "Epoch 1138/40000, Loss: 0.004086759872734547, Learning Rate: 0.001422\n",
      "Epoch 1139/40000, Loss: 0.0021978551521897316, Learning Rate: 0.001421\n",
      "Epoch 1140/40000, Loss: 0.002210833365097642, Learning Rate: 0.001421\n",
      "Epoch 1141/40000, Loss: 0.003940619062632322, Learning Rate: 0.001420\n",
      "Epoch 1142/40000, Loss: 0.003949210979044437, Learning Rate: 0.001420\n",
      "Epoch 1143/40000, Loss: 0.003026104299351573, Learning Rate: 0.001419\n",
      "Epoch 1144/40000, Loss: 0.004290771670639515, Learning Rate: 0.001419\n",
      "Epoch 1145/40000, Loss: 0.004365083761513233, Learning Rate: 0.001419\n",
      "Epoch 1146/40000, Loss: 0.0048719956539571285, Learning Rate: 0.001418\n",
      "Epoch 1147/40000, Loss: 0.003204699605703354, Learning Rate: 0.001418\n",
      "Epoch 1148/40000, Loss: 0.004467841237783432, Learning Rate: 0.001417\n",
      "Epoch 1149/40000, Loss: 0.004580543842166662, Learning Rate: 0.001417\n",
      "Epoch 1150/40000, Loss: 0.0042787231504917145, Learning Rate: 0.001416\n",
      "Epoch 1151/40000, Loss: 0.004028181079775095, Learning Rate: 0.001416\n",
      "Epoch 1152/40000, Loss: 0.003066755598410964, Learning Rate: 0.001416\n",
      "Epoch 1153/40000, Loss: 0.004029273986816406, Learning Rate: 0.001415\n",
      "Epoch 1154/40000, Loss: 0.004299027845263481, Learning Rate: 0.001415\n",
      "Epoch 1155/40000, Loss: 0.0042429594323039055, Learning Rate: 0.001414\n",
      "Epoch 1156/40000, Loss: 0.002288726856932044, Learning Rate: 0.001414\n",
      "Epoch 1157/40000, Loss: 0.002266607014462352, Learning Rate: 0.001413\n",
      "Epoch 1158/40000, Loss: 0.004338104743510485, Learning Rate: 0.001413\n",
      "Epoch 1159/40000, Loss: 0.002174744615331292, Learning Rate: 0.001413\n",
      "Epoch 1160/40000, Loss: 0.00421531917527318, Learning Rate: 0.001412\n",
      "Epoch 1161/40000, Loss: 0.004378605633974075, Learning Rate: 0.001412\n",
      "Epoch 1162/40000, Loss: 0.0022673795465379953, Learning Rate: 0.001411\n",
      "Epoch 1163/40000, Loss: 0.00429164944216609, Learning Rate: 0.001411\n",
      "Epoch 1164/40000, Loss: 0.004307180177420378, Learning Rate: 0.001410\n",
      "Epoch 1165/40000, Loss: 0.003900629235431552, Learning Rate: 0.001410\n",
      "Epoch 1166/40000, Loss: 0.002171084051951766, Learning Rate: 0.001410\n",
      "Epoch 1167/40000, Loss: 0.004297269508242607, Learning Rate: 0.001409\n",
      "Epoch 1168/40000, Loss: 0.004241893067955971, Learning Rate: 0.001409\n",
      "Epoch 1169/40000, Loss: 0.004319520201534033, Learning Rate: 0.001408\n",
      "Epoch 1170/40000, Loss: 0.004263601265847683, Learning Rate: 0.001408\n",
      "Epoch 1171/40000, Loss: 0.002130627166479826, Learning Rate: 0.001408\n",
      "Epoch 1172/40000, Loss: 0.002169607440009713, Learning Rate: 0.001407\n",
      "Epoch 1173/40000, Loss: 0.004541031084954739, Learning Rate: 0.001407\n",
      "Epoch 1174/40000, Loss: 0.005137283820658922, Learning Rate: 0.001406\n",
      "Epoch 1175/40000, Loss: 0.005022214259952307, Learning Rate: 0.001406\n",
      "Epoch 1176/40000, Loss: 0.0040582530200481415, Learning Rate: 0.001405\n",
      "Epoch 1177/40000, Loss: 0.00400985311716795, Learning Rate: 0.001405\n",
      "Epoch 1178/40000, Loss: 0.0031110979616642, Learning Rate: 0.001405\n",
      "Epoch 1179/40000, Loss: 0.0021733446046710014, Learning Rate: 0.001404\n",
      "Epoch 1180/40000, Loss: 0.0022395148407667875, Learning Rate: 0.001404\n",
      "Epoch 1181/40000, Loss: 0.00438126502558589, Learning Rate: 0.001403\n",
      "Epoch 1182/40000, Loss: 0.002216847613453865, Learning Rate: 0.001403\n",
      "Epoch 1183/40000, Loss: 0.004389107692986727, Learning Rate: 0.001402\n",
      "Epoch 1184/40000, Loss: 0.0023172928486019373, Learning Rate: 0.001402\n",
      "Epoch 1185/40000, Loss: 0.002287253737449646, Learning Rate: 0.001402\n",
      "Epoch 1186/40000, Loss: 0.002992779016494751, Learning Rate: 0.001401\n",
      "Epoch 1187/40000, Loss: 0.004003043752163649, Learning Rate: 0.001401\n",
      "Epoch 1188/40000, Loss: 0.004104300402104855, Learning Rate: 0.001400\n",
      "Epoch 1189/40000, Loss: 0.002916672732681036, Learning Rate: 0.001400\n",
      "Epoch 1190/40000, Loss: 0.0022284318692982197, Learning Rate: 0.001400\n",
      "Epoch 1191/40000, Loss: 0.002303606830537319, Learning Rate: 0.001399\n",
      "Epoch 1192/40000, Loss: 0.004397377837449312, Learning Rate: 0.001399\n",
      "Epoch 1193/40000, Loss: 0.0021803383715450764, Learning Rate: 0.001398\n",
      "Epoch 1194/40000, Loss: 0.002954981755465269, Learning Rate: 0.001398\n",
      "Epoch 1195/40000, Loss: 0.004482175689190626, Learning Rate: 0.001397\n",
      "Epoch 1196/40000, Loss: 0.002961362712085247, Learning Rate: 0.001397\n",
      "Epoch 1197/40000, Loss: 0.0022357888519763947, Learning Rate: 0.001397\n",
      "Epoch 1198/40000, Loss: 0.002203495241701603, Learning Rate: 0.001396\n",
      "Epoch 1199/40000, Loss: 0.00216799252666533, Learning Rate: 0.001396\n",
      "Epoch 1200/40000, Loss: 0.0029114382341504097, Learning Rate: 0.001395\n",
      "Epoch 1201/40000, Loss: 0.0039939843118190765, Learning Rate: 0.001395\n",
      "Epoch 1202/40000, Loss: 0.004237520042806864, Learning Rate: 0.001395\n",
      "Epoch 1203/40000, Loss: 0.004056893289089203, Learning Rate: 0.001394\n",
      "Epoch 1204/40000, Loss: 0.004603960085660219, Learning Rate: 0.001394\n",
      "Epoch 1205/40000, Loss: 0.004205565899610519, Learning Rate: 0.001393\n",
      "Epoch 1206/40000, Loss: 0.0025401548482477665, Learning Rate: 0.001393\n",
      "Epoch 1207/40000, Loss: 0.0043701897375285625, Learning Rate: 0.001392\n",
      "Epoch 1208/40000, Loss: 0.0043387627229094505, Learning Rate: 0.001392\n",
      "Epoch 1209/40000, Loss: 0.004254851955920458, Learning Rate: 0.001392\n",
      "Epoch 1210/40000, Loss: 0.002921252977102995, Learning Rate: 0.001391\n",
      "Epoch 1211/40000, Loss: 0.0031006706412881613, Learning Rate: 0.001391\n",
      "Epoch 1212/40000, Loss: 0.0021777423098683357, Learning Rate: 0.001390\n",
      "Epoch 1213/40000, Loss: 0.002220932859927416, Learning Rate: 0.001390\n",
      "Epoch 1214/40000, Loss: 0.004432499408721924, Learning Rate: 0.001389\n",
      "Epoch 1215/40000, Loss: 0.004302230663597584, Learning Rate: 0.001389\n",
      "Epoch 1216/40000, Loss: 0.004301322158426046, Learning Rate: 0.001389\n",
      "Epoch 1217/40000, Loss: 0.004035629332065582, Learning Rate: 0.001388\n",
      "Epoch 1218/40000, Loss: 0.004590910859405994, Learning Rate: 0.001388\n",
      "Epoch 1219/40000, Loss: 0.00399692403152585, Learning Rate: 0.001387\n",
      "Epoch 1220/40000, Loss: 0.002268932294100523, Learning Rate: 0.001387\n",
      "Epoch 1221/40000, Loss: 0.004256997257471085, Learning Rate: 0.001387\n",
      "Epoch 1222/40000, Loss: 0.0028516934253275394, Learning Rate: 0.001386\n",
      "Epoch 1223/40000, Loss: 0.003918930422514677, Learning Rate: 0.001386\n",
      "Epoch 1224/40000, Loss: 0.004182939883321524, Learning Rate: 0.001385\n",
      "Epoch 1225/40000, Loss: 0.002177836373448372, Learning Rate: 0.001385\n",
      "Epoch 1226/40000, Loss: 0.0029319273307919502, Learning Rate: 0.001384\n",
      "Epoch 1227/40000, Loss: 0.004342417232692242, Learning Rate: 0.001384\n",
      "Epoch 1228/40000, Loss: 0.002218875801190734, Learning Rate: 0.001384\n",
      "Epoch 1229/40000, Loss: 0.0042724707163870335, Learning Rate: 0.001383\n",
      "Epoch 1230/40000, Loss: 0.002416503382846713, Learning Rate: 0.001383\n",
      "Epoch 1231/40000, Loss: 0.00452415831387043, Learning Rate: 0.001382\n",
      "Epoch 1232/40000, Loss: 0.002628799993544817, Learning Rate: 0.001382\n",
      "Epoch 1233/40000, Loss: 0.004325705114752054, Learning Rate: 0.001382\n",
      "Epoch 1234/40000, Loss: 0.0045156623236835, Learning Rate: 0.001381\n",
      "Epoch 1235/40000, Loss: 0.003191551426425576, Learning Rate: 0.001381\n",
      "Epoch 1236/40000, Loss: 0.004292461555451155, Learning Rate: 0.001380\n",
      "Epoch 1237/40000, Loss: 0.0023595071397721767, Learning Rate: 0.001380\n",
      "Epoch 1238/40000, Loss: 0.004587417468428612, Learning Rate: 0.001380\n",
      "Epoch 1239/40000, Loss: 0.002229609526693821, Learning Rate: 0.001379\n",
      "Epoch 1240/40000, Loss: 0.002238598885014653, Learning Rate: 0.001379\n",
      "Epoch 1241/40000, Loss: 0.002253384795039892, Learning Rate: 0.001378\n",
      "Epoch 1242/40000, Loss: 0.0024165757931768894, Learning Rate: 0.001378\n",
      "Epoch 1243/40000, Loss: 0.0042912643402814865, Learning Rate: 0.001377\n",
      "Epoch 1244/40000, Loss: 0.004407786764204502, Learning Rate: 0.001377\n",
      "Epoch 1245/40000, Loss: 0.002304152585566044, Learning Rate: 0.001377\n",
      "Epoch 1246/40000, Loss: 0.004210724029690027, Learning Rate: 0.001376\n",
      "Epoch 1247/40000, Loss: 0.004263410810381174, Learning Rate: 0.001376\n",
      "Epoch 1248/40000, Loss: 0.0021461951546370983, Learning Rate: 0.001375\n",
      "Epoch 1249/40000, Loss: 0.004193559288978577, Learning Rate: 0.001375\n",
      "Epoch 1250/40000, Loss: 0.002180097158998251, Learning Rate: 0.001375\n",
      "Epoch 1251/40000, Loss: 0.002816859632730484, Learning Rate: 0.001374\n",
      "Epoch 1252/40000, Loss: 0.003873641137033701, Learning Rate: 0.001374\n",
      "Epoch 1253/40000, Loss: 0.0038931784220039845, Learning Rate: 0.001373\n",
      "Epoch 1254/40000, Loss: 0.00389246572740376, Learning Rate: 0.001373\n",
      "Epoch 1255/40000, Loss: 0.002113320864737034, Learning Rate: 0.001373\n",
      "Epoch 1256/40000, Loss: 0.004238379653543234, Learning Rate: 0.001372\n",
      "Epoch 1257/40000, Loss: 0.002168585080653429, Learning Rate: 0.001372\n",
      "Epoch 1258/40000, Loss: 0.0028365093749016523, Learning Rate: 0.001371\n",
      "Epoch 1259/40000, Loss: 0.0042654057033360004, Learning Rate: 0.001371\n",
      "Epoch 1260/40000, Loss: 0.002087457338348031, Learning Rate: 0.001370\n",
      "Epoch 1261/40000, Loss: 0.002167327329516411, Learning Rate: 0.001370\n",
      "Epoch 1262/40000, Loss: 0.002303679008036852, Learning Rate: 0.001370\n",
      "Epoch 1263/40000, Loss: 0.0021726496051996946, Learning Rate: 0.001369\n",
      "Epoch 1264/40000, Loss: 0.002888537710532546, Learning Rate: 0.001369\n",
      "Epoch 1265/40000, Loss: 0.004246379714459181, Learning Rate: 0.001368\n",
      "Epoch 1266/40000, Loss: 0.0038657360710203648, Learning Rate: 0.001368\n",
      "Epoch 1267/40000, Loss: 0.004229739774018526, Learning Rate: 0.001368\n",
      "Epoch 1268/40000, Loss: 0.003865106264129281, Learning Rate: 0.001367\n",
      "Epoch 1269/40000, Loss: 0.0022051832638680935, Learning Rate: 0.001367\n",
      "Epoch 1270/40000, Loss: 0.0021492852829396725, Learning Rate: 0.001366\n",
      "Epoch 1271/40000, Loss: 0.002168336184695363, Learning Rate: 0.001366\n",
      "Epoch 1272/40000, Loss: 0.0030646626837551594, Learning Rate: 0.001366\n",
      "Epoch 1273/40000, Loss: 0.004379387479275465, Learning Rate: 0.001365\n",
      "Epoch 1274/40000, Loss: 0.00226297858171165, Learning Rate: 0.001365\n",
      "Epoch 1275/40000, Loss: 0.0021820380352437496, Learning Rate: 0.001364\n",
      "Epoch 1276/40000, Loss: 0.004209708422422409, Learning Rate: 0.001364\n",
      "Epoch 1277/40000, Loss: 0.0042806160636246204, Learning Rate: 0.001363\n",
      "Epoch 1278/40000, Loss: 0.002184223150834441, Learning Rate: 0.001363\n",
      "Epoch 1279/40000, Loss: 0.0020735410507768393, Learning Rate: 0.001363\n",
      "Epoch 1280/40000, Loss: 0.002304724184796214, Learning Rate: 0.001362\n",
      "Epoch 1281/40000, Loss: 0.004442255478352308, Learning Rate: 0.001362\n",
      "Epoch 1282/40000, Loss: 0.0039061952847987413, Learning Rate: 0.001361\n",
      "Epoch 1283/40000, Loss: 0.004408067092299461, Learning Rate: 0.001361\n",
      "Epoch 1284/40000, Loss: 0.002296792808920145, Learning Rate: 0.001361\n",
      "Epoch 1285/40000, Loss: 0.002265069168061018, Learning Rate: 0.001360\n",
      "Epoch 1286/40000, Loss: 0.004351536277681589, Learning Rate: 0.001360\n",
      "Epoch 1287/40000, Loss: 0.0029344060458242893, Learning Rate: 0.001359\n",
      "Epoch 1288/40000, Loss: 0.0030862048733979464, Learning Rate: 0.001359\n",
      "Epoch 1289/40000, Loss: 0.00442079221829772, Learning Rate: 0.001359\n",
      "Epoch 1290/40000, Loss: 0.0021897652186453342, Learning Rate: 0.001358\n",
      "Epoch 1291/40000, Loss: 0.0021679014898836613, Learning Rate: 0.001358\n",
      "Epoch 1292/40000, Loss: 0.002226968063041568, Learning Rate: 0.001357\n",
      "Epoch 1293/40000, Loss: 0.002173586981371045, Learning Rate: 0.001357\n",
      "Epoch 1294/40000, Loss: 0.002246886258944869, Learning Rate: 0.001357\n",
      "Epoch 1295/40000, Loss: 0.004376912489533424, Learning Rate: 0.001356\n",
      "Epoch 1296/40000, Loss: 0.0031165198888629675, Learning Rate: 0.001356\n",
      "Epoch 1297/40000, Loss: 0.0042212000116705894, Learning Rate: 0.001355\n",
      "Epoch 1298/40000, Loss: 0.002227365504950285, Learning Rate: 0.001355\n",
      "Epoch 1299/40000, Loss: 0.004365603439509869, Learning Rate: 0.001355\n",
      "Epoch 1300/40000, Loss: 0.004444888327270746, Learning Rate: 0.001354\n",
      "Epoch 1301/40000, Loss: 0.004208737052977085, Learning Rate: 0.001354\n",
      "Epoch 1302/40000, Loss: 0.0028984935488551855, Learning Rate: 0.001353\n",
      "Epoch 1303/40000, Loss: 0.0030319395009428263, Learning Rate: 0.001353\n",
      "Epoch 1304/40000, Loss: 0.0023405253887176514, Learning Rate: 0.001352\n",
      "Epoch 1305/40000, Loss: 0.004300449043512344, Learning Rate: 0.001352\n",
      "Epoch 1306/40000, Loss: 0.004550053272396326, Learning Rate: 0.001352\n",
      "Epoch 1307/40000, Loss: 0.002325122943148017, Learning Rate: 0.001351\n",
      "Epoch 1308/40000, Loss: 0.004187441896647215, Learning Rate: 0.001351\n",
      "Epoch 1309/40000, Loss: 0.004263825248926878, Learning Rate: 0.001350\n",
      "Epoch 1310/40000, Loss: 0.004667436704039574, Learning Rate: 0.001350\n",
      "Epoch 1311/40000, Loss: 0.0023471335880458355, Learning Rate: 0.001350\n",
      "Epoch 1312/40000, Loss: 0.004368082154542208, Learning Rate: 0.001349\n",
      "Epoch 1313/40000, Loss: 0.002365185646340251, Learning Rate: 0.001349\n",
      "Epoch 1314/40000, Loss: 0.0022617848590016365, Learning Rate: 0.001348\n",
      "Epoch 1315/40000, Loss: 0.0024191574193537235, Learning Rate: 0.001348\n",
      "Epoch 1316/40000, Loss: 0.004366501234471798, Learning Rate: 0.001348\n",
      "Epoch 1317/40000, Loss: 0.00474222656339407, Learning Rate: 0.001347\n",
      "Epoch 1318/40000, Loss: 0.0024844955187290907, Learning Rate: 0.001347\n",
      "Epoch 1319/40000, Loss: 0.0042661456391215324, Learning Rate: 0.001346\n",
      "Epoch 1320/40000, Loss: 0.0029550937470048666, Learning Rate: 0.001346\n",
      "Epoch 1321/40000, Loss: 0.0041666519828140736, Learning Rate: 0.001346\n",
      "Epoch 1322/40000, Loss: 0.0021756712812930346, Learning Rate: 0.001345\n",
      "Epoch 1323/40000, Loss: 0.004196556750684977, Learning Rate: 0.001345\n",
      "Epoch 1324/40000, Loss: 0.003859751857817173, Learning Rate: 0.001344\n",
      "Epoch 1325/40000, Loss: 0.004293985664844513, Learning Rate: 0.001344\n",
      "Epoch 1326/40000, Loss: 0.003935037646442652, Learning Rate: 0.001344\n",
      "Epoch 1327/40000, Loss: 0.004011834505945444, Learning Rate: 0.001343\n",
      "Epoch 1328/40000, Loss: 0.00441545806825161, Learning Rate: 0.001343\n",
      "Epoch 1329/40000, Loss: 0.004190671723335981, Learning Rate: 0.001342\n",
      "Epoch 1330/40000, Loss: 0.0028184745460748672, Learning Rate: 0.001342\n",
      "Epoch 1331/40000, Loss: 0.004259987268596888, Learning Rate: 0.001342\n",
      "Epoch 1332/40000, Loss: 0.0021707757841795683, Learning Rate: 0.001341\n",
      "Epoch 1333/40000, Loss: 0.00418984005227685, Learning Rate: 0.001341\n",
      "Epoch 1334/40000, Loss: 0.0038819911424070597, Learning Rate: 0.001340\n",
      "Epoch 1335/40000, Loss: 0.00392255699262023, Learning Rate: 0.001340\n",
      "Epoch 1336/40000, Loss: 0.0029894053004682064, Learning Rate: 0.001340\n",
      "Epoch 1337/40000, Loss: 0.003925323486328125, Learning Rate: 0.001339\n",
      "Epoch 1338/40000, Loss: 0.0028374618850648403, Learning Rate: 0.001339\n",
      "Epoch 1339/40000, Loss: 0.003851791378110647, Learning Rate: 0.001338\n",
      "Epoch 1340/40000, Loss: 0.0038297760765999556, Learning Rate: 0.001338\n",
      "Epoch 1341/40000, Loss: 0.004292016848921776, Learning Rate: 0.001338\n",
      "Epoch 1342/40000, Loss: 0.004419503267854452, Learning Rate: 0.001337\n",
      "Epoch 1343/40000, Loss: 0.003838449018076062, Learning Rate: 0.001337\n",
      "Epoch 1344/40000, Loss: 0.002187915612012148, Learning Rate: 0.001336\n",
      "Epoch 1345/40000, Loss: 0.002825844334438443, Learning Rate: 0.001336\n",
      "Epoch 1346/40000, Loss: 0.002108238637447357, Learning Rate: 0.001336\n",
      "Epoch 1347/40000, Loss: 0.0041566151194274426, Learning Rate: 0.001335\n",
      "Epoch 1348/40000, Loss: 0.004213831387460232, Learning Rate: 0.001335\n",
      "Epoch 1349/40000, Loss: 0.002389068715274334, Learning Rate: 0.001334\n",
      "Epoch 1350/40000, Loss: 0.0021666160319000483, Learning Rate: 0.001334\n",
      "Epoch 1351/40000, Loss: 0.0023435871116816998, Learning Rate: 0.001334\n",
      "Epoch 1352/40000, Loss: 0.0028221909888088703, Learning Rate: 0.001333\n",
      "Epoch 1353/40000, Loss: 0.0023849476128816605, Learning Rate: 0.001333\n",
      "Epoch 1354/40000, Loss: 0.002123940270394087, Learning Rate: 0.001332\n",
      "Epoch 1355/40000, Loss: 0.004380849655717611, Learning Rate: 0.001332\n",
      "Epoch 1356/40000, Loss: 0.00426107132807374, Learning Rate: 0.001332\n",
      "Epoch 1357/40000, Loss: 0.002956579439342022, Learning Rate: 0.001331\n",
      "Epoch 1358/40000, Loss: 0.004194237757474184, Learning Rate: 0.001331\n",
      "Epoch 1359/40000, Loss: 0.004351586569100618, Learning Rate: 0.001330\n",
      "Epoch 1360/40000, Loss: 0.004223167430609465, Learning Rate: 0.001330\n",
      "Epoch 1361/40000, Loss: 0.003908262122422457, Learning Rate: 0.001330\n",
      "Epoch 1362/40000, Loss: 0.003974004648625851, Learning Rate: 0.001329\n",
      "Epoch 1363/40000, Loss: 0.004274954088032246, Learning Rate: 0.001329\n",
      "Epoch 1364/40000, Loss: 0.004108279012143612, Learning Rate: 0.001328\n",
      "Epoch 1365/40000, Loss: 0.0038414746522903442, Learning Rate: 0.001328\n",
      "Epoch 1366/40000, Loss: 0.003932060673832893, Learning Rate: 0.001328\n",
      "Epoch 1367/40000, Loss: 0.004373587667942047, Learning Rate: 0.001327\n",
      "Epoch 1368/40000, Loss: 0.0028841793537139893, Learning Rate: 0.001327\n",
      "Epoch 1369/40000, Loss: 0.004247140605002642, Learning Rate: 0.001326\n",
      "Epoch 1370/40000, Loss: 0.002855223836377263, Learning Rate: 0.001326\n",
      "Epoch 1371/40000, Loss: 0.0020664206240326166, Learning Rate: 0.001326\n",
      "Epoch 1372/40000, Loss: 0.004177364986389875, Learning Rate: 0.001325\n",
      "Epoch 1373/40000, Loss: 0.0021298539359122515, Learning Rate: 0.001325\n",
      "Epoch 1374/40000, Loss: 0.004127014894038439, Learning Rate: 0.001324\n",
      "Epoch 1375/40000, Loss: 0.0020759464241564274, Learning Rate: 0.001324\n",
      "Epoch 1376/40000, Loss: 0.004289749078452587, Learning Rate: 0.001324\n",
      "Epoch 1377/40000, Loss: 0.004701673984527588, Learning Rate: 0.001323\n",
      "Epoch 1378/40000, Loss: 0.004553072154521942, Learning Rate: 0.001323\n",
      "Epoch 1379/40000, Loss: 0.004506368655711412, Learning Rate: 0.001322\n",
      "Epoch 1380/40000, Loss: 0.004242639988660812, Learning Rate: 0.001322\n",
      "Epoch 1381/40000, Loss: 0.002417290350422263, Learning Rate: 0.001322\n",
      "Epoch 1382/40000, Loss: 0.0040354919619858265, Learning Rate: 0.001321\n",
      "Epoch 1383/40000, Loss: 0.004119825083762407, Learning Rate: 0.001321\n",
      "Epoch 1384/40000, Loss: 0.0028603775426745415, Learning Rate: 0.001320\n",
      "Epoch 1385/40000, Loss: 0.004266559612005949, Learning Rate: 0.001320\n",
      "Epoch 1386/40000, Loss: 0.0031374539248645306, Learning Rate: 0.001320\n",
      "Epoch 1387/40000, Loss: 0.0039943731389939785, Learning Rate: 0.001319\n",
      "Epoch 1388/40000, Loss: 0.004030010662972927, Learning Rate: 0.001319\n",
      "Epoch 1389/40000, Loss: 0.0026243815664201975, Learning Rate: 0.001318\n",
      "Epoch 1390/40000, Loss: 0.004660260397940874, Learning Rate: 0.001318\n",
      "Epoch 1391/40000, Loss: 0.003669531550258398, Learning Rate: 0.001318\n",
      "Epoch 1392/40000, Loss: 0.004376756027340889, Learning Rate: 0.001317\n",
      "Epoch 1393/40000, Loss: 0.004716571420431137, Learning Rate: 0.001317\n",
      "Epoch 1394/40000, Loss: 0.002237000735476613, Learning Rate: 0.001316\n",
      "Epoch 1395/40000, Loss: 0.004259625915437937, Learning Rate: 0.001316\n",
      "Epoch 1396/40000, Loss: 0.00457544531673193, Learning Rate: 0.001316\n",
      "Epoch 1397/40000, Loss: 0.002224813448265195, Learning Rate: 0.001315\n",
      "Epoch 1398/40000, Loss: 0.00419394439086318, Learning Rate: 0.001315\n",
      "Epoch 1399/40000, Loss: 0.002132717752829194, Learning Rate: 0.001314\n",
      "Epoch 1400/40000, Loss: 0.0021193805150687695, Learning Rate: 0.001314\n",
      "Epoch 1401/40000, Loss: 0.0028545253444463015, Learning Rate: 0.001314\n",
      "Epoch 1402/40000, Loss: 0.0021891645155847073, Learning Rate: 0.001313\n",
      "Epoch 1403/40000, Loss: 0.004162342753261328, Learning Rate: 0.001313\n",
      "Epoch 1404/40000, Loss: 0.0022621862590312958, Learning Rate: 0.001313\n",
      "Epoch 1405/40000, Loss: 0.00286211259663105, Learning Rate: 0.001312\n",
      "Epoch 1406/40000, Loss: 0.0028776146937161684, Learning Rate: 0.001312\n",
      "Epoch 1407/40000, Loss: 0.0022197815123945475, Learning Rate: 0.001311\n",
      "Epoch 1408/40000, Loss: 0.002832213882356882, Learning Rate: 0.001311\n",
      "Epoch 1409/40000, Loss: 0.0020692942198365927, Learning Rate: 0.001311\n",
      "Epoch 1410/40000, Loss: 0.0038289923686534166, Learning Rate: 0.001310\n",
      "Epoch 1411/40000, Loss: 0.0022632197942584753, Learning Rate: 0.001310\n",
      "Epoch 1412/40000, Loss: 0.0038286789786070585, Learning Rate: 0.001309\n",
      "Epoch 1413/40000, Loss: 0.0042747268453240395, Learning Rate: 0.001309\n",
      "Epoch 1414/40000, Loss: 0.002306852489709854, Learning Rate: 0.001309\n",
      "Epoch 1415/40000, Loss: 0.004191753454506397, Learning Rate: 0.001308\n",
      "Epoch 1416/40000, Loss: 0.004182959906756878, Learning Rate: 0.001308\n",
      "Epoch 1417/40000, Loss: 0.0020704707130789757, Learning Rate: 0.001307\n",
      "Epoch 1418/40000, Loss: 0.0021935233380645514, Learning Rate: 0.001307\n",
      "Epoch 1419/40000, Loss: 0.0022908642422407866, Learning Rate: 0.001307\n",
      "Epoch 1420/40000, Loss: 0.0024011158384382725, Learning Rate: 0.001306\n",
      "Epoch 1421/40000, Loss: 0.00427670544013381, Learning Rate: 0.001306\n",
      "Epoch 1422/40000, Loss: 0.0021331175230443478, Learning Rate: 0.001305\n",
      "Epoch 1423/40000, Loss: 0.0039052285719662905, Learning Rate: 0.001305\n",
      "Epoch 1424/40000, Loss: 0.0021454780362546444, Learning Rate: 0.001305\n",
      "Epoch 1425/40000, Loss: 0.0030011360067874193, Learning Rate: 0.001304\n",
      "Epoch 1426/40000, Loss: 0.00286938832141459, Learning Rate: 0.001304\n",
      "Epoch 1427/40000, Loss: 0.002122460398823023, Learning Rate: 0.001303\n",
      "Epoch 1428/40000, Loss: 0.004195156041532755, Learning Rate: 0.001303\n",
      "Epoch 1429/40000, Loss: 0.004071685019880533, Learning Rate: 0.001303\n",
      "Epoch 1430/40000, Loss: 0.0021460591815412045, Learning Rate: 0.001302\n",
      "Epoch 1431/40000, Loss: 0.0023364827502518892, Learning Rate: 0.001302\n",
      "Epoch 1432/40000, Loss: 0.00407280633226037, Learning Rate: 0.001302\n",
      "Epoch 1433/40000, Loss: 0.004273789003491402, Learning Rate: 0.001301\n",
      "Epoch 1434/40000, Loss: 0.0021403685677796602, Learning Rate: 0.001301\n",
      "Epoch 1435/40000, Loss: 0.0028938306495547295, Learning Rate: 0.001300\n",
      "Epoch 1436/40000, Loss: 0.002251820173114538, Learning Rate: 0.001300\n",
      "Epoch 1437/40000, Loss: 0.0027561509050428867, Learning Rate: 0.001300\n",
      "Epoch 1438/40000, Loss: 0.002874119905754924, Learning Rate: 0.001299\n",
      "Epoch 1439/40000, Loss: 0.004136198665946722, Learning Rate: 0.001299\n",
      "Epoch 1440/40000, Loss: 0.0027330282609909773, Learning Rate: 0.001298\n",
      "Epoch 1441/40000, Loss: 0.004194164648652077, Learning Rate: 0.001298\n",
      "Epoch 1442/40000, Loss: 0.003940914291888475, Learning Rate: 0.001298\n",
      "Epoch 1443/40000, Loss: 0.0037990736309438944, Learning Rate: 0.001297\n",
      "Epoch 1444/40000, Loss: 0.002111375331878662, Learning Rate: 0.001297\n",
      "Epoch 1445/40000, Loss: 0.0021613456774502993, Learning Rate: 0.001296\n",
      "Epoch 1446/40000, Loss: 0.004165389575064182, Learning Rate: 0.001296\n",
      "Epoch 1447/40000, Loss: 0.003810341004282236, Learning Rate: 0.001296\n",
      "Epoch 1448/40000, Loss: 0.0038730825763195753, Learning Rate: 0.001295\n",
      "Epoch 1449/40000, Loss: 0.004147137049585581, Learning Rate: 0.001295\n",
      "Epoch 1450/40000, Loss: 0.002244582399725914, Learning Rate: 0.001295\n",
      "Epoch 1451/40000, Loss: 0.0021383825223892927, Learning Rate: 0.001294\n",
      "Epoch 1452/40000, Loss: 0.004223955329507589, Learning Rate: 0.001294\n",
      "Epoch 1453/40000, Loss: 0.002155952388420701, Learning Rate: 0.001293\n",
      "Epoch 1454/40000, Loss: 0.004008837975561619, Learning Rate: 0.001293\n",
      "Epoch 1455/40000, Loss: 0.003859217744320631, Learning Rate: 0.001293\n",
      "Epoch 1456/40000, Loss: 0.0021371259354054928, Learning Rate: 0.001292\n",
      "Epoch 1457/40000, Loss: 0.0022925029043108225, Learning Rate: 0.001292\n",
      "Epoch 1458/40000, Loss: 0.0022288034670054913, Learning Rate: 0.001291\n",
      "Epoch 1459/40000, Loss: 0.0041411626152694225, Learning Rate: 0.001291\n",
      "Epoch 1460/40000, Loss: 0.004216837231069803, Learning Rate: 0.001291\n",
      "Epoch 1461/40000, Loss: 0.0021132456604391336, Learning Rate: 0.001290\n",
      "Epoch 1462/40000, Loss: 0.004136876203119755, Learning Rate: 0.001290\n",
      "Epoch 1463/40000, Loss: 0.002154707908630371, Learning Rate: 0.001289\n",
      "Epoch 1464/40000, Loss: 0.002182219410315156, Learning Rate: 0.001289\n",
      "Epoch 1465/40000, Loss: 0.002838989021256566, Learning Rate: 0.001289\n",
      "Epoch 1466/40000, Loss: 0.0021643354557454586, Learning Rate: 0.001288\n",
      "Epoch 1467/40000, Loss: 0.003812040202319622, Learning Rate: 0.001288\n",
      "Epoch 1468/40000, Loss: 0.0021214827429503202, Learning Rate: 0.001288\n",
      "Epoch 1469/40000, Loss: 0.0028840722516179085, Learning Rate: 0.001287\n",
      "Epoch 1470/40000, Loss: 0.0021648420952260494, Learning Rate: 0.001287\n",
      "Epoch 1471/40000, Loss: 0.0042128548957407475, Learning Rate: 0.001286\n",
      "Epoch 1472/40000, Loss: 0.0027735810726881027, Learning Rate: 0.001286\n",
      "Epoch 1473/40000, Loss: 0.0020839115604758263, Learning Rate: 0.001286\n",
      "Epoch 1474/40000, Loss: 0.0040992312133312225, Learning Rate: 0.001285\n",
      "Epoch 1475/40000, Loss: 0.0043385764583945274, Learning Rate: 0.001285\n",
      "Epoch 1476/40000, Loss: 0.004214683547616005, Learning Rate: 0.001284\n",
      "Epoch 1477/40000, Loss: 0.004295076709240675, Learning Rate: 0.001284\n",
      "Epoch 1478/40000, Loss: 0.003898737020790577, Learning Rate: 0.001284\n",
      "Epoch 1479/40000, Loss: 0.003803480649366975, Learning Rate: 0.001283\n",
      "Epoch 1480/40000, Loss: 0.002214604988694191, Learning Rate: 0.001283\n",
      "Epoch 1481/40000, Loss: 0.0039270371198654175, Learning Rate: 0.001283\n",
      "Epoch 1482/40000, Loss: 0.0022277876269072294, Learning Rate: 0.001282\n",
      "Epoch 1483/40000, Loss: 0.003744045738130808, Learning Rate: 0.001282\n",
      "Epoch 1484/40000, Loss: 0.0020873178727924824, Learning Rate: 0.001281\n",
      "Epoch 1485/40000, Loss: 0.003820696845650673, Learning Rate: 0.001281\n",
      "Epoch 1486/40000, Loss: 0.002865588292479515, Learning Rate: 0.001281\n",
      "Epoch 1487/40000, Loss: 0.004431232810020447, Learning Rate: 0.001280\n",
      "Epoch 1488/40000, Loss: 0.002131768735125661, Learning Rate: 0.001280\n",
      "Epoch 1489/40000, Loss: 0.0029275580309331417, Learning Rate: 0.001279\n",
      "Epoch 1490/40000, Loss: 0.004087063018232584, Learning Rate: 0.001279\n",
      "Epoch 1491/40000, Loss: 0.004120725207030773, Learning Rate: 0.001279\n",
      "Epoch 1492/40000, Loss: 0.002852407982572913, Learning Rate: 0.001278\n",
      "Epoch 1493/40000, Loss: 0.004292785190045834, Learning Rate: 0.001278\n",
      "Epoch 1494/40000, Loss: 0.0021949894726276398, Learning Rate: 0.001278\n",
      "Epoch 1495/40000, Loss: 0.002100499114021659, Learning Rate: 0.001277\n",
      "Epoch 1496/40000, Loss: 0.0020707270596176386, Learning Rate: 0.001277\n",
      "Epoch 1497/40000, Loss: 0.004183659330010414, Learning Rate: 0.001276\n",
      "Epoch 1498/40000, Loss: 0.0020820731297135353, Learning Rate: 0.001276\n",
      "Epoch 1499/40000, Loss: 0.002738700481131673, Learning Rate: 0.001276\n",
      "Epoch 1500/40000, Loss: 0.0020847416017204523, Learning Rate: 0.001275\n",
      "Epoch 1501/40000, Loss: 0.004133679438382387, Learning Rate: 0.001275\n",
      "Epoch 1502/40000, Loss: 0.0021672220900654793, Learning Rate: 0.001274\n",
      "Epoch 1503/40000, Loss: 0.0038747102953493595, Learning Rate: 0.001274\n",
      "Epoch 1504/40000, Loss: 0.004139583557844162, Learning Rate: 0.001274\n",
      "Epoch 1505/40000, Loss: 0.004234157968312502, Learning Rate: 0.001273\n",
      "Epoch 1506/40000, Loss: 0.002789005869999528, Learning Rate: 0.001273\n",
      "Epoch 1507/40000, Loss: 0.0027633013669401407, Learning Rate: 0.001273\n",
      "Epoch 1508/40000, Loss: 0.004198737908154726, Learning Rate: 0.001272\n",
      "Epoch 1509/40000, Loss: 0.002179596573114395, Learning Rate: 0.001272\n",
      "Epoch 1510/40000, Loss: 0.004180800169706345, Learning Rate: 0.001271\n",
      "Epoch 1511/40000, Loss: 0.004223557189106941, Learning Rate: 0.001271\n",
      "Epoch 1512/40000, Loss: 0.002167052123695612, Learning Rate: 0.001271\n",
      "Epoch 1513/40000, Loss: 0.004154833499342203, Learning Rate: 0.001270\n",
      "Epoch 1514/40000, Loss: 0.002234352519735694, Learning Rate: 0.001270\n",
      "Epoch 1515/40000, Loss: 0.0021316579077392817, Learning Rate: 0.001270\n",
      "Epoch 1516/40000, Loss: 0.00230629020370543, Learning Rate: 0.001269\n",
      "Epoch 1517/40000, Loss: 0.003038132796064019, Learning Rate: 0.001269\n",
      "Epoch 1518/40000, Loss: 0.002187170786783099, Learning Rate: 0.001268\n",
      "Epoch 1519/40000, Loss: 0.002231807680800557, Learning Rate: 0.001268\n",
      "Epoch 1520/40000, Loss: 0.0021829616744071245, Learning Rate: 0.001268\n",
      "Epoch 1521/40000, Loss: 0.002332942793145776, Learning Rate: 0.001267\n",
      "Epoch 1522/40000, Loss: 0.003939386922866106, Learning Rate: 0.001267\n",
      "Epoch 1523/40000, Loss: 0.002235895488411188, Learning Rate: 0.001266\n",
      "Epoch 1524/40000, Loss: 0.002330543240532279, Learning Rate: 0.001266\n",
      "Epoch 1525/40000, Loss: 0.002039037412032485, Learning Rate: 0.001266\n",
      "Epoch 1526/40000, Loss: 0.004151162225753069, Learning Rate: 0.001265\n",
      "Epoch 1527/40000, Loss: 0.004252699203789234, Learning Rate: 0.001265\n",
      "Epoch 1528/40000, Loss: 0.003808556590229273, Learning Rate: 0.001265\n",
      "Epoch 1529/40000, Loss: 0.004312328528612852, Learning Rate: 0.001264\n",
      "Epoch 1530/40000, Loss: 0.0040994989685714245, Learning Rate: 0.001264\n",
      "Epoch 1531/40000, Loss: 0.004193539265543222, Learning Rate: 0.001263\n",
      "Epoch 1532/40000, Loss: 0.004278930369764566, Learning Rate: 0.001263\n",
      "Epoch 1533/40000, Loss: 0.002815543208271265, Learning Rate: 0.001263\n",
      "Epoch 1534/40000, Loss: 0.002153233392164111, Learning Rate: 0.001262\n",
      "Epoch 1535/40000, Loss: 0.0041376203298568726, Learning Rate: 0.001262\n",
      "Epoch 1536/40000, Loss: 0.002764216624200344, Learning Rate: 0.001262\n",
      "Epoch 1537/40000, Loss: 0.004130036570131779, Learning Rate: 0.001261\n",
      "Epoch 1538/40000, Loss: 0.0040602050721645355, Learning Rate: 0.001261\n",
      "Epoch 1539/40000, Loss: 0.004217447247356176, Learning Rate: 0.001260\n",
      "Epoch 1540/40000, Loss: 0.0021139553282409906, Learning Rate: 0.001260\n",
      "Epoch 1541/40000, Loss: 0.002035620389506221, Learning Rate: 0.001260\n",
      "Epoch 1542/40000, Loss: 0.002733063418418169, Learning Rate: 0.001259\n",
      "Epoch 1543/40000, Loss: 0.003751015290617943, Learning Rate: 0.001259\n",
      "Epoch 1544/40000, Loss: 0.004169374238699675, Learning Rate: 0.001259\n",
      "Epoch 1545/40000, Loss: 0.0037945082876831293, Learning Rate: 0.001258\n",
      "Epoch 1546/40000, Loss: 0.004157568793743849, Learning Rate: 0.001258\n",
      "Epoch 1547/40000, Loss: 0.0037462767213582993, Learning Rate: 0.001257\n",
      "Epoch 1548/40000, Loss: 0.002052294323220849, Learning Rate: 0.001257\n",
      "Epoch 1549/40000, Loss: 0.00210410519503057, Learning Rate: 0.001257\n",
      "Epoch 1550/40000, Loss: 0.004030314274132252, Learning Rate: 0.001256\n",
      "Epoch 1551/40000, Loss: 0.003890067571774125, Learning Rate: 0.001256\n",
      "Epoch 1552/40000, Loss: 0.004118644632399082, Learning Rate: 0.001256\n",
      "Epoch 1553/40000, Loss: 0.0027353307232260704, Learning Rate: 0.001255\n",
      "Epoch 1554/40000, Loss: 0.0038594023790210485, Learning Rate: 0.001255\n",
      "Epoch 1555/40000, Loss: 0.003794921562075615, Learning Rate: 0.001254\n",
      "Epoch 1556/40000, Loss: 0.004185514524579048, Learning Rate: 0.001254\n",
      "Epoch 1557/40000, Loss: 0.0021148419473320246, Learning Rate: 0.001254\n",
      "Epoch 1558/40000, Loss: 0.004175273235887289, Learning Rate: 0.001253\n",
      "Epoch 1559/40000, Loss: 0.004338972736150026, Learning Rate: 0.001253\n",
      "Epoch 1560/40000, Loss: 0.002380510326474905, Learning Rate: 0.001252\n",
      "Epoch 1561/40000, Loss: 0.004200927447527647, Learning Rate: 0.001252\n",
      "Epoch 1562/40000, Loss: 0.00436623627319932, Learning Rate: 0.001252\n",
      "Epoch 1563/40000, Loss: 0.0038741701282560825, Learning Rate: 0.001251\n",
      "Epoch 1564/40000, Loss: 0.0027362676337361336, Learning Rate: 0.001251\n",
      "Epoch 1565/40000, Loss: 0.0023544980213046074, Learning Rate: 0.001251\n",
      "Epoch 1566/40000, Loss: 0.0023900694213807583, Learning Rate: 0.001250\n",
      "Epoch 1567/40000, Loss: 0.004575377330183983, Learning Rate: 0.001250\n",
      "Epoch 1568/40000, Loss: 0.0037660361267626286, Learning Rate: 0.001249\n",
      "Epoch 1569/40000, Loss: 0.0037753714714199305, Learning Rate: 0.001249\n",
      "Epoch 1570/40000, Loss: 0.004084758460521698, Learning Rate: 0.001249\n",
      "Epoch 1571/40000, Loss: 0.004060538951307535, Learning Rate: 0.001248\n",
      "Epoch 1572/40000, Loss: 0.004154724534600973, Learning Rate: 0.001248\n",
      "Epoch 1573/40000, Loss: 0.0037232397589832544, Learning Rate: 0.001248\n",
      "Epoch 1574/40000, Loss: 0.002076988574117422, Learning Rate: 0.001247\n",
      "Epoch 1575/40000, Loss: 0.004181121941655874, Learning Rate: 0.001247\n",
      "Epoch 1576/40000, Loss: 0.0028058234602212906, Learning Rate: 0.001246\n",
      "Epoch 1577/40000, Loss: 0.0039195651188492775, Learning Rate: 0.001246\n",
      "Epoch 1578/40000, Loss: 0.0029649122152477503, Learning Rate: 0.001246\n",
      "Epoch 1579/40000, Loss: 0.003028525970876217, Learning Rate: 0.001245\n",
      "Epoch 1580/40000, Loss: 0.004356251563876867, Learning Rate: 0.001245\n",
      "Epoch 1581/40000, Loss: 0.004341908264905214, Learning Rate: 0.001245\n",
      "Epoch 1582/40000, Loss: 0.00439923070371151, Learning Rate: 0.001244\n",
      "Epoch 1583/40000, Loss: 0.00434836745262146, Learning Rate: 0.001244\n",
      "Epoch 1584/40000, Loss: 0.004624973051249981, Learning Rate: 0.001244\n",
      "Epoch 1585/40000, Loss: 0.002359473379328847, Learning Rate: 0.001243\n",
      "Epoch 1586/40000, Loss: 0.003157694824039936, Learning Rate: 0.001243\n",
      "Epoch 1587/40000, Loss: 0.004276633262634277, Learning Rate: 0.001242\n",
      "Epoch 1588/40000, Loss: 0.0030389383900910616, Learning Rate: 0.001242\n",
      "Epoch 1589/40000, Loss: 0.0028038069140166044, Learning Rate: 0.001242\n",
      "Epoch 1590/40000, Loss: 0.0022068971302360296, Learning Rate: 0.001241\n",
      "Epoch 1591/40000, Loss: 0.002030374715104699, Learning Rate: 0.001241\n",
      "Epoch 1592/40000, Loss: 0.004175429232418537, Learning Rate: 0.001241\n",
      "Epoch 1593/40000, Loss: 0.0026875438634306192, Learning Rate: 0.001240\n",
      "Epoch 1594/40000, Loss: 0.004154086112976074, Learning Rate: 0.001240\n",
      "Epoch 1595/40000, Loss: 0.004144700244069099, Learning Rate: 0.001239\n",
      "Epoch 1596/40000, Loss: 0.004172003827989101, Learning Rate: 0.001239\n",
      "Epoch 1597/40000, Loss: 0.0038045619148761034, Learning Rate: 0.001239\n",
      "Epoch 1598/40000, Loss: 0.0028533865697681904, Learning Rate: 0.001238\n",
      "Epoch 1599/40000, Loss: 0.00428508035838604, Learning Rate: 0.001238\n",
      "Epoch 1600/40000, Loss: 0.004270960111171007, Learning Rate: 0.001238\n",
      "Epoch 1601/40000, Loss: 0.0027139338199049234, Learning Rate: 0.001237\n",
      "Epoch 1602/40000, Loss: 0.002071137772873044, Learning Rate: 0.001237\n",
      "Epoch 1603/40000, Loss: 0.0027140534948557615, Learning Rate: 0.001236\n",
      "Epoch 1604/40000, Loss: 0.002145936246961355, Learning Rate: 0.001236\n",
      "Epoch 1605/40000, Loss: 0.004127104766666889, Learning Rate: 0.001236\n",
      "Epoch 1606/40000, Loss: 0.0020148386247456074, Learning Rate: 0.001235\n",
      "Epoch 1607/40000, Loss: 0.003802674822509289, Learning Rate: 0.001235\n",
      "Epoch 1608/40000, Loss: 0.004080894403159618, Learning Rate: 0.001235\n",
      "Epoch 1609/40000, Loss: 0.0020813345909118652, Learning Rate: 0.001234\n",
      "Epoch 1610/40000, Loss: 0.004053191747516394, Learning Rate: 0.001234\n",
      "Epoch 1611/40000, Loss: 0.0021360854152590036, Learning Rate: 0.001233\n",
      "Epoch 1612/40000, Loss: 0.0027403340209275484, Learning Rate: 0.001233\n",
      "Epoch 1613/40000, Loss: 0.0028409739024937153, Learning Rate: 0.001233\n",
      "Epoch 1614/40000, Loss: 0.0027989819645881653, Learning Rate: 0.001232\n",
      "Epoch 1615/40000, Loss: 0.0021867670584470034, Learning Rate: 0.001232\n",
      "Epoch 1616/40000, Loss: 0.004300844389945269, Learning Rate: 0.001232\n",
      "Epoch 1617/40000, Loss: 0.003816541749984026, Learning Rate: 0.001231\n",
      "Epoch 1618/40000, Loss: 0.004232475068420172, Learning Rate: 0.001231\n",
      "Epoch 1619/40000, Loss: 0.002119550947099924, Learning Rate: 0.001231\n",
      "Epoch 1620/40000, Loss: 0.0041951462626457214, Learning Rate: 0.001230\n",
      "Epoch 1621/40000, Loss: 0.0021080824080854654, Learning Rate: 0.001230\n",
      "Epoch 1622/40000, Loss: 0.004116353113204241, Learning Rate: 0.001229\n",
      "Epoch 1623/40000, Loss: 0.002112069632858038, Learning Rate: 0.001229\n",
      "Epoch 1624/40000, Loss: 0.0038826509844511747, Learning Rate: 0.001229\n",
      "Epoch 1625/40000, Loss: 0.004132682457566261, Learning Rate: 0.001228\n",
      "Epoch 1626/40000, Loss: 0.0021734691690653563, Learning Rate: 0.001228\n",
      "Epoch 1627/40000, Loss: 0.004227315075695515, Learning Rate: 0.001228\n",
      "Epoch 1628/40000, Loss: 0.003716606879606843, Learning Rate: 0.001227\n",
      "Epoch 1629/40000, Loss: 0.003735695965588093, Learning Rate: 0.001227\n",
      "Epoch 1630/40000, Loss: 0.004114171024411917, Learning Rate: 0.001226\n",
      "Epoch 1631/40000, Loss: 0.0020810491405427456, Learning Rate: 0.001226\n",
      "Epoch 1632/40000, Loss: 0.002050706883892417, Learning Rate: 0.001226\n",
      "Epoch 1633/40000, Loss: 0.0040440126322209835, Learning Rate: 0.001225\n",
      "Epoch 1634/40000, Loss: 0.002062929328531027, Learning Rate: 0.001225\n",
      "Epoch 1635/40000, Loss: 0.003768267808482051, Learning Rate: 0.001225\n",
      "Epoch 1636/40000, Loss: 0.0026926419232040644, Learning Rate: 0.001224\n",
      "Epoch 1637/40000, Loss: 0.004187308251857758, Learning Rate: 0.001224\n",
      "Epoch 1638/40000, Loss: 0.004230510909110308, Learning Rate: 0.001224\n",
      "Epoch 1639/40000, Loss: 0.0021397771779447794, Learning Rate: 0.001223\n",
      "Epoch 1640/40000, Loss: 0.0021540531888604164, Learning Rate: 0.001223\n",
      "Epoch 1641/40000, Loss: 0.0027297709602862597, Learning Rate: 0.001222\n",
      "Epoch 1642/40000, Loss: 0.002104442333802581, Learning Rate: 0.001222\n",
      "Epoch 1643/40000, Loss: 0.002776418114081025, Learning Rate: 0.001222\n",
      "Epoch 1644/40000, Loss: 0.004362030886113644, Learning Rate: 0.001221\n",
      "Epoch 1645/40000, Loss: 0.002205511089414358, Learning Rate: 0.001221\n",
      "Epoch 1646/40000, Loss: 0.004126116167753935, Learning Rate: 0.001221\n",
      "Epoch 1647/40000, Loss: 0.004120416007936001, Learning Rate: 0.001220\n",
      "Epoch 1648/40000, Loss: 0.00203888863325119, Learning Rate: 0.001220\n",
      "Epoch 1649/40000, Loss: 0.0020545602310448885, Learning Rate: 0.001219\n",
      "Epoch 1650/40000, Loss: 0.0021971447858959436, Learning Rate: 0.001219\n",
      "Epoch 1651/40000, Loss: 0.003743940033018589, Learning Rate: 0.001219\n",
      "Epoch 1652/40000, Loss: 0.004129735287278891, Learning Rate: 0.001218\n",
      "Epoch 1653/40000, Loss: 0.0023400112986564636, Learning Rate: 0.001218\n",
      "Epoch 1654/40000, Loss: 0.004211619030684233, Learning Rate: 0.001218\n",
      "Epoch 1655/40000, Loss: 0.002801714465022087, Learning Rate: 0.001217\n",
      "Epoch 1656/40000, Loss: 0.002108901971951127, Learning Rate: 0.001217\n",
      "Epoch 1657/40000, Loss: 0.004011219833046198, Learning Rate: 0.001217\n",
      "Epoch 1658/40000, Loss: 0.004073932766914368, Learning Rate: 0.001216\n",
      "Epoch 1659/40000, Loss: 0.002036584774032235, Learning Rate: 0.001216\n",
      "Epoch 1660/40000, Loss: 0.004093784838914871, Learning Rate: 0.001215\n",
      "Epoch 1661/40000, Loss: 0.002043976215645671, Learning Rate: 0.001215\n",
      "Epoch 1662/40000, Loss: 0.0037121588829904795, Learning Rate: 0.001215\n",
      "Epoch 1663/40000, Loss: 0.003742286702618003, Learning Rate: 0.001214\n",
      "Epoch 1664/40000, Loss: 0.0037251771427690983, Learning Rate: 0.001214\n",
      "Epoch 1665/40000, Loss: 0.0027177594602108, Learning Rate: 0.001214\n",
      "Epoch 1666/40000, Loss: 0.002714963397011161, Learning Rate: 0.001213\n",
      "Epoch 1667/40000, Loss: 0.002836461877450347, Learning Rate: 0.001213\n",
      "Epoch 1668/40000, Loss: 0.004210864193737507, Learning Rate: 0.001213\n",
      "Epoch 1669/40000, Loss: 0.0020228964276611805, Learning Rate: 0.001212\n",
      "Epoch 1670/40000, Loss: 0.0037608002312481403, Learning Rate: 0.001212\n",
      "Epoch 1671/40000, Loss: 0.0037427227944135666, Learning Rate: 0.001211\n",
      "Epoch 1672/40000, Loss: 0.0027607108931988478, Learning Rate: 0.001211\n",
      "Epoch 1673/40000, Loss: 0.001999318366870284, Learning Rate: 0.001211\n",
      "Epoch 1674/40000, Loss: 0.004045720677822828, Learning Rate: 0.001210\n",
      "Epoch 1675/40000, Loss: 0.004189228639006615, Learning Rate: 0.001210\n",
      "Epoch 1676/40000, Loss: 0.002102423692122102, Learning Rate: 0.001210\n",
      "Epoch 1677/40000, Loss: 0.00222613918595016, Learning Rate: 0.001209\n",
      "Epoch 1678/40000, Loss: 0.0021307452116161585, Learning Rate: 0.001209\n",
      "Epoch 1679/40000, Loss: 0.0040457467548549175, Learning Rate: 0.001209\n",
      "Epoch 1680/40000, Loss: 0.0041187237948179245, Learning Rate: 0.001208\n",
      "Epoch 1681/40000, Loss: 0.0020827853586524725, Learning Rate: 0.001208\n",
      "Epoch 1682/40000, Loss: 0.002051234943792224, Learning Rate: 0.001207\n",
      "Epoch 1683/40000, Loss: 0.004118656273931265, Learning Rate: 0.001207\n",
      "Epoch 1684/40000, Loss: 0.0027064098976552486, Learning Rate: 0.001207\n",
      "Epoch 1685/40000, Loss: 0.002087624277919531, Learning Rate: 0.001206\n",
      "Epoch 1686/40000, Loss: 0.003712730249390006, Learning Rate: 0.001206\n",
      "Epoch 1687/40000, Loss: 0.004151441622525454, Learning Rate: 0.001206\n",
      "Epoch 1688/40000, Loss: 0.0037408953066915274, Learning Rate: 0.001205\n",
      "Epoch 1689/40000, Loss: 0.002047088462859392, Learning Rate: 0.001205\n",
      "Epoch 1690/40000, Loss: 0.0020418621134012938, Learning Rate: 0.001205\n",
      "Epoch 1691/40000, Loss: 0.004099385347217321, Learning Rate: 0.001204\n",
      "Epoch 1692/40000, Loss: 0.0020626101177185774, Learning Rate: 0.001204\n",
      "Epoch 1693/40000, Loss: 0.0037888113874942064, Learning Rate: 0.001204\n",
      "Epoch 1694/40000, Loss: 0.003952489700168371, Learning Rate: 0.001203\n",
      "Epoch 1695/40000, Loss: 0.004073692485690117, Learning Rate: 0.001203\n",
      "Epoch 1696/40000, Loss: 0.004352643620222807, Learning Rate: 0.001202\n",
      "Epoch 1697/40000, Loss: 0.0038315437268465757, Learning Rate: 0.001202\n",
      "Epoch 1698/40000, Loss: 0.004222598392516375, Learning Rate: 0.001202\n",
      "Epoch 1699/40000, Loss: 0.004435103852301836, Learning Rate: 0.001201\n",
      "Epoch 1700/40000, Loss: 0.002412509871646762, Learning Rate: 0.001201\n",
      "Epoch 1701/40000, Loss: 0.002237413777038455, Learning Rate: 0.001201\n",
      "Epoch 1702/40000, Loss: 0.003885607933625579, Learning Rate: 0.001200\n",
      "Epoch 1703/40000, Loss: 0.002198990900069475, Learning Rate: 0.001200\n",
      "Epoch 1704/40000, Loss: 0.0038941092789173126, Learning Rate: 0.001200\n",
      "Epoch 1705/40000, Loss: 0.0041267708875238895, Learning Rate: 0.001199\n",
      "Epoch 1706/40000, Loss: 0.004113427828997374, Learning Rate: 0.001199\n",
      "Epoch 1707/40000, Loss: 0.002011002041399479, Learning Rate: 0.001198\n",
      "Epoch 1708/40000, Loss: 0.0041031986474990845, Learning Rate: 0.001198\n",
      "Epoch 1709/40000, Loss: 0.003803034545853734, Learning Rate: 0.001198\n",
      "Epoch 1710/40000, Loss: 0.004194876179099083, Learning Rate: 0.001197\n",
      "Epoch 1711/40000, Loss: 0.004012759309262037, Learning Rate: 0.001197\n",
      "Epoch 1712/40000, Loss: 0.0027257241308689117, Learning Rate: 0.001197\n",
      "Epoch 1713/40000, Loss: 0.0020910247694700956, Learning Rate: 0.001196\n",
      "Epoch 1714/40000, Loss: 0.002057168399915099, Learning Rate: 0.001196\n",
      "Epoch 1715/40000, Loss: 0.004160724114626646, Learning Rate: 0.001196\n",
      "Epoch 1716/40000, Loss: 0.0021620867773890495, Learning Rate: 0.001195\n",
      "Epoch 1717/40000, Loss: 0.004199685994535685, Learning Rate: 0.001195\n",
      "Epoch 1718/40000, Loss: 0.00281137484125793, Learning Rate: 0.001195\n",
      "Epoch 1719/40000, Loss: 0.0021141748875379562, Learning Rate: 0.001194\n",
      "Epoch 1720/40000, Loss: 0.0027785669080913067, Learning Rate: 0.001194\n",
      "Epoch 1721/40000, Loss: 0.002687482861801982, Learning Rate: 0.001193\n",
      "Epoch 1722/40000, Loss: 0.002229474950581789, Learning Rate: 0.001193\n",
      "Epoch 1723/40000, Loss: 0.0020578450057655573, Learning Rate: 0.001193\n",
      "Epoch 1724/40000, Loss: 0.0021051957737654448, Learning Rate: 0.001192\n",
      "Epoch 1725/40000, Loss: 0.00407886877655983, Learning Rate: 0.001192\n",
      "Epoch 1726/40000, Loss: 0.0037784080486744642, Learning Rate: 0.001192\n",
      "Epoch 1727/40000, Loss: 0.0021428163163363934, Learning Rate: 0.001191\n",
      "Epoch 1728/40000, Loss: 0.0021413930226117373, Learning Rate: 0.001191\n",
      "Epoch 1729/40000, Loss: 0.003739976091310382, Learning Rate: 0.001191\n",
      "Epoch 1730/40000, Loss: 0.0027590692043304443, Learning Rate: 0.001190\n",
      "Epoch 1731/40000, Loss: 0.002047548070549965, Learning Rate: 0.001190\n",
      "Epoch 1732/40000, Loss: 0.004051714204251766, Learning Rate: 0.001190\n",
      "Epoch 1733/40000, Loss: 0.003684059949591756, Learning Rate: 0.001189\n",
      "Epoch 1734/40000, Loss: 0.002013089833781123, Learning Rate: 0.001189\n",
      "Epoch 1735/40000, Loss: 0.004019707906991243, Learning Rate: 0.001188\n",
      "Epoch 1736/40000, Loss: 0.002661388833075762, Learning Rate: 0.001188\n",
      "Epoch 1737/40000, Loss: 0.00373280793428421, Learning Rate: 0.001188\n",
      "Epoch 1738/40000, Loss: 0.004111464601010084, Learning Rate: 0.001187\n",
      "Epoch 1739/40000, Loss: 0.0027856132946908474, Learning Rate: 0.001187\n",
      "Epoch 1740/40000, Loss: 0.004051242023706436, Learning Rate: 0.001187\n",
      "Epoch 1741/40000, Loss: 0.0020216230768710375, Learning Rate: 0.001186\n",
      "Epoch 1742/40000, Loss: 0.002736018504947424, Learning Rate: 0.001186\n",
      "Epoch 1743/40000, Loss: 0.002046471694484353, Learning Rate: 0.001186\n",
      "Epoch 1744/40000, Loss: 0.004120773635804653, Learning Rate: 0.001185\n",
      "Epoch 1745/40000, Loss: 0.00267031486146152, Learning Rate: 0.001185\n",
      "Epoch 1746/40000, Loss: 0.0020247264765203, Learning Rate: 0.001185\n",
      "Epoch 1747/40000, Loss: 0.004186420235782862, Learning Rate: 0.001184\n",
      "Epoch 1748/40000, Loss: 0.0021179034374654293, Learning Rate: 0.001184\n",
      "Epoch 1749/40000, Loss: 0.004116652999073267, Learning Rate: 0.001183\n",
      "Epoch 1750/40000, Loss: 0.0027222062926739454, Learning Rate: 0.001183\n",
      "Epoch 1751/40000, Loss: 0.004134590737521648, Learning Rate: 0.001183\n",
      "Epoch 1752/40000, Loss: 0.004037737846374512, Learning Rate: 0.001182\n",
      "Epoch 1753/40000, Loss: 0.004173004999756813, Learning Rate: 0.001182\n",
      "Epoch 1754/40000, Loss: 0.0027128811925649643, Learning Rate: 0.001182\n",
      "Epoch 1755/40000, Loss: 0.0020335146691650152, Learning Rate: 0.001181\n",
      "Epoch 1756/40000, Loss: 0.004108883440494537, Learning Rate: 0.001181\n",
      "Epoch 1757/40000, Loss: 0.002716348972171545, Learning Rate: 0.001181\n",
      "Epoch 1758/40000, Loss: 0.002247154014185071, Learning Rate: 0.001180\n",
      "Epoch 1759/40000, Loss: 0.0020555194932967424, Learning Rate: 0.001180\n",
      "Epoch 1760/40000, Loss: 0.002077571116387844, Learning Rate: 0.001180\n",
      "Epoch 1761/40000, Loss: 0.003750378731638193, Learning Rate: 0.001179\n",
      "Epoch 1762/40000, Loss: 0.002774082124233246, Learning Rate: 0.001179\n",
      "Epoch 1763/40000, Loss: 0.0038171368651092052, Learning Rate: 0.001178\n",
      "Epoch 1764/40000, Loss: 0.004071053583174944, Learning Rate: 0.001178\n",
      "Epoch 1765/40000, Loss: 0.004251451697200537, Learning Rate: 0.001178\n",
      "Epoch 1766/40000, Loss: 0.0020650727674365044, Learning Rate: 0.001177\n",
      "Epoch 1767/40000, Loss: 0.0026725870557129383, Learning Rate: 0.001177\n",
      "Epoch 1768/40000, Loss: 0.002132478868588805, Learning Rate: 0.001177\n",
      "Epoch 1769/40000, Loss: 0.003703911090269685, Learning Rate: 0.001176\n",
      "Epoch 1770/40000, Loss: 0.0020114972721785307, Learning Rate: 0.001176\n",
      "Epoch 1771/40000, Loss: 0.002018451690673828, Learning Rate: 0.001176\n",
      "Epoch 1772/40000, Loss: 0.004053947515785694, Learning Rate: 0.001175\n",
      "Epoch 1773/40000, Loss: 0.002000359119847417, Learning Rate: 0.001175\n",
      "Epoch 1774/40000, Loss: 0.0041922926902771, Learning Rate: 0.001175\n",
      "Epoch 1775/40000, Loss: 0.00208628224208951, Learning Rate: 0.001174\n",
      "Epoch 1776/40000, Loss: 0.0020884659606963396, Learning Rate: 0.001174\n",
      "Epoch 1777/40000, Loss: 0.004062072839587927, Learning Rate: 0.001174\n",
      "Epoch 1778/40000, Loss: 0.0026414119638502598, Learning Rate: 0.001173\n",
      "Epoch 1779/40000, Loss: 0.004102241713553667, Learning Rate: 0.001173\n",
      "Epoch 1780/40000, Loss: 0.0021893989760428667, Learning Rate: 0.001172\n",
      "Epoch 1781/40000, Loss: 0.0019965586252510548, Learning Rate: 0.001172\n",
      "Epoch 1782/40000, Loss: 0.0021133970003575087, Learning Rate: 0.001172\n",
      "Epoch 1783/40000, Loss: 0.002123073674738407, Learning Rate: 0.001171\n",
      "Epoch 1784/40000, Loss: 0.002261559246107936, Learning Rate: 0.001171\n",
      "Epoch 1785/40000, Loss: 0.002092092763632536, Learning Rate: 0.001171\n",
      "Epoch 1786/40000, Loss: 0.0041519468650221825, Learning Rate: 0.001170\n",
      "Epoch 1787/40000, Loss: 0.002083155093714595, Learning Rate: 0.001170\n",
      "Epoch 1788/40000, Loss: 0.004164121579378843, Learning Rate: 0.001170\n",
      "Epoch 1789/40000, Loss: 0.004064920824021101, Learning Rate: 0.001169\n",
      "Epoch 1790/40000, Loss: 0.004301200620830059, Learning Rate: 0.001169\n",
      "Epoch 1791/40000, Loss: 0.004167668521404266, Learning Rate: 0.001169\n",
      "Epoch 1792/40000, Loss: 0.0026872819289565086, Learning Rate: 0.001168\n",
      "Epoch 1793/40000, Loss: 0.004223619122058153, Learning Rate: 0.001168\n",
      "Epoch 1794/40000, Loss: 0.002893720753490925, Learning Rate: 0.001168\n",
      "Epoch 1795/40000, Loss: 0.002090722555294633, Learning Rate: 0.001167\n",
      "Epoch 1796/40000, Loss: 0.0021781958639621735, Learning Rate: 0.001167\n",
      "Epoch 1797/40000, Loss: 0.00408209441229701, Learning Rate: 0.001167\n",
      "Epoch 1798/40000, Loss: 0.0020741126500070095, Learning Rate: 0.001166\n",
      "Epoch 1799/40000, Loss: 0.002661193488165736, Learning Rate: 0.001166\n",
      "Epoch 1800/40000, Loss: 0.004041504580527544, Learning Rate: 0.001165\n",
      "Epoch 1801/40000, Loss: 0.0019974312745034695, Learning Rate: 0.001165\n",
      "Epoch 1802/40000, Loss: 0.0037763663567602634, Learning Rate: 0.001165\n",
      "Epoch 1803/40000, Loss: 0.0020471476018428802, Learning Rate: 0.001164\n",
      "Epoch 1804/40000, Loss: 0.003695499151945114, Learning Rate: 0.001164\n",
      "Epoch 1805/40000, Loss: 0.003992160316556692, Learning Rate: 0.001164\n",
      "Epoch 1806/40000, Loss: 0.0039645167998969555, Learning Rate: 0.001163\n",
      "Epoch 1807/40000, Loss: 0.0019807706121355295, Learning Rate: 0.001163\n",
      "Epoch 1808/40000, Loss: 0.00221044197678566, Learning Rate: 0.001163\n",
      "Epoch 1809/40000, Loss: 0.00266654952429235, Learning Rate: 0.001162\n",
      "Epoch 1810/40000, Loss: 0.0021919242572039366, Learning Rate: 0.001162\n",
      "Epoch 1811/40000, Loss: 0.002793306950479746, Learning Rate: 0.001162\n",
      "Epoch 1812/40000, Loss: 0.003783558029681444, Learning Rate: 0.001161\n",
      "Epoch 1813/40000, Loss: 0.0037956945598125458, Learning Rate: 0.001161\n",
      "Epoch 1814/40000, Loss: 0.0037368249613791704, Learning Rate: 0.001161\n",
      "Epoch 1815/40000, Loss: 0.002098152181133628, Learning Rate: 0.001160\n",
      "Epoch 1816/40000, Loss: 0.004064586944878101, Learning Rate: 0.001160\n",
      "Epoch 1817/40000, Loss: 0.0019912919960916042, Learning Rate: 0.001160\n",
      "Epoch 1818/40000, Loss: 0.002715400652959943, Learning Rate: 0.001159\n",
      "Epoch 1819/40000, Loss: 0.003912791144102812, Learning Rate: 0.001159\n",
      "Epoch 1820/40000, Loss: 0.003985541872680187, Learning Rate: 0.001159\n",
      "Epoch 1821/40000, Loss: 0.0021671371068805456, Learning Rate: 0.001158\n",
      "Epoch 1822/40000, Loss: 0.002008856041356921, Learning Rate: 0.001158\n",
      "Epoch 1823/40000, Loss: 0.002077788580209017, Learning Rate: 0.001157\n",
      "Epoch 1824/40000, Loss: 0.002170001855120063, Learning Rate: 0.001157\n",
      "Epoch 1825/40000, Loss: 0.004148837178945541, Learning Rate: 0.001157\n",
      "Epoch 1826/40000, Loss: 0.002129974775016308, Learning Rate: 0.001156\n",
      "Epoch 1827/40000, Loss: 0.00421085674315691, Learning Rate: 0.001156\n",
      "Epoch 1828/40000, Loss: 0.0039201537147164345, Learning Rate: 0.001156\n",
      "Epoch 1829/40000, Loss: 0.0023366038221865892, Learning Rate: 0.001155\n",
      "Epoch 1830/40000, Loss: 0.004138890653848648, Learning Rate: 0.001155\n",
      "Epoch 1831/40000, Loss: 0.004449677653610706, Learning Rate: 0.001155\n",
      "Epoch 1832/40000, Loss: 0.0019979553762823343, Learning Rate: 0.001154\n",
      "Epoch 1833/40000, Loss: 0.002069006208330393, Learning Rate: 0.001154\n",
      "Epoch 1834/40000, Loss: 0.0027492379304021597, Learning Rate: 0.001154\n",
      "Epoch 1835/40000, Loss: 0.0020670676603913307, Learning Rate: 0.001153\n",
      "Epoch 1836/40000, Loss: 0.0020154821686446667, Learning Rate: 0.001153\n",
      "Epoch 1837/40000, Loss: 0.0041178688406944275, Learning Rate: 0.001153\n",
      "Epoch 1838/40000, Loss: 0.004365943372249603, Learning Rate: 0.001152\n",
      "Epoch 1839/40000, Loss: 0.0023348373360931873, Learning Rate: 0.001152\n",
      "Epoch 1840/40000, Loss: 0.003745975671336055, Learning Rate: 0.001152\n",
      "Epoch 1841/40000, Loss: 0.002316809957846999, Learning Rate: 0.001151\n",
      "Epoch 1842/40000, Loss: 0.004119694698601961, Learning Rate: 0.001151\n",
      "Epoch 1843/40000, Loss: 0.003901384538039565, Learning Rate: 0.001151\n",
      "Epoch 1844/40000, Loss: 0.0020209592767059803, Learning Rate: 0.001150\n",
      "Epoch 1845/40000, Loss: 0.0020944210700690746, Learning Rate: 0.001150\n",
      "Epoch 1846/40000, Loss: 0.002669682027772069, Learning Rate: 0.001150\n",
      "Epoch 1847/40000, Loss: 0.002733408473432064, Learning Rate: 0.001149\n",
      "Epoch 1848/40000, Loss: 0.003670177888125181, Learning Rate: 0.001149\n",
      "Epoch 1849/40000, Loss: 0.004066614434123039, Learning Rate: 0.001148\n",
      "Epoch 1850/40000, Loss: 0.0026221368461847305, Learning Rate: 0.001148\n",
      "Epoch 1851/40000, Loss: 0.003673003753647208, Learning Rate: 0.001148\n",
      "Epoch 1852/40000, Loss: 0.0021100162994116545, Learning Rate: 0.001147\n",
      "Epoch 1853/40000, Loss: 0.004094985313713551, Learning Rate: 0.001147\n",
      "Epoch 1854/40000, Loss: 0.004330654628574848, Learning Rate: 0.001147\n",
      "Epoch 1855/40000, Loss: 0.004394868854433298, Learning Rate: 0.001146\n",
      "Epoch 1856/40000, Loss: 0.004191602114588022, Learning Rate: 0.001146\n",
      "Epoch 1857/40000, Loss: 0.004452765919268131, Learning Rate: 0.001146\n",
      "Epoch 1858/40000, Loss: 0.0021007221657782793, Learning Rate: 0.001145\n",
      "Epoch 1859/40000, Loss: 0.002846150891855359, Learning Rate: 0.001145\n",
      "Epoch 1860/40000, Loss: 0.004118838347494602, Learning Rate: 0.001145\n",
      "Epoch 1861/40000, Loss: 0.002062063431367278, Learning Rate: 0.001144\n",
      "Epoch 1862/40000, Loss: 0.004086648114025593, Learning Rate: 0.001144\n",
      "Epoch 1863/40000, Loss: 0.004104953724890947, Learning Rate: 0.001144\n",
      "Epoch 1864/40000, Loss: 0.00426874915137887, Learning Rate: 0.001143\n",
      "Epoch 1865/40000, Loss: 0.004134619142860174, Learning Rate: 0.001143\n",
      "Epoch 1866/40000, Loss: 0.0019694059155881405, Learning Rate: 0.001143\n",
      "Epoch 1867/40000, Loss: 0.0020439941436052322, Learning Rate: 0.001142\n",
      "Epoch 1868/40000, Loss: 0.003706321120262146, Learning Rate: 0.001142\n",
      "Epoch 1869/40000, Loss: 0.0020433817990124226, Learning Rate: 0.001142\n",
      "Epoch 1870/40000, Loss: 0.002658318728208542, Learning Rate: 0.001141\n",
      "Epoch 1871/40000, Loss: 0.0027380383107811213, Learning Rate: 0.001141\n",
      "Epoch 1872/40000, Loss: 0.0040183113887906075, Learning Rate: 0.001141\n",
      "Epoch 1873/40000, Loss: 0.0020138646941632032, Learning Rate: 0.001140\n",
      "Epoch 1874/40000, Loss: 0.0026861196383833885, Learning Rate: 0.001140\n",
      "Epoch 1875/40000, Loss: 0.002030827570706606, Learning Rate: 0.001140\n",
      "Epoch 1876/40000, Loss: 0.0037298835813999176, Learning Rate: 0.001139\n",
      "Epoch 1877/40000, Loss: 0.002003410831093788, Learning Rate: 0.001139\n",
      "Epoch 1878/40000, Loss: 0.0027634501457214355, Learning Rate: 0.001139\n",
      "Epoch 1879/40000, Loss: 0.002804924501106143, Learning Rate: 0.001138\n",
      "Epoch 1880/40000, Loss: 0.004192447289824486, Learning Rate: 0.001138\n",
      "Epoch 1881/40000, Loss: 0.004488377831876278, Learning Rate: 0.001138\n",
      "Epoch 1882/40000, Loss: 0.0032211975194513798, Learning Rate: 0.001137\n",
      "Epoch 1883/40000, Loss: 0.0038525499403476715, Learning Rate: 0.001137\n",
      "Epoch 1884/40000, Loss: 0.002796309534460306, Learning Rate: 0.001136\n",
      "Epoch 1885/40000, Loss: 0.0026800688356161118, Learning Rate: 0.001136\n",
      "Epoch 1886/40000, Loss: 0.004047016613185406, Learning Rate: 0.001136\n",
      "Epoch 1887/40000, Loss: 0.002178054302930832, Learning Rate: 0.001135\n",
      "Epoch 1888/40000, Loss: 0.004103249404579401, Learning Rate: 0.001135\n",
      "Epoch 1889/40000, Loss: 0.002021318068727851, Learning Rate: 0.001135\n",
      "Epoch 1890/40000, Loss: 0.0027829655446112156, Learning Rate: 0.001134\n",
      "Epoch 1891/40000, Loss: 0.0027123994659632444, Learning Rate: 0.001134\n",
      "Epoch 1892/40000, Loss: 0.0020643926691263914, Learning Rate: 0.001134\n",
      "Epoch 1893/40000, Loss: 0.004151858855038881, Learning Rate: 0.001133\n",
      "Epoch 1894/40000, Loss: 0.0026259240694344044, Learning Rate: 0.001133\n",
      "Epoch 1895/40000, Loss: 0.0040358444675803185, Learning Rate: 0.001133\n",
      "Epoch 1896/40000, Loss: 0.0039627430960536, Learning Rate: 0.001132\n",
      "Epoch 1897/40000, Loss: 0.0026684231124818325, Learning Rate: 0.001132\n",
      "Epoch 1898/40000, Loss: 0.0021067303605377674, Learning Rate: 0.001132\n",
      "Epoch 1899/40000, Loss: 0.0020709463860839605, Learning Rate: 0.001131\n",
      "Epoch 1900/40000, Loss: 0.004015116021037102, Learning Rate: 0.001131\n",
      "Epoch 1901/40000, Loss: 0.0037376205436885357, Learning Rate: 0.001131\n",
      "Epoch 1902/40000, Loss: 0.002623928477987647, Learning Rate: 0.001130\n",
      "Epoch 1903/40000, Loss: 0.0037097171880304813, Learning Rate: 0.001130\n",
      "Epoch 1904/40000, Loss: 0.002669251523911953, Learning Rate: 0.001130\n",
      "Epoch 1905/40000, Loss: 0.0019800972659140825, Learning Rate: 0.001129\n",
      "Epoch 1906/40000, Loss: 0.0020428404677659273, Learning Rate: 0.001129\n",
      "Epoch 1907/40000, Loss: 0.0020143070723861456, Learning Rate: 0.001129\n",
      "Epoch 1908/40000, Loss: 0.002687444444745779, Learning Rate: 0.001128\n",
      "Epoch 1909/40000, Loss: 0.0021406456362456083, Learning Rate: 0.001128\n",
      "Epoch 1910/40000, Loss: 0.0040636565536260605, Learning Rate: 0.001128\n",
      "Epoch 1911/40000, Loss: 0.0020984490402042866, Learning Rate: 0.001127\n",
      "Epoch 1912/40000, Loss: 0.0037785996682941914, Learning Rate: 0.001127\n",
      "Epoch 1913/40000, Loss: 0.002230017678812146, Learning Rate: 0.001127\n",
      "Epoch 1914/40000, Loss: 0.0021666227839887142, Learning Rate: 0.001126\n",
      "Epoch 1915/40000, Loss: 0.004062368534505367, Learning Rate: 0.001126\n",
      "Epoch 1916/40000, Loss: 0.0042027984745800495, Learning Rate: 0.001126\n",
      "Epoch 1917/40000, Loss: 0.004347698763012886, Learning Rate: 0.001125\n",
      "Epoch 1918/40000, Loss: 0.0022334710229188204, Learning Rate: 0.001125\n",
      "Epoch 1919/40000, Loss: 0.004112489987164736, Learning Rate: 0.001125\n",
      "Epoch 1920/40000, Loss: 0.0039915363304317, Learning Rate: 0.001124\n",
      "Epoch 1921/40000, Loss: 0.0026541478000581264, Learning Rate: 0.001124\n",
      "Epoch 1922/40000, Loss: 0.0027072192169725895, Learning Rate: 0.001124\n",
      "Epoch 1923/40000, Loss: 0.004063221625983715, Learning Rate: 0.001123\n",
      "Epoch 1924/40000, Loss: 0.004271519836038351, Learning Rate: 0.001123\n",
      "Epoch 1925/40000, Loss: 0.002134361770004034, Learning Rate: 0.001123\n",
      "Epoch 1926/40000, Loss: 0.0027390511240810156, Learning Rate: 0.001122\n",
      "Epoch 1927/40000, Loss: 0.0037350363563746214, Learning Rate: 0.001122\n",
      "Epoch 1928/40000, Loss: 0.00208079325966537, Learning Rate: 0.001122\n",
      "Epoch 1929/40000, Loss: 0.004101141355931759, Learning Rate: 0.001121\n",
      "Epoch 1930/40000, Loss: 0.0021739371586591005, Learning Rate: 0.001121\n",
      "Epoch 1931/40000, Loss: 0.0037372226361185312, Learning Rate: 0.001121\n",
      "Epoch 1932/40000, Loss: 0.003983282949775457, Learning Rate: 0.001120\n",
      "Epoch 1933/40000, Loss: 0.002672940958291292, Learning Rate: 0.001120\n",
      "Epoch 1934/40000, Loss: 0.0027042501606047153, Learning Rate: 0.001120\n",
      "Epoch 1935/40000, Loss: 0.0036842331755906343, Learning Rate: 0.001119\n",
      "Epoch 1936/40000, Loss: 0.0026373814325779676, Learning Rate: 0.001119\n",
      "Epoch 1937/40000, Loss: 0.004031721968203783, Learning Rate: 0.001119\n",
      "Epoch 1938/40000, Loss: 0.0019602328538894653, Learning Rate: 0.001118\n",
      "Epoch 1939/40000, Loss: 0.0036649920511990786, Learning Rate: 0.001118\n",
      "Epoch 1940/40000, Loss: 0.0026314619462937117, Learning Rate: 0.001118\n",
      "Epoch 1941/40000, Loss: 0.003647403558716178, Learning Rate: 0.001117\n",
      "Epoch 1942/40000, Loss: 0.0019656112417578697, Learning Rate: 0.001117\n",
      "Epoch 1943/40000, Loss: 0.001975810853764415, Learning Rate: 0.001117\n",
      "Epoch 1944/40000, Loss: 0.0026813794393092394, Learning Rate: 0.001116\n",
      "Epoch 1945/40000, Loss: 0.003676374675706029, Learning Rate: 0.001116\n",
      "Epoch 1946/40000, Loss: 0.0038185250014066696, Learning Rate: 0.001116\n",
      "Epoch 1947/40000, Loss: 0.004164522513747215, Learning Rate: 0.001115\n",
      "Epoch 1948/40000, Loss: 0.002710841130465269, Learning Rate: 0.001115\n",
      "Epoch 1949/40000, Loss: 0.004236290231347084, Learning Rate: 0.001115\n",
      "Epoch 1950/40000, Loss: 0.0021602532360702753, Learning Rate: 0.001114\n",
      "Epoch 1951/40000, Loss: 0.004076108336448669, Learning Rate: 0.001114\n",
      "Epoch 1952/40000, Loss: 0.0027100187726318836, Learning Rate: 0.001114\n",
      "Epoch 1953/40000, Loss: 0.0027519066352397203, Learning Rate: 0.001113\n",
      "Epoch 1954/40000, Loss: 0.0021007107570767403, Learning Rate: 0.001113\n",
      "Epoch 1955/40000, Loss: 0.002621329389512539, Learning Rate: 0.001113\n",
      "Epoch 1956/40000, Loss: 0.002026101341471076, Learning Rate: 0.001112\n",
      "Epoch 1957/40000, Loss: 0.003976351115852594, Learning Rate: 0.001112\n",
      "Epoch 1958/40000, Loss: 0.0036524462047964334, Learning Rate: 0.001112\n",
      "Epoch 1959/40000, Loss: 0.002043509855866432, Learning Rate: 0.001111\n",
      "Epoch 1960/40000, Loss: 0.0020542964339256287, Learning Rate: 0.001111\n",
      "Epoch 1961/40000, Loss: 0.002120786579325795, Learning Rate: 0.001111\n",
      "Epoch 1962/40000, Loss: 0.0040803770534694195, Learning Rate: 0.001110\n",
      "Epoch 1963/40000, Loss: 0.002595956204459071, Learning Rate: 0.001110\n",
      "Epoch 1964/40000, Loss: 0.004069377668201923, Learning Rate: 0.001110\n",
      "Epoch 1965/40000, Loss: 0.0041141207329928875, Learning Rate: 0.001109\n",
      "Epoch 1966/40000, Loss: 0.003972820471972227, Learning Rate: 0.001109\n",
      "Epoch 1967/40000, Loss: 0.003939570859074593, Learning Rate: 0.001109\n",
      "Epoch 1968/40000, Loss: 0.004045250825583935, Learning Rate: 0.001108\n",
      "Epoch 1969/40000, Loss: 0.004088787827640772, Learning Rate: 0.001108\n",
      "Epoch 1970/40000, Loss: 0.004063894972205162, Learning Rate: 0.001108\n",
      "Epoch 1971/40000, Loss: 0.002840424422174692, Learning Rate: 0.001107\n",
      "Epoch 1972/40000, Loss: 0.002029850147664547, Learning Rate: 0.001107\n",
      "Epoch 1973/40000, Loss: 0.0020139236003160477, Learning Rate: 0.001107\n",
      "Epoch 1974/40000, Loss: 0.00207727262750268, Learning Rate: 0.001106\n",
      "Epoch 1975/40000, Loss: 0.002154792193323374, Learning Rate: 0.001106\n",
      "Epoch 1976/40000, Loss: 0.003955077845603228, Learning Rate: 0.001106\n",
      "Epoch 1977/40000, Loss: 0.002044953405857086, Learning Rate: 0.001105\n",
      "Epoch 1978/40000, Loss: 0.002063160762190819, Learning Rate: 0.001105\n",
      "Epoch 1979/40000, Loss: 0.0020950885955244303, Learning Rate: 0.001105\n",
      "Epoch 1980/40000, Loss: 0.0021808729507029057, Learning Rate: 0.001104\n",
      "Epoch 1981/40000, Loss: 0.003704190021380782, Learning Rate: 0.001104\n",
      "Epoch 1982/40000, Loss: 0.004048959352076054, Learning Rate: 0.001104\n",
      "Epoch 1983/40000, Loss: 0.0020689242519438267, Learning Rate: 0.001103\n",
      "Epoch 1984/40000, Loss: 0.0020969845354557037, Learning Rate: 0.001103\n",
      "Epoch 1985/40000, Loss: 0.0021414223592728376, Learning Rate: 0.001103\n",
      "Epoch 1986/40000, Loss: 0.004190764855593443, Learning Rate: 0.001102\n",
      "Epoch 1987/40000, Loss: 0.00425836443901062, Learning Rate: 0.001102\n",
      "Epoch 1988/40000, Loss: 0.002124766120687127, Learning Rate: 0.001102\n",
      "Epoch 1989/40000, Loss: 0.0028268438763916492, Learning Rate: 0.001101\n",
      "Epoch 1990/40000, Loss: 0.004151845816522837, Learning Rate: 0.001101\n",
      "Epoch 1991/40000, Loss: 0.002032313495874405, Learning Rate: 0.001101\n",
      "Epoch 1992/40000, Loss: 0.004017576575279236, Learning Rate: 0.001100\n",
      "Epoch 1993/40000, Loss: 0.0019736166577786207, Learning Rate: 0.001100\n",
      "Epoch 1994/40000, Loss: 0.002063646214082837, Learning Rate: 0.001100\n",
      "Epoch 1995/40000, Loss: 0.002615888137370348, Learning Rate: 0.001099\n",
      "Epoch 1996/40000, Loss: 0.00391587195917964, Learning Rate: 0.001099\n",
      "Epoch 1997/40000, Loss: 0.0041015916503965855, Learning Rate: 0.001099\n",
      "Epoch 1998/40000, Loss: 0.0019909737166017294, Learning Rate: 0.001098\n",
      "Epoch 1999/40000, Loss: 0.0040024747140705585, Learning Rate: 0.001098\n",
      "Epoch 2000/40000, Loss: 0.0019463291391730309, Learning Rate: 0.001098\n",
      "Epoch 2001/40000, Loss: 0.002626347355544567, Learning Rate: 0.001097\n",
      "Epoch 2002/40000, Loss: 0.0020393799059093, Learning Rate: 0.001097\n",
      "Epoch 2003/40000, Loss: 0.004126662854105234, Learning Rate: 0.001097\n",
      "Epoch 2004/40000, Loss: 0.004028035327792168, Learning Rate: 0.001096\n",
      "Epoch 2005/40000, Loss: 0.003789330366998911, Learning Rate: 0.001096\n",
      "Epoch 2006/40000, Loss: 0.0026195147074759007, Learning Rate: 0.001096\n",
      "Epoch 2007/40000, Loss: 0.00399948563426733, Learning Rate: 0.001095\n",
      "Epoch 2008/40000, Loss: 0.003987717442214489, Learning Rate: 0.001095\n",
      "Epoch 2009/40000, Loss: 0.0039996374398469925, Learning Rate: 0.001095\n",
      "Epoch 2010/40000, Loss: 0.002049130154773593, Learning Rate: 0.001094\n",
      "Epoch 2011/40000, Loss: 0.002006616909056902, Learning Rate: 0.001094\n",
      "Epoch 2012/40000, Loss: 0.0026575601659715176, Learning Rate: 0.001094\n",
      "Epoch 2013/40000, Loss: 0.002035716315731406, Learning Rate: 0.001093\n",
      "Epoch 2014/40000, Loss: 0.003660894464701414, Learning Rate: 0.001093\n",
      "Epoch 2015/40000, Loss: 0.0020903158001601696, Learning Rate: 0.001093\n",
      "Epoch 2016/40000, Loss: 0.0037277708761394024, Learning Rate: 0.001092\n",
      "Epoch 2017/40000, Loss: 0.003957646898925304, Learning Rate: 0.001092\n",
      "Epoch 2018/40000, Loss: 0.0025944779627025127, Learning Rate: 0.001092\n",
      "Epoch 2019/40000, Loss: 0.0020766211673617363, Learning Rate: 0.001091\n",
      "Epoch 2020/40000, Loss: 0.003933443687856197, Learning Rate: 0.001091\n",
      "Epoch 2021/40000, Loss: 0.0026549200993031263, Learning Rate: 0.001091\n",
      "Epoch 2022/40000, Loss: 0.0039302147924900055, Learning Rate: 0.001090\n",
      "Epoch 2023/40000, Loss: 0.002020573243498802, Learning Rate: 0.001090\n",
      "Epoch 2024/40000, Loss: 0.0019967244006693363, Learning Rate: 0.001090\n",
      "Epoch 2025/40000, Loss: 0.003939568065106869, Learning Rate: 0.001089\n",
      "Epoch 2026/40000, Loss: 0.0026406310498714447, Learning Rate: 0.001089\n",
      "Epoch 2027/40000, Loss: 0.0020205234177410603, Learning Rate: 0.001089\n",
      "Epoch 2028/40000, Loss: 0.004073016345500946, Learning Rate: 0.001088\n",
      "Epoch 2029/40000, Loss: 0.002656238153576851, Learning Rate: 0.001088\n",
      "Epoch 2030/40000, Loss: 0.003685499308630824, Learning Rate: 0.001088\n",
      "Epoch 2031/40000, Loss: 0.0025879191234707832, Learning Rate: 0.001087\n",
      "Epoch 2032/40000, Loss: 0.0026310067623853683, Learning Rate: 0.001087\n",
      "Epoch 2033/40000, Loss: 0.004074875731021166, Learning Rate: 0.001087\n",
      "Epoch 2034/40000, Loss: 0.003729680087417364, Learning Rate: 0.001086\n",
      "Epoch 2035/40000, Loss: 0.003756631864234805, Learning Rate: 0.001086\n",
      "Epoch 2036/40000, Loss: 0.0025923701468855143, Learning Rate: 0.001086\n",
      "Epoch 2037/40000, Loss: 0.0037378014530986547, Learning Rate: 0.001085\n",
      "Epoch 2038/40000, Loss: 0.0021131152752786875, Learning Rate: 0.001085\n",
      "Epoch 2039/40000, Loss: 0.003940409515053034, Learning Rate: 0.001085\n",
      "Epoch 2040/40000, Loss: 0.0020873970352113247, Learning Rate: 0.001085\n",
      "Epoch 2041/40000, Loss: 0.0020227793138474226, Learning Rate: 0.001084\n",
      "Epoch 2042/40000, Loss: 0.002967755775898695, Learning Rate: 0.001084\n",
      "Epoch 2043/40000, Loss: 0.003760548075661063, Learning Rate: 0.001084\n",
      "Epoch 2044/40000, Loss: 0.0026016910560429096, Learning Rate: 0.001083\n",
      "Epoch 2045/40000, Loss: 0.002592187374830246, Learning Rate: 0.001083\n",
      "Epoch 2046/40000, Loss: 0.003673510393127799, Learning Rate: 0.001083\n",
      "Epoch 2047/40000, Loss: 0.002012983663007617, Learning Rate: 0.001082\n",
      "Epoch 2048/40000, Loss: 0.0020435075275599957, Learning Rate: 0.001082\n",
      "Epoch 2049/40000, Loss: 0.0039659361355006695, Learning Rate: 0.001082\n",
      "Epoch 2050/40000, Loss: 0.003774417331442237, Learning Rate: 0.001081\n",
      "Epoch 2051/40000, Loss: 0.0026841252110898495, Learning Rate: 0.001081\n",
      "Epoch 2052/40000, Loss: 0.00215291790664196, Learning Rate: 0.001081\n",
      "Epoch 2053/40000, Loss: 0.0022013632114976645, Learning Rate: 0.001080\n",
      "Epoch 2054/40000, Loss: 0.002057452220469713, Learning Rate: 0.001080\n",
      "Epoch 2055/40000, Loss: 0.0041349465027451515, Learning Rate: 0.001080\n",
      "Epoch 2056/40000, Loss: 0.0020212060771882534, Learning Rate: 0.001079\n",
      "Epoch 2057/40000, Loss: 0.0020472959149628878, Learning Rate: 0.001079\n",
      "Epoch 2058/40000, Loss: 0.0020764947403222322, Learning Rate: 0.001079\n",
      "Epoch 2059/40000, Loss: 0.003976360894739628, Learning Rate: 0.001078\n",
      "Epoch 2060/40000, Loss: 0.00407122727483511, Learning Rate: 0.001078\n",
      "Epoch 2061/40000, Loss: 0.0041213021613657475, Learning Rate: 0.001078\n",
      "Epoch 2062/40000, Loss: 0.0038065447006374598, Learning Rate: 0.001077\n",
      "Epoch 2063/40000, Loss: 0.0021704956889152527, Learning Rate: 0.001077\n",
      "Epoch 2064/40000, Loss: 0.0042574405670166016, Learning Rate: 0.001077\n",
      "Epoch 2065/40000, Loss: 0.0026425805408507586, Learning Rate: 0.001076\n",
      "Epoch 2066/40000, Loss: 0.004013702739030123, Learning Rate: 0.001076\n",
      "Epoch 2067/40000, Loss: 0.0020522698760032654, Learning Rate: 0.001076\n",
      "Epoch 2068/40000, Loss: 0.001962425420060754, Learning Rate: 0.001075\n",
      "Epoch 2069/40000, Loss: 0.002109269145876169, Learning Rate: 0.001075\n",
      "Epoch 2070/40000, Loss: 0.0036748088896274567, Learning Rate: 0.001075\n",
      "Epoch 2071/40000, Loss: 0.003989303484559059, Learning Rate: 0.001074\n",
      "Epoch 2072/40000, Loss: 0.003678537206724286, Learning Rate: 0.001074\n",
      "Epoch 2073/40000, Loss: 0.0026837706100195646, Learning Rate: 0.001074\n",
      "Epoch 2074/40000, Loss: 0.00266368524171412, Learning Rate: 0.001074\n",
      "Epoch 2075/40000, Loss: 0.0021128004882484674, Learning Rate: 0.001073\n",
      "Epoch 2076/40000, Loss: 0.004055581521242857, Learning Rate: 0.001073\n",
      "Epoch 2077/40000, Loss: 0.0036722926888614893, Learning Rate: 0.001073\n",
      "Epoch 2078/40000, Loss: 0.0036462803836911917, Learning Rate: 0.001072\n",
      "Epoch 2079/40000, Loss: 0.003775169840082526, Learning Rate: 0.001072\n",
      "Epoch 2080/40000, Loss: 0.003959005698561668, Learning Rate: 0.001072\n",
      "Epoch 2081/40000, Loss: 0.0037798499688506126, Learning Rate: 0.001071\n",
      "Epoch 2082/40000, Loss: 0.002029232680797577, Learning Rate: 0.001071\n",
      "Epoch 2083/40000, Loss: 0.0036708246916532516, Learning Rate: 0.001071\n",
      "Epoch 2084/40000, Loss: 0.0020235730335116386, Learning Rate: 0.001070\n",
      "Epoch 2085/40000, Loss: 0.003918385598808527, Learning Rate: 0.001070\n",
      "Epoch 2086/40000, Loss: 0.003978373482823372, Learning Rate: 0.001070\n",
      "Epoch 2087/40000, Loss: 0.0038851448334753513, Learning Rate: 0.001069\n",
      "Epoch 2088/40000, Loss: 0.0019503905205056071, Learning Rate: 0.001069\n",
      "Epoch 2089/40000, Loss: 0.0039474754594266415, Learning Rate: 0.001069\n",
      "Epoch 2090/40000, Loss: 0.003936119377613068, Learning Rate: 0.001068\n",
      "Epoch 2091/40000, Loss: 0.0037528714165091515, Learning Rate: 0.001068\n",
      "Epoch 2092/40000, Loss: 0.0019269956974312663, Learning Rate: 0.001068\n",
      "Epoch 2093/40000, Loss: 0.003892053384333849, Learning Rate: 0.001067\n",
      "Epoch 2094/40000, Loss: 0.002652691211551428, Learning Rate: 0.001067\n",
      "Epoch 2095/40000, Loss: 0.0019143283134326339, Learning Rate: 0.001067\n",
      "Epoch 2096/40000, Loss: 0.0019517390755936503, Learning Rate: 0.001066\n",
      "Epoch 2097/40000, Loss: 0.0019693539943546057, Learning Rate: 0.001066\n",
      "Epoch 2098/40000, Loss: 0.0037622724194079638, Learning Rate: 0.001066\n",
      "Epoch 2099/40000, Loss: 0.004073472693562508, Learning Rate: 0.001065\n",
      "Epoch 2100/40000, Loss: 0.002133477246388793, Learning Rate: 0.001065\n",
      "Epoch 2101/40000, Loss: 0.004000749904662371, Learning Rate: 0.001065\n",
      "Epoch 2102/40000, Loss: 0.0036508538760244846, Learning Rate: 0.001065\n",
      "Epoch 2103/40000, Loss: 0.002601786283776164, Learning Rate: 0.001064\n",
      "Epoch 2104/40000, Loss: 0.0041527533903717995, Learning Rate: 0.001064\n",
      "Epoch 2105/40000, Loss: 0.0019602980464696884, Learning Rate: 0.001064\n",
      "Epoch 2106/40000, Loss: 0.0020443694666028023, Learning Rate: 0.001063\n",
      "Epoch 2107/40000, Loss: 0.003956789616495371, Learning Rate: 0.001063\n",
      "Epoch 2108/40000, Loss: 0.0019446356454864144, Learning Rate: 0.001063\n",
      "Epoch 2109/40000, Loss: 0.003930756822228432, Learning Rate: 0.001062\n",
      "Epoch 2110/40000, Loss: 0.0026070435997098684, Learning Rate: 0.001062\n",
      "Epoch 2111/40000, Loss: 0.0026023646350950003, Learning Rate: 0.001062\n",
      "Epoch 2112/40000, Loss: 0.004032074008136988, Learning Rate: 0.001061\n",
      "Epoch 2113/40000, Loss: 0.0020427992567420006, Learning Rate: 0.001061\n",
      "Epoch 2114/40000, Loss: 0.0041217682883143425, Learning Rate: 0.001061\n",
      "Epoch 2115/40000, Loss: 0.002681722631677985, Learning Rate: 0.001060\n",
      "Epoch 2116/40000, Loss: 0.0021204142831265926, Learning Rate: 0.001060\n",
      "Epoch 2117/40000, Loss: 0.002171466127038002, Learning Rate: 0.001060\n",
      "Epoch 2118/40000, Loss: 0.0036662553902715445, Learning Rate: 0.001059\n",
      "Epoch 2119/40000, Loss: 0.004021633882075548, Learning Rate: 0.001059\n",
      "Epoch 2120/40000, Loss: 0.00271103554405272, Learning Rate: 0.001059\n",
      "Epoch 2121/40000, Loss: 0.0036703080404549837, Learning Rate: 0.001058\n",
      "Epoch 2122/40000, Loss: 0.00266679422929883, Learning Rate: 0.001058\n",
      "Epoch 2123/40000, Loss: 0.003989892546087503, Learning Rate: 0.001058\n",
      "Epoch 2124/40000, Loss: 0.0026992126367986202, Learning Rate: 0.001058\n",
      "Epoch 2125/40000, Loss: 0.0019978901837021112, Learning Rate: 0.001057\n",
      "Epoch 2126/40000, Loss: 0.0021303053945302963, Learning Rate: 0.001057\n",
      "Epoch 2127/40000, Loss: 0.0021085762418806553, Learning Rate: 0.001057\n",
      "Epoch 2128/40000, Loss: 0.003964847419410944, Learning Rate: 0.001056\n",
      "Epoch 2129/40000, Loss: 0.002235235646367073, Learning Rate: 0.001056\n",
      "Epoch 2130/40000, Loss: 0.0040133679285645485, Learning Rate: 0.001056\n",
      "Epoch 2131/40000, Loss: 0.004052324686199427, Learning Rate: 0.001055\n",
      "Epoch 2132/40000, Loss: 0.004074852913618088, Learning Rate: 0.001055\n",
      "Epoch 2133/40000, Loss: 0.0020066204015165567, Learning Rate: 0.001055\n",
      "Epoch 2134/40000, Loss: 0.0036655841395258904, Learning Rate: 0.001054\n",
      "Epoch 2135/40000, Loss: 0.003940943628549576, Learning Rate: 0.001054\n",
      "Epoch 2136/40000, Loss: 0.003994655795395374, Learning Rate: 0.001054\n",
      "Epoch 2137/40000, Loss: 0.0036684854421764612, Learning Rate: 0.001053\n",
      "Epoch 2138/40000, Loss: 0.0019956259056925774, Learning Rate: 0.001053\n",
      "Epoch 2139/40000, Loss: 0.0019526013638824224, Learning Rate: 0.001053\n",
      "Epoch 2140/40000, Loss: 0.001999144908040762, Learning Rate: 0.001052\n",
      "Epoch 2141/40000, Loss: 0.0020326951052993536, Learning Rate: 0.001052\n",
      "Epoch 2142/40000, Loss: 0.003994090482592583, Learning Rate: 0.001052\n",
      "Epoch 2143/40000, Loss: 0.002097212476655841, Learning Rate: 0.001052\n",
      "Epoch 2144/40000, Loss: 0.00393779668956995, Learning Rate: 0.001051\n",
      "Epoch 2145/40000, Loss: 0.003641863353550434, Learning Rate: 0.001051\n",
      "Epoch 2146/40000, Loss: 0.004015585407614708, Learning Rate: 0.001051\n",
      "Epoch 2147/40000, Loss: 0.00398803036659956, Learning Rate: 0.001050\n",
      "Epoch 2148/40000, Loss: 0.0025570406578481197, Learning Rate: 0.001050\n",
      "Epoch 2149/40000, Loss: 0.0026588940527290106, Learning Rate: 0.001050\n",
      "Epoch 2150/40000, Loss: 0.0036467299796640873, Learning Rate: 0.001049\n",
      "Epoch 2151/40000, Loss: 0.003924597054719925, Learning Rate: 0.001049\n",
      "Epoch 2152/40000, Loss: 0.0025906278751790524, Learning Rate: 0.001049\n",
      "Epoch 2153/40000, Loss: 0.003920785617083311, Learning Rate: 0.001048\n",
      "Epoch 2154/40000, Loss: 0.002641657367348671, Learning Rate: 0.001048\n",
      "Epoch 2155/40000, Loss: 0.0036326826084405184, Learning Rate: 0.001048\n",
      "Epoch 2156/40000, Loss: 0.002052185358479619, Learning Rate: 0.001047\n",
      "Epoch 2157/40000, Loss: 0.002082773018628359, Learning Rate: 0.001047\n",
      "Epoch 2158/40000, Loss: 0.0019699642434716225, Learning Rate: 0.001047\n",
      "Epoch 2159/40000, Loss: 0.003789617447182536, Learning Rate: 0.001046\n",
      "Epoch 2160/40000, Loss: 0.0020442502573132515, Learning Rate: 0.001046\n",
      "Epoch 2161/40000, Loss: 0.003706222865730524, Learning Rate: 0.001046\n",
      "Epoch 2162/40000, Loss: 0.004048999864608049, Learning Rate: 0.001046\n",
      "Epoch 2163/40000, Loss: 0.002156931674107909, Learning Rate: 0.001045\n",
      "Epoch 2164/40000, Loss: 0.003948613069951534, Learning Rate: 0.001045\n",
      "Epoch 2165/40000, Loss: 0.0020400909706950188, Learning Rate: 0.001045\n",
      "Epoch 2166/40000, Loss: 0.0036482205614447594, Learning Rate: 0.001044\n",
      "Epoch 2167/40000, Loss: 0.004077156540006399, Learning Rate: 0.001044\n",
      "Epoch 2168/40000, Loss: 0.002032341668382287, Learning Rate: 0.001044\n",
      "Epoch 2169/40000, Loss: 0.002074470045045018, Learning Rate: 0.001043\n",
      "Epoch 2170/40000, Loss: 0.002606539987027645, Learning Rate: 0.001043\n",
      "Epoch 2171/40000, Loss: 0.0036476156674325466, Learning Rate: 0.001043\n",
      "Epoch 2172/40000, Loss: 0.0037075108848512173, Learning Rate: 0.001042\n",
      "Epoch 2173/40000, Loss: 0.0020304645877331495, Learning Rate: 0.001042\n",
      "Epoch 2174/40000, Loss: 0.003989949822425842, Learning Rate: 0.001042\n",
      "Epoch 2175/40000, Loss: 0.00210048770532012, Learning Rate: 0.001041\n",
      "Epoch 2176/40000, Loss: 0.003702379995957017, Learning Rate: 0.001041\n",
      "Epoch 2177/40000, Loss: 0.003984667360782623, Learning Rate: 0.001041\n",
      "Epoch 2178/40000, Loss: 0.002174822147935629, Learning Rate: 0.001041\n",
      "Epoch 2179/40000, Loss: 0.003944530617445707, Learning Rate: 0.001040\n",
      "Epoch 2180/40000, Loss: 0.003656857879832387, Learning Rate: 0.001040\n",
      "Epoch 2181/40000, Loss: 0.003990255296230316, Learning Rate: 0.001040\n",
      "Epoch 2182/40000, Loss: 0.0036834264174103737, Learning Rate: 0.001039\n",
      "Epoch 2183/40000, Loss: 0.004115914460271597, Learning Rate: 0.001039\n",
      "Epoch 2184/40000, Loss: 0.004023710731416941, Learning Rate: 0.001039\n",
      "Epoch 2185/40000, Loss: 0.001955878920853138, Learning Rate: 0.001038\n",
      "Epoch 2186/40000, Loss: 0.0019618982914835215, Learning Rate: 0.001038\n",
      "Epoch 2187/40000, Loss: 0.002055447781458497, Learning Rate: 0.001038\n",
      "Epoch 2188/40000, Loss: 0.0036116077098995447, Learning Rate: 0.001037\n",
      "Epoch 2189/40000, Loss: 0.003655204549431801, Learning Rate: 0.001037\n",
      "Epoch 2190/40000, Loss: 0.0036964835599064827, Learning Rate: 0.001037\n",
      "Epoch 2191/40000, Loss: 0.0019254196668043733, Learning Rate: 0.001036\n",
      "Epoch 2192/40000, Loss: 0.00258999434299767, Learning Rate: 0.001036\n",
      "Epoch 2193/40000, Loss: 0.003979573957622051, Learning Rate: 0.001036\n",
      "Epoch 2194/40000, Loss: 0.0026764338836073875, Learning Rate: 0.001036\n",
      "Epoch 2195/40000, Loss: 0.003942510578781366, Learning Rate: 0.001035\n",
      "Epoch 2196/40000, Loss: 0.004094671458005905, Learning Rate: 0.001035\n",
      "Epoch 2197/40000, Loss: 0.0020142749417573214, Learning Rate: 0.001035\n",
      "Epoch 2198/40000, Loss: 0.00194323412142694, Learning Rate: 0.001034\n",
      "Epoch 2199/40000, Loss: 0.002031896496191621, Learning Rate: 0.001034\n",
      "Epoch 2200/40000, Loss: 0.0020427496638149023, Learning Rate: 0.001034\n",
      "Epoch 2201/40000, Loss: 0.003624254371970892, Learning Rate: 0.001033\n",
      "Epoch 2202/40000, Loss: 0.003942806739360094, Learning Rate: 0.001033\n",
      "Epoch 2203/40000, Loss: 0.003640813985839486, Learning Rate: 0.001033\n",
      "Epoch 2204/40000, Loss: 0.0020186142064630985, Learning Rate: 0.001032\n",
      "Epoch 2205/40000, Loss: 0.002004445530474186, Learning Rate: 0.001032\n",
      "Epoch 2206/40000, Loss: 0.0020284168422222137, Learning Rate: 0.001032\n",
      "Epoch 2207/40000, Loss: 0.0020592601504176855, Learning Rate: 0.001032\n",
      "Epoch 2208/40000, Loss: 0.00200834427960217, Learning Rate: 0.001031\n",
      "Epoch 2209/40000, Loss: 0.0036316474433988333, Learning Rate: 0.001031\n",
      "Epoch 2210/40000, Loss: 0.0020877078641206026, Learning Rate: 0.001031\n",
      "Epoch 2211/40000, Loss: 0.0039724078960716724, Learning Rate: 0.001030\n",
      "Epoch 2212/40000, Loss: 0.003597929375246167, Learning Rate: 0.001030\n",
      "Epoch 2213/40000, Loss: 0.0019079879857599735, Learning Rate: 0.001030\n",
      "Epoch 2214/40000, Loss: 0.0025588476564735174, Learning Rate: 0.001029\n",
      "Epoch 2215/40000, Loss: 0.001922343042679131, Learning Rate: 0.001029\n",
      "Epoch 2216/40000, Loss: 0.0025721630081534386, Learning Rate: 0.001029\n",
      "Epoch 2217/40000, Loss: 0.001971386605873704, Learning Rate: 0.001028\n",
      "Epoch 2218/40000, Loss: 0.003955848049372435, Learning Rate: 0.001028\n",
      "Epoch 2219/40000, Loss: 0.003750614123418927, Learning Rate: 0.001028\n",
      "Epoch 2220/40000, Loss: 0.004121243022382259, Learning Rate: 0.001028\n",
      "Epoch 2221/40000, Loss: 0.0036505835596472025, Learning Rate: 0.001027\n",
      "Epoch 2222/40000, Loss: 0.0025330800563097, Learning Rate: 0.001027\n",
      "Epoch 2223/40000, Loss: 0.002537500113248825, Learning Rate: 0.001027\n",
      "Epoch 2224/40000, Loss: 0.003983113449066877, Learning Rate: 0.001026\n",
      "Epoch 2225/40000, Loss: 0.00400692131370306, Learning Rate: 0.001026\n",
      "Epoch 2226/40000, Loss: 0.004274583887308836, Learning Rate: 0.001026\n",
      "Epoch 2227/40000, Loss: 0.00397898256778717, Learning Rate: 0.001025\n",
      "Epoch 2228/40000, Loss: 0.0036377033684402704, Learning Rate: 0.001025\n",
      "Epoch 2229/40000, Loss: 0.0036008127499371767, Learning Rate: 0.001025\n",
      "Epoch 2230/40000, Loss: 0.0019571043085306883, Learning Rate: 0.001024\n",
      "Epoch 2231/40000, Loss: 0.0021052267402410507, Learning Rate: 0.001024\n",
      "Epoch 2232/40000, Loss: 0.003910352010279894, Learning Rate: 0.001024\n",
      "Epoch 2233/40000, Loss: 0.004024806898087263, Learning Rate: 0.001024\n",
      "Epoch 2234/40000, Loss: 0.0025489756371825933, Learning Rate: 0.001023\n",
      "Epoch 2235/40000, Loss: 0.0038922273088246584, Learning Rate: 0.001023\n",
      "Epoch 2236/40000, Loss: 0.003986955154687166, Learning Rate: 0.001023\n",
      "Epoch 2237/40000, Loss: 0.0027044047601521015, Learning Rate: 0.001022\n",
      "Epoch 2238/40000, Loss: 0.0039758444763720036, Learning Rate: 0.001022\n",
      "Epoch 2239/40000, Loss: 0.003907572478055954, Learning Rate: 0.001022\n",
      "Epoch 2240/40000, Loss: 0.003938389476388693, Learning Rate: 0.001021\n",
      "Epoch 2241/40000, Loss: 0.0019345051841810346, Learning Rate: 0.001021\n",
      "Epoch 2242/40000, Loss: 0.002054870128631592, Learning Rate: 0.001021\n",
      "Epoch 2243/40000, Loss: 0.0020545159932225943, Learning Rate: 0.001020\n",
      "Epoch 2244/40000, Loss: 0.004066376946866512, Learning Rate: 0.001020\n",
      "Epoch 2245/40000, Loss: 0.0036487614270299673, Learning Rate: 0.001020\n",
      "Epoch 2246/40000, Loss: 0.0019937376491725445, Learning Rate: 0.001020\n",
      "Epoch 2247/40000, Loss: 0.0019505290547385812, Learning Rate: 0.001019\n",
      "Epoch 2248/40000, Loss: 0.001995250815525651, Learning Rate: 0.001019\n",
      "Epoch 2249/40000, Loss: 0.0039436002261936665, Learning Rate: 0.001019\n",
      "Epoch 2250/40000, Loss: 0.001978050684556365, Learning Rate: 0.001018\n",
      "Epoch 2251/40000, Loss: 0.002589065581560135, Learning Rate: 0.001018\n",
      "Epoch 2252/40000, Loss: 0.0038723479956388474, Learning Rate: 0.001018\n",
      "Epoch 2253/40000, Loss: 0.0038892822340130806, Learning Rate: 0.001017\n",
      "Epoch 2254/40000, Loss: 0.0042063333094120026, Learning Rate: 0.001017\n",
      "Epoch 2255/40000, Loss: 0.002594485180452466, Learning Rate: 0.001017\n",
      "Epoch 2256/40000, Loss: 0.0026328070089221, Learning Rate: 0.001016\n",
      "Epoch 2257/40000, Loss: 0.001915684319101274, Learning Rate: 0.001016\n",
      "Epoch 2258/40000, Loss: 0.003995843697339296, Learning Rate: 0.001016\n",
      "Epoch 2259/40000, Loss: 0.003604032564908266, Learning Rate: 0.001016\n",
      "Epoch 2260/40000, Loss: 0.0019875371363013983, Learning Rate: 0.001015\n",
      "Epoch 2261/40000, Loss: 0.003984240349382162, Learning Rate: 0.001015\n",
      "Epoch 2262/40000, Loss: 0.0025564737152308226, Learning Rate: 0.001015\n",
      "Epoch 2263/40000, Loss: 0.003959298599511385, Learning Rate: 0.001014\n",
      "Epoch 2264/40000, Loss: 0.004003212321549654, Learning Rate: 0.001014\n",
      "Epoch 2265/40000, Loss: 0.003628720762208104, Learning Rate: 0.001014\n",
      "Epoch 2266/40000, Loss: 0.003707589814439416, Learning Rate: 0.001013\n",
      "Epoch 2267/40000, Loss: 0.004015006124973297, Learning Rate: 0.001013\n",
      "Epoch 2268/40000, Loss: 0.003777544479817152, Learning Rate: 0.001013\n",
      "Epoch 2269/40000, Loss: 0.004011150449514389, Learning Rate: 0.001013\n",
      "Epoch 2270/40000, Loss: 0.002688588574528694, Learning Rate: 0.001012\n",
      "Epoch 2271/40000, Loss: 0.0021549651864916086, Learning Rate: 0.001012\n",
      "Epoch 2272/40000, Loss: 0.003923135809600353, Learning Rate: 0.001012\n",
      "Epoch 2273/40000, Loss: 0.001994939986616373, Learning Rate: 0.001011\n",
      "Epoch 2274/40000, Loss: 0.002052205614745617, Learning Rate: 0.001011\n",
      "Epoch 2275/40000, Loss: 0.0036054360680282116, Learning Rate: 0.001011\n",
      "Epoch 2276/40000, Loss: 0.003861562581732869, Learning Rate: 0.001010\n",
      "Epoch 2277/40000, Loss: 0.0026548337191343307, Learning Rate: 0.001010\n",
      "Epoch 2278/40000, Loss: 0.003911649342626333, Learning Rate: 0.001010\n",
      "Epoch 2279/40000, Loss: 0.003960782196372747, Learning Rate: 0.001009\n",
      "Epoch 2280/40000, Loss: 0.002005710732191801, Learning Rate: 0.001009\n",
      "Epoch 2281/40000, Loss: 0.002624054905027151, Learning Rate: 0.001009\n",
      "Epoch 2282/40000, Loss: 0.003946326207369566, Learning Rate: 0.001009\n",
      "Epoch 2283/40000, Loss: 0.00392334908246994, Learning Rate: 0.001008\n",
      "Epoch 2284/40000, Loss: 0.004110312554985285, Learning Rate: 0.001008\n",
      "Epoch 2285/40000, Loss: 0.002131051616743207, Learning Rate: 0.001008\n",
      "Epoch 2286/40000, Loss: 0.0039611137472093105, Learning Rate: 0.001007\n",
      "Epoch 2287/40000, Loss: 0.003923513926565647, Learning Rate: 0.001007\n",
      "Epoch 2288/40000, Loss: 0.004026690497994423, Learning Rate: 0.001007\n",
      "Epoch 2289/40000, Loss: 0.0036063732113689184, Learning Rate: 0.001006\n",
      "Epoch 2290/40000, Loss: 0.002010094467550516, Learning Rate: 0.001006\n",
      "Epoch 2291/40000, Loss: 0.001980964792892337, Learning Rate: 0.001006\n",
      "Epoch 2292/40000, Loss: 0.0019870437681674957, Learning Rate: 0.001006\n",
      "Epoch 2293/40000, Loss: 0.0022036649752408266, Learning Rate: 0.001005\n",
      "Epoch 2294/40000, Loss: 0.0020406334660947323, Learning Rate: 0.001005\n",
      "Epoch 2295/40000, Loss: 0.00208080280572176, Learning Rate: 0.001005\n",
      "Epoch 2296/40000, Loss: 0.0019993213936686516, Learning Rate: 0.001004\n",
      "Epoch 2297/40000, Loss: 0.0021897212136536837, Learning Rate: 0.001004\n",
      "Epoch 2298/40000, Loss: 0.002585464622825384, Learning Rate: 0.001004\n",
      "Epoch 2299/40000, Loss: 0.002667794469743967, Learning Rate: 0.001003\n",
      "Epoch 2300/40000, Loss: 0.004309690557420254, Learning Rate: 0.001003\n",
      "Epoch 2301/40000, Loss: 0.0022939355112612247, Learning Rate: 0.001003\n",
      "Epoch 2302/40000, Loss: 0.002711473498493433, Learning Rate: 0.001003\n",
      "Epoch 2303/40000, Loss: 0.0025661743711680174, Learning Rate: 0.001002\n",
      "Epoch 2304/40000, Loss: 0.004043475724756718, Learning Rate: 0.001002\n",
      "Epoch 2305/40000, Loss: 0.002825448289513588, Learning Rate: 0.001002\n",
      "Epoch 2306/40000, Loss: 0.0039893863722682, Learning Rate: 0.001001\n",
      "Epoch 2307/40000, Loss: 0.002655562711879611, Learning Rate: 0.001001\n",
      "Epoch 2308/40000, Loss: 0.002034673700109124, Learning Rate: 0.001001\n",
      "Epoch 2309/40000, Loss: 0.00203649140894413, Learning Rate: 0.001000\n",
      "Epoch 2310/40000, Loss: 0.0025715769734233618, Learning Rate: 0.001000\n",
      "Epoch 2311/40000, Loss: 0.001969371922314167, Learning Rate: 0.001000\n",
      "Epoch 2312/40000, Loss: 0.0039120027795434, Learning Rate: 0.001000\n",
      "Epoch 2313/40000, Loss: 0.004031555727124214, Learning Rate: 0.000999\n",
      "Epoch 2314/40000, Loss: 0.002567203249782324, Learning Rate: 0.000999\n",
      "Epoch 2315/40000, Loss: 0.0019650571048259735, Learning Rate: 0.000999\n",
      "Epoch 2316/40000, Loss: 0.001957687083631754, Learning Rate: 0.000998\n",
      "Epoch 2317/40000, Loss: 0.0025790270883589983, Learning Rate: 0.000998\n",
      "Epoch 2318/40000, Loss: 0.0036836371291428804, Learning Rate: 0.000998\n",
      "Epoch 2319/40000, Loss: 0.004028747323900461, Learning Rate: 0.000997\n",
      "Epoch 2320/40000, Loss: 0.0025420167949050665, Learning Rate: 0.000997\n",
      "Epoch 2321/40000, Loss: 0.004034554585814476, Learning Rate: 0.000997\n",
      "Epoch 2322/40000, Loss: 0.0039316932670772076, Learning Rate: 0.000997\n",
      "Epoch 2323/40000, Loss: 0.0021260902285575867, Learning Rate: 0.000996\n",
      "Epoch 2324/40000, Loss: 0.003910766914486885, Learning Rate: 0.000996\n",
      "Epoch 2325/40000, Loss: 0.0019525910029187799, Learning Rate: 0.000996\n",
      "Epoch 2326/40000, Loss: 0.003996421582996845, Learning Rate: 0.000995\n",
      "Epoch 2327/40000, Loss: 0.0022395506966859102, Learning Rate: 0.000995\n",
      "Epoch 2328/40000, Loss: 0.004138872027397156, Learning Rate: 0.000995\n",
      "Epoch 2329/40000, Loss: 0.002076069824397564, Learning Rate: 0.000994\n",
      "Epoch 2330/40000, Loss: 0.002096352633088827, Learning Rate: 0.000994\n",
      "Epoch 2331/40000, Loss: 0.003916626330465078, Learning Rate: 0.000994\n",
      "Epoch 2332/40000, Loss: 0.002115336712449789, Learning Rate: 0.000994\n",
      "Epoch 2333/40000, Loss: 0.003928057383745909, Learning Rate: 0.000993\n",
      "Epoch 2334/40000, Loss: 0.0020083412528038025, Learning Rate: 0.000993\n",
      "Epoch 2335/40000, Loss: 0.003611921798437834, Learning Rate: 0.000993\n",
      "Epoch 2336/40000, Loss: 0.004002296831458807, Learning Rate: 0.000992\n",
      "Epoch 2337/40000, Loss: 0.00403662770986557, Learning Rate: 0.000992\n",
      "Epoch 2338/40000, Loss: 0.0036047915928065777, Learning Rate: 0.000992\n",
      "Epoch 2339/40000, Loss: 0.0019786693155765533, Learning Rate: 0.000991\n",
      "Epoch 2340/40000, Loss: 0.003896943759173155, Learning Rate: 0.000991\n",
      "Epoch 2341/40000, Loss: 0.0020499168895184994, Learning Rate: 0.000991\n",
      "Epoch 2342/40000, Loss: 0.004034097772091627, Learning Rate: 0.000991\n",
      "Epoch 2343/40000, Loss: 0.002054305514320731, Learning Rate: 0.000990\n",
      "Epoch 2344/40000, Loss: 0.003931019455194473, Learning Rate: 0.000990\n",
      "Epoch 2345/40000, Loss: 0.003927326761186123, Learning Rate: 0.000990\n",
      "Epoch 2346/40000, Loss: 0.001976568950340152, Learning Rate: 0.000989\n",
      "Epoch 2347/40000, Loss: 0.001994773978367448, Learning Rate: 0.000989\n",
      "Epoch 2348/40000, Loss: 0.003593663452193141, Learning Rate: 0.000989\n",
      "Epoch 2349/40000, Loss: 0.003884214675053954, Learning Rate: 0.000988\n",
      "Epoch 2350/40000, Loss: 0.0026472839526832104, Learning Rate: 0.000988\n",
      "Epoch 2351/40000, Loss: 0.002023739507421851, Learning Rate: 0.000988\n",
      "Epoch 2352/40000, Loss: 0.0035708597861230373, Learning Rate: 0.000988\n",
      "Epoch 2353/40000, Loss: 0.0038695638068020344, Learning Rate: 0.000987\n",
      "Epoch 2354/40000, Loss: 0.003911883570253849, Learning Rate: 0.000987\n",
      "Epoch 2355/40000, Loss: 0.00358391716144979, Learning Rate: 0.000987\n",
      "Epoch 2356/40000, Loss: 0.0038871143478900194, Learning Rate: 0.000986\n",
      "Epoch 2357/40000, Loss: 0.0039014273788779974, Learning Rate: 0.000986\n",
      "Epoch 2358/40000, Loss: 0.001916022738441825, Learning Rate: 0.000986\n",
      "Epoch 2359/40000, Loss: 0.002530395518988371, Learning Rate: 0.000986\n",
      "Epoch 2360/40000, Loss: 0.003898743772879243, Learning Rate: 0.000985\n",
      "Epoch 2361/40000, Loss: 0.003591863438487053, Learning Rate: 0.000985\n",
      "Epoch 2362/40000, Loss: 0.001975338440388441, Learning Rate: 0.000985\n",
      "Epoch 2363/40000, Loss: 0.0019711144268512726, Learning Rate: 0.000984\n",
      "Epoch 2364/40000, Loss: 0.0019655211362987757, Learning Rate: 0.000984\n",
      "Epoch 2365/40000, Loss: 0.0026761884801089764, Learning Rate: 0.000984\n",
      "Epoch 2366/40000, Loss: 0.0019424506463110447, Learning Rate: 0.000983\n",
      "Epoch 2367/40000, Loss: 0.003916171845048666, Learning Rate: 0.000983\n",
      "Epoch 2368/40000, Loss: 0.003579597221687436, Learning Rate: 0.000983\n",
      "Epoch 2369/40000, Loss: 0.0039145927876234055, Learning Rate: 0.000983\n",
      "Epoch 2370/40000, Loss: 0.001990653807297349, Learning Rate: 0.000982\n",
      "Epoch 2371/40000, Loss: 0.0019383345497772098, Learning Rate: 0.000982\n",
      "Epoch 2372/40000, Loss: 0.003864737693220377, Learning Rate: 0.000982\n",
      "Epoch 2373/40000, Loss: 0.0019519333727657795, Learning Rate: 0.000981\n",
      "Epoch 2374/40000, Loss: 0.0039630248211324215, Learning Rate: 0.000981\n",
      "Epoch 2375/40000, Loss: 0.0018993545090779662, Learning Rate: 0.000981\n",
      "Epoch 2376/40000, Loss: 0.0020118707325309515, Learning Rate: 0.000981\n",
      "Epoch 2377/40000, Loss: 0.0019698443356901407, Learning Rate: 0.000980\n",
      "Epoch 2378/40000, Loss: 0.002645581029355526, Learning Rate: 0.000980\n",
      "Epoch 2379/40000, Loss: 0.003902584547176957, Learning Rate: 0.000980\n",
      "Epoch 2380/40000, Loss: 0.004127741325646639, Learning Rate: 0.000979\n",
      "Epoch 2381/40000, Loss: 0.0026766490191221237, Learning Rate: 0.000979\n",
      "Epoch 2382/40000, Loss: 0.003928922116756439, Learning Rate: 0.000979\n",
      "Epoch 2383/40000, Loss: 0.0019191676983609796, Learning Rate: 0.000978\n",
      "Epoch 2384/40000, Loss: 0.003974507097154856, Learning Rate: 0.000978\n",
      "Epoch 2385/40000, Loss: 0.0036518031265586615, Learning Rate: 0.000978\n",
      "Epoch 2386/40000, Loss: 0.002056772354990244, Learning Rate: 0.000978\n",
      "Epoch 2387/40000, Loss: 0.003890309249982238, Learning Rate: 0.000977\n",
      "Epoch 2388/40000, Loss: 0.002585394075140357, Learning Rate: 0.000977\n",
      "Epoch 2389/40000, Loss: 0.0039037626702338457, Learning Rate: 0.000977\n",
      "Epoch 2390/40000, Loss: 0.0039365957491099834, Learning Rate: 0.000976\n",
      "Epoch 2391/40000, Loss: 0.0039267828688025475, Learning Rate: 0.000976\n",
      "Epoch 2392/40000, Loss: 0.00199179002083838, Learning Rate: 0.000976\n",
      "Epoch 2393/40000, Loss: 0.002076825825497508, Learning Rate: 0.000976\n",
      "Epoch 2394/40000, Loss: 0.0026446706615388393, Learning Rate: 0.000975\n",
      "Epoch 2395/40000, Loss: 0.00403287447988987, Learning Rate: 0.000975\n",
      "Epoch 2396/40000, Loss: 0.0038842535577714443, Learning Rate: 0.000975\n",
      "Epoch 2397/40000, Loss: 0.001964116469025612, Learning Rate: 0.000974\n",
      "Epoch 2398/40000, Loss: 0.0035886180121451616, Learning Rate: 0.000974\n",
      "Epoch 2399/40000, Loss: 0.003977482207119465, Learning Rate: 0.000974\n",
      "Epoch 2400/40000, Loss: 0.0020492617040872574, Learning Rate: 0.000973\n",
      "Epoch 2401/40000, Loss: 0.0019356980919837952, Learning Rate: 0.000973\n",
      "Epoch 2402/40000, Loss: 0.003598687704652548, Learning Rate: 0.000973\n",
      "Epoch 2403/40000, Loss: 0.00262370309792459, Learning Rate: 0.000973\n",
      "Epoch 2404/40000, Loss: 0.0025551035068929195, Learning Rate: 0.000972\n",
      "Epoch 2405/40000, Loss: 0.0037528234533965588, Learning Rate: 0.000972\n",
      "Epoch 2406/40000, Loss: 0.0023422210942953825, Learning Rate: 0.000972\n",
      "Epoch 2407/40000, Loss: 0.004037862177938223, Learning Rate: 0.000971\n",
      "Epoch 2408/40000, Loss: 0.002142156707122922, Learning Rate: 0.000971\n",
      "Epoch 2409/40000, Loss: 0.002121222671121359, Learning Rate: 0.000971\n",
      "Epoch 2410/40000, Loss: 0.003923704847693443, Learning Rate: 0.000971\n",
      "Epoch 2411/40000, Loss: 0.00427884329110384, Learning Rate: 0.000970\n",
      "Epoch 2412/40000, Loss: 0.0020351449493318796, Learning Rate: 0.000970\n",
      "Epoch 2413/40000, Loss: 0.004081439226865768, Learning Rate: 0.000970\n",
      "Epoch 2414/40000, Loss: 0.0035936273634433746, Learning Rate: 0.000969\n",
      "Epoch 2415/40000, Loss: 0.00393718620762229, Learning Rate: 0.000969\n",
      "Epoch 2416/40000, Loss: 0.004043797496706247, Learning Rate: 0.000969\n",
      "Epoch 2417/40000, Loss: 0.003578201401978731, Learning Rate: 0.000969\n",
      "Epoch 2418/40000, Loss: 0.0038988967426121235, Learning Rate: 0.000968\n",
      "Epoch 2419/40000, Loss: 0.0025896034203469753, Learning Rate: 0.000968\n",
      "Epoch 2420/40000, Loss: 0.003928338177502155, Learning Rate: 0.000968\n",
      "Epoch 2421/40000, Loss: 0.0025710042100399733, Learning Rate: 0.000967\n",
      "Epoch 2422/40000, Loss: 0.0038882719818502665, Learning Rate: 0.000967\n",
      "Epoch 2423/40000, Loss: 0.004018810577690601, Learning Rate: 0.000967\n",
      "Epoch 2424/40000, Loss: 0.004059929400682449, Learning Rate: 0.000967\n",
      "Epoch 2425/40000, Loss: 0.0019630652386695147, Learning Rate: 0.000966\n",
      "Epoch 2426/40000, Loss: 0.001953615341335535, Learning Rate: 0.000966\n",
      "Epoch 2427/40000, Loss: 0.002029599156230688, Learning Rate: 0.000966\n",
      "Epoch 2428/40000, Loss: 0.0025507044047117233, Learning Rate: 0.000965\n",
      "Epoch 2429/40000, Loss: 0.002040700288489461, Learning Rate: 0.000965\n",
      "Epoch 2430/40000, Loss: 0.002632385352626443, Learning Rate: 0.000965\n",
      "Epoch 2431/40000, Loss: 0.003931037150323391, Learning Rate: 0.000964\n",
      "Epoch 2432/40000, Loss: 0.003987804055213928, Learning Rate: 0.000964\n",
      "Epoch 2433/40000, Loss: 0.003980634734034538, Learning Rate: 0.000964\n",
      "Epoch 2434/40000, Loss: 0.003916843794286251, Learning Rate: 0.000964\n",
      "Epoch 2435/40000, Loss: 0.002692732261493802, Learning Rate: 0.000963\n",
      "Epoch 2436/40000, Loss: 0.0025617266073822975, Learning Rate: 0.000963\n",
      "Epoch 2437/40000, Loss: 0.0025503700599074364, Learning Rate: 0.000963\n",
      "Epoch 2438/40000, Loss: 0.00290517951361835, Learning Rate: 0.000962\n",
      "Epoch 2439/40000, Loss: 0.0026206723414361477, Learning Rate: 0.000962\n",
      "Epoch 2440/40000, Loss: 0.003947125282138586, Learning Rate: 0.000962\n",
      "Epoch 2441/40000, Loss: 0.002020637970417738, Learning Rate: 0.000962\n",
      "Epoch 2442/40000, Loss: 0.002159831579774618, Learning Rate: 0.000961\n",
      "Epoch 2443/40000, Loss: 0.002115837763994932, Learning Rate: 0.000961\n",
      "Epoch 2444/40000, Loss: 0.0036836036015301943, Learning Rate: 0.000961\n",
      "Epoch 2445/40000, Loss: 0.001981124049052596, Learning Rate: 0.000960\n",
      "Epoch 2446/40000, Loss: 0.0020441117230802774, Learning Rate: 0.000960\n",
      "Epoch 2447/40000, Loss: 0.002545461291447282, Learning Rate: 0.000960\n",
      "Epoch 2448/40000, Loss: 0.003965057898312807, Learning Rate: 0.000960\n",
      "Epoch 2449/40000, Loss: 0.0020086499862372875, Learning Rate: 0.000959\n",
      "Epoch 2450/40000, Loss: 0.003945327829569578, Learning Rate: 0.000959\n",
      "Epoch 2451/40000, Loss: 0.0019377003191038966, Learning Rate: 0.000959\n",
      "Epoch 2452/40000, Loss: 0.0019659572280943394, Learning Rate: 0.000958\n",
      "Epoch 2453/40000, Loss: 0.003617466427385807, Learning Rate: 0.000958\n",
      "Epoch 2454/40000, Loss: 0.0038747957441955805, Learning Rate: 0.000958\n",
      "Epoch 2455/40000, Loss: 0.0019511028658598661, Learning Rate: 0.000958\n",
      "Epoch 2456/40000, Loss: 0.004008179064840078, Learning Rate: 0.000957\n",
      "Epoch 2457/40000, Loss: 0.003959119785577059, Learning Rate: 0.000957\n",
      "Epoch 2458/40000, Loss: 0.004039636347442865, Learning Rate: 0.000957\n",
      "Epoch 2459/40000, Loss: 0.0019945327658206224, Learning Rate: 0.000956\n",
      "Epoch 2460/40000, Loss: 0.0020318366587162018, Learning Rate: 0.000956\n",
      "Epoch 2461/40000, Loss: 0.002001402201130986, Learning Rate: 0.000956\n",
      "Epoch 2462/40000, Loss: 0.002012968296185136, Learning Rate: 0.000956\n",
      "Epoch 2463/40000, Loss: 0.0038550051394850016, Learning Rate: 0.000955\n",
      "Epoch 2464/40000, Loss: 0.003561836900189519, Learning Rate: 0.000955\n",
      "Epoch 2465/40000, Loss: 0.00195854133926332, Learning Rate: 0.000955\n",
      "Epoch 2466/40000, Loss: 0.0038287064526230097, Learning Rate: 0.000954\n",
      "Epoch 2467/40000, Loss: 0.003888106904923916, Learning Rate: 0.000954\n",
      "Epoch 2468/40000, Loss: 0.0020470903255045414, Learning Rate: 0.000954\n",
      "Epoch 2469/40000, Loss: 0.0019620016682893038, Learning Rate: 0.000954\n",
      "Epoch 2470/40000, Loss: 0.0026809866540133953, Learning Rate: 0.000953\n",
      "Epoch 2471/40000, Loss: 0.003972453996539116, Learning Rate: 0.000953\n",
      "Epoch 2472/40000, Loss: 0.0039968350902199745, Learning Rate: 0.000953\n",
      "Epoch 2473/40000, Loss: 0.004046888090670109, Learning Rate: 0.000952\n",
      "Epoch 2474/40000, Loss: 0.0019514424493536353, Learning Rate: 0.000952\n",
      "Epoch 2475/40000, Loss: 0.0038459135685116053, Learning Rate: 0.000952\n",
      "Epoch 2476/40000, Loss: 0.0039053219370543957, Learning Rate: 0.000952\n",
      "Epoch 2477/40000, Loss: 0.001895619323477149, Learning Rate: 0.000951\n",
      "Epoch 2478/40000, Loss: 0.0025931624695658684, Learning Rate: 0.000951\n",
      "Epoch 2479/40000, Loss: 0.0020525965373963118, Learning Rate: 0.000951\n",
      "Epoch 2480/40000, Loss: 0.003865231992676854, Learning Rate: 0.000950\n",
      "Epoch 2481/40000, Loss: 0.004163294564932585, Learning Rate: 0.000950\n",
      "Epoch 2482/40000, Loss: 0.004095570649951696, Learning Rate: 0.000950\n",
      "Epoch 2483/40000, Loss: 0.003618059679865837, Learning Rate: 0.000950\n",
      "Epoch 2484/40000, Loss: 0.004112273454666138, Learning Rate: 0.000949\n",
      "Epoch 2485/40000, Loss: 0.004036652389913797, Learning Rate: 0.000949\n",
      "Epoch 2486/40000, Loss: 0.003930636215955019, Learning Rate: 0.000949\n",
      "Epoch 2487/40000, Loss: 0.0041783652268350124, Learning Rate: 0.000948\n",
      "Epoch 2488/40000, Loss: 0.002544376766309142, Learning Rate: 0.000948\n",
      "Epoch 2489/40000, Loss: 0.0020580380223691463, Learning Rate: 0.000948\n",
      "Epoch 2490/40000, Loss: 0.0018791081383824348, Learning Rate: 0.000948\n",
      "Epoch 2491/40000, Loss: 0.0036511202342808247, Learning Rate: 0.000947\n",
      "Epoch 2492/40000, Loss: 0.0038204609882086515, Learning Rate: 0.000947\n",
      "Epoch 2493/40000, Loss: 0.002559274435043335, Learning Rate: 0.000947\n",
      "Epoch 2494/40000, Loss: 0.0019071956630796194, Learning Rate: 0.000946\n",
      "Epoch 2495/40000, Loss: 0.0038525036070495844, Learning Rate: 0.000946\n",
      "Epoch 2496/40000, Loss: 0.002521669724956155, Learning Rate: 0.000946\n",
      "Epoch 2497/40000, Loss: 0.0019684340804815292, Learning Rate: 0.000946\n",
      "Epoch 2498/40000, Loss: 0.001986890332773328, Learning Rate: 0.000945\n",
      "Epoch 2499/40000, Loss: 0.003922532312572002, Learning Rate: 0.000945\n",
      "Epoch 2500/40000, Loss: 0.0025317473337054253, Learning Rate: 0.000945\n",
      "Epoch 2501/40000, Loss: 0.0038374171126633883, Learning Rate: 0.000944\n",
      "Epoch 2502/40000, Loss: 0.0018806036096066236, Learning Rate: 0.000944\n",
      "Epoch 2503/40000, Loss: 0.003916115500032902, Learning Rate: 0.000944\n",
      "Epoch 2504/40000, Loss: 0.003952586557716131, Learning Rate: 0.000944\n",
      "Epoch 2505/40000, Loss: 0.0038873665034770966, Learning Rate: 0.000943\n",
      "Epoch 2506/40000, Loss: 0.0036217712331563234, Learning Rate: 0.000943\n",
      "Epoch 2507/40000, Loss: 0.003980153705924749, Learning Rate: 0.000943\n",
      "Epoch 2508/40000, Loss: 0.00269812042824924, Learning Rate: 0.000942\n",
      "Epoch 2509/40000, Loss: 0.002049598377197981, Learning Rate: 0.000942\n",
      "Epoch 2510/40000, Loss: 0.004006674513220787, Learning Rate: 0.000942\n",
      "Epoch 2511/40000, Loss: 0.0021088700741529465, Learning Rate: 0.000942\n",
      "Epoch 2512/40000, Loss: 0.0035755732096731663, Learning Rate: 0.000941\n",
      "Epoch 2513/40000, Loss: 0.003844460705295205, Learning Rate: 0.000941\n",
      "Epoch 2514/40000, Loss: 0.0037207244895398617, Learning Rate: 0.000941\n",
      "Epoch 2515/40000, Loss: 0.003859588410705328, Learning Rate: 0.000940\n",
      "Epoch 2516/40000, Loss: 0.0039059496484696865, Learning Rate: 0.000940\n",
      "Epoch 2517/40000, Loss: 0.0019093191949650645, Learning Rate: 0.000940\n",
      "Epoch 2518/40000, Loss: 0.003841840196400881, Learning Rate: 0.000940\n",
      "Epoch 2519/40000, Loss: 0.0020149576012045145, Learning Rate: 0.000939\n",
      "Epoch 2520/40000, Loss: 0.003839376149699092, Learning Rate: 0.000939\n",
      "Epoch 2521/40000, Loss: 0.0025573037564754486, Learning Rate: 0.000939\n",
      "Epoch 2522/40000, Loss: 0.0038812810089439154, Learning Rate: 0.000939\n",
      "Epoch 2523/40000, Loss: 0.0036654348950833082, Learning Rate: 0.000938\n",
      "Epoch 2524/40000, Loss: 0.0039005326107144356, Learning Rate: 0.000938\n",
      "Epoch 2525/40000, Loss: 0.0035391799174249172, Learning Rate: 0.000938\n",
      "Epoch 2526/40000, Loss: 0.0019330789800733328, Learning Rate: 0.000937\n",
      "Epoch 2527/40000, Loss: 0.001981070265173912, Learning Rate: 0.000937\n",
      "Epoch 2528/40000, Loss: 0.001980795757845044, Learning Rate: 0.000937\n",
      "Epoch 2529/40000, Loss: 0.003586637554690242, Learning Rate: 0.000937\n",
      "Epoch 2530/40000, Loss: 0.00391031987965107, Learning Rate: 0.000936\n",
      "Epoch 2531/40000, Loss: 0.003786905435845256, Learning Rate: 0.000936\n",
      "Epoch 2532/40000, Loss: 0.0039354125037789345, Learning Rate: 0.000936\n",
      "Epoch 2533/40000, Loss: 0.0018762018298730254, Learning Rate: 0.000935\n",
      "Epoch 2534/40000, Loss: 0.0019853985868394375, Learning Rate: 0.000935\n",
      "Epoch 2535/40000, Loss: 0.0021586923394352198, Learning Rate: 0.000935\n",
      "Epoch 2536/40000, Loss: 0.002554139820858836, Learning Rate: 0.000935\n",
      "Epoch 2537/40000, Loss: 0.0021830028854310513, Learning Rate: 0.000934\n",
      "Epoch 2538/40000, Loss: 0.0038603167049586773, Learning Rate: 0.000934\n",
      "Epoch 2539/40000, Loss: 0.003603527322411537, Learning Rate: 0.000934\n",
      "Epoch 2540/40000, Loss: 0.002023630542680621, Learning Rate: 0.000933\n",
      "Epoch 2541/40000, Loss: 0.002523857867345214, Learning Rate: 0.000933\n",
      "Epoch 2542/40000, Loss: 0.0025508722756057978, Learning Rate: 0.000933\n",
      "Epoch 2543/40000, Loss: 0.003817495424300432, Learning Rate: 0.000933\n",
      "Epoch 2544/40000, Loss: 0.003550000488758087, Learning Rate: 0.000932\n",
      "Epoch 2545/40000, Loss: 0.003890787251293659, Learning Rate: 0.000932\n",
      "Epoch 2546/40000, Loss: 0.003987965639680624, Learning Rate: 0.000932\n",
      "Epoch 2547/40000, Loss: 0.002004166366532445, Learning Rate: 0.000931\n",
      "Epoch 2548/40000, Loss: 0.0019034388242289424, Learning Rate: 0.000931\n",
      "Epoch 2549/40000, Loss: 0.001972852274775505, Learning Rate: 0.000931\n",
      "Epoch 2550/40000, Loss: 0.002507145283743739, Learning Rate: 0.000931\n",
      "Epoch 2551/40000, Loss: 0.0025627880822867155, Learning Rate: 0.000930\n",
      "Epoch 2552/40000, Loss: 0.003944437485188246, Learning Rate: 0.000930\n",
      "Epoch 2553/40000, Loss: 0.0025341876316815615, Learning Rate: 0.000930\n",
      "Epoch 2554/40000, Loss: 0.0019488873658701777, Learning Rate: 0.000930\n",
      "Epoch 2555/40000, Loss: 0.0018751403549686074, Learning Rate: 0.000929\n",
      "Epoch 2556/40000, Loss: 0.0019941285718232393, Learning Rate: 0.000929\n",
      "Epoch 2557/40000, Loss: 0.003937549889087677, Learning Rate: 0.000929\n",
      "Epoch 2558/40000, Loss: 0.003903654171153903, Learning Rate: 0.000928\n",
      "Epoch 2559/40000, Loss: 0.002545677125453949, Learning Rate: 0.000928\n",
      "Epoch 2560/40000, Loss: 0.0019005895592272282, Learning Rate: 0.000928\n",
      "Epoch 2561/40000, Loss: 0.003916717134416103, Learning Rate: 0.000928\n",
      "Epoch 2562/40000, Loss: 0.002565306844189763, Learning Rate: 0.000927\n",
      "Epoch 2563/40000, Loss: 0.003653099061921239, Learning Rate: 0.000927\n",
      "Epoch 2564/40000, Loss: 0.0018746248679235578, Learning Rate: 0.000927\n",
      "Epoch 2565/40000, Loss: 0.0020524244755506516, Learning Rate: 0.000926\n",
      "Epoch 2566/40000, Loss: 0.002010507509112358, Learning Rate: 0.000926\n",
      "Epoch 2567/40000, Loss: 0.002504384610801935, Learning Rate: 0.000926\n",
      "Epoch 2568/40000, Loss: 0.004094497766345739, Learning Rate: 0.000926\n",
      "Epoch 2569/40000, Loss: 0.004056967329233885, Learning Rate: 0.000925\n",
      "Epoch 2570/40000, Loss: 0.004149311687797308, Learning Rate: 0.000925\n",
      "Epoch 2571/40000, Loss: 0.0020013193134218454, Learning Rate: 0.000925\n",
      "Epoch 2572/40000, Loss: 0.0035631272476166487, Learning Rate: 0.000925\n",
      "Epoch 2573/40000, Loss: 0.0019008229719474912, Learning Rate: 0.000924\n",
      "Epoch 2574/40000, Loss: 0.003929540514945984, Learning Rate: 0.000924\n",
      "Epoch 2575/40000, Loss: 0.00409675482660532, Learning Rate: 0.000924\n",
      "Epoch 2576/40000, Loss: 0.0035771136172115803, Learning Rate: 0.000923\n",
      "Epoch 2577/40000, Loss: 0.003650982165709138, Learning Rate: 0.000923\n",
      "Epoch 2578/40000, Loss: 0.0037170518189668655, Learning Rate: 0.000923\n",
      "Epoch 2579/40000, Loss: 0.004002096131443977, Learning Rate: 0.000923\n",
      "Epoch 2580/40000, Loss: 0.0024793650954961777, Learning Rate: 0.000922\n",
      "Epoch 2581/40000, Loss: 0.0025695248041301966, Learning Rate: 0.000922\n",
      "Epoch 2582/40000, Loss: 0.0038332457188516855, Learning Rate: 0.000922\n",
      "Epoch 2583/40000, Loss: 0.001875595422461629, Learning Rate: 0.000921\n",
      "Epoch 2584/40000, Loss: 0.0036437108647078276, Learning Rate: 0.000921\n",
      "Epoch 2585/40000, Loss: 0.0019392863614484668, Learning Rate: 0.000921\n",
      "Epoch 2586/40000, Loss: 0.002514643594622612, Learning Rate: 0.000921\n",
      "Epoch 2587/40000, Loss: 0.0036022246349602938, Learning Rate: 0.000920\n",
      "Epoch 2588/40000, Loss: 0.0038249825593084097, Learning Rate: 0.000920\n",
      "Epoch 2589/40000, Loss: 0.0019217473454773426, Learning Rate: 0.000920\n",
      "Epoch 2590/40000, Loss: 0.003920773975551128, Learning Rate: 0.000920\n",
      "Epoch 2591/40000, Loss: 0.003915729001164436, Learning Rate: 0.000919\n",
      "Epoch 2592/40000, Loss: 0.001884334720671177, Learning Rate: 0.000919\n",
      "Epoch 2593/40000, Loss: 0.0019401799654588103, Learning Rate: 0.000919\n",
      "Epoch 2594/40000, Loss: 0.0035518358927220106, Learning Rate: 0.000918\n",
      "Epoch 2595/40000, Loss: 0.0035861621145159006, Learning Rate: 0.000918\n",
      "Epoch 2596/40000, Loss: 0.0019192666513845325, Learning Rate: 0.000918\n",
      "Epoch 2597/40000, Loss: 0.0025527020916342735, Learning Rate: 0.000918\n",
      "Epoch 2598/40000, Loss: 0.0025284583680331707, Learning Rate: 0.000917\n",
      "Epoch 2599/40000, Loss: 0.002058418933302164, Learning Rate: 0.000917\n",
      "Epoch 2600/40000, Loss: 0.0035361377522349358, Learning Rate: 0.000917\n",
      "Epoch 2601/40000, Loss: 0.0038486917037516832, Learning Rate: 0.000917\n",
      "Epoch 2602/40000, Loss: 0.0038088669534772635, Learning Rate: 0.000916\n",
      "Epoch 2603/40000, Loss: 0.0039047575555741787, Learning Rate: 0.000916\n",
      "Epoch 2604/40000, Loss: 0.00399186834692955, Learning Rate: 0.000916\n",
      "Epoch 2605/40000, Loss: 0.0037671804893761873, Learning Rate: 0.000915\n",
      "Epoch 2606/40000, Loss: 0.004086633212864399, Learning Rate: 0.000915\n",
      "Epoch 2607/40000, Loss: 0.0022184315603226423, Learning Rate: 0.000915\n",
      "Epoch 2608/40000, Loss: 0.0038855192251503468, Learning Rate: 0.000915\n",
      "Epoch 2609/40000, Loss: 0.004042473156005144, Learning Rate: 0.000914\n",
      "Epoch 2610/40000, Loss: 0.004140069242566824, Learning Rate: 0.000914\n",
      "Epoch 2611/40000, Loss: 0.004119161982089281, Learning Rate: 0.000914\n",
      "Epoch 2612/40000, Loss: 0.00197233515791595, Learning Rate: 0.000913\n",
      "Epoch 2613/40000, Loss: 0.0038601267151534557, Learning Rate: 0.000913\n",
      "Epoch 2614/40000, Loss: 0.003950333222746849, Learning Rate: 0.000913\n",
      "Epoch 2615/40000, Loss: 0.0025895906146615744, Learning Rate: 0.000913\n",
      "Epoch 2616/40000, Loss: 0.003914904315024614, Learning Rate: 0.000912\n",
      "Epoch 2617/40000, Loss: 0.0019380805315449834, Learning Rate: 0.000912\n",
      "Epoch 2618/40000, Loss: 0.003528106026351452, Learning Rate: 0.000912\n",
      "Epoch 2619/40000, Loss: 0.00205672113224864, Learning Rate: 0.000912\n",
      "Epoch 2620/40000, Loss: 0.0024955628905445337, Learning Rate: 0.000911\n",
      "Epoch 2621/40000, Loss: 0.0026402256917208433, Learning Rate: 0.000911\n",
      "Epoch 2622/40000, Loss: 0.0018803395796567202, Learning Rate: 0.000911\n",
      "Epoch 2623/40000, Loss: 0.0025083194486796856, Learning Rate: 0.000910\n",
      "Epoch 2624/40000, Loss: 0.0035352862905710936, Learning Rate: 0.000910\n",
      "Epoch 2625/40000, Loss: 0.001909203827381134, Learning Rate: 0.000910\n",
      "Epoch 2626/40000, Loss: 0.0024643349461257458, Learning Rate: 0.000910\n",
      "Epoch 2627/40000, Loss: 0.0035397373139858246, Learning Rate: 0.000909\n",
      "Epoch 2628/40000, Loss: 0.001892994623631239, Learning Rate: 0.000909\n",
      "Epoch 2629/40000, Loss: 0.003816233715042472, Learning Rate: 0.000909\n",
      "Epoch 2630/40000, Loss: 0.0024832836352288723, Learning Rate: 0.000909\n",
      "Epoch 2631/40000, Loss: 0.003532739821821451, Learning Rate: 0.000908\n",
      "Epoch 2632/40000, Loss: 0.0025347599294036627, Learning Rate: 0.000908\n",
      "Epoch 2633/40000, Loss: 0.003893214976415038, Learning Rate: 0.000908\n",
      "Epoch 2634/40000, Loss: 0.00409160740673542, Learning Rate: 0.000907\n",
      "Epoch 2635/40000, Loss: 0.00400701817125082, Learning Rate: 0.000907\n",
      "Epoch 2636/40000, Loss: 0.0020217285491526127, Learning Rate: 0.000907\n",
      "Epoch 2637/40000, Loss: 0.002087811240926385, Learning Rate: 0.000907\n",
      "Epoch 2638/40000, Loss: 0.0027736439369618893, Learning Rate: 0.000906\n",
      "Epoch 2639/40000, Loss: 0.0041004931554198265, Learning Rate: 0.000906\n",
      "Epoch 2640/40000, Loss: 0.001915689092129469, Learning Rate: 0.000906\n",
      "Epoch 2641/40000, Loss: 0.003979171626269817, Learning Rate: 0.000906\n",
      "Epoch 2642/40000, Loss: 0.003930911887437105, Learning Rate: 0.000905\n",
      "Epoch 2643/40000, Loss: 0.002626011846587062, Learning Rate: 0.000905\n",
      "Epoch 2644/40000, Loss: 0.001954444916918874, Learning Rate: 0.000905\n",
      "Epoch 2645/40000, Loss: 0.00261191138997674, Learning Rate: 0.000905\n",
      "Epoch 2646/40000, Loss: 0.002020357409492135, Learning Rate: 0.000904\n",
      "Epoch 2647/40000, Loss: 0.0036133849062025547, Learning Rate: 0.000904\n",
      "Epoch 2648/40000, Loss: 0.0020103980787098408, Learning Rate: 0.000904\n",
      "Epoch 2649/40000, Loss: 0.0024819921236485243, Learning Rate: 0.000903\n",
      "Epoch 2650/40000, Loss: 0.002587302587926388, Learning Rate: 0.000903\n",
      "Epoch 2651/40000, Loss: 0.0035571029875427485, Learning Rate: 0.000903\n",
      "Epoch 2652/40000, Loss: 0.0037605685647577047, Learning Rate: 0.000903\n",
      "Epoch 2653/40000, Loss: 0.0020121142733842134, Learning Rate: 0.000902\n",
      "Epoch 2654/40000, Loss: 0.0019342022715136409, Learning Rate: 0.000902\n",
      "Epoch 2655/40000, Loss: 0.0019848961383104324, Learning Rate: 0.000902\n",
      "Epoch 2656/40000, Loss: 0.0038318654987961054, Learning Rate: 0.000902\n",
      "Epoch 2657/40000, Loss: 0.002518465043976903, Learning Rate: 0.000901\n",
      "Epoch 2658/40000, Loss: 0.004008847754448652, Learning Rate: 0.000901\n",
      "Epoch 2659/40000, Loss: 0.004069237504154444, Learning Rate: 0.000901\n",
      "Epoch 2660/40000, Loss: 0.0038640606217086315, Learning Rate: 0.000900\n",
      "Epoch 2661/40000, Loss: 0.0026347958482801914, Learning Rate: 0.000900\n",
      "Epoch 2662/40000, Loss: 0.003833552123978734, Learning Rate: 0.000900\n",
      "Epoch 2663/40000, Loss: 0.00248106406070292, Learning Rate: 0.000900\n",
      "Epoch 2664/40000, Loss: 0.0019423336489126086, Learning Rate: 0.000899\n",
      "Epoch 2665/40000, Loss: 0.0025423720944672823, Learning Rate: 0.000899\n",
      "Epoch 2666/40000, Loss: 0.0019521302310749888, Learning Rate: 0.000899\n",
      "Epoch 2667/40000, Loss: 0.0020403300877660513, Learning Rate: 0.000899\n",
      "Epoch 2668/40000, Loss: 0.0025202345568686724, Learning Rate: 0.000898\n",
      "Epoch 2669/40000, Loss: 0.0035634352825582027, Learning Rate: 0.000898\n",
      "Epoch 2670/40000, Loss: 0.003924861084669828, Learning Rate: 0.000898\n",
      "Epoch 2671/40000, Loss: 0.003868673462420702, Learning Rate: 0.000897\n",
      "Epoch 2672/40000, Loss: 0.003948265686631203, Learning Rate: 0.000897\n",
      "Epoch 2673/40000, Loss: 0.001963031478226185, Learning Rate: 0.000897\n",
      "Epoch 2674/40000, Loss: 0.0019319988787174225, Learning Rate: 0.000897\n",
      "Epoch 2675/40000, Loss: 0.0019371991511434317, Learning Rate: 0.000896\n",
      "Epoch 2676/40000, Loss: 0.0024997733999043703, Learning Rate: 0.000896\n",
      "Epoch 2677/40000, Loss: 0.002614630851894617, Learning Rate: 0.000896\n",
      "Epoch 2678/40000, Loss: 0.0020597565453499556, Learning Rate: 0.000896\n",
      "Epoch 2679/40000, Loss: 0.003735944628715515, Learning Rate: 0.000895\n",
      "Epoch 2680/40000, Loss: 0.0024930627550929785, Learning Rate: 0.000895\n",
      "Epoch 2681/40000, Loss: 0.003907882142812014, Learning Rate: 0.000895\n",
      "Epoch 2682/40000, Loss: 0.0037284388672560453, Learning Rate: 0.000895\n",
      "Epoch 2683/40000, Loss: 0.003583411918953061, Learning Rate: 0.000894\n",
      "Epoch 2684/40000, Loss: 0.0039604222401976585, Learning Rate: 0.000894\n",
      "Epoch 2685/40000, Loss: 0.0019694510847330093, Learning Rate: 0.000894\n",
      "Epoch 2686/40000, Loss: 0.0026375644374638796, Learning Rate: 0.000893\n",
      "Epoch 2687/40000, Loss: 0.003970839083194733, Learning Rate: 0.000893\n",
      "Epoch 2688/40000, Loss: 0.003645771648734808, Learning Rate: 0.000893\n",
      "Epoch 2689/40000, Loss: 0.0018836769741028547, Learning Rate: 0.000893\n",
      "Epoch 2690/40000, Loss: 0.003558955155313015, Learning Rate: 0.000892\n",
      "Epoch 2691/40000, Loss: 0.0035922802053391933, Learning Rate: 0.000892\n",
      "Epoch 2692/40000, Loss: 0.0038461810909211636, Learning Rate: 0.000892\n",
      "Epoch 2693/40000, Loss: 0.0038827629759907722, Learning Rate: 0.000892\n",
      "Epoch 2694/40000, Loss: 0.0024585984647274017, Learning Rate: 0.000891\n",
      "Epoch 2695/40000, Loss: 0.0038794991560280323, Learning Rate: 0.000891\n",
      "Epoch 2696/40000, Loss: 0.004088434856384993, Learning Rate: 0.000891\n",
      "Epoch 2697/40000, Loss: 0.002092131646350026, Learning Rate: 0.000890\n",
      "Epoch 2698/40000, Loss: 0.003838250180706382, Learning Rate: 0.000890\n",
      "Epoch 2699/40000, Loss: 0.004095320124179125, Learning Rate: 0.000890\n",
      "Epoch 2700/40000, Loss: 0.00400663772597909, Learning Rate: 0.000890\n",
      "Epoch 2701/40000, Loss: 0.003800968872383237, Learning Rate: 0.000889\n",
      "Epoch 2702/40000, Loss: 0.002022310858592391, Learning Rate: 0.000889\n",
      "Epoch 2703/40000, Loss: 0.0025032286066561937, Learning Rate: 0.000889\n",
      "Epoch 2704/40000, Loss: 0.0019035483710467815, Learning Rate: 0.000889\n",
      "Epoch 2705/40000, Loss: 0.003628719365224242, Learning Rate: 0.000888\n",
      "Epoch 2706/40000, Loss: 0.003805335611104965, Learning Rate: 0.000888\n",
      "Epoch 2707/40000, Loss: 0.0039782291278243065, Learning Rate: 0.000888\n",
      "Epoch 2708/40000, Loss: 0.003984861075878143, Learning Rate: 0.000888\n",
      "Epoch 2709/40000, Loss: 0.001991849159821868, Learning Rate: 0.000887\n",
      "Epoch 2710/40000, Loss: 0.0036580744199454784, Learning Rate: 0.000887\n",
      "Epoch 2711/40000, Loss: 0.0021211362909525633, Learning Rate: 0.000887\n",
      "Epoch 2712/40000, Loss: 0.0025343268644064665, Learning Rate: 0.000887\n",
      "Epoch 2713/40000, Loss: 0.003857346484437585, Learning Rate: 0.000886\n",
      "Epoch 2714/40000, Loss: 0.004003186244517565, Learning Rate: 0.000886\n",
      "Epoch 2715/40000, Loss: 0.0018840094562619925, Learning Rate: 0.000886\n",
      "Epoch 2716/40000, Loss: 0.003567726584151387, Learning Rate: 0.000885\n",
      "Epoch 2717/40000, Loss: 0.0019519648049026728, Learning Rate: 0.000885\n",
      "Epoch 2718/40000, Loss: 0.003839254379272461, Learning Rate: 0.000885\n",
      "Epoch 2719/40000, Loss: 0.0039541530422866344, Learning Rate: 0.000885\n",
      "Epoch 2720/40000, Loss: 0.0025010767858475447, Learning Rate: 0.000884\n",
      "Epoch 2721/40000, Loss: 0.0038269474171102047, Learning Rate: 0.000884\n",
      "Epoch 2722/40000, Loss: 0.003959765657782555, Learning Rate: 0.000884\n",
      "Epoch 2723/40000, Loss: 0.0018963729962706566, Learning Rate: 0.000884\n",
      "Epoch 2724/40000, Loss: 0.0025331764481961727, Learning Rate: 0.000883\n",
      "Epoch 2725/40000, Loss: 0.003853089874610305, Learning Rate: 0.000883\n",
      "Epoch 2726/40000, Loss: 0.0019757694099098444, Learning Rate: 0.000883\n",
      "Epoch 2727/40000, Loss: 0.002458831761032343, Learning Rate: 0.000883\n",
      "Epoch 2728/40000, Loss: 0.0018385287839919329, Learning Rate: 0.000882\n",
      "Epoch 2729/40000, Loss: 0.0038448066916316748, Learning Rate: 0.000882\n",
      "Epoch 2730/40000, Loss: 0.002059811493381858, Learning Rate: 0.000882\n",
      "Epoch 2731/40000, Loss: 0.0021339673548936844, Learning Rate: 0.000881\n",
      "Epoch 2732/40000, Loss: 0.0019312059739604592, Learning Rate: 0.000881\n",
      "Epoch 2733/40000, Loss: 0.0019416713621467352, Learning Rate: 0.000881\n",
      "Epoch 2734/40000, Loss: 0.0035998427774757147, Learning Rate: 0.000881\n",
      "Epoch 2735/40000, Loss: 0.002478770911693573, Learning Rate: 0.000880\n",
      "Epoch 2736/40000, Loss: 0.0037816455587744713, Learning Rate: 0.000880\n",
      "Epoch 2737/40000, Loss: 0.0038487310521304607, Learning Rate: 0.000880\n",
      "Epoch 2738/40000, Loss: 0.003829892724752426, Learning Rate: 0.000880\n",
      "Epoch 2739/40000, Loss: 0.002488103462383151, Learning Rate: 0.000879\n",
      "Epoch 2740/40000, Loss: 0.0036133569665253162, Learning Rate: 0.000879\n",
      "Epoch 2741/40000, Loss: 0.0035373761784285307, Learning Rate: 0.000879\n",
      "Epoch 2742/40000, Loss: 0.0025034593418240547, Learning Rate: 0.000879\n",
      "Epoch 2743/40000, Loss: 0.00190812221262604, Learning Rate: 0.000878\n",
      "Epoch 2744/40000, Loss: 0.0035513711627572775, Learning Rate: 0.000878\n",
      "Epoch 2745/40000, Loss: 0.003907713573426008, Learning Rate: 0.000878\n",
      "Epoch 2746/40000, Loss: 0.0026936426293104887, Learning Rate: 0.000878\n",
      "Epoch 2747/40000, Loss: 0.003869251813739538, Learning Rate: 0.000877\n",
      "Epoch 2748/40000, Loss: 0.0018929161597043276, Learning Rate: 0.000877\n",
      "Epoch 2749/40000, Loss: 0.0025939345359802246, Learning Rate: 0.000877\n",
      "Epoch 2750/40000, Loss: 0.0021913230884820223, Learning Rate: 0.000876\n",
      "Epoch 2751/40000, Loss: 0.003630541730672121, Learning Rate: 0.000876\n",
      "Epoch 2752/40000, Loss: 0.0019412727560847998, Learning Rate: 0.000876\n",
      "Epoch 2753/40000, Loss: 0.004078545607626438, Learning Rate: 0.000876\n",
      "Epoch 2754/40000, Loss: 0.003988882526755333, Learning Rate: 0.000875\n",
      "Epoch 2755/40000, Loss: 0.0024890871718525887, Learning Rate: 0.000875\n",
      "Epoch 2756/40000, Loss: 0.0018917281413450837, Learning Rate: 0.000875\n",
      "Epoch 2757/40000, Loss: 0.002099865349009633, Learning Rate: 0.000875\n",
      "Epoch 2758/40000, Loss: 0.003932025749236345, Learning Rate: 0.000874\n",
      "Epoch 2759/40000, Loss: 0.002524039940908551, Learning Rate: 0.000874\n",
      "Epoch 2760/40000, Loss: 0.003905392484739423, Learning Rate: 0.000874\n",
      "Epoch 2761/40000, Loss: 0.003911606036126614, Learning Rate: 0.000874\n",
      "Epoch 2762/40000, Loss: 0.0019326061010360718, Learning Rate: 0.000873\n",
      "Epoch 2763/40000, Loss: 0.0024587647058069706, Learning Rate: 0.000873\n",
      "Epoch 2764/40000, Loss: 0.00381077011115849, Learning Rate: 0.000873\n",
      "Epoch 2765/40000, Loss: 0.0019403593614697456, Learning Rate: 0.000873\n",
      "Epoch 2766/40000, Loss: 0.0035275109112262726, Learning Rate: 0.000872\n",
      "Epoch 2767/40000, Loss: 0.0037879575975239277, Learning Rate: 0.000872\n",
      "Epoch 2768/40000, Loss: 0.002494778484106064, Learning Rate: 0.000872\n",
      "Epoch 2769/40000, Loss: 0.0038014710880815983, Learning Rate: 0.000871\n",
      "Epoch 2770/40000, Loss: 0.003888916689902544, Learning Rate: 0.000871\n",
      "Epoch 2771/40000, Loss: 0.0018561944598332047, Learning Rate: 0.000871\n",
      "Epoch 2772/40000, Loss: 0.0024758779909461737, Learning Rate: 0.000871\n",
      "Epoch 2773/40000, Loss: 0.003807127010077238, Learning Rate: 0.000870\n",
      "Epoch 2774/40000, Loss: 0.0025682393461465836, Learning Rate: 0.000870\n",
      "Epoch 2775/40000, Loss: 0.003940103575587273, Learning Rate: 0.000870\n",
      "Epoch 2776/40000, Loss: 0.0025251638144254684, Learning Rate: 0.000870\n",
      "Epoch 2777/40000, Loss: 0.003808292793110013, Learning Rate: 0.000869\n",
      "Epoch 2778/40000, Loss: 0.0019622687250375748, Learning Rate: 0.000869\n",
      "Epoch 2779/40000, Loss: 0.0035262638702988625, Learning Rate: 0.000869\n",
      "Epoch 2780/40000, Loss: 0.0019572691526263952, Learning Rate: 0.000869\n",
      "Epoch 2781/40000, Loss: 0.0035842328798025846, Learning Rate: 0.000868\n",
      "Epoch 2782/40000, Loss: 0.0019998308271169662, Learning Rate: 0.000868\n",
      "Epoch 2783/40000, Loss: 0.0025467174127697945, Learning Rate: 0.000868\n",
      "Epoch 2784/40000, Loss: 0.001993178855627775, Learning Rate: 0.000868\n",
      "Epoch 2785/40000, Loss: 0.003941232338547707, Learning Rate: 0.000867\n",
      "Epoch 2786/40000, Loss: 0.003928008489310741, Learning Rate: 0.000867\n",
      "Epoch 2787/40000, Loss: 0.0019415536662563682, Learning Rate: 0.000867\n",
      "Epoch 2788/40000, Loss: 0.0025323855224996805, Learning Rate: 0.000867\n",
      "Epoch 2789/40000, Loss: 0.001969186356291175, Learning Rate: 0.000866\n",
      "Epoch 2790/40000, Loss: 0.001972484402358532, Learning Rate: 0.000866\n",
      "Epoch 2791/40000, Loss: 0.0038569029420614243, Learning Rate: 0.000866\n",
      "Epoch 2792/40000, Loss: 0.0037722080014646053, Learning Rate: 0.000865\n",
      "Epoch 2793/40000, Loss: 0.0035590960178524256, Learning Rate: 0.000865\n",
      "Epoch 2794/40000, Loss: 0.0019276122329756618, Learning Rate: 0.000865\n",
      "Epoch 2795/40000, Loss: 0.0024885181337594986, Learning Rate: 0.000865\n",
      "Epoch 2796/40000, Loss: 0.0024916441179811954, Learning Rate: 0.000864\n",
      "Epoch 2797/40000, Loss: 0.003839238779619336, Learning Rate: 0.000864\n",
      "Epoch 2798/40000, Loss: 0.0024678341578692198, Learning Rate: 0.000864\n",
      "Epoch 2799/40000, Loss: 0.0037912309635430574, Learning Rate: 0.000864\n",
      "Epoch 2800/40000, Loss: 0.003539500990882516, Learning Rate: 0.000863\n",
      "Epoch 2801/40000, Loss: 0.002427103463560343, Learning Rate: 0.000863\n",
      "Epoch 2802/40000, Loss: 0.0038193066138774157, Learning Rate: 0.000863\n",
      "Epoch 2803/40000, Loss: 0.002523242263123393, Learning Rate: 0.000863\n",
      "Epoch 2804/40000, Loss: 0.001878521405160427, Learning Rate: 0.000862\n",
      "Epoch 2805/40000, Loss: 0.003636443056166172, Learning Rate: 0.000862\n",
      "Epoch 2806/40000, Loss: 0.003811508184298873, Learning Rate: 0.000862\n",
      "Epoch 2807/40000, Loss: 0.00392929557710886, Learning Rate: 0.000862\n",
      "Epoch 2808/40000, Loss: 0.0025273561477661133, Learning Rate: 0.000861\n",
      "Epoch 2809/40000, Loss: 0.0019139789510518312, Learning Rate: 0.000861\n",
      "Epoch 2810/40000, Loss: 0.001917429850436747, Learning Rate: 0.000861\n",
      "Epoch 2811/40000, Loss: 0.0037598952185362577, Learning Rate: 0.000861\n",
      "Epoch 2812/40000, Loss: 0.0019214156782254577, Learning Rate: 0.000860\n",
      "Epoch 2813/40000, Loss: 0.003536579431965947, Learning Rate: 0.000860\n",
      "Epoch 2814/40000, Loss: 0.00356026878580451, Learning Rate: 0.000860\n",
      "Epoch 2815/40000, Loss: 0.003812431590631604, Learning Rate: 0.000860\n",
      "Epoch 2816/40000, Loss: 0.0039047994650900364, Learning Rate: 0.000859\n",
      "Epoch 2817/40000, Loss: 0.0018837782554328442, Learning Rate: 0.000859\n",
      "Epoch 2818/40000, Loss: 0.003824857994914055, Learning Rate: 0.000859\n",
      "Epoch 2819/40000, Loss: 0.0019187924917787313, Learning Rate: 0.000858\n",
      "Epoch 2820/40000, Loss: 0.0038102599792182446, Learning Rate: 0.000858\n",
      "Epoch 2821/40000, Loss: 0.0018486634362488985, Learning Rate: 0.000858\n",
      "Epoch 2822/40000, Loss: 0.0019153974717482924, Learning Rate: 0.000858\n",
      "Epoch 2823/40000, Loss: 0.003906599711626768, Learning Rate: 0.000857\n",
      "Epoch 2824/40000, Loss: 0.001973237842321396, Learning Rate: 0.000857\n",
      "Epoch 2825/40000, Loss: 0.002018764615058899, Learning Rate: 0.000857\n",
      "Epoch 2826/40000, Loss: 0.0019233895000070333, Learning Rate: 0.000857\n",
      "Epoch 2827/40000, Loss: 0.0038338929880410433, Learning Rate: 0.000856\n",
      "Epoch 2828/40000, Loss: 0.0019215259235352278, Learning Rate: 0.000856\n",
      "Epoch 2829/40000, Loss: 0.0035586217418313026, Learning Rate: 0.000856\n",
      "Epoch 2830/40000, Loss: 0.0038210353814065456, Learning Rate: 0.000856\n",
      "Epoch 2831/40000, Loss: 0.0019294399535283446, Learning Rate: 0.000855\n",
      "Epoch 2832/40000, Loss: 0.001943330280482769, Learning Rate: 0.000855\n",
      "Epoch 2833/40000, Loss: 0.001981220906600356, Learning Rate: 0.000855\n",
      "Epoch 2834/40000, Loss: 0.0019063956569880247, Learning Rate: 0.000855\n",
      "Epoch 2835/40000, Loss: 0.0025156924966722727, Learning Rate: 0.000854\n",
      "Epoch 2836/40000, Loss: 0.0019448574166744947, Learning Rate: 0.000854\n",
      "Epoch 2837/40000, Loss: 0.0025292187929153442, Learning Rate: 0.000854\n",
      "Epoch 2838/40000, Loss: 0.00356308720074594, Learning Rate: 0.000854\n",
      "Epoch 2839/40000, Loss: 0.001992262899875641, Learning Rate: 0.000853\n",
      "Epoch 2840/40000, Loss: 0.0019135824404656887, Learning Rate: 0.000853\n",
      "Epoch 2841/40000, Loss: 0.003809532383456826, Learning Rate: 0.000853\n",
      "Epoch 2842/40000, Loss: 0.004034203942865133, Learning Rate: 0.000853\n",
      "Epoch 2843/40000, Loss: 0.0038275679107755423, Learning Rate: 0.000852\n",
      "Epoch 2844/40000, Loss: 0.0038259890861809254, Learning Rate: 0.000852\n",
      "Epoch 2845/40000, Loss: 0.0039509134367108345, Learning Rate: 0.000852\n",
      "Epoch 2846/40000, Loss: 0.004030648618936539, Learning Rate: 0.000852\n",
      "Epoch 2847/40000, Loss: 0.0037693025078624487, Learning Rate: 0.000851\n",
      "Epoch 2848/40000, Loss: 0.0038123622070997953, Learning Rate: 0.000851\n",
      "Epoch 2849/40000, Loss: 0.003520139493048191, Learning Rate: 0.000851\n",
      "Epoch 2850/40000, Loss: 0.0035335684660822153, Learning Rate: 0.000851\n",
      "Epoch 2851/40000, Loss: 0.002512182341888547, Learning Rate: 0.000850\n",
      "Epoch 2852/40000, Loss: 0.0035307679791003466, Learning Rate: 0.000850\n",
      "Epoch 2853/40000, Loss: 0.0039041093550622463, Learning Rate: 0.000850\n",
      "Epoch 2854/40000, Loss: 0.0036081073340028524, Learning Rate: 0.000850\n",
      "Epoch 2855/40000, Loss: 0.001957845175638795, Learning Rate: 0.000849\n",
      "Epoch 2856/40000, Loss: 0.0038286715280264616, Learning Rate: 0.000849\n",
      "Epoch 2857/40000, Loss: 0.00392901711165905, Learning Rate: 0.000849\n",
      "Epoch 2858/40000, Loss: 0.0025574201717972755, Learning Rate: 0.000849\n",
      "Epoch 2859/40000, Loss: 0.0037699874956160784, Learning Rate: 0.000848\n",
      "Epoch 2860/40000, Loss: 0.0018970760283991694, Learning Rate: 0.000848\n",
      "Epoch 2861/40000, Loss: 0.0038642750587314367, Learning Rate: 0.000848\n",
      "Epoch 2862/40000, Loss: 0.002021861495450139, Learning Rate: 0.000847\n",
      "Epoch 2863/40000, Loss: 0.003937618341296911, Learning Rate: 0.000847\n",
      "Epoch 2864/40000, Loss: 0.00189728825353086, Learning Rate: 0.000847\n",
      "Epoch 2865/40000, Loss: 0.0039013931527733803, Learning Rate: 0.000847\n",
      "Epoch 2866/40000, Loss: 0.003805952612310648, Learning Rate: 0.000846\n",
      "Epoch 2867/40000, Loss: 0.0025843381881713867, Learning Rate: 0.000846\n",
      "Epoch 2868/40000, Loss: 0.003824014915153384, Learning Rate: 0.000846\n",
      "Epoch 2869/40000, Loss: 0.003680963534861803, Learning Rate: 0.000846\n",
      "Epoch 2870/40000, Loss: 0.001905615208670497, Learning Rate: 0.000845\n",
      "Epoch 2871/40000, Loss: 0.001923727453686297, Learning Rate: 0.000845\n",
      "Epoch 2872/40000, Loss: 0.004171937704086304, Learning Rate: 0.000845\n",
      "Epoch 2873/40000, Loss: 0.0020013723988085985, Learning Rate: 0.000845\n",
      "Epoch 2874/40000, Loss: 0.002466693054884672, Learning Rate: 0.000844\n",
      "Epoch 2875/40000, Loss: 0.0018435402307659388, Learning Rate: 0.000844\n",
      "Epoch 2876/40000, Loss: 0.0038501997478306293, Learning Rate: 0.000844\n",
      "Epoch 2877/40000, Loss: 0.0037967811804264784, Learning Rate: 0.000844\n",
      "Epoch 2878/40000, Loss: 0.0019454590510576963, Learning Rate: 0.000843\n",
      "Epoch 2879/40000, Loss: 0.003945434931665659, Learning Rate: 0.000843\n",
      "Epoch 2880/40000, Loss: 0.003968813922256231, Learning Rate: 0.000843\n",
      "Epoch 2881/40000, Loss: 0.003917653579264879, Learning Rate: 0.000843\n",
      "Epoch 2882/40000, Loss: 0.001961988629773259, Learning Rate: 0.000842\n",
      "Epoch 2883/40000, Loss: 0.003961544018238783, Learning Rate: 0.000842\n",
      "Epoch 2884/40000, Loss: 0.0035612478386610746, Learning Rate: 0.000842\n",
      "Epoch 2885/40000, Loss: 0.00197165017016232, Learning Rate: 0.000842\n",
      "Epoch 2886/40000, Loss: 0.0038264391478151083, Learning Rate: 0.000841\n",
      "Epoch 2887/40000, Loss: 0.0019178714137524366, Learning Rate: 0.000841\n",
      "Epoch 2888/40000, Loss: 0.0035146460868418217, Learning Rate: 0.000841\n",
      "Epoch 2889/40000, Loss: 0.0037922104820609093, Learning Rate: 0.000841\n",
      "Epoch 2890/40000, Loss: 0.001972653204575181, Learning Rate: 0.000840\n",
      "Epoch 2891/40000, Loss: 0.003786665853112936, Learning Rate: 0.000840\n",
      "Epoch 2892/40000, Loss: 0.001957056811079383, Learning Rate: 0.000840\n",
      "Epoch 2893/40000, Loss: 0.0024593293201178312, Learning Rate: 0.000840\n",
      "Epoch 2894/40000, Loss: 0.003924349322915077, Learning Rate: 0.000839\n",
      "Epoch 2895/40000, Loss: 0.004195182118564844, Learning Rate: 0.000839\n",
      "Epoch 2896/40000, Loss: 0.0020869155414402485, Learning Rate: 0.000839\n",
      "Epoch 2897/40000, Loss: 0.0038340624887496233, Learning Rate: 0.000839\n",
      "Epoch 2898/40000, Loss: 0.0019862798508256674, Learning Rate: 0.000838\n",
      "Epoch 2899/40000, Loss: 0.0038119135424494743, Learning Rate: 0.000838\n",
      "Epoch 2900/40000, Loss: 0.0035431580618023872, Learning Rate: 0.000838\n",
      "Epoch 2901/40000, Loss: 0.0018711305456236005, Learning Rate: 0.000838\n",
      "Epoch 2902/40000, Loss: 0.003946466837078333, Learning Rate: 0.000837\n",
      "Epoch 2903/40000, Loss: 0.0038131133187562227, Learning Rate: 0.000837\n",
      "Epoch 2904/40000, Loss: 0.0019658496603369713, Learning Rate: 0.000837\n",
      "Epoch 2905/40000, Loss: 0.0037686408031731844, Learning Rate: 0.000837\n",
      "Epoch 2906/40000, Loss: 0.003543250961229205, Learning Rate: 0.000836\n",
      "Epoch 2907/40000, Loss: 0.001854263129644096, Learning Rate: 0.000836\n",
      "Epoch 2908/40000, Loss: 0.002482813782989979, Learning Rate: 0.000836\n",
      "Epoch 2909/40000, Loss: 0.0025113087613135576, Learning Rate: 0.000836\n",
      "Epoch 2910/40000, Loss: 0.001859237439930439, Learning Rate: 0.000835\n",
      "Epoch 2911/40000, Loss: 0.0038074057083576918, Learning Rate: 0.000835\n",
      "Epoch 2912/40000, Loss: 0.00242555676959455, Learning Rate: 0.000835\n",
      "Epoch 2913/40000, Loss: 0.0038315618876367807, Learning Rate: 0.000835\n",
      "Epoch 2914/40000, Loss: 0.0035058320499956608, Learning Rate: 0.000834\n",
      "Epoch 2915/40000, Loss: 0.0024553206749260426, Learning Rate: 0.000834\n",
      "Epoch 2916/40000, Loss: 0.0038445089012384415, Learning Rate: 0.000834\n",
      "Epoch 2917/40000, Loss: 0.003538447432219982, Learning Rate: 0.000834\n",
      "Epoch 2918/40000, Loss: 0.0020192754454910755, Learning Rate: 0.000833\n",
      "Epoch 2919/40000, Loss: 0.0038018240593373775, Learning Rate: 0.000833\n",
      "Epoch 2920/40000, Loss: 0.003810062538832426, Learning Rate: 0.000833\n",
      "Epoch 2921/40000, Loss: 0.0024974681437015533, Learning Rate: 0.000833\n",
      "Epoch 2922/40000, Loss: 0.0018724618712440133, Learning Rate: 0.000832\n",
      "Epoch 2923/40000, Loss: 0.003918785136193037, Learning Rate: 0.000832\n",
      "Epoch 2924/40000, Loss: 0.0019177643116563559, Learning Rate: 0.000832\n",
      "Epoch 2925/40000, Loss: 0.001989066367968917, Learning Rate: 0.000832\n",
      "Epoch 2926/40000, Loss: 0.003886510618031025, Learning Rate: 0.000831\n",
      "Epoch 2927/40000, Loss: 0.001960265450179577, Learning Rate: 0.000831\n",
      "Epoch 2928/40000, Loss: 0.0038083097897469997, Learning Rate: 0.000831\n",
      "Epoch 2929/40000, Loss: 0.003954119049012661, Learning Rate: 0.000831\n",
      "Epoch 2930/40000, Loss: 0.0037759551778435707, Learning Rate: 0.000830\n",
      "Epoch 2931/40000, Loss: 0.0037592938169837, Learning Rate: 0.000830\n",
      "Epoch 2932/40000, Loss: 0.001898102113045752, Learning Rate: 0.000830\n",
      "Epoch 2933/40000, Loss: 0.0019630147144198418, Learning Rate: 0.000830\n",
      "Epoch 2934/40000, Loss: 0.0019052916904911399, Learning Rate: 0.000829\n",
      "Epoch 2935/40000, Loss: 0.003766176989302039, Learning Rate: 0.000829\n",
      "Epoch 2936/40000, Loss: 0.0024992094840854406, Learning Rate: 0.000829\n",
      "Epoch 2937/40000, Loss: 0.0025617792271077633, Learning Rate: 0.000829\n",
      "Epoch 2938/40000, Loss: 0.0038049533031880856, Learning Rate: 0.000828\n",
      "Epoch 2939/40000, Loss: 0.002523815957829356, Learning Rate: 0.000828\n",
      "Epoch 2940/40000, Loss: 0.0025562273804098368, Learning Rate: 0.000828\n",
      "Epoch 2941/40000, Loss: 0.0019238203531131148, Learning Rate: 0.000828\n",
      "Epoch 2942/40000, Loss: 0.002040275139734149, Learning Rate: 0.000827\n",
      "Epoch 2943/40000, Loss: 0.0038660899735987186, Learning Rate: 0.000827\n",
      "Epoch 2944/40000, Loss: 0.0035924490075558424, Learning Rate: 0.000827\n",
      "Epoch 2945/40000, Loss: 0.0018621711060404778, Learning Rate: 0.000827\n",
      "Epoch 2946/40000, Loss: 0.0035576066002249718, Learning Rate: 0.000826\n",
      "Epoch 2947/40000, Loss: 0.0035540880635380745, Learning Rate: 0.000826\n",
      "Epoch 2948/40000, Loss: 0.001933135325089097, Learning Rate: 0.000826\n",
      "Epoch 2949/40000, Loss: 0.002454322762787342, Learning Rate: 0.000826\n",
      "Epoch 2950/40000, Loss: 0.0018710365984588861, Learning Rate: 0.000825\n",
      "Epoch 2951/40000, Loss: 0.0019514489686116576, Learning Rate: 0.000825\n",
      "Epoch 2952/40000, Loss: 0.0019434054847806692, Learning Rate: 0.000825\n",
      "Epoch 2953/40000, Loss: 0.002549875294789672, Learning Rate: 0.000825\n",
      "Epoch 2954/40000, Loss: 0.0035961256362497807, Learning Rate: 0.000824\n",
      "Epoch 2955/40000, Loss: 0.0035431559663265944, Learning Rate: 0.000824\n",
      "Epoch 2956/40000, Loss: 0.001910993130877614, Learning Rate: 0.000824\n",
      "Epoch 2957/40000, Loss: 0.001853244611993432, Learning Rate: 0.000824\n",
      "Epoch 2958/40000, Loss: 0.003773433156311512, Learning Rate: 0.000823\n",
      "Epoch 2959/40000, Loss: 0.0035189520567655563, Learning Rate: 0.000823\n",
      "Epoch 2960/40000, Loss: 0.0037431789096444845, Learning Rate: 0.000823\n",
      "Epoch 2961/40000, Loss: 0.003792000701650977, Learning Rate: 0.000823\n",
      "Epoch 2962/40000, Loss: 0.003574020927771926, Learning Rate: 0.000822\n",
      "Epoch 2963/40000, Loss: 0.0019458436872810125, Learning Rate: 0.000822\n",
      "Epoch 2964/40000, Loss: 0.0037673970218747854, Learning Rate: 0.000822\n",
      "Epoch 2965/40000, Loss: 0.003869147039949894, Learning Rate: 0.000822\n",
      "Epoch 2966/40000, Loss: 0.003934294451028109, Learning Rate: 0.000821\n",
      "Epoch 2967/40000, Loss: 0.004078431520611048, Learning Rate: 0.000821\n",
      "Epoch 2968/40000, Loss: 0.001933503895998001, Learning Rate: 0.000821\n",
      "Epoch 2969/40000, Loss: 0.004071817733347416, Learning Rate: 0.000821\n",
      "Epoch 2970/40000, Loss: 0.00391370290890336, Learning Rate: 0.000820\n",
      "Epoch 2971/40000, Loss: 0.0025141409132629633, Learning Rate: 0.000820\n",
      "Epoch 2972/40000, Loss: 0.0038354122079908848, Learning Rate: 0.000820\n",
      "Epoch 2973/40000, Loss: 0.0039087338373064995, Learning Rate: 0.000820\n",
      "Epoch 2974/40000, Loss: 0.001836281269788742, Learning Rate: 0.000819\n",
      "Epoch 2975/40000, Loss: 0.002446451224386692, Learning Rate: 0.000819\n",
      "Epoch 2976/40000, Loss: 0.003787020454183221, Learning Rate: 0.000819\n",
      "Epoch 2977/40000, Loss: 0.0039108735509216785, Learning Rate: 0.000819\n",
      "Epoch 2978/40000, Loss: 0.0019147418206557631, Learning Rate: 0.000819\n",
      "Epoch 2979/40000, Loss: 0.0035279719159007072, Learning Rate: 0.000818\n",
      "Epoch 2980/40000, Loss: 0.003855629125609994, Learning Rate: 0.000818\n",
      "Epoch 2981/40000, Loss: 0.0019415075657889247, Learning Rate: 0.000818\n",
      "Epoch 2982/40000, Loss: 0.0035206261090934277, Learning Rate: 0.000818\n",
      "Epoch 2983/40000, Loss: 0.0036083932500332594, Learning Rate: 0.000817\n",
      "Epoch 2984/40000, Loss: 0.003930595237761736, Learning Rate: 0.000817\n",
      "Epoch 2985/40000, Loss: 0.004105816595256329, Learning Rate: 0.000817\n",
      "Epoch 2986/40000, Loss: 0.0021819439716637135, Learning Rate: 0.000817\n",
      "Epoch 2987/40000, Loss: 0.001911706174723804, Learning Rate: 0.000816\n",
      "Epoch 2988/40000, Loss: 0.001889069564640522, Learning Rate: 0.000816\n",
      "Epoch 2989/40000, Loss: 0.001908416161313653, Learning Rate: 0.000816\n",
      "Epoch 2990/40000, Loss: 0.0019065657397732139, Learning Rate: 0.000816\n",
      "Epoch 2991/40000, Loss: 0.003803085535764694, Learning Rate: 0.000815\n",
      "Epoch 2992/40000, Loss: 0.003817399498075247, Learning Rate: 0.000815\n",
      "Epoch 2993/40000, Loss: 0.001855224254541099, Learning Rate: 0.000815\n",
      "Epoch 2994/40000, Loss: 0.0038098893128335476, Learning Rate: 0.000815\n",
      "Epoch 2995/40000, Loss: 0.0038378729950636625, Learning Rate: 0.000814\n",
      "Epoch 2996/40000, Loss: 0.0035993054043501616, Learning Rate: 0.000814\n",
      "Epoch 2997/40000, Loss: 0.0035817883908748627, Learning Rate: 0.000814\n",
      "Epoch 2998/40000, Loss: 0.0019236268708482385, Learning Rate: 0.000814\n",
      "Epoch 2999/40000, Loss: 0.0019093990558758378, Learning Rate: 0.000813\n",
      "Epoch 3000/40000, Loss: 0.0039090984500944614, Learning Rate: 0.000813\n",
      "Epoch 3001/40000, Loss: 0.0024762293323874474, Learning Rate: 0.000813\n",
      "Epoch 3002/40000, Loss: 0.0019437293522059917, Learning Rate: 0.000813\n",
      "Epoch 3003/40000, Loss: 0.0037996326573193073, Learning Rate: 0.000812\n",
      "Epoch 3004/40000, Loss: 0.003936018794775009, Learning Rate: 0.000812\n",
      "Epoch 3005/40000, Loss: 0.003936853259801865, Learning Rate: 0.000812\n",
      "Epoch 3006/40000, Loss: 0.001996059902012348, Learning Rate: 0.000812\n",
      "Epoch 3007/40000, Loss: 0.0037666629068553448, Learning Rate: 0.000811\n",
      "Epoch 3008/40000, Loss: 0.0018810102483257651, Learning Rate: 0.000811\n",
      "Epoch 3009/40000, Loss: 0.003738243132829666, Learning Rate: 0.000811\n",
      "Epoch 3010/40000, Loss: 0.0024766651913523674, Learning Rate: 0.000811\n",
      "Epoch 3011/40000, Loss: 0.001873824279755354, Learning Rate: 0.000810\n",
      "Epoch 3012/40000, Loss: 0.0038389796391129494, Learning Rate: 0.000810\n",
      "Epoch 3013/40000, Loss: 0.002467593178153038, Learning Rate: 0.000810\n",
      "Epoch 3014/40000, Loss: 0.0024712570011615753, Learning Rate: 0.000810\n",
      "Epoch 3015/40000, Loss: 0.0018985344795510173, Learning Rate: 0.000809\n",
      "Epoch 3016/40000, Loss: 0.0019482687348499894, Learning Rate: 0.000809\n",
      "Epoch 3017/40000, Loss: 0.001840741140767932, Learning Rate: 0.000809\n",
      "Epoch 3018/40000, Loss: 0.0019422234036028385, Learning Rate: 0.000809\n",
      "Epoch 3019/40000, Loss: 0.004016284830868244, Learning Rate: 0.000808\n",
      "Epoch 3020/40000, Loss: 0.0038542968686670065, Learning Rate: 0.000808\n",
      "Epoch 3021/40000, Loss: 0.003916747868061066, Learning Rate: 0.000808\n",
      "Epoch 3022/40000, Loss: 0.001876624533906579, Learning Rate: 0.000808\n",
      "Epoch 3023/40000, Loss: 0.003562154481187463, Learning Rate: 0.000808\n",
      "Epoch 3024/40000, Loss: 0.003775332821533084, Learning Rate: 0.000807\n",
      "Epoch 3025/40000, Loss: 0.0024206810630857944, Learning Rate: 0.000807\n",
      "Epoch 3026/40000, Loss: 0.0038227136246860027, Learning Rate: 0.000807\n",
      "Epoch 3027/40000, Loss: 0.002457150723785162, Learning Rate: 0.000807\n",
      "Epoch 3028/40000, Loss: 0.0037197100464254618, Learning Rate: 0.000806\n",
      "Epoch 3029/40000, Loss: 0.0018377761589363217, Learning Rate: 0.000806\n",
      "Epoch 3030/40000, Loss: 0.003771628253161907, Learning Rate: 0.000806\n",
      "Epoch 3031/40000, Loss: 0.0038860610220581293, Learning Rate: 0.000806\n",
      "Epoch 3032/40000, Loss: 0.003920336719602346, Learning Rate: 0.000805\n",
      "Epoch 3033/40000, Loss: 0.0024467427283525467, Learning Rate: 0.000805\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (numpy array): A numpy array of shape [N, 100, 200, 19] where N is the number of samples.\n",
    "        \"\"\"\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.target = torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.x[idx]\n",
    "        target = self.target[idx]\n",
    "        return input, target\n",
    "\n",
    "#criterion3 = SmoothnessLoss(.1, .1)\n",
    "dataset = CustomDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#criterion1 = DivLoss()\n",
    "criterion2 = nn.MSELoss()\n",
    "optimizer1 = optim.Adam(model1.parameters(), lr=0.002)\n",
    "#optimizer2 = optim.Adam(model2.parameters(), lr=0.0004)\n",
    "#optimizer3 = optim.Adam(model3.parameters(), lr=0.0004)\n",
    "#optimizer4 = optim.Adam(model4.parameters(), lr=0.0004)\n",
    "#optimizer5 = optim.Adam(model5.parameters(), lr=0.0004)\n",
    "\n",
    "gamma = 0.99995  # The exponential decay factor (adjust as needed)\n",
    "scheduler1 = ExponentialLR(optimizer1, gamma=gamma)\n",
    "#scheduler2 = ExponentialLR(optimizer2, gamma=gamma)\n",
    "#scheduler3 = ExponentialLR(optimizer3, gamma=gamma)\n",
    "#3scheduler4 = ExponentialLR(optimizer4, gamma=gamma)\n",
    "#scheduler5 = ExponentialLR(optimizer5, gamma=gamma)\n",
    "\n",
    "#import gc\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 40000\n",
    "for epoch in range(num_epochs):\n",
    "    model1.train()\n",
    "    #model2.train()\n",
    "    #model3.train()\n",
    "    #model4.train()\n",
    "    #model5.train()\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        labels = labels[0,:,:,:,:].permute(0,2,1,3)\n",
    "        #labels1 = labels[:,0:400,:,:]\n",
    "        #labels2 = labels[:,0:800,:,:]\n",
    "        #labels3 = labels[:,0:1200,:,:]\n",
    "        #labels4 = labels[:,0:1600,:,:]\n",
    "        optimizer1.zero_grad()\n",
    "        #optimizer2.zero_grad()\n",
    "        #optimizer3.zero_grad()\n",
    "        #optimizer4.zero_grad()\n",
    "        #optimizer5.zero_grad()\n",
    "        #outputs1 = model1(inputs)\n",
    "        #outputs2 = model2(outputs1)\n",
    "        #outputs3 = model3(outputs2)\n",
    "        #outputs4 = model4(outputs3)\n",
    "        outputs = model1(inputs)\n",
    "        #outputs2 = outputs2[:,400:800,:,:]\n",
    "        #outputs3 = outputs3[:,800:1200,:,:]\n",
    "        #outputs4 = outputs4[:,1200:1600,:,:]\n",
    "        \n",
    "        #loss1 = criterion2(outputs1, labels1)\n",
    "        #loss2 = criterion2(outputs2, labels2)\n",
    "        #loss3 = criterion2(outputs3, labels3)\n",
    "        #loss4 = criterion2(outputs4, labels4)\n",
    "        loss = criterion2(outputs, labels)\n",
    "\n",
    "        #loss = (loss1*4+loss2+loss3+loss4+loss5)/8\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        #optimizer2.step()\n",
    "        #optimizer3.step()\n",
    "        #optimizer4.step()\n",
    "        #optimizer5.step()\n",
    "        scheduler1.step()\n",
    "        ##scheduler2.step()\n",
    "        #scheduler3.step()\n",
    "        #scheduler4.step()\n",
    "        #scheduler5.step()\n",
    "    current_lr = optimizer1.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Learning Rate: {current_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e879dd9c-9ced-4852-9e0f-a48dddc0efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.cat((outputs1,outputs2,outputs3,outputs4,outputs5), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7020ea62-458c-45d8-ad0b-b84c0cc901f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def aggregate_horizontal_means(tensor):\n",
    "    \"\"\"\n",
    "    Aggregate horizontal means for every 10 rows along the 100-length dimension.\n",
    "    Args:\n",
    "        tensor (torch.Tensor): Tensor of shape [batch_size, 100, 2000, 19].\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of shape [batch_size, 10, 19].\n",
    "    \"\"\"\n",
    "    batch_size, _, width, channels = tensor.shape\n",
    "    aggregated_means = []\n",
    "\n",
    "    for start_row in range(0, 100, 10):\n",
    "        # Extract 10-row block and compute mean\n",
    "        block = tensor[:, start_row:start_row + 10, :, :]\n",
    "        block_mean = torch.mean(block, dim=2)  # Mean along the width (2000)\n",
    "        block_mean = torch.mean(block_mean, dim=1)  # Mean along the 10 rows\n",
    "        aggregated_means.append(block_mean)\n",
    "\n",
    "    # Combine the means into a single tensor\n",
    "    aggregated_means_tensor = torch.stack(aggregated_means, dim=1)\n",
    "    return aggregated_means_tensor\n",
    "\n",
    "class CustomMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomMSELoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Aggregate horizontal means\n",
    "        y_pred_agg = aggregate_horizontal_means(y_pred)\n",
    "        y_true_agg = aggregate_horizontal_means(y_true)\n",
    "\n",
    "        # Calculate MSE between the aggregated tensors\n",
    "        return self.mse_loss(y_pred_agg, y_true_agg)\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming y_pred and y_true are your model's output and ground truth tensors respectively\n",
    "# with shape [batch_size, 100, 2000, 19]\n",
    "custom_loss = CustomMSELoss()\n",
    "#loss = custom_loss(y_pred, y_true)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SmoothnessLoss(nn.Module):\n",
    "    def __init__(self, dx, dy):\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        super(SmoothnessLoss, self).__init__()\n",
    "        self.dx = dx\n",
    "        self.dy = dy\n",
    "\n",
    "    def forward(self, v):\n",
    "        v1 = v[:,:,:,0].permute(0,2,1)\n",
    "        v2 = v[:,:,:,1].permute(0,2,1)\n",
    "\n",
    "        # Calculate horizontal and vertical differences\n",
    "        horizontal_diff = torch.abs(torch.diff(v1, dim=2) / self.dx)\n",
    "        vertical_diff = torch.abs(torch.diff(v2, dim=1) / self.dy)\n",
    "\n",
    "        # Calculate the squared differences (L2 norm)\n",
    "        horizontal_loss = torch.sum(horizontal_diff)\n",
    "        vertical_loss = torch.sum(vertical_diff)\n",
    "\n",
    "        # Total loss\n",
    "        loss = horizontal_loss + vertical_loss\n",
    "        #if loss > .001:\n",
    "\n",
    "        return loss\n",
    "\n",
    "# Example usage with dummy data\n",
    "# Create sample velocity fields as PyTorch tensors\n",
    "\n",
    "\n",
    "\n",
    "def calculate_divergence(vx, vy, dx, dy):\n",
    "    # Use PyTorch's built-in functions for gradient computation\n",
    "    dvx_dx = torch.diff(vx, dim=1) / dx\n",
    "    dvy_dy = torch.diff(vy, dim=2) / dy\n",
    "\n",
    "\n",
    "    # Pad the last dimension to match the original size\n",
    "    #dvx_dx = F.pad(dvx_dx, (0, 1), \"constant\", 0)\n",
    "    #dvy_dy = F.pad(dvy_dy, (0, 0, 0, 1), \"constant\", 0)\n",
    "\n",
    "    return dvx_dx[:,:,0:1999] + dvy_dy[:,0:99,:]\n",
    "\n",
    "class DivLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DivLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        dx = .1\n",
    "        dy = dx\n",
    "        \n",
    "        ypred_vx = y_pred[:,:,:,0].permute(0,2,1)\n",
    "        ypred_vy = y_pred[:,:,:,1].permute(0,2,1)\n",
    "    \n",
    "        ytrue_vx = y_true[:,:,:,0].permute(0,2,1)\n",
    "        ytrue_vy = y_true[:,:,:,1].permute(0,2,1)\n",
    "        \n",
    "        y_pred_div = calculate_divergence(ypred_vx, ypred_vy, dx, dy)\n",
    "        y_true_div = calculate_divergence(ytrue_vx, ytrue_vy, dx, dy)\n",
    "\n",
    "        return torch.mean(torch.abs(torch.abs(y_pred_div)-torch.abs(y_true_div)))\n",
    "\n",
    "class NormLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NormLoss, self).__init__()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        true_means=[]\n",
    "        pred_means=[]\n",
    "        for i in y_pred.shape[3]:\n",
    "            ypred_flat = torch.flatten(y_pred[:,:,:,i])\n",
    "            ypred_mean = torch.mean(ypred_flat)\n",
    "            ypred_std = torch.std(ypred_flat)\n",
    "            pred_means.append([ypred_mean, ypred_std])\n",
    "\n",
    "            ytrue_flat = torch.flatten(y_true[:,:,:,i])\n",
    "            ytrue_mean = torch.mean(ytrue_flat)\n",
    "            ytrue_std = torch.std(ytrue_flat)\n",
    "            true_means.append([ytrue_mean, ytrue_std])\n",
    "\n",
    "        return self.mse_loss(pred_means, true_means)\n",
    "# Example usage (assuming y_pred and y_true are PyTorch tensors with appropriate dimensions)\n",
    "# loss_fn = DivLoss()\n",
    "# loss = loss_fn(y_pred, y_true)\n",
    "\n",
    "\n",
    "\n",
    "#divergence = calculate_divergence(vx, vy, dx, dy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "24d03a26-b2dc-4c98-8e9a-097902cef6c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m smoothness_loss_fn \u001b[38;5;241m=\u001b[39m SmoothnessLoss(dx, dy)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Calculate smoothness loss for vx and vy\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m smoothness_loss_vx \u001b[38;5;241m=\u001b[39m \u001b[43msmoothness_loss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m smoothness_loss_vy \u001b[38;5;241m=\u001b[39m smoothness_loss_fn(vy)\n\u001b[1;32m     15\u001b[0m (smoothness_loss_vx\u001b[38;5;241m.\u001b[39mitem(), smoothness_loss_vy\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/anaconda3/envs/timm_new/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[144], line 62\u001b[0m, in \u001b[0;36mSmoothnessLoss.forward\u001b[0;34m(self, v)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, v):\n\u001b[0;32m---> 62\u001b[0m     v1 \u001b[38;5;241m=\u001b[39m \u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m     v2 \u001b[38;5;241m=\u001b[39m v[:,:,:,\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Calculate horizontal and vertical differences\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "vx = torch.rand(114, 100, 2000)  # Assuming the shape [batch_size, height, width]\n",
    "vy = torch.rand(114, 100, 2000)\n",
    "\n",
    "# Assume uniform grid spacing\n",
    "dx = 0.1  # x-spacing\n",
    "dy = 0.1  # y-spacing\n",
    "\n",
    "# Create an instance of the SmoothnessLoss class\n",
    "smoothness_loss_fn = SmoothnessLoss(dx, dy)\n",
    "\n",
    "# Calculate smoothness loss for vx and vy\n",
    "smoothness_loss_vx = smoothness_loss_fn(vx)\n",
    "smoothness_loss_vy = smoothness_loss_fn(vy)\n",
    "\n",
    "(smoothness_loss_vx.item(), smoothness_loss_vy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b6195e17-9228-4b72-b1cb-d2ec2bd168d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.0072, -0.0046, -0.0054,  ..., -0.0108, -0.0112, -0.0103],\n",
      "         [-0.0061, -0.0039, -0.0042,  ..., -0.0101, -0.0103, -0.0093],\n",
      "         [-0.0049, -0.0032, -0.0034,  ..., -0.0082, -0.0083, -0.0074],\n",
      "         ...,\n",
      "         [ 0.0011,  0.0007,  0.0007,  ...,  0.0018,  0.0018,  0.0016],\n",
      "         [ 0.0007,  0.0003,  0.0004,  ...,  0.0013,  0.0013,  0.0012],\n",
      "         [ 0.0004,  0.0002,  0.0003,  ...,  0.0010,  0.0010,  0.0010]],\n",
      "\n",
      "        [[-0.0097, -0.0061, -0.0073,  ..., -0.0144, -0.0151, -0.0138],\n",
      "         [-0.0082, -0.0052, -0.0057,  ..., -0.0138, -0.0141, -0.0127],\n",
      "         [-0.0068, -0.0044, -0.0047,  ..., -0.0112, -0.0114, -0.0102],\n",
      "         ...,\n",
      "         [ 0.0008,  0.0004,  0.0005,  ...,  0.0014,  0.0014,  0.0012],\n",
      "         [ 0.0009,  0.0005,  0.0005,  ...,  0.0018,  0.0017,  0.0015],\n",
      "         [ 0.0005,  0.0003,  0.0003,  ...,  0.0013,  0.0013,  0.0013]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0134, -0.0086, -0.0100,  ..., -0.0202, -0.0210, -0.0193],\n",
      "         [-0.0115, -0.0073, -0.0080,  ..., -0.0194, -0.0197, -0.0178],\n",
      "         [-0.0092, -0.0058, -0.0063,  ..., -0.0156, -0.0158, -0.0141],\n",
      "         ...,\n",
      "         [ 0.0009,  0.0006,  0.0005,  ...,  0.0017,  0.0016,  0.0015],\n",
      "         [ 0.0008,  0.0004,  0.0004,  ...,  0.0016,  0.0016,  0.0014],\n",
      "         [ 0.0010,  0.0007,  0.0007,  ...,  0.0019,  0.0018,  0.0018]],\n",
      "\n",
      "        [[-0.0266, -0.0170, -0.0203,  ..., -0.0420, -0.0440, -0.0409],\n",
      "         [-0.0278, -0.0176, -0.0194,  ..., -0.0467, -0.0477, -0.0429],\n",
      "         [-0.0227, -0.0146, -0.0155,  ..., -0.0372, -0.0378, -0.0339],\n",
      "         ...,\n",
      "         [ 0.0013,  0.0009,  0.0008,  ...,  0.0019,  0.0017,  0.0017],\n",
      "         [ 0.0005,  0.0003,  0.0003,  ...,  0.0012,  0.0011,  0.0010],\n",
      "         [ 0.0011,  0.0006,  0.0007,  ...,  0.0026,  0.0026,  0.0023]],\n",
      "\n",
      "        [[-0.0083, -0.0054, -0.0062,  ..., -0.0122, -0.0127, -0.0116],\n",
      "         [-0.0065, -0.0040, -0.0045,  ..., -0.0113, -0.0115, -0.0104],\n",
      "         [-0.0060, -0.0039, -0.0040,  ..., -0.0098, -0.0099, -0.0089],\n",
      "         ...,\n",
      "         [ 0.0007,  0.0004,  0.0004,  ...,  0.0015,  0.0015,  0.0013],\n",
      "         [ 0.0008,  0.0005,  0.0006,  ...,  0.0018,  0.0018,  0.0016],\n",
      "         [ 0.0007,  0.0005,  0.0005,  ...,  0.0013,  0.0013,  0.0014]]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([114, 99, 2000])\n",
      "tensor([[[ 0.2738,  0.3209,  0.0013,  ..., -0.5893,  0.1305, -0.0241],\n",
      "         [ 0.2738,  0.3209,  0.0013,  ..., -0.5893,  0.1305, -0.0241],\n",
      "         [ 0.2738,  0.3209,  0.0013,  ..., -0.5893,  0.1305, -0.0241],\n",
      "         ...,\n",
      "         [ 0.2738,  0.3209,  0.0013,  ..., -0.5893,  0.1305, -0.0241],\n",
      "         [ 0.2738,  0.3209,  0.0013,  ..., -0.5893,  0.1305, -0.0241],\n",
      "         [ 0.2738,  0.3209,  0.0013,  ..., -0.5893,  0.1305, -0.0241]],\n",
      "\n",
      "        [[ 0.2740,  0.3211,  0.0014,  ..., -0.5892,  0.1294, -0.0230],\n",
      "         [ 0.2743,  0.3213,  0.0012,  ..., -0.5892,  0.1296, -0.0232],\n",
      "         [ 0.2738,  0.3214,  0.0014,  ..., -0.5892,  0.1294, -0.0232],\n",
      "         ...,\n",
      "         [ 0.2731,  0.3214,  0.0014,  ..., -0.5884,  0.1297, -0.0242],\n",
      "         [ 0.2740,  0.3211,  0.0011,  ..., -0.5896,  0.1304, -0.0238],\n",
      "         [ 0.2740,  0.3211,  0.0010,  ..., -0.5894,  0.1301, -0.0235]],\n",
      "\n",
      "        [[ 0.2742,  0.3211,  0.0009,  ..., -0.5895,  0.1290, -0.0228],\n",
      "         [ 0.2743,  0.3210,  0.0015,  ..., -0.5888,  0.1290, -0.0230],\n",
      "         [ 0.2744,  0.3212,  0.0014,  ..., -0.5894,  0.1294, -0.0232],\n",
      "         ...,\n",
      "         [ 0.2736,  0.3210,  0.0012,  ..., -0.5893,  0.1301, -0.0237],\n",
      "         [ 0.2740,  0.3211,  0.0011,  ..., -0.5896,  0.1304, -0.0238],\n",
      "         [ 0.2740,  0.3211,  0.0010,  ..., -0.5894,  0.1301, -0.0235]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.2744,  0.3211,  0.0013,  ..., -0.5901,  0.1299, -0.0232],\n",
      "         [ 0.2747,  0.3217,  0.0015,  ..., -0.5901,  0.1298, -0.0233],\n",
      "         [ 0.2731,  0.3215,  0.0017,  ..., -0.5895,  0.1296, -0.0237],\n",
      "         ...,\n",
      "         [ 0.2736,  0.3213,  0.0013,  ..., -0.5896,  0.1297, -0.0232],\n",
      "         [ 0.2740,  0.3211,  0.0010,  ..., -0.5894,  0.1301, -0.0235],\n",
      "         [ 0.2740,  0.3211,  0.0010,  ..., -0.5894,  0.1301, -0.0235]],\n",
      "\n",
      "        [[ 0.2736,  0.3216,  0.0012,  ..., -0.5900,  0.1304, -0.0238],\n",
      "         [ 0.2731,  0.3216,  0.0008,  ..., -0.5896,  0.1297, -0.0242],\n",
      "         [ 0.2723,  0.3212,  0.0013,  ..., -0.5908,  0.1307, -0.0238],\n",
      "         ...,\n",
      "         [ 0.2739,  0.3215,  0.0020,  ..., -0.5888,  0.1297, -0.0236],\n",
      "         [ 0.2732,  0.3209,  0.0015,  ..., -0.5895,  0.1303, -0.0236],\n",
      "         [ 0.2740,  0.3211,  0.0010,  ..., -0.5894,  0.1301, -0.0235]],\n",
      "\n",
      "        [[ 0.2745,  0.3209,  0.0013,  ..., -0.5897,  0.1298, -0.0235],\n",
      "         [ 0.2743,  0.3210,  0.0015,  ..., -0.5888,  0.1290, -0.0230],\n",
      "         [ 0.2742,  0.3213,  0.0013,  ..., -0.5890,  0.1293, -0.0235],\n",
      "         ...,\n",
      "         [ 0.2741,  0.3212,  0.0013,  ..., -0.5889,  0.1298, -0.0241],\n",
      "         [ 0.2740,  0.3211,  0.0011,  ..., -0.5896,  0.1304, -0.0238],\n",
      "         [ 0.2740,  0.3211,  0.0010,  ..., -0.5894,  0.1301, -0.0235]]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "torch.Size([114, 100, 1999])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[-1.0296e+00, -3.4664e-01, -1.3572e-01,  ..., -2.3475e-02,\n",
      "          -6.1061e-02, -1.8342e-01],\n",
      "         [-2.6344e-01, -2.1436e-01, -1.3162e-01,  ..., -2.2216e-02,\n",
      "          -3.7042e-02, -4.6417e-02],\n",
      "         [-8.1960e-02, -1.1518e-01, -9.6545e-02,  ..., -1.5675e-02,\n",
      "          -1.9139e-02, -1.4079e-02],\n",
      "         ...,\n",
      "         [-6.0070e-07, -2.0349e-06, -4.2003e-06,  ...,  5.3982e-04,\n",
      "           6.7816e-04,  5.4841e-04],\n",
      "         [-4.1444e-07, -1.4342e-06, -2.9383e-06,  ...,  8.0081e-04,\n",
      "           1.3980e-03,  1.8416e-03],\n",
      "         [-1.3039e-07, -4.3772e-07, -7.3109e-07,  ...,  8.4933e-04,\n",
      "           2.2939e-03,  7.1120e-03]],\n",
      "\n",
      "        [[-1.4072e+00, -4.7370e-01, -1.8546e-01,  ..., -3.2928e-02,\n",
      "          -8.5775e-02, -2.5841e-01],\n",
      "         [-3.5998e-01, -2.9295e-01, -1.7988e-01,  ..., -3.1131e-02,\n",
      "          -5.1944e-02, -6.5232e-02],\n",
      "         [-1.1199e-01, -1.5741e-01, -1.3194e-01,  ..., -2.1954e-02,\n",
      "          -2.6818e-02, -1.9753e-02],\n",
      "         ...,\n",
      "         [-8.2422e-07, -2.7940e-06, -5.7556e-06,  ...,  5.2495e-04,\n",
      "           6.5945e-04,  5.3394e-04],\n",
      "         [-5.6345e-07, -1.9651e-06, -4.0233e-06,  ...,  7.7819e-04,\n",
      "           1.3597e-03,  1.7929e-03],\n",
      "         [-1.8161e-07, -6.0070e-07, -1.0012e-06,  ...,  8.2515e-04,\n",
      "           2.2307e-03,  6.9223e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.9100e+00, -6.4274e-01, -2.5158e-01,  ..., -4.6095e-02,\n",
      "          -1.1968e-01, -3.5895e-01],\n",
      "         [-4.8842e-01, -3.9744e-01, -2.4399e-01,  ..., -4.3685e-02,\n",
      "          -7.2731e-02, -9.0943e-02],\n",
      "         [-1.5189e-01, -2.1348e-01, -1.7893e-01,  ..., -3.0855e-02,\n",
      "          -3.7633e-02, -2.7631e-02],\n",
      "         ...,\n",
      "         [-1.1269e-06, -3.8138e-06, -7.8324e-06,  ...,  5.7579e-04,\n",
      "           7.2024e-04,  5.7876e-04],\n",
      "         [-7.6834e-07, -2.6682e-06, -5.4622e-06,  ...,  8.4979e-04,\n",
      "           1.4783e-03,  1.9405e-03],\n",
      "         [-2.4214e-07, -8.1956e-07, -1.3690e-06,  ...,  9.0043e-04,\n",
      "           2.4250e-03,  7.4987e-03]],\n",
      "\n",
      "        [[-4.5534e+00, -1.5303e+00, -5.9860e-01,  ..., -1.1356e-01,\n",
      "          -2.9493e-01, -8.8553e-01],\n",
      "         [-1.1624e+00, -9.4617e-01, -5.8063e-01,  ..., -1.0767e-01,\n",
      "          -1.7927e-01, -2.2403e-01],\n",
      "         [-3.6120e-01, -5.0789e-01, -4.2572e-01,  ..., -7.6060e-02,\n",
      "          -9.2736e-02, -6.8032e-02],\n",
      "         ...,\n",
      "         [-2.7660e-06, -9.1735e-06, -1.8580e-05,  ...,  6.1159e-04,\n",
      "           7.5747e-04,  6.0192e-04],\n",
      "         [-1.8626e-06, -6.4168e-06, -1.2978e-05,  ...,  8.9368e-04,\n",
      "           1.5431e-03,  2.0106e-03],\n",
      "         [-5.7742e-07, -1.9372e-06, -3.2410e-06,  ...,  9.4844e-04,\n",
      "           2.5345e-03,  7.7823e-03]],\n",
      "\n",
      "        [[-1.1628e+00, -3.9145e-01, -1.5325e-01,  ..., -2.6950e-02,\n",
      "          -7.0010e-02, -2.1000e-01],\n",
      "         [-2.9749e-01, -2.4206e-01, -1.4862e-01,  ..., -2.5528e-02,\n",
      "          -4.2524e-02, -5.3208e-02],\n",
      "         [-9.2543e-02, -1.3005e-01, -1.0900e-01,  ..., -1.8023e-02,\n",
      "          -2.1992e-02, -1.6159e-02],\n",
      "         ...,\n",
      "         [-6.7987e-07, -2.3050e-06, -4.7497e-06,  ...,  5.5685e-04,\n",
      "           6.9867e-04,  5.6362e-04],\n",
      "         [-4.6566e-07, -1.6158e-06, -3.3155e-06,  ...,  8.2497e-04,\n",
      "           1.4382e-03,  1.8918e-03],\n",
      "         [-1.5367e-07, -4.9826e-07, -8.2888e-07,  ...,  8.7473e-04,\n",
      "           2.3597e-03,  7.3080e-03]]], device='cuda:0')\n",
      "torch.Size([114, 99, 2000])\n",
      "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         ...,\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
      "           0.0000e+00,  0.0000e+00]],\n",
      "\n",
      "        [[ 6.3806e-01,  1.6536e-01,  5.1451e-02,  ...,  8.9794e-03,\n",
      "           2.9265e-02,  1.1376e-01],\n",
      "         [ 2.1259e-01,  1.3275e-01,  7.0667e-02,  ...,  1.1982e-02,\n",
      "           2.2979e-02,  3.7358e-02],\n",
      "         [ 8.2753e-02,  8.1080e-02,  5.9098e-02,  ...,  9.7048e-03,\n",
      "           1.3643e-02,  1.4230e-02],\n",
      "         ...,\n",
      "         [ 1.1921e-06,  1.7881e-06,  4.1723e-06,  ..., -3.4094e-04,\n",
      "          -4.9949e-04, -5.4777e-04],\n",
      "         [ 0.0000e+00,  5.9605e-07,  1.1921e-06,  ..., -4.2856e-04,\n",
      "          -8.5950e-04, -1.4544e-03],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.1948e-04,\n",
      "          -1.0824e-03, -4.3577e-03]],\n",
      "\n",
      "        [[ 8.7207e-01,  2.2596e-01,  7.0301e-02,  ...,  1.2603e-02,\n",
      "           4.1136e-02,  1.6027e-01],\n",
      "         [ 2.9052e-01,  1.8141e-01,  9.6568e-02,  ...,  1.6791e-02,\n",
      "           3.2226e-02,  5.2474e-02],\n",
      "         [ 1.1308e-01,  1.1081e-01,  8.0763e-02,  ...,  1.3593e-02,\n",
      "           1.9116e-02,  1.9956e-02],\n",
      "         ...,\n",
      "         [ 1.1921e-06,  2.9802e-06,  5.9605e-06,  ..., -3.3081e-04,\n",
      "          -4.8578e-04, -5.3287e-04],\n",
      "         [ 5.9605e-07,  5.9605e-07,  1.1921e-06,  ..., -4.1664e-04,\n",
      "          -8.3566e-04, -1.4156e-03],\n",
      "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.1054e-04,\n",
      "          -1.0526e-03, -4.2403e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.1836e+00,  3.0659e-01,  9.5365e-02,  ...,  1.7622e-02,\n",
      "           5.7334e-02,  2.2261e-01],\n",
      "         [ 3.9418e-01,  2.4613e-01,  1.3099e-01,  ...,  2.3554e-02,\n",
      "           4.5119e-02,  7.3225e-02],\n",
      "         [ 1.5338e-01,  1.5029e-01,  1.0954e-01,  ...,  1.9104e-02,\n",
      "           2.6829e-02,  2.7945e-02],\n",
      "         ...,\n",
      "         [ 1.7881e-06,  3.5763e-06,  8.3447e-06,  ..., -3.6240e-04,\n",
      "          -5.2989e-04, -5.7817e-04],\n",
      "         [ 5.9605e-07,  1.1921e-06,  1.1921e-06,  ..., -4.5538e-04,\n",
      "          -9.0897e-04, -1.5342e-03],\n",
      "         [ 0.0000e+00,  5.9605e-07,  0.0000e+00,  ..., -3.3915e-04,\n",
      "          -1.1456e-03, -4.5979e-03]],\n",
      "\n",
      "        [[ 2.8216e+00,  7.2977e-01,  2.2683e-01,  ...,  4.3395e-02,\n",
      "           1.4125e-01,  5.4915e-01],\n",
      "         [ 9.3841e-01,  5.8596e-01,  3.1168e-01,  ...,  5.8044e-02,\n",
      "           1.1121e-01,  1.8043e-01],\n",
      "         [ 3.6486e-01,  3.5764e-01,  2.6063e-01,  ...,  4.7092e-02,\n",
      "           6.6127e-02,  6.8832e-02],\n",
      "         ...,\n",
      "         [ 3.5763e-06,  9.5367e-06,  1.9670e-05,  ..., -3.8326e-04,\n",
      "          -5.5611e-04, -6.0320e-04],\n",
      "         [ 1.7881e-06,  2.3842e-06,  3.5763e-06,  ..., -4.7863e-04,\n",
      "          -9.5010e-04, -1.5932e-03],\n",
      "         [ 5.9605e-07,  5.9605e-07,  0.0000e+00,  ..., -3.5942e-04,\n",
      "          -1.2022e-03, -4.7815e-03]],\n",
      "\n",
      "        [[ 7.2059e-01,  1.8673e-01,  5.8097e-02,  ...,  1.0304e-02,\n",
      "           3.3544e-02,  1.3024e-01],\n",
      "         [ 2.4007e-01,  1.4990e-01,  7.9792e-02,  ...,  1.3766e-02,\n",
      "           2.6380e-02,  4.2835e-02],\n",
      "         [ 9.3439e-02,  9.1549e-02,  6.6726e-02,  ...,  1.1159e-02,\n",
      "           1.5678e-02,  1.6339e-02],\n",
      "         ...,\n",
      "         [ 5.9605e-07,  2.3842e-06,  5.3644e-06,  ..., -3.5107e-04,\n",
      "          -5.1498e-04, -5.6267e-04],\n",
      "         [ 5.9605e-07,  5.9605e-07,  1.1921e-06,  ..., -4.4227e-04,\n",
      "          -8.8394e-04, -1.4949e-03],\n",
      "         [ 0.0000e+00,  5.9605e-07,  0.0000e+00,  ..., -3.2902e-04,\n",
      "          -1.1146e-03, -4.4787e-03]]], device='cuda:0')\n",
      "torch.Size([114, 100, 1999])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f5f00038dc0>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dx = .1\n",
    "dy = dx\n",
    "\n",
    "ypred_vx = outputs[:,:,:,0].permute(0,2,1)\n",
    "ypred_vy = outputs[:,:,:,1].permute(0,2,1)\n",
    "\n",
    "ytrue_vx = labels[:,:,:,0].permute(0,2,1)\n",
    "ytrue_vy = labels[:,:,:,1].permute(0,2,1)\n",
    "\n",
    "y_pred_div = calculate_divergence(ypred_vx, ypred_vy, dx, dy)\n",
    "y_true_div = calculate_divergence(ytrue_vx, ytrue_vy, dx, dy)\n",
    "plt.figure(figsize=(12,12))\n",
    "plt.imshow(np.abs(y_pred_div.cpu().detach().numpy()[50,:,:]), norm=LogNorm())\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccf22742-c41d-4fda-88a2-9052f1073e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40000, Loss: 0.06273537874221802, Learning Rate: 0.002000\n",
      "Epoch 2/40000, Loss: 0.09435981512069702, Learning Rate: 0.002000\n",
      "Epoch 3/40000, Loss: 0.04889047145843506, Learning Rate: 0.001999\n",
      "Epoch 4/40000, Loss: 0.04934140667319298, Learning Rate: 0.001999\n",
      "Epoch 5/40000, Loss: 0.0368013009428978, Learning Rate: 0.001999\n",
      "Epoch 6/40000, Loss: 0.043831076472997665, Learning Rate: 0.001999\n",
      "Epoch 7/40000, Loss: 0.034796375781297684, Learning Rate: 0.001998\n",
      "Epoch 8/40000, Loss: 0.026932083070278168, Learning Rate: 0.001998\n",
      "Epoch 9/40000, Loss: 0.030524227768182755, Learning Rate: 0.001998\n",
      "Epoch 10/40000, Loss: 0.026211276650428772, Learning Rate: 0.001998\n",
      "Epoch 11/40000, Loss: 0.024436693638563156, Learning Rate: 0.001997\n",
      "Epoch 12/40000, Loss: 0.020341766998171806, Learning Rate: 0.001997\n",
      "Epoch 13/40000, Loss: 0.021008726209402084, Learning Rate: 0.001997\n",
      "Epoch 14/40000, Loss: 0.02424204722046852, Learning Rate: 0.001997\n",
      "Epoch 15/40000, Loss: 0.021023984998464584, Learning Rate: 0.001996\n",
      "Epoch 16/40000, Loss: 0.019804857671260834, Learning Rate: 0.001996\n",
      "Epoch 17/40000, Loss: 0.017813079059123993, Learning Rate: 0.001996\n",
      "Epoch 18/40000, Loss: 0.018761049956083298, Learning Rate: 0.001996\n",
      "Epoch 19/40000, Loss: 0.019003085792064667, Learning Rate: 0.001995\n",
      "Epoch 20/40000, Loss: 0.015234231017529964, Learning Rate: 0.001995\n",
      "Epoch 21/40000, Loss: 0.01507007610052824, Learning Rate: 0.001995\n",
      "Epoch 22/40000, Loss: 0.0174289308488369, Learning Rate: 0.001995\n",
      "Epoch 23/40000, Loss: 0.019244328141212463, Learning Rate: 0.001994\n",
      "Epoch 24/40000, Loss: 0.01806424930691719, Learning Rate: 0.001994\n",
      "Epoch 25/40000, Loss: 0.01606549695134163, Learning Rate: 0.001994\n",
      "Epoch 26/40000, Loss: 0.014351407997310162, Learning Rate: 0.001994\n",
      "Epoch 27/40000, Loss: 0.01670730486512184, Learning Rate: 0.001994\n",
      "Epoch 28/40000, Loss: 0.014695274643599987, Learning Rate: 0.001993\n",
      "Epoch 29/40000, Loss: 0.016920028254389763, Learning Rate: 0.001993\n",
      "Epoch 30/40000, Loss: 0.016317015513777733, Learning Rate: 0.001993\n",
      "Epoch 31/40000, Loss: 0.016561787575483322, Learning Rate: 0.001993\n",
      "Epoch 32/40000, Loss: 0.016247346997261047, Learning Rate: 0.001992\n",
      "Epoch 33/40000, Loss: 0.014421595260500908, Learning Rate: 0.001992\n",
      "Epoch 34/40000, Loss: 0.014250628650188446, Learning Rate: 0.001992\n",
      "Epoch 35/40000, Loss: 0.013171682134270668, Learning Rate: 0.001992\n",
      "Epoch 36/40000, Loss: 0.012964529916644096, Learning Rate: 0.001991\n",
      "Epoch 37/40000, Loss: 0.0157961193472147, Learning Rate: 0.001991\n",
      "Epoch 38/40000, Loss: 0.015727143734693527, Learning Rate: 0.001991\n",
      "Epoch 39/40000, Loss: 0.014786857180297375, Learning Rate: 0.001991\n",
      "Epoch 40/40000, Loss: 0.014693763107061386, Learning Rate: 0.001990\n",
      "Epoch 41/40000, Loss: 0.013887623324990273, Learning Rate: 0.001990\n",
      "Epoch 42/40000, Loss: 0.013646719977259636, Learning Rate: 0.001990\n",
      "Epoch 43/40000, Loss: 0.01271513756364584, Learning Rate: 0.001990\n",
      "Epoch 44/40000, Loss: 0.011988993734121323, Learning Rate: 0.001989\n",
      "Epoch 45/40000, Loss: 0.01305738277733326, Learning Rate: 0.001989\n",
      "Epoch 46/40000, Loss: 0.015038622543215752, Learning Rate: 0.001989\n",
      "Epoch 47/40000, Loss: 0.01258873101323843, Learning Rate: 0.001989\n",
      "Epoch 48/40000, Loss: 0.014524641446769238, Learning Rate: 0.001989\n",
      "Epoch 49/40000, Loss: 0.011658580973744392, Learning Rate: 0.001988\n",
      "Epoch 50/40000, Loss: 0.012866111472249031, Learning Rate: 0.001988\n",
      "Epoch 51/40000, Loss: 0.01479385793209076, Learning Rate: 0.001988\n",
      "Epoch 52/40000, Loss: 0.014396768063306808, Learning Rate: 0.001988\n",
      "Epoch 53/40000, Loss: 0.01445102971047163, Learning Rate: 0.001987\n",
      "Epoch 54/40000, Loss: 0.011592375114560127, Learning Rate: 0.001987\n",
      "Epoch 55/40000, Loss: 0.012325957417488098, Learning Rate: 0.001987\n",
      "Epoch 56/40000, Loss: 0.01362019032239914, Learning Rate: 0.001987\n",
      "Epoch 57/40000, Loss: 0.014872088097035885, Learning Rate: 0.001986\n",
      "Epoch 58/40000, Loss: 0.012309448793530464, Learning Rate: 0.001986\n",
      "Epoch 59/40000, Loss: 0.014015364460647106, Learning Rate: 0.001986\n",
      "Epoch 60/40000, Loss: 0.01155549194663763, Learning Rate: 0.001986\n",
      "Epoch 61/40000, Loss: 0.011000602506101131, Learning Rate: 0.001985\n",
      "Epoch 62/40000, Loss: 0.012581898830831051, Learning Rate: 0.001985\n",
      "Epoch 63/40000, Loss: 0.010795613750815392, Learning Rate: 0.001985\n",
      "Epoch 64/40000, Loss: 0.011715217493474483, Learning Rate: 0.001985\n",
      "Epoch 65/40000, Loss: 0.01083142589777708, Learning Rate: 0.001984\n",
      "Epoch 66/40000, Loss: 0.011247573420405388, Learning Rate: 0.001984\n",
      "Epoch 67/40000, Loss: 0.013578011654317379, Learning Rate: 0.001984\n",
      "Epoch 68/40000, Loss: 0.01103618647903204, Learning Rate: 0.001984\n",
      "Epoch 69/40000, Loss: 0.010619137436151505, Learning Rate: 0.001984\n",
      "Epoch 70/40000, Loss: 0.011637495830655098, Learning Rate: 0.001983\n",
      "Epoch 71/40000, Loss: 0.012142796069383621, Learning Rate: 0.001983\n",
      "Epoch 72/40000, Loss: 0.012734593823552132, Learning Rate: 0.001983\n",
      "Epoch 73/40000, Loss: 0.01036040112376213, Learning Rate: 0.001983\n",
      "Epoch 74/40000, Loss: 0.009500584565103054, Learning Rate: 0.001982\n",
      "Epoch 75/40000, Loss: 0.009600028395652771, Learning Rate: 0.001982\n",
      "Epoch 76/40000, Loss: 0.012039462104439735, Learning Rate: 0.001982\n",
      "Epoch 77/40000, Loss: 0.011509612202644348, Learning Rate: 0.001982\n",
      "Epoch 78/40000, Loss: 0.00992543064057827, Learning Rate: 0.001981\n",
      "Epoch 79/40000, Loss: 0.00947039294987917, Learning Rate: 0.001981\n",
      "Epoch 80/40000, Loss: 0.010037751868367195, Learning Rate: 0.001981\n",
      "Epoch 81/40000, Loss: 0.00953567586839199, Learning Rate: 0.001981\n",
      "Epoch 82/40000, Loss: 0.009829972870647907, Learning Rate: 0.001980\n",
      "Epoch 83/40000, Loss: 0.008895283564925194, Learning Rate: 0.001980\n",
      "Epoch 84/40000, Loss: 0.010569657199084759, Learning Rate: 0.001980\n",
      "Epoch 85/40000, Loss: 0.00903494842350483, Learning Rate: 0.001980\n",
      "Epoch 86/40000, Loss: 0.011390912346541882, Learning Rate: 0.001979\n",
      "Epoch 87/40000, Loss: 0.008986447937786579, Learning Rate: 0.001979\n",
      "Epoch 88/40000, Loss: 0.010678160935640335, Learning Rate: 0.001979\n",
      "Epoch 89/40000, Loss: 0.008715912699699402, Learning Rate: 0.001979\n",
      "Epoch 90/40000, Loss: 0.008957874961197376, Learning Rate: 0.001979\n",
      "Epoch 91/40000, Loss: 0.008796168491244316, Learning Rate: 0.001978\n",
      "Epoch 92/40000, Loss: 0.008042407222092152, Learning Rate: 0.001978\n",
      "Epoch 93/40000, Loss: 0.008878071792423725, Learning Rate: 0.001978\n",
      "Epoch 94/40000, Loss: 0.008820637129247189, Learning Rate: 0.001978\n",
      "Epoch 95/40000, Loss: 0.010325348936021328, Learning Rate: 0.001977\n",
      "Epoch 96/40000, Loss: 0.00940779596567154, Learning Rate: 0.001977\n",
      "Epoch 97/40000, Loss: 0.007409276440739632, Learning Rate: 0.001977\n",
      "Epoch 98/40000, Loss: 0.009249947965145111, Learning Rate: 0.001977\n",
      "Epoch 99/40000, Loss: 0.009526008740067482, Learning Rate: 0.001976\n",
      "Epoch 100/40000, Loss: 0.007952475920319557, Learning Rate: 0.001976\n",
      "Epoch 101/40000, Loss: 0.00781544391065836, Learning Rate: 0.001976\n",
      "Epoch 102/40000, Loss: 0.007572522386908531, Learning Rate: 0.001976\n",
      "Epoch 103/40000, Loss: 0.0067873382940888405, Learning Rate: 0.001975\n",
      "Epoch 104/40000, Loss: 0.006953540723770857, Learning Rate: 0.001975\n",
      "Epoch 105/40000, Loss: 0.006741497199982405, Learning Rate: 0.001975\n",
      "Epoch 106/40000, Loss: 0.00706998398527503, Learning Rate: 0.001975\n",
      "Epoch 107/40000, Loss: 0.0071588605642318726, Learning Rate: 0.001974\n",
      "Epoch 108/40000, Loss: 0.01053070928901434, Learning Rate: 0.001974\n",
      "Epoch 109/40000, Loss: 0.007740216329693794, Learning Rate: 0.001974\n",
      "Epoch 110/40000, Loss: 0.007741224020719528, Learning Rate: 0.001974\n",
      "Epoch 111/40000, Loss: 0.007757994811981916, Learning Rate: 0.001974\n",
      "Epoch 112/40000, Loss: 0.006964478641748428, Learning Rate: 0.001973\n",
      "Epoch 113/40000, Loss: 0.0075900922529399395, Learning Rate: 0.001973\n",
      "Epoch 114/40000, Loss: 0.010042373090982437, Learning Rate: 0.001973\n",
      "Epoch 115/40000, Loss: 0.007161518558859825, Learning Rate: 0.001973\n",
      "Epoch 116/40000, Loss: 0.007114493753761053, Learning Rate: 0.001972\n",
      "Epoch 117/40000, Loss: 0.006875853054225445, Learning Rate: 0.001972\n",
      "Epoch 118/40000, Loss: 0.006979299243539572, Learning Rate: 0.001972\n",
      "Epoch 119/40000, Loss: 0.006696605589240789, Learning Rate: 0.001972\n",
      "Epoch 120/40000, Loss: 0.007936259731650352, Learning Rate: 0.001971\n",
      "Epoch 121/40000, Loss: 0.006644998677074909, Learning Rate: 0.001971\n",
      "Epoch 122/40000, Loss: 0.0063509284518659115, Learning Rate: 0.001971\n",
      "Epoch 123/40000, Loss: 0.007815116085112095, Learning Rate: 0.001971\n",
      "Epoch 124/40000, Loss: 0.007097004912793636, Learning Rate: 0.001970\n",
      "Epoch 125/40000, Loss: 0.005901265889406204, Learning Rate: 0.001970\n",
      "Epoch 126/40000, Loss: 0.006791756954044104, Learning Rate: 0.001970\n",
      "Epoch 127/40000, Loss: 0.008005775511264801, Learning Rate: 0.001970\n",
      "Epoch 128/40000, Loss: 0.007548103109002113, Learning Rate: 0.001970\n",
      "Epoch 129/40000, Loss: 0.0061128949746489525, Learning Rate: 0.001969\n",
      "Epoch 130/40000, Loss: 0.005815668031573296, Learning Rate: 0.001969\n",
      "Epoch 131/40000, Loss: 0.007150407414883375, Learning Rate: 0.001969\n",
      "Epoch 132/40000, Loss: 0.006151051260530949, Learning Rate: 0.001969\n",
      "Epoch 133/40000, Loss: 0.005907414481043816, Learning Rate: 0.001968\n",
      "Epoch 134/40000, Loss: 0.00594596890732646, Learning Rate: 0.001968\n",
      "Epoch 135/40000, Loss: 0.0062469784170389175, Learning Rate: 0.001968\n",
      "Epoch 136/40000, Loss: 0.006769259925931692, Learning Rate: 0.001968\n",
      "Epoch 137/40000, Loss: 0.006943678017705679, Learning Rate: 0.001967\n",
      "Epoch 138/40000, Loss: 0.00587534811347723, Learning Rate: 0.001967\n",
      "Epoch 139/40000, Loss: 0.0066030374728143215, Learning Rate: 0.001967\n",
      "Epoch 140/40000, Loss: 0.007646012119948864, Learning Rate: 0.001967\n",
      "Epoch 141/40000, Loss: 0.006864823400974274, Learning Rate: 0.001966\n",
      "Epoch 142/40000, Loss: 0.00545818917453289, Learning Rate: 0.001966\n",
      "Epoch 143/40000, Loss: 0.005450425203889608, Learning Rate: 0.001966\n",
      "Epoch 144/40000, Loss: 0.006179244257509708, Learning Rate: 0.001966\n",
      "Epoch 145/40000, Loss: 0.006851836107671261, Learning Rate: 0.001966\n",
      "Epoch 146/40000, Loss: 0.006598320789635181, Learning Rate: 0.001965\n",
      "Epoch 147/40000, Loss: 0.006776119582355022, Learning Rate: 0.001965\n",
      "Epoch 148/40000, Loss: 0.0057970015332102776, Learning Rate: 0.001965\n",
      "Epoch 149/40000, Loss: 0.006415753625333309, Learning Rate: 0.001965\n",
      "Epoch 150/40000, Loss: 0.00595866609364748, Learning Rate: 0.001964\n",
      "Epoch 151/40000, Loss: 0.006594400852918625, Learning Rate: 0.001964\n",
      "Epoch 152/40000, Loss: 0.005638808943331242, Learning Rate: 0.001964\n",
      "Epoch 153/40000, Loss: 0.005810407921671867, Learning Rate: 0.001964\n",
      "Epoch 154/40000, Loss: 0.005994800012558699, Learning Rate: 0.001963\n",
      "Epoch 155/40000, Loss: 0.006398916244506836, Learning Rate: 0.001963\n",
      "Epoch 156/40000, Loss: 0.006865392904728651, Learning Rate: 0.001963\n",
      "Epoch 157/40000, Loss: 0.005852635484188795, Learning Rate: 0.001963\n",
      "Epoch 158/40000, Loss: 0.005763617344200611, Learning Rate: 0.001962\n",
      "Epoch 159/40000, Loss: 0.005307412706315517, Learning Rate: 0.001962\n",
      "Epoch 160/40000, Loss: 0.006161673925817013, Learning Rate: 0.001962\n",
      "Epoch 161/40000, Loss: 0.005623647477477789, Learning Rate: 0.001962\n",
      "Epoch 162/40000, Loss: 0.005965732969343662, Learning Rate: 0.001961\n",
      "Epoch 163/40000, Loss: 0.005571385379880667, Learning Rate: 0.001961\n",
      "Epoch 164/40000, Loss: 0.00717898178845644, Learning Rate: 0.001961\n",
      "Epoch 165/40000, Loss: 0.00583077035844326, Learning Rate: 0.001961\n",
      "Epoch 166/40000, Loss: 0.005695413798093796, Learning Rate: 0.001961\n",
      "Epoch 167/40000, Loss: 0.005647727288305759, Learning Rate: 0.001960\n",
      "Epoch 168/40000, Loss: 0.005567335989326239, Learning Rate: 0.001960\n",
      "Epoch 169/40000, Loss: 0.005222958978265524, Learning Rate: 0.001960\n",
      "Epoch 170/40000, Loss: 0.006525048054754734, Learning Rate: 0.001960\n",
      "Epoch 171/40000, Loss: 0.0053266119211912155, Learning Rate: 0.001959\n",
      "Epoch 172/40000, Loss: 0.005173849873244762, Learning Rate: 0.001959\n",
      "Epoch 173/40000, Loss: 0.0057165352627635, Learning Rate: 0.001959\n",
      "Epoch 174/40000, Loss: 0.0047990623861551285, Learning Rate: 0.001959\n",
      "Epoch 175/40000, Loss: 0.004769683815538883, Learning Rate: 0.001958\n",
      "Epoch 176/40000, Loss: 0.005179624073207378, Learning Rate: 0.001958\n",
      "Epoch 177/40000, Loss: 0.004978683311492205, Learning Rate: 0.001958\n",
      "Epoch 178/40000, Loss: 0.0051280297338962555, Learning Rate: 0.001958\n",
      "Epoch 179/40000, Loss: 0.005031333304941654, Learning Rate: 0.001957\n",
      "Epoch 180/40000, Loss: 0.004939992446452379, Learning Rate: 0.001957\n",
      "Epoch 181/40000, Loss: 0.005137116182595491, Learning Rate: 0.001957\n",
      "Epoch 182/40000, Loss: 0.005177218466997147, Learning Rate: 0.001957\n",
      "Epoch 183/40000, Loss: 0.006655213423073292, Learning Rate: 0.001957\n",
      "Epoch 184/40000, Loss: 0.004910020157694817, Learning Rate: 0.001956\n",
      "Epoch 185/40000, Loss: 0.004818476736545563, Learning Rate: 0.001956\n",
      "Epoch 186/40000, Loss: 0.004631392657756805, Learning Rate: 0.001956\n",
      "Epoch 187/40000, Loss: 0.005900238640606403, Learning Rate: 0.001956\n",
      "Epoch 188/40000, Loss: 0.0051355729810893536, Learning Rate: 0.001955\n",
      "Epoch 189/40000, Loss: 0.006502001546323299, Learning Rate: 0.001955\n",
      "Epoch 190/40000, Loss: 0.005202747881412506, Learning Rate: 0.001955\n",
      "Epoch 191/40000, Loss: 0.004467024467885494, Learning Rate: 0.001955\n",
      "Epoch 192/40000, Loss: 0.005542782600969076, Learning Rate: 0.001954\n",
      "Epoch 193/40000, Loss: 0.004929180257022381, Learning Rate: 0.001954\n",
      "Epoch 194/40000, Loss: 0.004638501442968845, Learning Rate: 0.001954\n",
      "Epoch 195/40000, Loss: 0.005411210469901562, Learning Rate: 0.001954\n",
      "Epoch 196/40000, Loss: 0.0044861165806651115, Learning Rate: 0.001954\n",
      "Epoch 197/40000, Loss: 0.004946236498653889, Learning Rate: 0.001953\n",
      "Epoch 198/40000, Loss: 0.004737840034067631, Learning Rate: 0.001953\n",
      "Epoch 199/40000, Loss: 0.005991511046886444, Learning Rate: 0.001953\n",
      "Epoch 200/40000, Loss: 0.00497389305382967, Learning Rate: 0.001953\n",
      "Epoch 201/40000, Loss: 0.006169341504573822, Learning Rate: 0.001952\n",
      "Epoch 202/40000, Loss: 0.0053095147013664246, Learning Rate: 0.001952\n",
      "Epoch 203/40000, Loss: 0.0057396311312913895, Learning Rate: 0.001952\n",
      "Epoch 204/40000, Loss: 0.006970077287405729, Learning Rate: 0.001952\n",
      "Epoch 205/40000, Loss: 0.00507030775770545, Learning Rate: 0.001951\n",
      "Epoch 206/40000, Loss: 0.004975146148353815, Learning Rate: 0.001951\n",
      "Epoch 207/40000, Loss: 0.006262248381972313, Learning Rate: 0.001951\n",
      "Epoch 208/40000, Loss: 0.005249993409961462, Learning Rate: 0.001951\n",
      "Epoch 209/40000, Loss: 0.004903784021735191, Learning Rate: 0.001950\n",
      "Epoch 210/40000, Loss: 0.0054345279932022095, Learning Rate: 0.001950\n",
      "Epoch 211/40000, Loss: 0.005281283985823393, Learning Rate: 0.001950\n",
      "Epoch 212/40000, Loss: 0.0054343752562999725, Learning Rate: 0.001950\n",
      "Epoch 213/40000, Loss: 0.005347173195332289, Learning Rate: 0.001950\n",
      "Epoch 214/40000, Loss: 0.005335506983101368, Learning Rate: 0.001949\n",
      "Epoch 215/40000, Loss: 0.004970488138496876, Learning Rate: 0.001949\n",
      "Epoch 216/40000, Loss: 0.005360637791454792, Learning Rate: 0.001949\n",
      "Epoch 217/40000, Loss: 0.004726084880530834, Learning Rate: 0.001949\n",
      "Epoch 218/40000, Loss: 0.004484659060835838, Learning Rate: 0.001948\n",
      "Epoch 219/40000, Loss: 0.005262303631752729, Learning Rate: 0.001948\n",
      "Epoch 220/40000, Loss: 0.00503316568210721, Learning Rate: 0.001948\n",
      "Epoch 221/40000, Loss: 0.00513530895113945, Learning Rate: 0.001948\n",
      "Epoch 222/40000, Loss: 0.005079762078821659, Learning Rate: 0.001947\n",
      "Epoch 223/40000, Loss: 0.004306969698518515, Learning Rate: 0.001947\n",
      "Epoch 224/40000, Loss: 0.005283661186695099, Learning Rate: 0.001947\n",
      "Epoch 225/40000, Loss: 0.00446728290989995, Learning Rate: 0.001947\n",
      "Epoch 226/40000, Loss: 0.0053679728880524635, Learning Rate: 0.001946\n",
      "Epoch 227/40000, Loss: 0.004991089925169945, Learning Rate: 0.001946\n",
      "Epoch 228/40000, Loss: 0.0047371480613946915, Learning Rate: 0.001946\n",
      "Epoch 229/40000, Loss: 0.005088623147457838, Learning Rate: 0.001946\n",
      "Epoch 230/40000, Loss: 0.004898586310446262, Learning Rate: 0.001946\n",
      "Epoch 231/40000, Loss: 0.004402222111821175, Learning Rate: 0.001945\n",
      "Epoch 232/40000, Loss: 0.004617086611688137, Learning Rate: 0.001945\n",
      "Epoch 233/40000, Loss: 0.004344669170677662, Learning Rate: 0.001945\n",
      "Epoch 234/40000, Loss: 0.004229976329952478, Learning Rate: 0.001945\n",
      "Epoch 235/40000, Loss: 0.004878174047917128, Learning Rate: 0.001944\n",
      "Epoch 236/40000, Loss: 0.004526922944933176, Learning Rate: 0.001944\n",
      "Epoch 237/40000, Loss: 0.004472595639526844, Learning Rate: 0.001944\n",
      "Epoch 238/40000, Loss: 0.004722891375422478, Learning Rate: 0.001944\n",
      "Epoch 239/40000, Loss: 0.004343861248344183, Learning Rate: 0.001943\n",
      "Epoch 240/40000, Loss: 0.0045883567072451115, Learning Rate: 0.001943\n",
      "Epoch 241/40000, Loss: 0.004239467903971672, Learning Rate: 0.001943\n",
      "Epoch 242/40000, Loss: 0.004696615040302277, Learning Rate: 0.001943\n",
      "Epoch 243/40000, Loss: 0.004258955828845501, Learning Rate: 0.001943\n",
      "Epoch 244/40000, Loss: 0.004355876240879297, Learning Rate: 0.001942\n",
      "Epoch 245/40000, Loss: 0.004158397670835257, Learning Rate: 0.001942\n",
      "Epoch 246/40000, Loss: 0.004064270760864019, Learning Rate: 0.001942\n",
      "Epoch 247/40000, Loss: 0.004151071421802044, Learning Rate: 0.001942\n",
      "Epoch 248/40000, Loss: 0.0047104922123253345, Learning Rate: 0.001941\n",
      "Epoch 249/40000, Loss: 0.004110811278223991, Learning Rate: 0.001941\n",
      "Epoch 250/40000, Loss: 0.004181643016636372, Learning Rate: 0.001941\n",
      "Epoch 251/40000, Loss: 0.00412810780107975, Learning Rate: 0.001941\n",
      "Epoch 252/40000, Loss: 0.004769321996718645, Learning Rate: 0.001940\n",
      "Epoch 253/40000, Loss: 0.004128266591578722, Learning Rate: 0.001940\n",
      "Epoch 254/40000, Loss: 0.004179061856120825, Learning Rate: 0.001940\n",
      "Epoch 255/40000, Loss: 0.004508467391133308, Learning Rate: 0.001940\n",
      "Epoch 256/40000, Loss: 0.004249504301697016, Learning Rate: 0.001939\n",
      "Epoch 257/40000, Loss: 0.004217255860567093, Learning Rate: 0.001939\n",
      "Epoch 258/40000, Loss: 0.004275888204574585, Learning Rate: 0.001939\n",
      "Epoch 259/40000, Loss: 0.004184337332844734, Learning Rate: 0.001939\n",
      "Epoch 260/40000, Loss: 0.004459944553673267, Learning Rate: 0.001939\n",
      "Epoch 261/40000, Loss: 0.004372780676931143, Learning Rate: 0.001938\n",
      "Epoch 262/40000, Loss: 0.004510635510087013, Learning Rate: 0.001938\n",
      "Epoch 263/40000, Loss: 0.0040406957268714905, Learning Rate: 0.001938\n",
      "Epoch 264/40000, Loss: 0.004149690270423889, Learning Rate: 0.001938\n",
      "Epoch 265/40000, Loss: 0.004246211145073175, Learning Rate: 0.001937\n",
      "Epoch 266/40000, Loss: 0.004276270978152752, Learning Rate: 0.001937\n",
      "Epoch 267/40000, Loss: 0.004203861113637686, Learning Rate: 0.001937\n",
      "Epoch 268/40000, Loss: 0.004108532331883907, Learning Rate: 0.001937\n",
      "Epoch 269/40000, Loss: 0.004455088172107935, Learning Rate: 0.001936\n",
      "Epoch 270/40000, Loss: 0.004296743776649237, Learning Rate: 0.001936\n",
      "Epoch 271/40000, Loss: 0.004060036037117243, Learning Rate: 0.001936\n",
      "Epoch 272/40000, Loss: 0.004159384407103062, Learning Rate: 0.001936\n",
      "Epoch 273/40000, Loss: 0.004280600231140852, Learning Rate: 0.001936\n",
      "Epoch 274/40000, Loss: 0.004427968990057707, Learning Rate: 0.001935\n",
      "Epoch 275/40000, Loss: 0.004094456788152456, Learning Rate: 0.001935\n",
      "Epoch 276/40000, Loss: 0.004104240331798792, Learning Rate: 0.001935\n",
      "Epoch 277/40000, Loss: 0.00403294712305069, Learning Rate: 0.001935\n",
      "Epoch 278/40000, Loss: 0.00384012283757329, Learning Rate: 0.001934\n",
      "Epoch 279/40000, Loss: 0.004011752083897591, Learning Rate: 0.001934\n",
      "Epoch 280/40000, Loss: 0.004031960386782885, Learning Rate: 0.001934\n",
      "Epoch 281/40000, Loss: 0.004133678041398525, Learning Rate: 0.001934\n",
      "Epoch 282/40000, Loss: 0.004332471638917923, Learning Rate: 0.001933\n",
      "Epoch 283/40000, Loss: 0.004192893393337727, Learning Rate: 0.001933\n",
      "Epoch 284/40000, Loss: 0.003977004438638687, Learning Rate: 0.001933\n",
      "Epoch 285/40000, Loss: 0.004092076793313026, Learning Rate: 0.001933\n",
      "Epoch 286/40000, Loss: 0.003909548744559288, Learning Rate: 0.001933\n",
      "Epoch 287/40000, Loss: 0.003840449033305049, Learning Rate: 0.001932\n",
      "Epoch 288/40000, Loss: 0.003951597958803177, Learning Rate: 0.001932\n",
      "Epoch 289/40000, Loss: 0.003855316434055567, Learning Rate: 0.001932\n",
      "Epoch 290/40000, Loss: 0.004084712825715542, Learning Rate: 0.001932\n",
      "Epoch 291/40000, Loss: 0.0040456922724843025, Learning Rate: 0.001931\n",
      "Epoch 292/40000, Loss: 0.0037477556616067886, Learning Rate: 0.001931\n",
      "Epoch 293/40000, Loss: 0.003755041165277362, Learning Rate: 0.001931\n",
      "Epoch 294/40000, Loss: 0.00425669364631176, Learning Rate: 0.001931\n",
      "Epoch 295/40000, Loss: 0.0037874653935432434, Learning Rate: 0.001930\n",
      "Epoch 296/40000, Loss: 0.004120077006518841, Learning Rate: 0.001930\n",
      "Epoch 297/40000, Loss: 0.004895612597465515, Learning Rate: 0.001930\n",
      "Epoch 298/40000, Loss: 0.003741491585969925, Learning Rate: 0.001930\n",
      "Epoch 299/40000, Loss: 0.0038376767188310623, Learning Rate: 0.001930\n",
      "Epoch 300/40000, Loss: 0.003916863352060318, Learning Rate: 0.001929\n",
      "Epoch 301/40000, Loss: 0.0051260776817798615, Learning Rate: 0.001929\n",
      "Epoch 302/40000, Loss: 0.004151010420173407, Learning Rate: 0.001929\n",
      "Epoch 303/40000, Loss: 0.003959403373301029, Learning Rate: 0.001929\n",
      "Epoch 304/40000, Loss: 0.004274018108844757, Learning Rate: 0.001928\n",
      "Epoch 305/40000, Loss: 0.0038777701556682587, Learning Rate: 0.001928\n",
      "Epoch 306/40000, Loss: 0.004333699122071266, Learning Rate: 0.001928\n",
      "Epoch 307/40000, Loss: 0.004064731299877167, Learning Rate: 0.001928\n",
      "Epoch 308/40000, Loss: 0.00398839870467782, Learning Rate: 0.001927\n",
      "Epoch 309/40000, Loss: 0.003960302099585533, Learning Rate: 0.001927\n",
      "Epoch 310/40000, Loss: 0.0038416122552007437, Learning Rate: 0.001927\n",
      "Epoch 311/40000, Loss: 0.0042778244242072105, Learning Rate: 0.001927\n",
      "Epoch 312/40000, Loss: 0.0045033786445856094, Learning Rate: 0.001927\n",
      "Epoch 313/40000, Loss: 0.004150271415710449, Learning Rate: 0.001926\n",
      "Epoch 314/40000, Loss: 0.004411177709698677, Learning Rate: 0.001926\n",
      "Epoch 315/40000, Loss: 0.003889017505571246, Learning Rate: 0.001926\n",
      "Epoch 316/40000, Loss: 0.003776918863877654, Learning Rate: 0.001926\n",
      "Epoch 317/40000, Loss: 0.0037470939569175243, Learning Rate: 0.001925\n",
      "Epoch 318/40000, Loss: 0.004163466859608889, Learning Rate: 0.001925\n",
      "Epoch 319/40000, Loss: 0.003857563715428114, Learning Rate: 0.001925\n",
      "Epoch 320/40000, Loss: 0.004119437653571367, Learning Rate: 0.001925\n",
      "Epoch 321/40000, Loss: 0.0038573830388486385, Learning Rate: 0.001924\n",
      "Epoch 322/40000, Loss: 0.0037087928503751755, Learning Rate: 0.001924\n",
      "Epoch 323/40000, Loss: 0.0038196705281734467, Learning Rate: 0.001924\n",
      "Epoch 324/40000, Loss: 0.003918401896953583, Learning Rate: 0.001924\n",
      "Epoch 325/40000, Loss: 0.003689751960337162, Learning Rate: 0.001924\n",
      "Epoch 326/40000, Loss: 0.003770673181861639, Learning Rate: 0.001923\n",
      "Epoch 327/40000, Loss: 0.003993607591837645, Learning Rate: 0.001923\n",
      "Epoch 328/40000, Loss: 0.003856523660942912, Learning Rate: 0.001923\n",
      "Epoch 329/40000, Loss: 0.00426881667226553, Learning Rate: 0.001923\n",
      "Epoch 330/40000, Loss: 0.0038693291135132313, Learning Rate: 0.001922\n",
      "Epoch 331/40000, Loss: 0.004349338822066784, Learning Rate: 0.001922\n",
      "Epoch 332/40000, Loss: 0.004178400617092848, Learning Rate: 0.001922\n",
      "Epoch 333/40000, Loss: 0.004086471628397703, Learning Rate: 0.001922\n",
      "Epoch 334/40000, Loss: 0.00399446627125144, Learning Rate: 0.001921\n",
      "Epoch 335/40000, Loss: 0.0039093978703022, Learning Rate: 0.001921\n",
      "Epoch 336/40000, Loss: 0.0041970000602304935, Learning Rate: 0.001921\n",
      "Epoch 337/40000, Loss: 0.004213657230138779, Learning Rate: 0.001921\n",
      "Epoch 338/40000, Loss: 0.004226205870509148, Learning Rate: 0.001921\n",
      "Epoch 339/40000, Loss: 0.0037522295024245977, Learning Rate: 0.001920\n",
      "Epoch 340/40000, Loss: 0.004045178648084402, Learning Rate: 0.001920\n",
      "Epoch 341/40000, Loss: 0.004050542134791613, Learning Rate: 0.001920\n",
      "Epoch 342/40000, Loss: 0.0036202464252710342, Learning Rate: 0.001920\n",
      "Epoch 343/40000, Loss: 0.004035694524645805, Learning Rate: 0.001919\n",
      "Epoch 344/40000, Loss: 0.0038615711964666843, Learning Rate: 0.001919\n",
      "Epoch 345/40000, Loss: 0.004316933918744326, Learning Rate: 0.001919\n",
      "Epoch 346/40000, Loss: 0.0036499532870948315, Learning Rate: 0.001919\n",
      "Epoch 347/40000, Loss: 0.0035413436125963926, Learning Rate: 0.001918\n",
      "Epoch 348/40000, Loss: 0.003796177916228771, Learning Rate: 0.001918\n",
      "Epoch 349/40000, Loss: 0.004003996029496193, Learning Rate: 0.001918\n",
      "Epoch 350/40000, Loss: 0.0035111780744045973, Learning Rate: 0.001918\n",
      "Epoch 351/40000, Loss: 0.003562489291653037, Learning Rate: 0.001918\n",
      "Epoch 352/40000, Loss: 0.0038776975125074387, Learning Rate: 0.001917\n",
      "Epoch 353/40000, Loss: 0.004085320979356766, Learning Rate: 0.001917\n",
      "Epoch 354/40000, Loss: 0.0035814386792480946, Learning Rate: 0.001917\n",
      "Epoch 355/40000, Loss: 0.0035213190130889416, Learning Rate: 0.001917\n",
      "Epoch 356/40000, Loss: 0.0037603972014039755, Learning Rate: 0.001916\n",
      "Epoch 357/40000, Loss: 0.003943257499486208, Learning Rate: 0.001916\n",
      "Epoch 358/40000, Loss: 0.003481685183942318, Learning Rate: 0.001916\n",
      "Epoch 359/40000, Loss: 0.0035546261351555586, Learning Rate: 0.001916\n",
      "Epoch 360/40000, Loss: 0.0038124737329781055, Learning Rate: 0.001915\n",
      "Epoch 361/40000, Loss: 0.003700505942106247, Learning Rate: 0.001915\n",
      "Epoch 362/40000, Loss: 0.0035542387049645185, Learning Rate: 0.001915\n",
      "Epoch 363/40000, Loss: 0.003907481208443642, Learning Rate: 0.001915\n",
      "Epoch 364/40000, Loss: 0.0038150320760905743, Learning Rate: 0.001915\n",
      "Epoch 365/40000, Loss: 0.0044756862334907055, Learning Rate: 0.001914\n",
      "Epoch 366/40000, Loss: 0.004781309515237808, Learning Rate: 0.001914\n",
      "Epoch 367/40000, Loss: 0.004320949781686068, Learning Rate: 0.001914\n",
      "Epoch 368/40000, Loss: 0.004148164764046669, Learning Rate: 0.001914\n",
      "Epoch 369/40000, Loss: 0.004354805685579777, Learning Rate: 0.001913\n",
      "Epoch 370/40000, Loss: 0.0037030023522675037, Learning Rate: 0.001913\n",
      "Epoch 371/40000, Loss: 0.004034388344734907, Learning Rate: 0.001913\n",
      "Epoch 372/40000, Loss: 0.004083194769918919, Learning Rate: 0.001913\n",
      "Epoch 373/40000, Loss: 0.0037286062724888325, Learning Rate: 0.001912\n",
      "Epoch 374/40000, Loss: 0.003707996103912592, Learning Rate: 0.001912\n",
      "Epoch 375/40000, Loss: 0.0037741796113550663, Learning Rate: 0.001912\n",
      "Epoch 376/40000, Loss: 0.003431650809943676, Learning Rate: 0.001912\n",
      "Epoch 377/40000, Loss: 0.003376728855073452, Learning Rate: 0.001912\n",
      "Epoch 378/40000, Loss: 0.003760280553251505, Learning Rate: 0.001911\n",
      "Epoch 379/40000, Loss: 0.0038784132339060307, Learning Rate: 0.001911\n",
      "Epoch 380/40000, Loss: 0.0039010390173643827, Learning Rate: 0.001911\n",
      "Epoch 381/40000, Loss: 0.0034780397545546293, Learning Rate: 0.001911\n",
      "Epoch 382/40000, Loss: 0.003663504496216774, Learning Rate: 0.001910\n",
      "Epoch 383/40000, Loss: 0.0036807225551456213, Learning Rate: 0.001910\n",
      "Epoch 384/40000, Loss: 0.004576509818434715, Learning Rate: 0.001910\n",
      "Epoch 385/40000, Loss: 0.003928835503757, Learning Rate: 0.001910\n",
      "Epoch 386/40000, Loss: 0.004477764014154673, Learning Rate: 0.001909\n",
      "Epoch 387/40000, Loss: 0.004803851712495089, Learning Rate: 0.001909\n",
      "Epoch 388/40000, Loss: 0.004287920892238617, Learning Rate: 0.001909\n",
      "Epoch 389/40000, Loss: 0.0039250897243618965, Learning Rate: 0.001909\n",
      "Epoch 390/40000, Loss: 0.00355251831933856, Learning Rate: 0.001909\n",
      "Epoch 391/40000, Loss: 0.0035530580207705498, Learning Rate: 0.001908\n",
      "Epoch 392/40000, Loss: 0.004110990557819605, Learning Rate: 0.001908\n",
      "Epoch 393/40000, Loss: 0.0035613595973700285, Learning Rate: 0.001908\n",
      "Epoch 394/40000, Loss: 0.003488437971100211, Learning Rate: 0.001908\n",
      "Epoch 395/40000, Loss: 0.0034775719977915287, Learning Rate: 0.001907\n",
      "Epoch 396/40000, Loss: 0.0036875619553029537, Learning Rate: 0.001907\n",
      "Epoch 397/40000, Loss: 0.003945305477827787, Learning Rate: 0.001907\n",
      "Epoch 398/40000, Loss: 0.0037044778000563383, Learning Rate: 0.001907\n",
      "Epoch 399/40000, Loss: 0.0041674040257930756, Learning Rate: 0.001906\n",
      "Epoch 400/40000, Loss: 0.0035019784700125456, Learning Rate: 0.001906\n",
      "Epoch 401/40000, Loss: 0.0038627523463219404, Learning Rate: 0.001906\n",
      "Epoch 402/40000, Loss: 0.003681357717141509, Learning Rate: 0.001906\n",
      "Epoch 403/40000, Loss: 0.0036492138169705868, Learning Rate: 0.001906\n",
      "Epoch 404/40000, Loss: 0.00341515289619565, Learning Rate: 0.001905\n",
      "Epoch 405/40000, Loss: 0.003455644939094782, Learning Rate: 0.001905\n",
      "Epoch 406/40000, Loss: 0.00380763690918684, Learning Rate: 0.001905\n",
      "Epoch 407/40000, Loss: 0.003503179643303156, Learning Rate: 0.001905\n",
      "Epoch 408/40000, Loss: 0.003336508758366108, Learning Rate: 0.001904\n",
      "Epoch 409/40000, Loss: 0.003392731538042426, Learning Rate: 0.001904\n",
      "Epoch 410/40000, Loss: 0.0034432781394571066, Learning Rate: 0.001904\n",
      "Epoch 411/40000, Loss: 0.0034915031865239143, Learning Rate: 0.001904\n",
      "Epoch 412/40000, Loss: 0.003618251532316208, Learning Rate: 0.001904\n",
      "Epoch 413/40000, Loss: 0.0033815773203969, Learning Rate: 0.001903\n",
      "Epoch 414/40000, Loss: 0.003170563606545329, Learning Rate: 0.001903\n",
      "Epoch 415/40000, Loss: 0.0033252909779548645, Learning Rate: 0.001903\n",
      "Epoch 416/40000, Loss: 0.0035963391419500113, Learning Rate: 0.001903\n",
      "Epoch 417/40000, Loss: 0.0035150833427906036, Learning Rate: 0.001902\n",
      "Epoch 418/40000, Loss: 0.003228657878935337, Learning Rate: 0.001902\n",
      "Epoch 419/40000, Loss: 0.0033315764740109444, Learning Rate: 0.001902\n",
      "Epoch 420/40000, Loss: 0.0032688684295862913, Learning Rate: 0.001902\n",
      "Epoch 421/40000, Loss: 0.0035146332811564207, Learning Rate: 0.001901\n",
      "Epoch 422/40000, Loss: 0.0032499339431524277, Learning Rate: 0.001901\n",
      "Epoch 423/40000, Loss: 0.003648508107289672, Learning Rate: 0.001901\n",
      "Epoch 424/40000, Loss: 0.0034398415591567755, Learning Rate: 0.001901\n",
      "Epoch 425/40000, Loss: 0.004039247054606676, Learning Rate: 0.001901\n",
      "Epoch 426/40000, Loss: 0.003616345813497901, Learning Rate: 0.001900\n",
      "Epoch 427/40000, Loss: 0.0034353467635810375, Learning Rate: 0.001900\n",
      "Epoch 428/40000, Loss: 0.003225287888199091, Learning Rate: 0.001900\n",
      "Epoch 429/40000, Loss: 0.003420711262151599, Learning Rate: 0.001900\n",
      "Epoch 430/40000, Loss: 0.003271168563514948, Learning Rate: 0.001899\n",
      "Epoch 431/40000, Loss: 0.0031974059529602528, Learning Rate: 0.001899\n",
      "Epoch 432/40000, Loss: 0.003268886124715209, Learning Rate: 0.001899\n",
      "Epoch 433/40000, Loss: 0.003424751339480281, Learning Rate: 0.001899\n",
      "Epoch 434/40000, Loss: 0.0031996448524296284, Learning Rate: 0.001899\n",
      "Epoch 435/40000, Loss: 0.003225271590054035, Learning Rate: 0.001898\n",
      "Epoch 436/40000, Loss: 0.0032463555689901114, Learning Rate: 0.001898\n",
      "Epoch 437/40000, Loss: 0.0033171901013702154, Learning Rate: 0.001898\n",
      "Epoch 438/40000, Loss: 0.0036140340380370617, Learning Rate: 0.001898\n",
      "Epoch 439/40000, Loss: 0.0033587913494557142, Learning Rate: 0.001897\n",
      "Epoch 440/40000, Loss: 0.003658735891804099, Learning Rate: 0.001897\n",
      "Epoch 441/40000, Loss: 0.0034941276535391808, Learning Rate: 0.001897\n",
      "Epoch 442/40000, Loss: 0.0037044878117740154, Learning Rate: 0.001897\n",
      "Epoch 443/40000, Loss: 0.003519788384437561, Learning Rate: 0.001896\n",
      "Epoch 444/40000, Loss: 0.003396580694243312, Learning Rate: 0.001896\n",
      "Epoch 445/40000, Loss: 0.0034070906694978476, Learning Rate: 0.001896\n",
      "Epoch 446/40000, Loss: 0.0035126819275319576, Learning Rate: 0.001896\n",
      "Epoch 447/40000, Loss: 0.0031654490157961845, Learning Rate: 0.001896\n",
      "Epoch 448/40000, Loss: 0.0032411792781203985, Learning Rate: 0.001895\n",
      "Epoch 449/40000, Loss: 0.003214795608073473, Learning Rate: 0.001895\n",
      "Epoch 450/40000, Loss: 0.003270156681537628, Learning Rate: 0.001895\n",
      "Epoch 451/40000, Loss: 0.003582819364964962, Learning Rate: 0.001895\n",
      "Epoch 452/40000, Loss: 0.0033003482967615128, Learning Rate: 0.001894\n",
      "Epoch 453/40000, Loss: 0.003482719650492072, Learning Rate: 0.001894\n",
      "Epoch 454/40000, Loss: 0.003529809881001711, Learning Rate: 0.001894\n",
      "Epoch 455/40000, Loss: 0.0036350712180137634, Learning Rate: 0.001894\n",
      "Epoch 456/40000, Loss: 0.0034212851896882057, Learning Rate: 0.001893\n",
      "Epoch 457/40000, Loss: 0.003252874594181776, Learning Rate: 0.001893\n",
      "Epoch 458/40000, Loss: 0.0034553343430161476, Learning Rate: 0.001893\n",
      "Epoch 459/40000, Loss: 0.0032468889839947224, Learning Rate: 0.001893\n",
      "Epoch 460/40000, Loss: 0.003601066768169403, Learning Rate: 0.001893\n",
      "Epoch 461/40000, Loss: 0.0033186087384819984, Learning Rate: 0.001892\n",
      "Epoch 462/40000, Loss: 0.0032911677844822407, Learning Rate: 0.001892\n",
      "Epoch 463/40000, Loss: 0.003214373718947172, Learning Rate: 0.001892\n",
      "Epoch 464/40000, Loss: 0.003221763065084815, Learning Rate: 0.001892\n",
      "Epoch 465/40000, Loss: 0.003516600001603365, Learning Rate: 0.001891\n",
      "Epoch 466/40000, Loss: 0.0033611757680773735, Learning Rate: 0.001891\n",
      "Epoch 467/40000, Loss: 0.003301971359178424, Learning Rate: 0.001891\n",
      "Epoch 468/40000, Loss: 0.003185366280376911, Learning Rate: 0.001891\n",
      "Epoch 469/40000, Loss: 0.003214745782315731, Learning Rate: 0.001891\n",
      "Epoch 470/40000, Loss: 0.003467656672000885, Learning Rate: 0.001890\n",
      "Epoch 471/40000, Loss: 0.0032046623528003693, Learning Rate: 0.001890\n",
      "Epoch 472/40000, Loss: 0.0030520556028932333, Learning Rate: 0.001890\n",
      "Epoch 473/40000, Loss: 0.0035808838438242674, Learning Rate: 0.001890\n",
      "Epoch 474/40000, Loss: 0.0034515862353146076, Learning Rate: 0.001889\n",
      "Epoch 475/40000, Loss: 0.0033073252998292446, Learning Rate: 0.001889\n",
      "Epoch 476/40000, Loss: 0.0036389154847711325, Learning Rate: 0.001889\n",
      "Epoch 477/40000, Loss: 0.0034424997866153717, Learning Rate: 0.001889\n",
      "Epoch 478/40000, Loss: 0.003366768127307296, Learning Rate: 0.001889\n",
      "Epoch 479/40000, Loss: 0.003419402986764908, Learning Rate: 0.001888\n",
      "Epoch 480/40000, Loss: 0.003743839915841818, Learning Rate: 0.001888\n",
      "Epoch 481/40000, Loss: 0.0035001838114112616, Learning Rate: 0.001888\n",
      "Epoch 482/40000, Loss: 0.004055641125887632, Learning Rate: 0.001888\n",
      "Epoch 483/40000, Loss: 0.003988098353147507, Learning Rate: 0.001887\n",
      "Epoch 484/40000, Loss: 0.0039114030078053474, Learning Rate: 0.001887\n",
      "Epoch 485/40000, Loss: 0.0041543468832969666, Learning Rate: 0.001887\n",
      "Epoch 486/40000, Loss: 0.003939887508749962, Learning Rate: 0.001887\n",
      "Epoch 487/40000, Loss: 0.003780951490625739, Learning Rate: 0.001886\n",
      "Epoch 488/40000, Loss: 0.0034888614900410175, Learning Rate: 0.001886\n",
      "Epoch 489/40000, Loss: 0.003508050460368395, Learning Rate: 0.001886\n",
      "Epoch 490/40000, Loss: 0.0033014588989317417, Learning Rate: 0.001886\n",
      "Epoch 491/40000, Loss: 0.003183324821293354, Learning Rate: 0.001886\n",
      "Epoch 492/40000, Loss: 0.0030962848104536533, Learning Rate: 0.001885\n",
      "Epoch 493/40000, Loss: 0.003303809557110071, Learning Rate: 0.001885\n",
      "Epoch 494/40000, Loss: 0.0033214977011084557, Learning Rate: 0.001885\n",
      "Epoch 495/40000, Loss: 0.0032334662973880768, Learning Rate: 0.001885\n",
      "Epoch 496/40000, Loss: 0.0030691633000969887, Learning Rate: 0.001884\n",
      "Epoch 497/40000, Loss: 0.00303114065900445, Learning Rate: 0.001884\n",
      "Epoch 498/40000, Loss: 0.0033022412098944187, Learning Rate: 0.001884\n",
      "Epoch 499/40000, Loss: 0.0030293879099190235, Learning Rate: 0.001884\n",
      "Epoch 500/40000, Loss: 0.0029637606348842382, Learning Rate: 0.001884\n",
      "Epoch 501/40000, Loss: 0.003042371477931738, Learning Rate: 0.001883\n",
      "Epoch 502/40000, Loss: 0.0032222920563071966, Learning Rate: 0.001883\n",
      "Epoch 503/40000, Loss: 0.003101467853412032, Learning Rate: 0.001883\n",
      "Epoch 504/40000, Loss: 0.003250846639275551, Learning Rate: 0.001883\n",
      "Epoch 505/40000, Loss: 0.0030678973998874426, Learning Rate: 0.001882\n",
      "Epoch 506/40000, Loss: 0.002937380224466324, Learning Rate: 0.001882\n",
      "Epoch 507/40000, Loss: 0.0030471202917397022, Learning Rate: 0.001882\n",
      "Epoch 508/40000, Loss: 0.0031303472351282835, Learning Rate: 0.001882\n",
      "Epoch 509/40000, Loss: 0.0030962862074375153, Learning Rate: 0.001881\n",
      "Epoch 510/40000, Loss: 0.0029571480117738247, Learning Rate: 0.001881\n",
      "Epoch 511/40000, Loss: 0.0030310936272144318, Learning Rate: 0.001881\n",
      "Epoch 512/40000, Loss: 0.0032004001550376415, Learning Rate: 0.001881\n",
      "Epoch 513/40000, Loss: 0.003038895782083273, Learning Rate: 0.001881\n",
      "Epoch 514/40000, Loss: 0.003117935499176383, Learning Rate: 0.001880\n",
      "Epoch 515/40000, Loss: 0.003155394457280636, Learning Rate: 0.001880\n",
      "Epoch 516/40000, Loss: 0.0029979373794049025, Learning Rate: 0.001880\n",
      "Epoch 517/40000, Loss: 0.0030659388285130262, Learning Rate: 0.001880\n",
      "Epoch 518/40000, Loss: 0.0030060461722314358, Learning Rate: 0.001879\n",
      "Epoch 519/40000, Loss: 0.0031467843800783157, Learning Rate: 0.001879\n",
      "Epoch 520/40000, Loss: 0.00344518362544477, Learning Rate: 0.001879\n",
      "Epoch 521/40000, Loss: 0.003173557575792074, Learning Rate: 0.001879\n",
      "Epoch 522/40000, Loss: 0.0031865411438047886, Learning Rate: 0.001879\n",
      "Epoch 523/40000, Loss: 0.0033783894032239914, Learning Rate: 0.001878\n",
      "Epoch 524/40000, Loss: 0.0034790029749274254, Learning Rate: 0.001878\n",
      "Epoch 525/40000, Loss: 0.002943697152659297, Learning Rate: 0.001878\n",
      "Epoch 526/40000, Loss: 0.003391656558960676, Learning Rate: 0.001878\n",
      "Epoch 527/40000, Loss: 0.003232790157198906, Learning Rate: 0.001877\n",
      "Epoch 528/40000, Loss: 0.0029732626862823963, Learning Rate: 0.001877\n",
      "Epoch 529/40000, Loss: 0.003091034945100546, Learning Rate: 0.001877\n",
      "Epoch 530/40000, Loss: 0.0028954460285604, Learning Rate: 0.001877\n",
      "Epoch 531/40000, Loss: 0.0033098733983933926, Learning Rate: 0.001877\n",
      "Epoch 532/40000, Loss: 0.0029462184756994247, Learning Rate: 0.001876\n",
      "Epoch 533/40000, Loss: 0.0028991743456572294, Learning Rate: 0.001876\n",
      "Epoch 534/40000, Loss: 0.002922859974205494, Learning Rate: 0.001876\n",
      "Epoch 535/40000, Loss: 0.0030610389076173306, Learning Rate: 0.001876\n",
      "Epoch 536/40000, Loss: 0.0028522955253720284, Learning Rate: 0.001875\n",
      "Epoch 537/40000, Loss: 0.0027716706972569227, Learning Rate: 0.001875\n",
      "Epoch 538/40000, Loss: 0.0030195810832083225, Learning Rate: 0.001875\n",
      "Epoch 539/40000, Loss: 0.003049009246751666, Learning Rate: 0.001875\n",
      "Epoch 540/40000, Loss: 0.0029164208099246025, Learning Rate: 0.001875\n",
      "Epoch 541/40000, Loss: 0.003272324800491333, Learning Rate: 0.001874\n",
      "Epoch 542/40000, Loss: 0.003058513393625617, Learning Rate: 0.001874\n",
      "Epoch 543/40000, Loss: 0.0032233595848083496, Learning Rate: 0.001874\n",
      "Epoch 544/40000, Loss: 0.0029761900659650564, Learning Rate: 0.001874\n",
      "Epoch 545/40000, Loss: 0.0029842276126146317, Learning Rate: 0.001873\n",
      "Epoch 546/40000, Loss: 0.003001951612532139, Learning Rate: 0.001873\n",
      "Epoch 547/40000, Loss: 0.003451451426371932, Learning Rate: 0.001873\n",
      "Epoch 548/40000, Loss: 0.002918524667620659, Learning Rate: 0.001873\n",
      "Epoch 549/40000, Loss: 0.003045829711481929, Learning Rate: 0.001872\n",
      "Epoch 550/40000, Loss: 0.0029825661331415176, Learning Rate: 0.001872\n",
      "Epoch 551/40000, Loss: 0.0034392718225717545, Learning Rate: 0.001872\n",
      "Epoch 552/40000, Loss: 0.003945555537939072, Learning Rate: 0.001872\n",
      "Epoch 553/40000, Loss: 0.003201219718903303, Learning Rate: 0.001872\n",
      "Epoch 554/40000, Loss: 0.0033577082213014364, Learning Rate: 0.001871\n",
      "Epoch 555/40000, Loss: 0.003191534196957946, Learning Rate: 0.001871\n",
      "Epoch 556/40000, Loss: 0.0035552303306758404, Learning Rate: 0.001871\n",
      "Epoch 557/40000, Loss: 0.0033493670634925365, Learning Rate: 0.001871\n",
      "Epoch 558/40000, Loss: 0.0031019416637718678, Learning Rate: 0.001870\n",
      "Epoch 559/40000, Loss: 0.003046160563826561, Learning Rate: 0.001870\n",
      "Epoch 560/40000, Loss: 0.002969077555462718, Learning Rate: 0.001870\n",
      "Epoch 561/40000, Loss: 0.003266701241955161, Learning Rate: 0.001870\n",
      "Epoch 562/40000, Loss: 0.0030831776093691587, Learning Rate: 0.001870\n",
      "Epoch 563/40000, Loss: 0.0030176497530192137, Learning Rate: 0.001869\n",
      "Epoch 564/40000, Loss: 0.0029812492430210114, Learning Rate: 0.001869\n",
      "Epoch 565/40000, Loss: 0.002856612205505371, Learning Rate: 0.001869\n",
      "Epoch 566/40000, Loss: 0.0030374564230442047, Learning Rate: 0.001869\n",
      "Epoch 567/40000, Loss: 0.0029037005733698606, Learning Rate: 0.001868\n",
      "Epoch 568/40000, Loss: 0.003010156564414501, Learning Rate: 0.001868\n",
      "Epoch 569/40000, Loss: 0.0030019651167094707, Learning Rate: 0.001868\n",
      "Epoch 570/40000, Loss: 0.0032941761892288923, Learning Rate: 0.001868\n",
      "Epoch 571/40000, Loss: 0.0028443026822060347, Learning Rate: 0.001868\n",
      "Epoch 572/40000, Loss: 0.002976170741021633, Learning Rate: 0.001867\n",
      "Epoch 573/40000, Loss: 0.0028617121279239655, Learning Rate: 0.001867\n",
      "Epoch 574/40000, Loss: 0.0030430806800723076, Learning Rate: 0.001867\n",
      "Epoch 575/40000, Loss: 0.0030337839853018522, Learning Rate: 0.001867\n",
      "Epoch 576/40000, Loss: 0.002915207063779235, Learning Rate: 0.001866\n",
      "Epoch 577/40000, Loss: 0.0032817404717206955, Learning Rate: 0.001866\n",
      "Epoch 578/40000, Loss: 0.00285104988142848, Learning Rate: 0.001866\n",
      "Epoch 579/40000, Loss: 0.0028374986723065376, Learning Rate: 0.001866\n",
      "Epoch 580/40000, Loss: 0.0030272183939814568, Learning Rate: 0.001866\n",
      "Epoch 581/40000, Loss: 0.002881956985220313, Learning Rate: 0.001865\n",
      "Epoch 582/40000, Loss: 0.003122505731880665, Learning Rate: 0.001865\n",
      "Epoch 583/40000, Loss: 0.0030728140845894814, Learning Rate: 0.001865\n",
      "Epoch 584/40000, Loss: 0.0029970151372253895, Learning Rate: 0.001865\n",
      "Epoch 585/40000, Loss: 0.0032600925769656897, Learning Rate: 0.001864\n",
      "Epoch 586/40000, Loss: 0.0030965739861130714, Learning Rate: 0.001864\n",
      "Epoch 587/40000, Loss: 0.0031782507430762053, Learning Rate: 0.001864\n",
      "Epoch 588/40000, Loss: 0.003117901273071766, Learning Rate: 0.001864\n",
      "Epoch 589/40000, Loss: 0.0029994607903063297, Learning Rate: 0.001864\n",
      "Epoch 590/40000, Loss: 0.0029329597018659115, Learning Rate: 0.001863\n",
      "Epoch 591/40000, Loss: 0.002939456608146429, Learning Rate: 0.001863\n",
      "Epoch 592/40000, Loss: 0.002738812705501914, Learning Rate: 0.001863\n",
      "Epoch 593/40000, Loss: 0.0029356356244534254, Learning Rate: 0.001863\n",
      "Epoch 594/40000, Loss: 0.002811780199408531, Learning Rate: 0.001862\n",
      "Epoch 595/40000, Loss: 0.0030932303052395582, Learning Rate: 0.001862\n",
      "Epoch 596/40000, Loss: 0.0030022894497960806, Learning Rate: 0.001862\n",
      "Epoch 597/40000, Loss: 0.0031181692611426115, Learning Rate: 0.001862\n",
      "Epoch 598/40000, Loss: 0.003018522635102272, Learning Rate: 0.001862\n",
      "Epoch 599/40000, Loss: 0.004445383790880442, Learning Rate: 0.001861\n",
      "Epoch 600/40000, Loss: 0.0033445414155721664, Learning Rate: 0.001861\n",
      "Epoch 601/40000, Loss: 0.0029812040738761425, Learning Rate: 0.001861\n",
      "Epoch 602/40000, Loss: 0.002895550336688757, Learning Rate: 0.001861\n",
      "Epoch 603/40000, Loss: 0.0031995531171560287, Learning Rate: 0.001860\n",
      "Epoch 604/40000, Loss: 0.0028177134227007627, Learning Rate: 0.001860\n",
      "Epoch 605/40000, Loss: 0.0030615273863077164, Learning Rate: 0.001860\n",
      "Epoch 606/40000, Loss: 0.0030876509845256805, Learning Rate: 0.001860\n",
      "Epoch 607/40000, Loss: 0.0032751644030213356, Learning Rate: 0.001859\n",
      "Epoch 608/40000, Loss: 0.003057443303987384, Learning Rate: 0.001859\n",
      "Epoch 609/40000, Loss: 0.0029543843120336533, Learning Rate: 0.001859\n",
      "Epoch 610/40000, Loss: 0.0030749987345188856, Learning Rate: 0.001859\n",
      "Epoch 611/40000, Loss: 0.0033160815946757793, Learning Rate: 0.001859\n",
      "Epoch 612/40000, Loss: 0.0030384722631424665, Learning Rate: 0.001858\n",
      "Epoch 613/40000, Loss: 0.0031629945151507854, Learning Rate: 0.001858\n",
      "Epoch 614/40000, Loss: 0.0030842951964586973, Learning Rate: 0.001858\n",
      "Epoch 615/40000, Loss: 0.003071294631808996, Learning Rate: 0.001858\n",
      "Epoch 616/40000, Loss: 0.0027969330549240112, Learning Rate: 0.001857\n",
      "Epoch 617/40000, Loss: 0.0027800777461379766, Learning Rate: 0.001857\n",
      "Epoch 618/40000, Loss: 0.0027247872203588486, Learning Rate: 0.001857\n",
      "Epoch 619/40000, Loss: 0.0028555523604154587, Learning Rate: 0.001857\n",
      "Epoch 620/40000, Loss: 0.0027624459471553564, Learning Rate: 0.001857\n",
      "Epoch 621/40000, Loss: 0.0026824972592294216, Learning Rate: 0.001856\n",
      "Epoch 622/40000, Loss: 0.0026832479052245617, Learning Rate: 0.001856\n",
      "Epoch 623/40000, Loss: 0.0028525753878057003, Learning Rate: 0.001856\n",
      "Epoch 624/40000, Loss: 0.0027350930031389, Learning Rate: 0.001856\n",
      "Epoch 625/40000, Loss: 0.002676455769687891, Learning Rate: 0.001855\n",
      "Epoch 626/40000, Loss: 0.0026455800980329514, Learning Rate: 0.001855\n",
      "Epoch 627/40000, Loss: 0.002841319888830185, Learning Rate: 0.001855\n",
      "Epoch 628/40000, Loss: 0.0028853993862867355, Learning Rate: 0.001855\n",
      "Epoch 629/40000, Loss: 0.0029304518830031157, Learning Rate: 0.001855\n",
      "Epoch 630/40000, Loss: 0.0026291469112038612, Learning Rate: 0.001854\n",
      "Epoch 631/40000, Loss: 0.00285022659227252, Learning Rate: 0.001854\n",
      "Epoch 632/40000, Loss: 0.0030916696414351463, Learning Rate: 0.001854\n",
      "Epoch 633/40000, Loss: 0.0029140000697225332, Learning Rate: 0.001854\n",
      "Epoch 634/40000, Loss: 0.0029369164258241653, Learning Rate: 0.001853\n",
      "Epoch 635/40000, Loss: 0.0028627559076994658, Learning Rate: 0.001853\n",
      "Epoch 636/40000, Loss: 0.0027100916486233473, Learning Rate: 0.001853\n",
      "Epoch 637/40000, Loss: 0.0025961739011108875, Learning Rate: 0.001853\n",
      "Epoch 638/40000, Loss: 0.002779922913759947, Learning Rate: 0.001853\n",
      "Epoch 639/40000, Loss: 0.0028435038402676582, Learning Rate: 0.001852\n",
      "Epoch 640/40000, Loss: 0.0027184165082871914, Learning Rate: 0.001852\n",
      "Epoch 641/40000, Loss: 0.0029000169597566128, Learning Rate: 0.001852\n",
      "Epoch 642/40000, Loss: 0.0027862461283802986, Learning Rate: 0.001852\n",
      "Epoch 643/40000, Loss: 0.0027248309925198555, Learning Rate: 0.001851\n",
      "Epoch 644/40000, Loss: 0.0028866061475127935, Learning Rate: 0.001851\n",
      "Epoch 645/40000, Loss: 0.0027754956390708685, Learning Rate: 0.001851\n",
      "Epoch 646/40000, Loss: 0.0033516341354697943, Learning Rate: 0.001851\n",
      "Epoch 647/40000, Loss: 0.002883985172957182, Learning Rate: 0.001851\n",
      "Epoch 648/40000, Loss: 0.0029116403311491013, Learning Rate: 0.001850\n",
      "Epoch 649/40000, Loss: 0.002927213441580534, Learning Rate: 0.001850\n",
      "Epoch 650/40000, Loss: 0.0028228857554495335, Learning Rate: 0.001850\n",
      "Epoch 651/40000, Loss: 0.0027551797684282064, Learning Rate: 0.001850\n",
      "Epoch 652/40000, Loss: 0.002789687365293503, Learning Rate: 0.001849\n",
      "Epoch 653/40000, Loss: 0.002775076776742935, Learning Rate: 0.001849\n",
      "Epoch 654/40000, Loss: 0.002684913342818618, Learning Rate: 0.001849\n",
      "Epoch 655/40000, Loss: 0.0027957044076174498, Learning Rate: 0.001849\n",
      "Epoch 656/40000, Loss: 0.002739112824201584, Learning Rate: 0.001849\n",
      "Epoch 657/40000, Loss: 0.0027124746702611446, Learning Rate: 0.001848\n",
      "Epoch 658/40000, Loss: 0.0030069597996771336, Learning Rate: 0.001848\n",
      "Epoch 659/40000, Loss: 0.0029466059058904648, Learning Rate: 0.001848\n",
      "Epoch 660/40000, Loss: 0.0028958092443645, Learning Rate: 0.001848\n",
      "Epoch 661/40000, Loss: 0.002945605432614684, Learning Rate: 0.001847\n",
      "Epoch 662/40000, Loss: 0.00286435941234231, Learning Rate: 0.001847\n",
      "Epoch 663/40000, Loss: 0.003255989868193865, Learning Rate: 0.001847\n",
      "Epoch 664/40000, Loss: 0.002930855844169855, Learning Rate: 0.001847\n",
      "Epoch 665/40000, Loss: 0.002724775578826666, Learning Rate: 0.001847\n",
      "Epoch 666/40000, Loss: 0.0028559789061546326, Learning Rate: 0.001846\n",
      "Epoch 667/40000, Loss: 0.0027100001461803913, Learning Rate: 0.001846\n",
      "Epoch 668/40000, Loss: 0.0028967985417693853, Learning Rate: 0.001846\n",
      "Epoch 669/40000, Loss: 0.002895175013691187, Learning Rate: 0.001846\n",
      "Epoch 670/40000, Loss: 0.0027534407563507557, Learning Rate: 0.001845\n",
      "Epoch 671/40000, Loss: 0.002991889137774706, Learning Rate: 0.001845\n",
      "Epoch 672/40000, Loss: 0.0027956864796578884, Learning Rate: 0.001845\n",
      "Epoch 673/40000, Loss: 0.002816882450133562, Learning Rate: 0.001845\n",
      "Epoch 674/40000, Loss: 0.0028349552303552628, Learning Rate: 0.001845\n",
      "Epoch 675/40000, Loss: 0.0031264121644198895, Learning Rate: 0.001844\n",
      "Epoch 676/40000, Loss: 0.0032074162736535072, Learning Rate: 0.001844\n",
      "Epoch 677/40000, Loss: 0.0027223052456974983, Learning Rate: 0.001844\n",
      "Epoch 678/40000, Loss: 0.0026136646047234535, Learning Rate: 0.001844\n",
      "Epoch 679/40000, Loss: 0.003010917454957962, Learning Rate: 0.001844\n",
      "Epoch 680/40000, Loss: 0.0027653034776449203, Learning Rate: 0.001843\n",
      "Epoch 681/40000, Loss: 0.002854976337403059, Learning Rate: 0.001843\n",
      "Epoch 682/40000, Loss: 0.002746890764683485, Learning Rate: 0.001843\n",
      "Epoch 683/40000, Loss: 0.002664192346855998, Learning Rate: 0.001843\n",
      "Epoch 684/40000, Loss: 0.0027561690658330917, Learning Rate: 0.001842\n",
      "Epoch 685/40000, Loss: 0.002663905266672373, Learning Rate: 0.001842\n",
      "Epoch 686/40000, Loss: 0.003101722337305546, Learning Rate: 0.001842\n",
      "Epoch 687/40000, Loss: 0.002833783393725753, Learning Rate: 0.001842\n",
      "Epoch 688/40000, Loss: 0.003200724720954895, Learning Rate: 0.001842\n",
      "Epoch 689/40000, Loss: 0.0028889626264572144, Learning Rate: 0.001841\n",
      "Epoch 690/40000, Loss: 0.002802083268761635, Learning Rate: 0.001841\n",
      "Epoch 691/40000, Loss: 0.0029084207490086555, Learning Rate: 0.001841\n",
      "Epoch 692/40000, Loss: 0.002721475437283516, Learning Rate: 0.001841\n",
      "Epoch 693/40000, Loss: 0.002975122770294547, Learning Rate: 0.001840\n",
      "Epoch 694/40000, Loss: 0.0030398834496736526, Learning Rate: 0.001840\n",
      "Epoch 695/40000, Loss: 0.0029108882881700993, Learning Rate: 0.001840\n",
      "Epoch 696/40000, Loss: 0.0027111005038022995, Learning Rate: 0.001840\n",
      "Epoch 697/40000, Loss: 0.00289626675657928, Learning Rate: 0.001840\n",
      "Epoch 698/40000, Loss: 0.002562705660238862, Learning Rate: 0.001839\n",
      "Epoch 699/40000, Loss: 0.00296476180665195, Learning Rate: 0.001839\n",
      "Epoch 700/40000, Loss: 0.002751838183030486, Learning Rate: 0.001839\n",
      "Epoch 701/40000, Loss: 0.0026230819057673216, Learning Rate: 0.001839\n",
      "Epoch 702/40000, Loss: 0.002591937780380249, Learning Rate: 0.001838\n",
      "Epoch 703/40000, Loss: 0.0028534862212836742, Learning Rate: 0.001838\n",
      "Epoch 704/40000, Loss: 0.0026305951178073883, Learning Rate: 0.001838\n",
      "Epoch 705/40000, Loss: 0.0029403669759631157, Learning Rate: 0.001838\n",
      "Epoch 706/40000, Loss: 0.002728030551224947, Learning Rate: 0.001838\n",
      "Epoch 707/40000, Loss: 0.0028787131886929274, Learning Rate: 0.001837\n",
      "Epoch 708/40000, Loss: 0.002786717377603054, Learning Rate: 0.001837\n",
      "Epoch 709/40000, Loss: 0.0029335464350879192, Learning Rate: 0.001837\n",
      "Epoch 710/40000, Loss: 0.002967339474707842, Learning Rate: 0.001837\n",
      "Epoch 711/40000, Loss: 0.0034759650006890297, Learning Rate: 0.001836\n",
      "Epoch 712/40000, Loss: 0.0029147672466933727, Learning Rate: 0.001836\n",
      "Epoch 713/40000, Loss: 0.002855726983398199, Learning Rate: 0.001836\n",
      "Epoch 714/40000, Loss: 0.0025920371990650892, Learning Rate: 0.001836\n",
      "Epoch 715/40000, Loss: 0.002877721330150962, Learning Rate: 0.001836\n",
      "Epoch 716/40000, Loss: 0.0025939466431736946, Learning Rate: 0.001835\n",
      "Epoch 717/40000, Loss: 0.002899253275245428, Learning Rate: 0.001835\n",
      "Epoch 718/40000, Loss: 0.0027821618132293224, Learning Rate: 0.001835\n",
      "Epoch 719/40000, Loss: 0.0027229608967900276, Learning Rate: 0.001835\n",
      "Epoch 720/40000, Loss: 0.002732518594712019, Learning Rate: 0.001834\n",
      "Epoch 721/40000, Loss: 0.0027950413059443235, Learning Rate: 0.001834\n",
      "Epoch 722/40000, Loss: 0.002653710078448057, Learning Rate: 0.001834\n",
      "Epoch 723/40000, Loss: 0.0027615397702902555, Learning Rate: 0.001834\n",
      "Epoch 724/40000, Loss: 0.0027630687691271305, Learning Rate: 0.001834\n",
      "Epoch 725/40000, Loss: 0.002539512701332569, Learning Rate: 0.001833\n",
      "Epoch 726/40000, Loss: 0.002910447074100375, Learning Rate: 0.001833\n",
      "Epoch 727/40000, Loss: 0.002577611245214939, Learning Rate: 0.001833\n",
      "Epoch 728/40000, Loss: 0.0026117495726794004, Learning Rate: 0.001833\n",
      "Epoch 729/40000, Loss: 0.002441832795739174, Learning Rate: 0.001832\n",
      "Epoch 730/40000, Loss: 0.002764231525361538, Learning Rate: 0.001832\n",
      "Epoch 731/40000, Loss: 0.0028164288960397243, Learning Rate: 0.001832\n",
      "Epoch 732/40000, Loss: 0.0027931954246014357, Learning Rate: 0.001832\n",
      "Epoch 733/40000, Loss: 0.0025483916979283094, Learning Rate: 0.001832\n",
      "Epoch 734/40000, Loss: 0.0025010942481458187, Learning Rate: 0.001831\n",
      "Epoch 735/40000, Loss: 0.002648613415658474, Learning Rate: 0.001831\n",
      "Epoch 736/40000, Loss: 0.002631590235978365, Learning Rate: 0.001831\n",
      "Epoch 737/40000, Loss: 0.0028487208765000105, Learning Rate: 0.001831\n",
      "Epoch 738/40000, Loss: 0.002957017859444022, Learning Rate: 0.001830\n",
      "Epoch 739/40000, Loss: 0.002793214749544859, Learning Rate: 0.001830\n",
      "Epoch 740/40000, Loss: 0.002785776276141405, Learning Rate: 0.001830\n",
      "Epoch 741/40000, Loss: 0.0026102466508746147, Learning Rate: 0.001830\n",
      "Epoch 742/40000, Loss: 0.0025795157998800278, Learning Rate: 0.001830\n",
      "Epoch 743/40000, Loss: 0.0028552506119012833, Learning Rate: 0.001829\n",
      "Epoch 744/40000, Loss: 0.0027357458602637053, Learning Rate: 0.001829\n",
      "Epoch 745/40000, Loss: 0.0026211715303361416, Learning Rate: 0.001829\n",
      "Epoch 746/40000, Loss: 0.0027385288849473, Learning Rate: 0.001829\n",
      "Epoch 747/40000, Loss: 0.0025696102529764175, Learning Rate: 0.001829\n",
      "Epoch 748/40000, Loss: 0.002637153258547187, Learning Rate: 0.001828\n",
      "Epoch 749/40000, Loss: 0.0026965299621224403, Learning Rate: 0.001828\n",
      "Epoch 750/40000, Loss: 0.0026943935081362724, Learning Rate: 0.001828\n",
      "Epoch 751/40000, Loss: 0.0024858678225427866, Learning Rate: 0.001828\n",
      "Epoch 752/40000, Loss: 0.002468152204528451, Learning Rate: 0.001827\n",
      "Epoch 753/40000, Loss: 0.0024898629635572433, Learning Rate: 0.001827\n",
      "Epoch 754/40000, Loss: 0.0027159047313034534, Learning Rate: 0.001827\n",
      "Epoch 755/40000, Loss: 0.0025218618102371693, Learning Rate: 0.001827\n",
      "Epoch 756/40000, Loss: 0.002513483865186572, Learning Rate: 0.001827\n",
      "Epoch 757/40000, Loss: 0.002590371761471033, Learning Rate: 0.001826\n",
      "Epoch 758/40000, Loss: 0.0024808410089462996, Learning Rate: 0.001826\n",
      "Epoch 759/40000, Loss: 0.0024590473622083664, Learning Rate: 0.001826\n",
      "Epoch 760/40000, Loss: 0.0027375370264053345, Learning Rate: 0.001826\n",
      "Epoch 761/40000, Loss: 0.002625823952257633, Learning Rate: 0.001825\n",
      "Epoch 762/40000, Loss: 0.002540476154536009, Learning Rate: 0.001825\n",
      "Epoch 763/40000, Loss: 0.0026123933494091034, Learning Rate: 0.001825\n",
      "Epoch 764/40000, Loss: 0.0025355243124067783, Learning Rate: 0.001825\n",
      "Epoch 765/40000, Loss: 0.0026388950645923615, Learning Rate: 0.001825\n",
      "Epoch 766/40000, Loss: 0.0026747570373117924, Learning Rate: 0.001824\n",
      "Epoch 767/40000, Loss: 0.0025881812907755375, Learning Rate: 0.001824\n",
      "Epoch 768/40000, Loss: 0.0026736711151897907, Learning Rate: 0.001824\n",
      "Epoch 769/40000, Loss: 0.002638635691255331, Learning Rate: 0.001824\n",
      "Epoch 770/40000, Loss: 0.0024846727028489113, Learning Rate: 0.001823\n",
      "Epoch 771/40000, Loss: 0.0023712688125669956, Learning Rate: 0.001823\n",
      "Epoch 772/40000, Loss: 0.002655262826010585, Learning Rate: 0.001823\n",
      "Epoch 773/40000, Loss: 0.0024826470762491226, Learning Rate: 0.001823\n",
      "Epoch 774/40000, Loss: 0.0025087292306125164, Learning Rate: 0.001823\n",
      "Epoch 775/40000, Loss: 0.0026727819349616766, Learning Rate: 0.001822\n",
      "Epoch 776/40000, Loss: 0.002498784800991416, Learning Rate: 0.001822\n",
      "Epoch 777/40000, Loss: 0.002671577502042055, Learning Rate: 0.001822\n",
      "Epoch 778/40000, Loss: 0.0026735635474324226, Learning Rate: 0.001822\n",
      "Epoch 779/40000, Loss: 0.0026447807904332876, Learning Rate: 0.001822\n",
      "Epoch 780/40000, Loss: 0.002671262016519904, Learning Rate: 0.001821\n",
      "Epoch 781/40000, Loss: 0.002597860060632229, Learning Rate: 0.001821\n",
      "Epoch 782/40000, Loss: 0.0025833742693066597, Learning Rate: 0.001821\n",
      "Epoch 783/40000, Loss: 0.002536419779062271, Learning Rate: 0.001821\n",
      "Epoch 784/40000, Loss: 0.002507994184270501, Learning Rate: 0.001820\n",
      "Epoch 785/40000, Loss: 0.002537402557209134, Learning Rate: 0.001820\n",
      "Epoch 786/40000, Loss: 0.0025885794311761856, Learning Rate: 0.001820\n",
      "Epoch 787/40000, Loss: 0.0024701757356524467, Learning Rate: 0.001820\n",
      "Epoch 788/40000, Loss: 0.0025730363558977842, Learning Rate: 0.001820\n",
      "Epoch 789/40000, Loss: 0.00240636826492846, Learning Rate: 0.001819\n",
      "Epoch 790/40000, Loss: 0.002429249929264188, Learning Rate: 0.001819\n",
      "Epoch 791/40000, Loss: 0.0027483708690851927, Learning Rate: 0.001819\n",
      "Epoch 792/40000, Loss: 0.002501741051673889, Learning Rate: 0.001819\n",
      "Epoch 793/40000, Loss: 0.0027953190729022026, Learning Rate: 0.001818\n",
      "Epoch 794/40000, Loss: 0.00247626518830657, Learning Rate: 0.001818\n",
      "Epoch 795/40000, Loss: 0.0025243712589144707, Learning Rate: 0.001818\n",
      "Epoch 796/40000, Loss: 0.0025275577791035175, Learning Rate: 0.001818\n",
      "Epoch 797/40000, Loss: 0.0026798113249242306, Learning Rate: 0.001818\n",
      "Epoch 798/40000, Loss: 0.002549553057178855, Learning Rate: 0.001817\n",
      "Epoch 799/40000, Loss: 0.002789833350107074, Learning Rate: 0.001817\n",
      "Epoch 800/40000, Loss: 0.0024880599230527878, Learning Rate: 0.001817\n",
      "Epoch 801/40000, Loss: 0.0025083813816308975, Learning Rate: 0.001817\n",
      "Epoch 802/40000, Loss: 0.0026357804890722036, Learning Rate: 0.001816\n",
      "Epoch 803/40000, Loss: 0.00255033141002059, Learning Rate: 0.001816\n",
      "Epoch 804/40000, Loss: 0.0032614460214972496, Learning Rate: 0.001816\n",
      "Epoch 805/40000, Loss: 0.002815320622175932, Learning Rate: 0.001816\n",
      "Epoch 806/40000, Loss: 0.0029815658926963806, Learning Rate: 0.001816\n",
      "Epoch 807/40000, Loss: 0.003011454362422228, Learning Rate: 0.001815\n",
      "Epoch 808/40000, Loss: 0.0033150676172226667, Learning Rate: 0.001815\n",
      "Epoch 809/40000, Loss: 0.002814335748553276, Learning Rate: 0.001815\n",
      "Epoch 810/40000, Loss: 0.002724447986111045, Learning Rate: 0.001815\n",
      "Epoch 811/40000, Loss: 0.002945385407656431, Learning Rate: 0.001815\n",
      "Epoch 812/40000, Loss: 0.002788993064314127, Learning Rate: 0.001814\n",
      "Epoch 813/40000, Loss: 0.0036251461133360863, Learning Rate: 0.001814\n",
      "Epoch 814/40000, Loss: 0.004003249574452639, Learning Rate: 0.001814\n",
      "Epoch 815/40000, Loss: 0.003645391669124365, Learning Rate: 0.001814\n",
      "Epoch 816/40000, Loss: 0.003803595434874296, Learning Rate: 0.001813\n",
      "Epoch 817/40000, Loss: 0.0028457180596888065, Learning Rate: 0.001813\n",
      "Epoch 818/40000, Loss: 0.002881983993574977, Learning Rate: 0.001813\n",
      "Epoch 819/40000, Loss: 0.0029911240562796593, Learning Rate: 0.001813\n",
      "Epoch 820/40000, Loss: 0.0026062852703034878, Learning Rate: 0.001813\n",
      "Epoch 821/40000, Loss: 0.0029416007455438375, Learning Rate: 0.001812\n",
      "Epoch 822/40000, Loss: 0.0028749348130077124, Learning Rate: 0.001812\n",
      "Epoch 823/40000, Loss: 0.0027867546305060387, Learning Rate: 0.001812\n",
      "Epoch 824/40000, Loss: 0.002673223614692688, Learning Rate: 0.001812\n",
      "Epoch 825/40000, Loss: 0.002605636138468981, Learning Rate: 0.001811\n",
      "Epoch 826/40000, Loss: 0.002737764036282897, Learning Rate: 0.001811\n",
      "Epoch 827/40000, Loss: 0.0034205967094749212, Learning Rate: 0.001811\n",
      "Epoch 828/40000, Loss: 0.0029297745786607265, Learning Rate: 0.001811\n",
      "Epoch 829/40000, Loss: 0.002820348832756281, Learning Rate: 0.001811\n",
      "Epoch 830/40000, Loss: 0.0026766997762024403, Learning Rate: 0.001810\n",
      "Epoch 831/40000, Loss: 0.0031687431037425995, Learning Rate: 0.001810\n",
      "Epoch 832/40000, Loss: 0.002490248531103134, Learning Rate: 0.001810\n",
      "Epoch 833/40000, Loss: 0.0026732641272246838, Learning Rate: 0.001810\n",
      "Epoch 834/40000, Loss: 0.002769764047116041, Learning Rate: 0.001810\n",
      "Epoch 835/40000, Loss: 0.0029839733615517616, Learning Rate: 0.001809\n",
      "Epoch 836/40000, Loss: 0.0029493565671145916, Learning Rate: 0.001809\n",
      "Epoch 837/40000, Loss: 0.0028709869366139174, Learning Rate: 0.001809\n",
      "Epoch 838/40000, Loss: 0.0026889857836067677, Learning Rate: 0.001809\n",
      "Epoch 839/40000, Loss: 0.002597913844510913, Learning Rate: 0.001808\n",
      "Epoch 840/40000, Loss: 0.0026357013266533613, Learning Rate: 0.001808\n",
      "Epoch 841/40000, Loss: 0.0030448762699961662, Learning Rate: 0.001808\n",
      "Epoch 842/40000, Loss: 0.002840678906068206, Learning Rate: 0.001808\n",
      "Epoch 843/40000, Loss: 0.0026040743105113506, Learning Rate: 0.001808\n",
      "Epoch 844/40000, Loss: 0.00288293045014143, Learning Rate: 0.001807\n",
      "Epoch 845/40000, Loss: 0.0029585694428533316, Learning Rate: 0.001807\n",
      "Epoch 846/40000, Loss: 0.002566539216786623, Learning Rate: 0.001807\n",
      "Epoch 847/40000, Loss: 0.0026075912173837423, Learning Rate: 0.001807\n",
      "Epoch 848/40000, Loss: 0.002790404949337244, Learning Rate: 0.001806\n",
      "Epoch 849/40000, Loss: 0.0026574204675853252, Learning Rate: 0.001806\n",
      "Epoch 850/40000, Loss: 0.0028764032758772373, Learning Rate: 0.001806\n",
      "Epoch 851/40000, Loss: 0.002787800505757332, Learning Rate: 0.001806\n",
      "Epoch 852/40000, Loss: 0.002695894567295909, Learning Rate: 0.001806\n",
      "Epoch 853/40000, Loss: 0.002417625393718481, Learning Rate: 0.001805\n",
      "Epoch 854/40000, Loss: 0.002511377912014723, Learning Rate: 0.001805\n",
      "Epoch 855/40000, Loss: 0.0024536289274692535, Learning Rate: 0.001805\n",
      "Epoch 856/40000, Loss: 0.002410551533102989, Learning Rate: 0.001805\n",
      "Epoch 857/40000, Loss: 0.0026985190343111753, Learning Rate: 0.001805\n",
      "Epoch 858/40000, Loss: 0.002451474079862237, Learning Rate: 0.001804\n",
      "Epoch 859/40000, Loss: 0.0027288775891065598, Learning Rate: 0.001804\n",
      "Epoch 860/40000, Loss: 0.002600087784230709, Learning Rate: 0.001804\n",
      "Epoch 861/40000, Loss: 0.002346646972000599, Learning Rate: 0.001804\n",
      "Epoch 862/40000, Loss: 0.002534277271479368, Learning Rate: 0.001803\n",
      "Epoch 863/40000, Loss: 0.0026668438222259283, Learning Rate: 0.001803\n",
      "Epoch 864/40000, Loss: 0.002504617441445589, Learning Rate: 0.001803\n",
      "Epoch 865/40000, Loss: 0.002502296818420291, Learning Rate: 0.001803\n",
      "Epoch 866/40000, Loss: 0.002394090872257948, Learning Rate: 0.001803\n",
      "Epoch 867/40000, Loss: 0.002469779923558235, Learning Rate: 0.001802\n",
      "Epoch 868/40000, Loss: 0.0027059013955295086, Learning Rate: 0.001802\n",
      "Epoch 869/40000, Loss: 0.0026738259475678205, Learning Rate: 0.001802\n",
      "Epoch 870/40000, Loss: 0.0025259635876864195, Learning Rate: 0.001802\n",
      "Epoch 871/40000, Loss: 0.0024982071481645107, Learning Rate: 0.001802\n",
      "Epoch 872/40000, Loss: 0.0024050844367593527, Learning Rate: 0.001801\n",
      "Epoch 873/40000, Loss: 0.002627644222229719, Learning Rate: 0.001801\n",
      "Epoch 874/40000, Loss: 0.0025980588980019093, Learning Rate: 0.001801\n",
      "Epoch 875/40000, Loss: 0.002545837312936783, Learning Rate: 0.001801\n",
      "Epoch 876/40000, Loss: 0.0025252713821828365, Learning Rate: 0.001800\n",
      "Epoch 877/40000, Loss: 0.0025739246048033237, Learning Rate: 0.001800\n",
      "Epoch 878/40000, Loss: 0.002433564979583025, Learning Rate: 0.001800\n",
      "Epoch 879/40000, Loss: 0.0023501559626311064, Learning Rate: 0.001800\n",
      "Epoch 880/40000, Loss: 0.0023947139270603657, Learning Rate: 0.001800\n",
      "Epoch 881/40000, Loss: 0.002284715883433819, Learning Rate: 0.001799\n",
      "Epoch 882/40000, Loss: 0.0022099618799984455, Learning Rate: 0.001799\n",
      "Epoch 883/40000, Loss: 0.0023645288310945034, Learning Rate: 0.001799\n",
      "Epoch 884/40000, Loss: 0.0024054651148617268, Learning Rate: 0.001799\n",
      "Epoch 885/40000, Loss: 0.002273453399538994, Learning Rate: 0.001798\n",
      "Epoch 886/40000, Loss: 0.0024044313468039036, Learning Rate: 0.001798\n",
      "Epoch 887/40000, Loss: 0.0023398727644234896, Learning Rate: 0.001798\n",
      "Epoch 888/40000, Loss: 0.0024917658884078264, Learning Rate: 0.001798\n",
      "Epoch 889/40000, Loss: 0.0022963862866163254, Learning Rate: 0.001798\n",
      "Epoch 890/40000, Loss: 0.0024093501269817352, Learning Rate: 0.001797\n",
      "Epoch 891/40000, Loss: 0.002429027110338211, Learning Rate: 0.001797\n",
      "Epoch 892/40000, Loss: 0.0024683307856321335, Learning Rate: 0.001797\n",
      "Epoch 893/40000, Loss: 0.002318389480933547, Learning Rate: 0.001797\n",
      "Epoch 894/40000, Loss: 0.0022806948982179165, Learning Rate: 0.001797\n",
      "Epoch 895/40000, Loss: 0.002306983107700944, Learning Rate: 0.001796\n",
      "Epoch 896/40000, Loss: 0.0022612493485212326, Learning Rate: 0.001796\n",
      "Epoch 897/40000, Loss: 0.002284666057676077, Learning Rate: 0.001796\n",
      "Epoch 898/40000, Loss: 0.0022718384861946106, Learning Rate: 0.001796\n",
      "Epoch 899/40000, Loss: 0.0023267848882824183, Learning Rate: 0.001795\n",
      "Epoch 900/40000, Loss: 0.0024158088490366936, Learning Rate: 0.001795\n",
      "Epoch 901/40000, Loss: 0.00232529966160655, Learning Rate: 0.001795\n",
      "Epoch 902/40000, Loss: 0.0025081573985517025, Learning Rate: 0.001795\n",
      "Epoch 903/40000, Loss: 0.002370060421526432, Learning Rate: 0.001795\n",
      "Epoch 904/40000, Loss: 0.002401458565145731, Learning Rate: 0.001794\n",
      "Epoch 905/40000, Loss: 0.002256924519315362, Learning Rate: 0.001794\n",
      "Epoch 906/40000, Loss: 0.0024894175585359335, Learning Rate: 0.001794\n",
      "Epoch 907/40000, Loss: 0.0025182117242366076, Learning Rate: 0.001794\n",
      "Epoch 908/40000, Loss: 0.0029621259309351444, Learning Rate: 0.001794\n",
      "Epoch 909/40000, Loss: 0.0029598146211355925, Learning Rate: 0.001793\n",
      "Epoch 910/40000, Loss: 0.002607958158478141, Learning Rate: 0.001793\n",
      "Epoch 911/40000, Loss: 0.0025773211382329464, Learning Rate: 0.001793\n",
      "Epoch 912/40000, Loss: 0.0028067806269973516, Learning Rate: 0.001793\n",
      "Epoch 913/40000, Loss: 0.0024838342797011137, Learning Rate: 0.001792\n",
      "Epoch 914/40000, Loss: 0.002534304978325963, Learning Rate: 0.001792\n",
      "Epoch 915/40000, Loss: 0.002464295132085681, Learning Rate: 0.001792\n",
      "Epoch 916/40000, Loss: 0.002473704982548952, Learning Rate: 0.001792\n",
      "Epoch 917/40000, Loss: 0.0026661595329642296, Learning Rate: 0.001792\n",
      "Epoch 918/40000, Loss: 0.002407737774774432, Learning Rate: 0.001791\n",
      "Epoch 919/40000, Loss: 0.0026176287792623043, Learning Rate: 0.001791\n",
      "Epoch 920/40000, Loss: 0.002663771156221628, Learning Rate: 0.001791\n",
      "Epoch 921/40000, Loss: 0.0026420631911605597, Learning Rate: 0.001791\n",
      "Epoch 922/40000, Loss: 0.0026877722702920437, Learning Rate: 0.001791\n",
      "Epoch 923/40000, Loss: 0.0025966474786400795, Learning Rate: 0.001790\n",
      "Epoch 924/40000, Loss: 0.0027227511163800955, Learning Rate: 0.001790\n",
      "Epoch 925/40000, Loss: 0.002491750055924058, Learning Rate: 0.001790\n",
      "Epoch 926/40000, Loss: 0.002545361639931798, Learning Rate: 0.001790\n",
      "Epoch 927/40000, Loss: 0.002771413652226329, Learning Rate: 0.001789\n",
      "Epoch 928/40000, Loss: 0.0023924540728330612, Learning Rate: 0.001789\n",
      "Epoch 929/40000, Loss: 0.0024279309436678886, Learning Rate: 0.001789\n",
      "Epoch 930/40000, Loss: 0.0027245176024734974, Learning Rate: 0.001789\n",
      "Epoch 931/40000, Loss: 0.0027871294878423214, Learning Rate: 0.001789\n",
      "Epoch 932/40000, Loss: 0.0026632649824023247, Learning Rate: 0.001788\n",
      "Epoch 933/40000, Loss: 0.0025956870522350073, Learning Rate: 0.001788\n",
      "Epoch 934/40000, Loss: 0.002659631660208106, Learning Rate: 0.001788\n",
      "Epoch 935/40000, Loss: 0.0026340053882449865, Learning Rate: 0.001788\n",
      "Epoch 936/40000, Loss: 0.0024550845846533775, Learning Rate: 0.001788\n",
      "Epoch 937/40000, Loss: 0.0025762494187802076, Learning Rate: 0.001787\n",
      "Epoch 938/40000, Loss: 0.002513857325538993, Learning Rate: 0.001787\n",
      "Epoch 939/40000, Loss: 0.0024291090667247772, Learning Rate: 0.001787\n",
      "Epoch 940/40000, Loss: 0.0024280152283608913, Learning Rate: 0.001787\n",
      "Epoch 941/40000, Loss: 0.0024389084428548813, Learning Rate: 0.001786\n",
      "Epoch 942/40000, Loss: 0.002270835218951106, Learning Rate: 0.001786\n",
      "Epoch 943/40000, Loss: 0.002379413926973939, Learning Rate: 0.001786\n",
      "Epoch 944/40000, Loss: 0.0024140276946127415, Learning Rate: 0.001786\n",
      "Epoch 945/40000, Loss: 0.0024269134737551212, Learning Rate: 0.001786\n",
      "Epoch 946/40000, Loss: 0.0022092238068580627, Learning Rate: 0.001785\n",
      "Epoch 947/40000, Loss: 0.0023041442036628723, Learning Rate: 0.001785\n",
      "Epoch 948/40000, Loss: 0.002459658309817314, Learning Rate: 0.001785\n",
      "Epoch 949/40000, Loss: 0.0023687647189944983, Learning Rate: 0.001785\n",
      "Epoch 950/40000, Loss: 0.0023276223801076412, Learning Rate: 0.001785\n",
      "Epoch 951/40000, Loss: 0.0022511067800223827, Learning Rate: 0.001784\n",
      "Epoch 952/40000, Loss: 0.0024226359091699123, Learning Rate: 0.001784\n",
      "Epoch 953/40000, Loss: 0.0022634922061115503, Learning Rate: 0.001784\n",
      "Epoch 954/40000, Loss: 0.0024461904540657997, Learning Rate: 0.001784\n",
      "Epoch 955/40000, Loss: 0.0024338439106941223, Learning Rate: 0.001783\n",
      "Epoch 956/40000, Loss: 0.0024113317485898733, Learning Rate: 0.001783\n",
      "Epoch 957/40000, Loss: 0.002460182411596179, Learning Rate: 0.001783\n",
      "Epoch 958/40000, Loss: 0.0023551317863166332, Learning Rate: 0.001783\n",
      "Epoch 959/40000, Loss: 0.0024028378538787365, Learning Rate: 0.001783\n",
      "Epoch 960/40000, Loss: 0.0022260472178459167, Learning Rate: 0.001782\n",
      "Epoch 961/40000, Loss: 0.002307484159246087, Learning Rate: 0.001782\n",
      "Epoch 962/40000, Loss: 0.002378683304414153, Learning Rate: 0.001782\n",
      "Epoch 963/40000, Loss: 0.002169117797166109, Learning Rate: 0.001782\n",
      "Epoch 964/40000, Loss: 0.0022428184747695923, Learning Rate: 0.001782\n",
      "Epoch 965/40000, Loss: 0.002136637456715107, Learning Rate: 0.001781\n",
      "Epoch 966/40000, Loss: 0.0021525011397898197, Learning Rate: 0.001781\n",
      "Epoch 967/40000, Loss: 0.002387216780334711, Learning Rate: 0.001781\n",
      "Epoch 968/40000, Loss: 0.002270185388624668, Learning Rate: 0.001781\n",
      "Epoch 969/40000, Loss: 0.0022143793758004904, Learning Rate: 0.001780\n",
      "Epoch 970/40000, Loss: 0.002376183634623885, Learning Rate: 0.001780\n",
      "Epoch 971/40000, Loss: 0.002316438127309084, Learning Rate: 0.001780\n",
      "Epoch 972/40000, Loss: 0.0023480330128222704, Learning Rate: 0.001780\n",
      "Epoch 973/40000, Loss: 0.0022093995939940214, Learning Rate: 0.001780\n",
      "Epoch 974/40000, Loss: 0.0022183547262102365, Learning Rate: 0.001779\n",
      "Epoch 975/40000, Loss: 0.002329818205907941, Learning Rate: 0.001779\n",
      "Epoch 976/40000, Loss: 0.002292236778885126, Learning Rate: 0.001779\n",
      "Epoch 977/40000, Loss: 0.00220767455175519, Learning Rate: 0.001779\n",
      "Epoch 978/40000, Loss: 0.0022950994316488504, Learning Rate: 0.001779\n",
      "Epoch 979/40000, Loss: 0.0021247719414532185, Learning Rate: 0.001778\n",
      "Epoch 980/40000, Loss: 0.0021068663336336613, Learning Rate: 0.001778\n",
      "Epoch 981/40000, Loss: 0.0022537270560860634, Learning Rate: 0.001778\n",
      "Epoch 982/40000, Loss: 0.002359208185225725, Learning Rate: 0.001778\n",
      "Epoch 983/40000, Loss: 0.0023188325576484203, Learning Rate: 0.001777\n",
      "Epoch 984/40000, Loss: 0.0022155744954943657, Learning Rate: 0.001777\n",
      "Epoch 985/40000, Loss: 0.0022498315665870905, Learning Rate: 0.001777\n",
      "Epoch 986/40000, Loss: 0.00210782908834517, Learning Rate: 0.001777\n",
      "Epoch 987/40000, Loss: 0.0021848154719918966, Learning Rate: 0.001777\n",
      "Epoch 988/40000, Loss: 0.0022087981924414635, Learning Rate: 0.001776\n",
      "Epoch 989/40000, Loss: 0.0021643424406647682, Learning Rate: 0.001776\n",
      "Epoch 990/40000, Loss: 0.002235760912299156, Learning Rate: 0.001776\n",
      "Epoch 991/40000, Loss: 0.0022231056354939938, Learning Rate: 0.001776\n",
      "Epoch 992/40000, Loss: 0.002199585549533367, Learning Rate: 0.001776\n",
      "Epoch 993/40000, Loss: 0.002319157589226961, Learning Rate: 0.001775\n",
      "Epoch 994/40000, Loss: 0.0023036187049001455, Learning Rate: 0.001775\n",
      "Epoch 995/40000, Loss: 0.0022592004388570786, Learning Rate: 0.001775\n",
      "Epoch 996/40000, Loss: 0.0023086927831172943, Learning Rate: 0.001775\n",
      "Epoch 997/40000, Loss: 0.0022750506177544594, Learning Rate: 0.001774\n",
      "Epoch 998/40000, Loss: 0.002586374059319496, Learning Rate: 0.001774\n",
      "Epoch 999/40000, Loss: 0.002546203788369894, Learning Rate: 0.001774\n",
      "Epoch 1000/40000, Loss: 0.0023475666530430317, Learning Rate: 0.001774\n",
      "Epoch 1001/40000, Loss: 0.002775577362626791, Learning Rate: 0.001774\n",
      "Epoch 1002/40000, Loss: 0.0027823857963085175, Learning Rate: 0.001773\n",
      "Epoch 1003/40000, Loss: 0.002471823710948229, Learning Rate: 0.001773\n",
      "Epoch 1004/40000, Loss: 0.0028310520574450493, Learning Rate: 0.001773\n",
      "Epoch 1005/40000, Loss: 0.0027944729663431644, Learning Rate: 0.001773\n",
      "Epoch 1006/40000, Loss: 0.0025610551238059998, Learning Rate: 0.001773\n",
      "Epoch 1007/40000, Loss: 0.002560521475970745, Learning Rate: 0.001772\n",
      "Epoch 1008/40000, Loss: 0.002299803774803877, Learning Rate: 0.001772\n",
      "Epoch 1009/40000, Loss: 0.0026072547771036625, Learning Rate: 0.001772\n",
      "Epoch 1010/40000, Loss: 0.002624381333589554, Learning Rate: 0.001772\n",
      "Epoch 1011/40000, Loss: 0.0024590748362243176, Learning Rate: 0.001771\n",
      "Epoch 1012/40000, Loss: 0.003409821540117264, Learning Rate: 0.001771\n",
      "Epoch 1013/40000, Loss: 0.0028338402044028044, Learning Rate: 0.001771\n",
      "Epoch 1014/40000, Loss: 0.002906390931457281, Learning Rate: 0.001771\n",
      "Epoch 1015/40000, Loss: 0.002856491832062602, Learning Rate: 0.001771\n",
      "Epoch 1016/40000, Loss: 0.003057661233469844, Learning Rate: 0.001770\n",
      "Epoch 1017/40000, Loss: 0.003214502241462469, Learning Rate: 0.001770\n",
      "Epoch 1018/40000, Loss: 0.0033271280117332935, Learning Rate: 0.001770\n",
      "Epoch 1019/40000, Loss: 0.002973026130348444, Learning Rate: 0.001770\n",
      "Epoch 1020/40000, Loss: 0.003090575337409973, Learning Rate: 0.001770\n",
      "Epoch 1021/40000, Loss: 0.0024787266738712788, Learning Rate: 0.001769\n",
      "Epoch 1022/40000, Loss: 0.0028711440972983837, Learning Rate: 0.001769\n",
      "Epoch 1023/40000, Loss: 0.002407185733318329, Learning Rate: 0.001769\n",
      "Epoch 1024/40000, Loss: 0.0026278155855834484, Learning Rate: 0.001769\n",
      "Epoch 1025/40000, Loss: 0.002602673601359129, Learning Rate: 0.001769\n",
      "Epoch 1026/40000, Loss: 0.0022910505067557096, Learning Rate: 0.001768\n",
      "Epoch 1027/40000, Loss: 0.002399215940386057, Learning Rate: 0.001768\n",
      "Epoch 1028/40000, Loss: 0.0023109607864171267, Learning Rate: 0.001768\n",
      "Epoch 1029/40000, Loss: 0.002387154148891568, Learning Rate: 0.001768\n",
      "Epoch 1030/40000, Loss: 0.0025504492223262787, Learning Rate: 0.001767\n",
      "Epoch 1031/40000, Loss: 0.0024252834264189005, Learning Rate: 0.001767\n",
      "Epoch 1032/40000, Loss: 0.0022375634871423244, Learning Rate: 0.001767\n",
      "Epoch 1033/40000, Loss: 0.002555249957367778, Learning Rate: 0.001767\n",
      "Epoch 1034/40000, Loss: 0.00230106501840055, Learning Rate: 0.001767\n",
      "Epoch 1035/40000, Loss: 0.0022256416268646717, Learning Rate: 0.001766\n",
      "Epoch 1036/40000, Loss: 0.0025126261170953512, Learning Rate: 0.001766\n",
      "Epoch 1037/40000, Loss: 0.0024028155021369457, Learning Rate: 0.001766\n",
      "Epoch 1038/40000, Loss: 0.002599224913865328, Learning Rate: 0.001766\n",
      "Epoch 1039/40000, Loss: 0.002468874678015709, Learning Rate: 0.001766\n",
      "Epoch 1040/40000, Loss: 0.0022268237080425024, Learning Rate: 0.001765\n",
      "Epoch 1041/40000, Loss: 0.0022338093258440495, Learning Rate: 0.001765\n",
      "Epoch 1042/40000, Loss: 0.002157307229936123, Learning Rate: 0.001765\n",
      "Epoch 1043/40000, Loss: 0.002383508952334523, Learning Rate: 0.001765\n",
      "Epoch 1044/40000, Loss: 0.0021689627319574356, Learning Rate: 0.001764\n",
      "Epoch 1045/40000, Loss: 0.002248861361294985, Learning Rate: 0.001764\n",
      "Epoch 1046/40000, Loss: 0.002218867652118206, Learning Rate: 0.001764\n",
      "Epoch 1047/40000, Loss: 0.00246843951754272, Learning Rate: 0.001764\n",
      "Epoch 1048/40000, Loss: 0.0022847377695143223, Learning Rate: 0.001764\n",
      "Epoch 1049/40000, Loss: 0.0022285040467977524, Learning Rate: 0.001763\n",
      "Epoch 1050/40000, Loss: 0.002089194254949689, Learning Rate: 0.001763\n",
      "Epoch 1051/40000, Loss: 0.0023138101678341627, Learning Rate: 0.001763\n",
      "Epoch 1052/40000, Loss: 0.0023281751200556755, Learning Rate: 0.001763\n",
      "Epoch 1053/40000, Loss: 0.002320453990250826, Learning Rate: 0.001763\n",
      "Epoch 1054/40000, Loss: 0.002366979606449604, Learning Rate: 0.001762\n",
      "Epoch 1055/40000, Loss: 0.0024176095612347126, Learning Rate: 0.001762\n",
      "Epoch 1056/40000, Loss: 0.0022944542579352856, Learning Rate: 0.001762\n",
      "Epoch 1057/40000, Loss: 0.0022637145593762398, Learning Rate: 0.001762\n",
      "Epoch 1058/40000, Loss: 0.002240325789898634, Learning Rate: 0.001762\n",
      "Epoch 1059/40000, Loss: 0.0021059871651232243, Learning Rate: 0.001761\n",
      "Epoch 1060/40000, Loss: 0.002157315146178007, Learning Rate: 0.001761\n",
      "Epoch 1061/40000, Loss: 0.002221370115876198, Learning Rate: 0.001761\n",
      "Epoch 1062/40000, Loss: 0.0022551296278834343, Learning Rate: 0.001761\n",
      "Epoch 1063/40000, Loss: 0.002173852641135454, Learning Rate: 0.001760\n",
      "Epoch 1064/40000, Loss: 0.0020909986924380064, Learning Rate: 0.001760\n",
      "Epoch 1065/40000, Loss: 0.002090581925585866, Learning Rate: 0.001760\n",
      "Epoch 1066/40000, Loss: 0.00273779290728271, Learning Rate: 0.001760\n",
      "Epoch 1067/40000, Loss: 0.0024401149712502956, Learning Rate: 0.001760\n",
      "Epoch 1068/40000, Loss: 0.002243444789201021, Learning Rate: 0.001759\n",
      "Epoch 1069/40000, Loss: 0.0022150943987071514, Learning Rate: 0.001759\n",
      "Epoch 1070/40000, Loss: 0.002449392108246684, Learning Rate: 0.001759\n",
      "Epoch 1071/40000, Loss: 0.002486724406480789, Learning Rate: 0.001759\n",
      "Epoch 1072/40000, Loss: 0.0025179265066981316, Learning Rate: 0.001759\n",
      "Epoch 1073/40000, Loss: 0.002477057743817568, Learning Rate: 0.001758\n",
      "Epoch 1074/40000, Loss: 0.003859536489471793, Learning Rate: 0.001758\n",
      "Epoch 1075/40000, Loss: 0.005636853631585836, Learning Rate: 0.001758\n",
      "Epoch 1076/40000, Loss: 0.006897850893437862, Learning Rate: 0.001758\n",
      "Epoch 1077/40000, Loss: 0.0033344547264277935, Learning Rate: 0.001758\n",
      "Epoch 1078/40000, Loss: 0.005012210458517075, Learning Rate: 0.001757\n",
      "Epoch 1079/40000, Loss: 0.004172110930085182, Learning Rate: 0.001757\n",
      "Epoch 1080/40000, Loss: 0.003336748108267784, Learning Rate: 0.001757\n",
      "Epoch 1081/40000, Loss: 0.002845568349584937, Learning Rate: 0.001757\n",
      "Epoch 1082/40000, Loss: 0.0031873704865574837, Learning Rate: 0.001756\n",
      "Epoch 1083/40000, Loss: 0.0037561487406492233, Learning Rate: 0.001756\n",
      "Epoch 1084/40000, Loss: 0.0027634415309876204, Learning Rate: 0.001756\n",
      "Epoch 1085/40000, Loss: 0.002680558478459716, Learning Rate: 0.001756\n",
      "Epoch 1086/40000, Loss: 0.0047217439860105515, Learning Rate: 0.001756\n",
      "Epoch 1087/40000, Loss: 0.003126715309917927, Learning Rate: 0.001755\n",
      "Epoch 1088/40000, Loss: 0.0061287423595786095, Learning Rate: 0.001755\n",
      "Epoch 1089/40000, Loss: 0.003131329780444503, Learning Rate: 0.001755\n",
      "Epoch 1090/40000, Loss: 0.0024862608406692743, Learning Rate: 0.001755\n",
      "Epoch 1091/40000, Loss: 0.002905785571783781, Learning Rate: 0.001755\n",
      "Epoch 1092/40000, Loss: 0.002724668011069298, Learning Rate: 0.001754\n",
      "Epoch 1093/40000, Loss: 0.002507398836314678, Learning Rate: 0.001754\n",
      "Epoch 1094/40000, Loss: 0.0023731589317321777, Learning Rate: 0.001754\n",
      "Epoch 1095/40000, Loss: 0.0024901418946683407, Learning Rate: 0.001754\n",
      "Epoch 1096/40000, Loss: 0.0023240181617438793, Learning Rate: 0.001754\n",
      "Epoch 1097/40000, Loss: 0.002366537693887949, Learning Rate: 0.001753\n",
      "Epoch 1098/40000, Loss: 0.0028603083919733763, Learning Rate: 0.001753\n",
      "Epoch 1099/40000, Loss: 0.0023454083129763603, Learning Rate: 0.001753\n",
      "Epoch 1100/40000, Loss: 0.0023368527181446552, Learning Rate: 0.001753\n",
      "Epoch 1101/40000, Loss: 0.002211056649684906, Learning Rate: 0.001752\n",
      "Epoch 1102/40000, Loss: 0.0022129639983177185, Learning Rate: 0.001752\n",
      "Epoch 1103/40000, Loss: 0.002270217053592205, Learning Rate: 0.001752\n",
      "Epoch 1104/40000, Loss: 0.0022574574686586857, Learning Rate: 0.001752\n",
      "Epoch 1105/40000, Loss: 0.002198167145252228, Learning Rate: 0.001752\n",
      "Epoch 1106/40000, Loss: 0.002348216250538826, Learning Rate: 0.001751\n",
      "Epoch 1107/40000, Loss: 0.002409708686172962, Learning Rate: 0.001751\n",
      "Epoch 1108/40000, Loss: 0.0022250842303037643, Learning Rate: 0.001751\n",
      "Epoch 1109/40000, Loss: 0.002492325147613883, Learning Rate: 0.001751\n",
      "Epoch 1110/40000, Loss: 0.0022124499082565308, Learning Rate: 0.001751\n",
      "Epoch 1111/40000, Loss: 0.002970026573166251, Learning Rate: 0.001750\n",
      "Epoch 1112/40000, Loss: 0.0022694403305649757, Learning Rate: 0.001750\n",
      "Epoch 1113/40000, Loss: 0.0023197061382234097, Learning Rate: 0.001750\n",
      "Epoch 1114/40000, Loss: 0.0024029172491282225, Learning Rate: 0.001750\n",
      "Epoch 1115/40000, Loss: 0.0023362752981483936, Learning Rate: 0.001750\n",
      "Epoch 1116/40000, Loss: 0.0027083586901426315, Learning Rate: 0.001749\n",
      "Epoch 1117/40000, Loss: 0.002469005761668086, Learning Rate: 0.001749\n",
      "Epoch 1118/40000, Loss: 0.0022552963346242905, Learning Rate: 0.001749\n",
      "Epoch 1119/40000, Loss: 0.0024658695328980684, Learning Rate: 0.001749\n",
      "Epoch 1120/40000, Loss: 0.0022123788949102163, Learning Rate: 0.001748\n",
      "Epoch 1121/40000, Loss: 0.0023060007952153683, Learning Rate: 0.001748\n",
      "Epoch 1122/40000, Loss: 0.002259941538795829, Learning Rate: 0.001748\n",
      "Epoch 1123/40000, Loss: 0.0024123124312609434, Learning Rate: 0.001748\n",
      "Epoch 1124/40000, Loss: 0.002302108332514763, Learning Rate: 0.001748\n",
      "Epoch 1125/40000, Loss: 0.0022734603844583035, Learning Rate: 0.001747\n",
      "Epoch 1126/40000, Loss: 0.0023144888691604137, Learning Rate: 0.001747\n",
      "Epoch 1127/40000, Loss: 0.0024204407818615437, Learning Rate: 0.001747\n",
      "Epoch 1128/40000, Loss: 0.0023816816974431276, Learning Rate: 0.001747\n",
      "Epoch 1129/40000, Loss: 0.0023039740044623613, Learning Rate: 0.001747\n",
      "Epoch 1130/40000, Loss: 0.002250010147690773, Learning Rate: 0.001746\n",
      "Epoch 1131/40000, Loss: 0.002418557647615671, Learning Rate: 0.001746\n",
      "Epoch 1132/40000, Loss: 0.002258653985336423, Learning Rate: 0.001746\n",
      "Epoch 1133/40000, Loss: 0.002183216158300638, Learning Rate: 0.001746\n",
      "Epoch 1134/40000, Loss: 0.0021405417937785387, Learning Rate: 0.001746\n",
      "Epoch 1135/40000, Loss: 0.0022304803133010864, Learning Rate: 0.001745\n",
      "Epoch 1136/40000, Loss: 0.0020960750989615917, Learning Rate: 0.001745\n",
      "Epoch 1137/40000, Loss: 0.002083591651171446, Learning Rate: 0.001745\n",
      "Epoch 1138/40000, Loss: 0.00216706027276814, Learning Rate: 0.001745\n",
      "Epoch 1139/40000, Loss: 0.002176429145038128, Learning Rate: 0.001744\n",
      "Epoch 1140/40000, Loss: 0.0021736600901931524, Learning Rate: 0.001744\n",
      "Epoch 1141/40000, Loss: 0.0021073664538562298, Learning Rate: 0.001744\n",
      "Epoch 1142/40000, Loss: 0.002286369213834405, Learning Rate: 0.001744\n",
      "Epoch 1143/40000, Loss: 0.0022311918437480927, Learning Rate: 0.001744\n",
      "Epoch 1144/40000, Loss: 0.00223635439760983, Learning Rate: 0.001743\n",
      "Epoch 1145/40000, Loss: 0.002133518224582076, Learning Rate: 0.001743\n",
      "Epoch 1146/40000, Loss: 0.002299474086612463, Learning Rate: 0.001743\n",
      "Epoch 1147/40000, Loss: 0.0022606474813073874, Learning Rate: 0.001743\n",
      "Epoch 1148/40000, Loss: 0.0021785697899758816, Learning Rate: 0.001743\n",
      "Epoch 1149/40000, Loss: 0.002325741806998849, Learning Rate: 0.001742\n",
      "Epoch 1150/40000, Loss: 0.0021706875413656235, Learning Rate: 0.001742\n",
      "Epoch 1151/40000, Loss: 0.002179880626499653, Learning Rate: 0.001742\n",
      "Epoch 1152/40000, Loss: 0.002606998896226287, Learning Rate: 0.001742\n",
      "Epoch 1153/40000, Loss: 0.0021026143804192543, Learning Rate: 0.001742\n",
      "Epoch 1154/40000, Loss: 0.0021168615203350782, Learning Rate: 0.001741\n",
      "Epoch 1155/40000, Loss: 0.0022193416953086853, Learning Rate: 0.001741\n",
      "Epoch 1156/40000, Loss: 0.002062371000647545, Learning Rate: 0.001741\n",
      "Epoch 1157/40000, Loss: 0.0020856796763837337, Learning Rate: 0.001741\n",
      "Epoch 1158/40000, Loss: 0.002283515641465783, Learning Rate: 0.001741\n",
      "Epoch 1159/40000, Loss: 0.0020857900381088257, Learning Rate: 0.001740\n",
      "Epoch 1160/40000, Loss: 0.002222143579274416, Learning Rate: 0.001740\n",
      "Epoch 1161/40000, Loss: 0.002056957921013236, Learning Rate: 0.001740\n",
      "Epoch 1162/40000, Loss: 0.0020953472703695297, Learning Rate: 0.001740\n",
      "Epoch 1163/40000, Loss: 0.002143038436770439, Learning Rate: 0.001739\n",
      "Epoch 1164/40000, Loss: 0.0021546187344938517, Learning Rate: 0.001739\n",
      "Epoch 1165/40000, Loss: 0.002022300148382783, Learning Rate: 0.001739\n",
      "Epoch 1166/40000, Loss: 0.001981991808861494, Learning Rate: 0.001739\n",
      "Epoch 1167/40000, Loss: 0.0020146681927144527, Learning Rate: 0.001739\n",
      "Epoch 1168/40000, Loss: 0.002156771020963788, Learning Rate: 0.001738\n",
      "Epoch 1169/40000, Loss: 0.001972690224647522, Learning Rate: 0.001738\n",
      "Epoch 1170/40000, Loss: 0.002038454171270132, Learning Rate: 0.001738\n",
      "Epoch 1171/40000, Loss: 0.002089319285005331, Learning Rate: 0.001738\n",
      "Epoch 1172/40000, Loss: 0.002132178982719779, Learning Rate: 0.001738\n",
      "Epoch 1173/40000, Loss: 0.0020112665370106697, Learning Rate: 0.001737\n",
      "Epoch 1174/40000, Loss: 0.0020086257718503475, Learning Rate: 0.001737\n",
      "Epoch 1175/40000, Loss: 0.002187656704336405, Learning Rate: 0.001737\n",
      "Epoch 1176/40000, Loss: 0.002092912094667554, Learning Rate: 0.001737\n",
      "Epoch 1177/40000, Loss: 0.0020421037916094065, Learning Rate: 0.001737\n",
      "Epoch 1178/40000, Loss: 0.002146249171346426, Learning Rate: 0.001736\n",
      "Epoch 1179/40000, Loss: 0.0022074952721595764, Learning Rate: 0.001736\n",
      "Epoch 1180/40000, Loss: 0.0021849232725799084, Learning Rate: 0.001736\n",
      "Epoch 1181/40000, Loss: 0.0021367399021983147, Learning Rate: 0.001736\n",
      "Epoch 1182/40000, Loss: 0.002157666953280568, Learning Rate: 0.001736\n",
      "Epoch 1183/40000, Loss: 0.0020425610709935427, Learning Rate: 0.001735\n",
      "Epoch 1184/40000, Loss: 0.0021570436656475067, Learning Rate: 0.001735\n",
      "Epoch 1185/40000, Loss: 0.00217220326885581, Learning Rate: 0.001735\n",
      "Epoch 1186/40000, Loss: 0.002005157992243767, Learning Rate: 0.001735\n",
      "Epoch 1187/40000, Loss: 0.0021051776129752398, Learning Rate: 0.001734\n",
      "Epoch 1188/40000, Loss: 0.0021466254256665707, Learning Rate: 0.001734\n",
      "Epoch 1189/40000, Loss: 0.0021129632368683815, Learning Rate: 0.001734\n",
      "Epoch 1190/40000, Loss: 0.0020761038176715374, Learning Rate: 0.001734\n",
      "Epoch 1191/40000, Loss: 0.002119028475135565, Learning Rate: 0.001734\n",
      "Epoch 1192/40000, Loss: 0.0021234829910099506, Learning Rate: 0.001733\n",
      "Epoch 1193/40000, Loss: 0.0020985878072679043, Learning Rate: 0.001733\n",
      "Epoch 1194/40000, Loss: 0.001961243338882923, Learning Rate: 0.001733\n",
      "Epoch 1195/40000, Loss: 0.002101391088217497, Learning Rate: 0.001733\n",
      "Epoch 1196/40000, Loss: 0.0020411363802850246, Learning Rate: 0.001733\n",
      "Epoch 1197/40000, Loss: 0.0020940559916198254, Learning Rate: 0.001732\n",
      "Epoch 1198/40000, Loss: 0.0020273090340197086, Learning Rate: 0.001732\n",
      "Epoch 1199/40000, Loss: 0.002033994533121586, Learning Rate: 0.001732\n",
      "Epoch 1200/40000, Loss: 0.0020549141336232424, Learning Rate: 0.001732\n",
      "Epoch 1201/40000, Loss: 0.002003495115786791, Learning Rate: 0.001732\n",
      "Epoch 1202/40000, Loss: 0.0021575719583779573, Learning Rate: 0.001731\n",
      "Epoch 1203/40000, Loss: 0.0019855834543704987, Learning Rate: 0.001731\n",
      "Epoch 1204/40000, Loss: 0.0019363242899999022, Learning Rate: 0.001731\n",
      "Epoch 1205/40000, Loss: 0.0021123678889125586, Learning Rate: 0.001731\n",
      "Epoch 1206/40000, Loss: 0.0020901099778711796, Learning Rate: 0.001731\n",
      "Epoch 1207/40000, Loss: 0.0020270682871341705, Learning Rate: 0.001730\n",
      "Epoch 1208/40000, Loss: 0.002164566656574607, Learning Rate: 0.001730\n",
      "Epoch 1209/40000, Loss: 0.0020955472718924284, Learning Rate: 0.001730\n",
      "Epoch 1210/40000, Loss: 0.0019929884001612663, Learning Rate: 0.001730\n",
      "Epoch 1211/40000, Loss: 0.0020001668017357588, Learning Rate: 0.001729\n",
      "Epoch 1212/40000, Loss: 0.002126475563272834, Learning Rate: 0.001729\n",
      "Epoch 1213/40000, Loss: 0.00216990290209651, Learning Rate: 0.001729\n",
      "Epoch 1214/40000, Loss: 0.0021075215190649033, Learning Rate: 0.001729\n",
      "Epoch 1215/40000, Loss: 0.0021890869829803705, Learning Rate: 0.001729\n",
      "Epoch 1216/40000, Loss: 0.002163121709600091, Learning Rate: 0.001728\n",
      "Epoch 1217/40000, Loss: 0.002264837734401226, Learning Rate: 0.001728\n",
      "Epoch 1218/40000, Loss: 0.0021868725307285786, Learning Rate: 0.001728\n",
      "Epoch 1219/40000, Loss: 0.0021206745877861977, Learning Rate: 0.001728\n",
      "Epoch 1220/40000, Loss: 0.0022421455942094326, Learning Rate: 0.001728\n",
      "Epoch 1221/40000, Loss: 0.0020854384638369083, Learning Rate: 0.001727\n",
      "Epoch 1222/40000, Loss: 0.0020937644876539707, Learning Rate: 0.001727\n",
      "Epoch 1223/40000, Loss: 0.0021575838327407837, Learning Rate: 0.001727\n",
      "Epoch 1224/40000, Loss: 0.002096528187394142, Learning Rate: 0.001727\n",
      "Epoch 1225/40000, Loss: 0.0020843965467065573, Learning Rate: 0.001727\n",
      "Epoch 1226/40000, Loss: 0.0021087368950247765, Learning Rate: 0.001726\n",
      "Epoch 1227/40000, Loss: 0.0024115508422255516, Learning Rate: 0.001726\n",
      "Epoch 1228/40000, Loss: 0.00270548602566123, Learning Rate: 0.001726\n",
      "Epoch 1229/40000, Loss: 0.002470744773745537, Learning Rate: 0.001726\n",
      "Epoch 1230/40000, Loss: 0.002352494280785322, Learning Rate: 0.001726\n",
      "Epoch 1231/40000, Loss: 0.002197694731876254, Learning Rate: 0.001725\n",
      "Epoch 1232/40000, Loss: 0.002154456451535225, Learning Rate: 0.001725\n",
      "Epoch 1233/40000, Loss: 0.0020885979756712914, Learning Rate: 0.001725\n",
      "Epoch 1234/40000, Loss: 0.0022116403561085463, Learning Rate: 0.001725\n",
      "Epoch 1235/40000, Loss: 0.002107103355228901, Learning Rate: 0.001725\n",
      "Epoch 1236/40000, Loss: 0.0020356783643364906, Learning Rate: 0.001724\n",
      "Epoch 1237/40000, Loss: 0.0021046409383416176, Learning Rate: 0.001724\n",
      "Epoch 1238/40000, Loss: 0.0019271841738373041, Learning Rate: 0.001724\n",
      "Epoch 1239/40000, Loss: 0.001937446533702314, Learning Rate: 0.001724\n",
      "Epoch 1240/40000, Loss: 0.002036359626799822, Learning Rate: 0.001723\n",
      "Epoch 1241/40000, Loss: 0.001974442508071661, Learning Rate: 0.001723\n",
      "Epoch 1242/40000, Loss: 0.0019094919553026557, Learning Rate: 0.001723\n",
      "Epoch 1243/40000, Loss: 0.0019574144389480352, Learning Rate: 0.001723\n",
      "Epoch 1244/40000, Loss: 0.0019499377813190222, Learning Rate: 0.001723\n",
      "Epoch 1245/40000, Loss: 0.002036126796156168, Learning Rate: 0.001722\n",
      "Epoch 1246/40000, Loss: 0.002013278193771839, Learning Rate: 0.001722\n",
      "Epoch 1247/40000, Loss: 0.0020354893058538437, Learning Rate: 0.001722\n",
      "Epoch 1248/40000, Loss: 0.0020285011269152164, Learning Rate: 0.001722\n",
      "Epoch 1249/40000, Loss: 0.002084349747747183, Learning Rate: 0.001722\n",
      "Epoch 1250/40000, Loss: 0.0018722767708823085, Learning Rate: 0.001721\n",
      "Epoch 1251/40000, Loss: 0.00195961631834507, Learning Rate: 0.001721\n",
      "Epoch 1252/40000, Loss: 0.0020412204321473837, Learning Rate: 0.001721\n",
      "Epoch 1253/40000, Loss: 0.001969647593796253, Learning Rate: 0.001721\n",
      "Epoch 1254/40000, Loss: 0.002049787435680628, Learning Rate: 0.001721\n",
      "Epoch 1255/40000, Loss: 0.002103727776557207, Learning Rate: 0.001720\n",
      "Epoch 1256/40000, Loss: 0.002014429308474064, Learning Rate: 0.001720\n",
      "Epoch 1257/40000, Loss: 0.001907486468553543, Learning Rate: 0.001720\n",
      "Epoch 1258/40000, Loss: 0.0020055659115314484, Learning Rate: 0.001720\n",
      "Epoch 1259/40000, Loss: 0.001899452181532979, Learning Rate: 0.001720\n",
      "Epoch 1260/40000, Loss: 0.0019598323851823807, Learning Rate: 0.001719\n",
      "Epoch 1261/40000, Loss: 0.001889631850644946, Learning Rate: 0.001719\n",
      "Epoch 1262/40000, Loss: 0.0019432248082011938, Learning Rate: 0.001719\n",
      "Epoch 1263/40000, Loss: 0.001960433553904295, Learning Rate: 0.001719\n",
      "Epoch 1264/40000, Loss: 0.0019153421744704247, Learning Rate: 0.001719\n",
      "Epoch 1265/40000, Loss: 0.002085731830447912, Learning Rate: 0.001718\n",
      "Epoch 1266/40000, Loss: 0.002061468316242099, Learning Rate: 0.001718\n",
      "Epoch 1267/40000, Loss: 0.001915761036798358, Learning Rate: 0.001718\n",
      "Epoch 1268/40000, Loss: 0.001975980121642351, Learning Rate: 0.001718\n",
      "Epoch 1269/40000, Loss: 0.001976432278752327, Learning Rate: 0.001717\n",
      "Epoch 1270/40000, Loss: 0.0020310450345277786, Learning Rate: 0.001717\n",
      "Epoch 1271/40000, Loss: 0.0020637570414692163, Learning Rate: 0.001717\n",
      "Epoch 1272/40000, Loss: 0.001906279823742807, Learning Rate: 0.001717\n",
      "Epoch 1273/40000, Loss: 0.0019652664195746183, Learning Rate: 0.001717\n",
      "Epoch 1274/40000, Loss: 0.00190000981092453, Learning Rate: 0.001716\n",
      "Epoch 1275/40000, Loss: 0.0020434909965842962, Learning Rate: 0.001716\n",
      "Epoch 1276/40000, Loss: 0.0020507406443357468, Learning Rate: 0.001716\n",
      "Epoch 1277/40000, Loss: 0.0020493981428444386, Learning Rate: 0.001716\n",
      "Epoch 1278/40000, Loss: 0.0020720399916172028, Learning Rate: 0.001716\n",
      "Epoch 1279/40000, Loss: 0.001984434900805354, Learning Rate: 0.001715\n",
      "Epoch 1280/40000, Loss: 0.0019887785892933607, Learning Rate: 0.001715\n",
      "Epoch 1281/40000, Loss: 0.0020366949029266834, Learning Rate: 0.001715\n",
      "Epoch 1282/40000, Loss: 0.0020616950932890177, Learning Rate: 0.001715\n",
      "Epoch 1283/40000, Loss: 0.0020982110872864723, Learning Rate: 0.001715\n",
      "Epoch 1284/40000, Loss: 0.0019329822389408946, Learning Rate: 0.001714\n",
      "Epoch 1285/40000, Loss: 0.0019666021689772606, Learning Rate: 0.001714\n",
      "Epoch 1286/40000, Loss: 0.001919784932397306, Learning Rate: 0.001714\n",
      "Epoch 1287/40000, Loss: 0.001909076701849699, Learning Rate: 0.001714\n",
      "Epoch 1288/40000, Loss: 0.0020772074349224567, Learning Rate: 0.001714\n",
      "Epoch 1289/40000, Loss: 0.0020217495039105415, Learning Rate: 0.001713\n",
      "Epoch 1290/40000, Loss: 0.001928266603499651, Learning Rate: 0.001713\n",
      "Epoch 1291/40000, Loss: 0.001832469366490841, Learning Rate: 0.001713\n",
      "Epoch 1292/40000, Loss: 0.0020164961460977793, Learning Rate: 0.001713\n",
      "Epoch 1293/40000, Loss: 0.001915386295877397, Learning Rate: 0.001713\n",
      "Epoch 1294/40000, Loss: 0.001934137544594705, Learning Rate: 0.001712\n",
      "Epoch 1295/40000, Loss: 0.0019437517039477825, Learning Rate: 0.001712\n",
      "Epoch 1296/40000, Loss: 0.001996079459786415, Learning Rate: 0.001712\n",
      "Epoch 1297/40000, Loss: 0.0020832945592701435, Learning Rate: 0.001712\n",
      "Epoch 1298/40000, Loss: 0.0019400401506572962, Learning Rate: 0.001712\n",
      "Epoch 1299/40000, Loss: 0.0021981257013976574, Learning Rate: 0.001711\n",
      "Epoch 1300/40000, Loss: 0.0019333715317770839, Learning Rate: 0.001711\n",
      "Epoch 1301/40000, Loss: 0.0020237616263329983, Learning Rate: 0.001711\n",
      "Epoch 1302/40000, Loss: 0.0022729411721229553, Learning Rate: 0.001711\n",
      "Epoch 1303/40000, Loss: 0.0019862065091729164, Learning Rate: 0.001710\n",
      "Epoch 1304/40000, Loss: 0.003314139787107706, Learning Rate: 0.001710\n",
      "Epoch 1305/40000, Loss: 0.0024506901390850544, Learning Rate: 0.001710\n",
      "Epoch 1306/40000, Loss: 0.002157697919756174, Learning Rate: 0.001710\n",
      "Epoch 1307/40000, Loss: 0.0025039254687726498, Learning Rate: 0.001710\n",
      "Epoch 1308/40000, Loss: 0.0022369942162185907, Learning Rate: 0.001709\n",
      "Epoch 1309/40000, Loss: 0.0021198089234530926, Learning Rate: 0.001709\n",
      "Epoch 1310/40000, Loss: 0.0022379918955266476, Learning Rate: 0.001709\n",
      "Epoch 1311/40000, Loss: 0.002021149732172489, Learning Rate: 0.001709\n",
      "Epoch 1312/40000, Loss: 0.002031786600127816, Learning Rate: 0.001709\n",
      "Epoch 1313/40000, Loss: 0.002330083167180419, Learning Rate: 0.001708\n",
      "Epoch 1314/40000, Loss: 0.0018248159904032946, Learning Rate: 0.001708\n",
      "Epoch 1315/40000, Loss: 0.0018729346338659525, Learning Rate: 0.001708\n",
      "Epoch 1316/40000, Loss: 0.0018753919284790754, Learning Rate: 0.001708\n",
      "Epoch 1317/40000, Loss: 0.001880343072116375, Learning Rate: 0.001708\n",
      "Epoch 1318/40000, Loss: 0.0018870357889682055, Learning Rate: 0.001707\n",
      "Epoch 1319/40000, Loss: 0.0019641269464045763, Learning Rate: 0.001707\n",
      "Epoch 1320/40000, Loss: 0.0020637642592191696, Learning Rate: 0.001707\n",
      "Epoch 1321/40000, Loss: 0.0020141296554356813, Learning Rate: 0.001707\n",
      "Epoch 1322/40000, Loss: 0.001986416755244136, Learning Rate: 0.001707\n",
      "Epoch 1323/40000, Loss: 0.0019307732582092285, Learning Rate: 0.001706\n",
      "Epoch 1324/40000, Loss: 0.0020641149021685123, Learning Rate: 0.001706\n",
      "Epoch 1325/40000, Loss: 0.002142647048458457, Learning Rate: 0.001706\n",
      "Epoch 1326/40000, Loss: 0.0019122471567243338, Learning Rate: 0.001706\n",
      "Epoch 1327/40000, Loss: 0.002134861657395959, Learning Rate: 0.001706\n",
      "Epoch 1328/40000, Loss: 0.002001540269702673, Learning Rate: 0.001705\n",
      "Epoch 1329/40000, Loss: 0.0020300596952438354, Learning Rate: 0.001705\n",
      "Epoch 1330/40000, Loss: 0.0018952966202050447, Learning Rate: 0.001705\n",
      "Epoch 1331/40000, Loss: 0.001889438135549426, Learning Rate: 0.001705\n",
      "Epoch 1332/40000, Loss: 0.001974301878362894, Learning Rate: 0.001705\n",
      "Epoch 1333/40000, Loss: 0.0020225404296070337, Learning Rate: 0.001704\n",
      "Epoch 1334/40000, Loss: 0.0021084530744701624, Learning Rate: 0.001704\n",
      "Epoch 1335/40000, Loss: 0.0019985008984804153, Learning Rate: 0.001704\n",
      "Epoch 1336/40000, Loss: 0.001961515750735998, Learning Rate: 0.001704\n",
      "Epoch 1337/40000, Loss: 0.0019837808795273304, Learning Rate: 0.001704\n",
      "Epoch 1338/40000, Loss: 0.002095696981996298, Learning Rate: 0.001703\n",
      "Epoch 1339/40000, Loss: 0.002173937391489744, Learning Rate: 0.001703\n",
      "Epoch 1340/40000, Loss: 0.0020415715407580137, Learning Rate: 0.001703\n",
      "Epoch 1341/40000, Loss: 0.002253348473459482, Learning Rate: 0.001703\n",
      "Epoch 1342/40000, Loss: 0.0021386202424764633, Learning Rate: 0.001703\n",
      "Epoch 1343/40000, Loss: 0.0019652063492685556, Learning Rate: 0.001702\n",
      "Epoch 1344/40000, Loss: 0.0019144187681376934, Learning Rate: 0.001702\n",
      "Epoch 1345/40000, Loss: 0.0021055955439805984, Learning Rate: 0.001702\n",
      "Epoch 1346/40000, Loss: 0.0019818302243947983, Learning Rate: 0.001702\n",
      "Epoch 1347/40000, Loss: 0.0019507065881043673, Learning Rate: 0.001701\n",
      "Epoch 1348/40000, Loss: 0.002161317504942417, Learning Rate: 0.001701\n",
      "Epoch 1349/40000, Loss: 0.0019959094934165478, Learning Rate: 0.001701\n",
      "Epoch 1350/40000, Loss: 0.001959173707291484, Learning Rate: 0.001701\n",
      "Epoch 1351/40000, Loss: 0.001839713891968131, Learning Rate: 0.001701\n",
      "Epoch 1352/40000, Loss: 0.0020345384255051613, Learning Rate: 0.001700\n",
      "Epoch 1353/40000, Loss: 0.0019966564141213894, Learning Rate: 0.001700\n",
      "Epoch 1354/40000, Loss: 0.0018521002493798733, Learning Rate: 0.001700\n",
      "Epoch 1355/40000, Loss: 0.002034698147326708, Learning Rate: 0.001700\n",
      "Epoch 1356/40000, Loss: 0.0018672769656404853, Learning Rate: 0.001700\n",
      "Epoch 1357/40000, Loss: 0.001965493429452181, Learning Rate: 0.001699\n",
      "Epoch 1358/40000, Loss: 0.0019288499606773257, Learning Rate: 0.001699\n",
      "Epoch 1359/40000, Loss: 0.0018536685965955257, Learning Rate: 0.001699\n",
      "Epoch 1360/40000, Loss: 0.0018327017314732075, Learning Rate: 0.001699\n",
      "Epoch 1361/40000, Loss: 0.0019321049330756068, Learning Rate: 0.001699\n",
      "Epoch 1362/40000, Loss: 0.001854444737546146, Learning Rate: 0.001698\n",
      "Epoch 1363/40000, Loss: 0.0019217338413000107, Learning Rate: 0.001698\n",
      "Epoch 1364/40000, Loss: 0.0019127451814711094, Learning Rate: 0.001698\n",
      "Epoch 1365/40000, Loss: 0.0019804188050329685, Learning Rate: 0.001698\n",
      "Epoch 1366/40000, Loss: 0.001964147435501218, Learning Rate: 0.001698\n",
      "Epoch 1367/40000, Loss: 0.0019675998482853174, Learning Rate: 0.001697\n",
      "Epoch 1368/40000, Loss: 0.0019528495613485575, Learning Rate: 0.001697\n",
      "Epoch 1369/40000, Loss: 0.0020100120455026627, Learning Rate: 0.001697\n",
      "Epoch 1370/40000, Loss: 0.0018639112822711468, Learning Rate: 0.001697\n",
      "Epoch 1371/40000, Loss: 0.0019063906511291862, Learning Rate: 0.001697\n",
      "Epoch 1372/40000, Loss: 0.0019788783974945545, Learning Rate: 0.001696\n",
      "Epoch 1373/40000, Loss: 0.001933376770466566, Learning Rate: 0.001696\n",
      "Epoch 1374/40000, Loss: 0.001853261492215097, Learning Rate: 0.001696\n",
      "Epoch 1375/40000, Loss: 0.0019837538711726665, Learning Rate: 0.001696\n",
      "Epoch 1376/40000, Loss: 0.0020137124229222536, Learning Rate: 0.001696\n",
      "Epoch 1377/40000, Loss: 0.0017394507303833961, Learning Rate: 0.001695\n",
      "Epoch 1378/40000, Loss: 0.0018419837579131126, Learning Rate: 0.001695\n",
      "Epoch 1379/40000, Loss: 0.0019252500496804714, Learning Rate: 0.001695\n",
      "Epoch 1380/40000, Loss: 0.0019148772116750479, Learning Rate: 0.001695\n",
      "Epoch 1381/40000, Loss: 0.001801277045160532, Learning Rate: 0.001695\n",
      "Epoch 1382/40000, Loss: 0.0017972039058804512, Learning Rate: 0.001694\n",
      "Epoch 1383/40000, Loss: 0.001807649852707982, Learning Rate: 0.001694\n",
      "Epoch 1384/40000, Loss: 0.001917489105835557, Learning Rate: 0.001694\n",
      "Epoch 1385/40000, Loss: 0.001875541522167623, Learning Rate: 0.001694\n",
      "Epoch 1386/40000, Loss: 0.001912283943966031, Learning Rate: 0.001694\n",
      "Epoch 1387/40000, Loss: 0.0017783287912607193, Learning Rate: 0.001693\n",
      "Epoch 1388/40000, Loss: 0.0018596936715766788, Learning Rate: 0.001693\n",
      "Epoch 1389/40000, Loss: 0.0018379593966528773, Learning Rate: 0.001693\n",
      "Epoch 1390/40000, Loss: 0.001811136375181377, Learning Rate: 0.001693\n",
      "Epoch 1391/40000, Loss: 0.0018348374869674444, Learning Rate: 0.001693\n",
      "Epoch 1392/40000, Loss: 0.0019104959210380912, Learning Rate: 0.001692\n",
      "Epoch 1393/40000, Loss: 0.001955030020326376, Learning Rate: 0.001692\n",
      "Epoch 1394/40000, Loss: 0.0019667837768793106, Learning Rate: 0.001692\n",
      "Epoch 1395/40000, Loss: 0.0018121872562915087, Learning Rate: 0.001692\n",
      "Epoch 1396/40000, Loss: 0.001996937207877636, Learning Rate: 0.001692\n",
      "Epoch 1397/40000, Loss: 0.001851662527769804, Learning Rate: 0.001691\n",
      "Epoch 1398/40000, Loss: 0.001842499477788806, Learning Rate: 0.001691\n",
      "Epoch 1399/40000, Loss: 0.0018761864630505443, Learning Rate: 0.001691\n",
      "Epoch 1400/40000, Loss: 0.0021311650052666664, Learning Rate: 0.001691\n",
      "Epoch 1401/40000, Loss: 0.0019846546929329634, Learning Rate: 0.001691\n",
      "Epoch 1402/40000, Loss: 0.0019271979108452797, Learning Rate: 0.001690\n",
      "Epoch 1403/40000, Loss: 0.0021531027741730213, Learning Rate: 0.001690\n",
      "Epoch 1404/40000, Loss: 0.0019877455197274685, Learning Rate: 0.001690\n",
      "Epoch 1405/40000, Loss: 0.001998176798224449, Learning Rate: 0.001690\n",
      "Epoch 1406/40000, Loss: 0.0019752527587115765, Learning Rate: 0.001689\n",
      "Epoch 1407/40000, Loss: 0.0019787861965596676, Learning Rate: 0.001689\n",
      "Epoch 1408/40000, Loss: 0.0018795666983351111, Learning Rate: 0.001689\n",
      "Epoch 1409/40000, Loss: 0.0018452669028192759, Learning Rate: 0.001689\n",
      "Epoch 1410/40000, Loss: 0.001965543255209923, Learning Rate: 0.001689\n",
      "Epoch 1411/40000, Loss: 0.0019156571943312883, Learning Rate: 0.001688\n",
      "Epoch 1412/40000, Loss: 0.0019457482267171144, Learning Rate: 0.001688\n",
      "Epoch 1413/40000, Loss: 0.001833735965192318, Learning Rate: 0.001688\n",
      "Epoch 1414/40000, Loss: 0.002008855575695634, Learning Rate: 0.001688\n",
      "Epoch 1415/40000, Loss: 0.0018651331774890423, Learning Rate: 0.001688\n",
      "Epoch 1416/40000, Loss: 0.001782062230631709, Learning Rate: 0.001687\n",
      "Epoch 1417/40000, Loss: 0.0018133504781872034, Learning Rate: 0.001687\n",
      "Epoch 1418/40000, Loss: 0.001925612217746675, Learning Rate: 0.001687\n",
      "Epoch 1419/40000, Loss: 0.001875654561445117, Learning Rate: 0.001687\n",
      "Epoch 1420/40000, Loss: 0.0018075210973620415, Learning Rate: 0.001687\n",
      "Epoch 1421/40000, Loss: 0.0020015558693557978, Learning Rate: 0.001686\n",
      "Epoch 1422/40000, Loss: 0.0018545686034485698, Learning Rate: 0.001686\n",
      "Epoch 1423/40000, Loss: 0.0017900057137012482, Learning Rate: 0.001686\n",
      "Epoch 1424/40000, Loss: 0.002027879934757948, Learning Rate: 0.001686\n",
      "Epoch 1425/40000, Loss: 0.0019342128653079271, Learning Rate: 0.001686\n",
      "Epoch 1426/40000, Loss: 0.0018341158283874393, Learning Rate: 0.001685\n",
      "Epoch 1427/40000, Loss: 0.0018769650487229228, Learning Rate: 0.001685\n",
      "Epoch 1428/40000, Loss: 0.0019258244428783655, Learning Rate: 0.001685\n",
      "Epoch 1429/40000, Loss: 0.001863245852291584, Learning Rate: 0.001685\n",
      "Epoch 1430/40000, Loss: 0.0019423987250775099, Learning Rate: 0.001685\n",
      "Epoch 1431/40000, Loss: 0.002050228649750352, Learning Rate: 0.001684\n",
      "Epoch 1432/40000, Loss: 0.0019444808131083846, Learning Rate: 0.001684\n",
      "Epoch 1433/40000, Loss: 0.001924202311784029, Learning Rate: 0.001684\n",
      "Epoch 1434/40000, Loss: 0.001971066929399967, Learning Rate: 0.001684\n",
      "Epoch 1435/40000, Loss: 0.0019699884578585625, Learning Rate: 0.001684\n",
      "Epoch 1436/40000, Loss: 0.0019028331153094769, Learning Rate: 0.001683\n",
      "Epoch 1437/40000, Loss: 0.0019224187126383185, Learning Rate: 0.001683\n",
      "Epoch 1438/40000, Loss: 0.00182915513869375, Learning Rate: 0.001683\n",
      "Epoch 1439/40000, Loss: 0.001838977332226932, Learning Rate: 0.001683\n",
      "Epoch 1440/40000, Loss: 0.0017778719775378704, Learning Rate: 0.001683\n",
      "Epoch 1441/40000, Loss: 0.0018041455186903477, Learning Rate: 0.001682\n",
      "Epoch 1442/40000, Loss: 0.0018585724756121635, Learning Rate: 0.001682\n",
      "Epoch 1443/40000, Loss: 0.0018372090999037027, Learning Rate: 0.001682\n",
      "Epoch 1444/40000, Loss: 0.0018458366394042969, Learning Rate: 0.001682\n",
      "Epoch 1445/40000, Loss: 0.0018276192713528872, Learning Rate: 0.001682\n",
      "Epoch 1446/40000, Loss: 0.0019113047746941447, Learning Rate: 0.001681\n",
      "Epoch 1447/40000, Loss: 0.001853934838436544, Learning Rate: 0.001681\n",
      "Epoch 1448/40000, Loss: 0.0017762028146535158, Learning Rate: 0.001681\n",
      "Epoch 1449/40000, Loss: 0.0018790791509673, Learning Rate: 0.001681\n",
      "Epoch 1450/40000, Loss: 0.0018558300798758864, Learning Rate: 0.001681\n",
      "Epoch 1451/40000, Loss: 0.0017733101267367601, Learning Rate: 0.001680\n",
      "Epoch 1452/40000, Loss: 0.0018437509424984455, Learning Rate: 0.001680\n",
      "Epoch 1453/40000, Loss: 0.0018654249142855406, Learning Rate: 0.001680\n",
      "Epoch 1454/40000, Loss: 0.0019082672661170363, Learning Rate: 0.001680\n",
      "Epoch 1455/40000, Loss: 0.001754693454131484, Learning Rate: 0.001680\n",
      "Epoch 1456/40000, Loss: 0.0018787756562232971, Learning Rate: 0.001679\n",
      "Epoch 1457/40000, Loss: 0.0018508420325815678, Learning Rate: 0.001679\n",
      "Epoch 1458/40000, Loss: 0.0017469972372055054, Learning Rate: 0.001679\n",
      "Epoch 1459/40000, Loss: 0.0018689779099076986, Learning Rate: 0.001679\n",
      "Epoch 1460/40000, Loss: 0.0018770757596939802, Learning Rate: 0.001679\n",
      "Epoch 1461/40000, Loss: 0.0018332281615585089, Learning Rate: 0.001678\n",
      "Epoch 1462/40000, Loss: 0.0017784936353564262, Learning Rate: 0.001678\n",
      "Epoch 1463/40000, Loss: 0.0018086693016812205, Learning Rate: 0.001678\n",
      "Epoch 1464/40000, Loss: 0.0019172015599906445, Learning Rate: 0.001678\n",
      "Epoch 1465/40000, Loss: 0.0017304237699136138, Learning Rate: 0.001678\n",
      "Epoch 1466/40000, Loss: 0.0018574576824903488, Learning Rate: 0.001677\n",
      "Epoch 1467/40000, Loss: 0.0017868231516331434, Learning Rate: 0.001677\n",
      "Epoch 1468/40000, Loss: 0.001829793443903327, Learning Rate: 0.001677\n",
      "Epoch 1469/40000, Loss: 0.0019078243058174849, Learning Rate: 0.001677\n",
      "Epoch 1470/40000, Loss: 0.001815947936847806, Learning Rate: 0.001677\n",
      "Epoch 1471/40000, Loss: 0.001971049699932337, Learning Rate: 0.001676\n",
      "Epoch 1472/40000, Loss: 0.001804071944206953, Learning Rate: 0.001676\n",
      "Epoch 1473/40000, Loss: 0.0018838983960449696, Learning Rate: 0.001676\n",
      "Epoch 1474/40000, Loss: 0.0019343056483194232, Learning Rate: 0.001676\n",
      "Epoch 1475/40000, Loss: 0.001931599574163556, Learning Rate: 0.001676\n",
      "Epoch 1476/40000, Loss: 0.0018328982405364513, Learning Rate: 0.001675\n",
      "Epoch 1477/40000, Loss: 0.001764341606758535, Learning Rate: 0.001675\n",
      "Epoch 1478/40000, Loss: 0.0017857042839750648, Learning Rate: 0.001675\n",
      "Epoch 1479/40000, Loss: 0.0017494510393589735, Learning Rate: 0.001675\n",
      "Epoch 1480/40000, Loss: 0.0017489175079390407, Learning Rate: 0.001675\n",
      "Epoch 1481/40000, Loss: 0.0018553126137703657, Learning Rate: 0.001674\n",
      "Epoch 1482/40000, Loss: 0.0019121345831081271, Learning Rate: 0.001674\n",
      "Epoch 1483/40000, Loss: 0.0018456662073731422, Learning Rate: 0.001674\n",
      "Epoch 1484/40000, Loss: 0.001830916735343635, Learning Rate: 0.001674\n",
      "Epoch 1485/40000, Loss: 0.0017859748331829906, Learning Rate: 0.001674\n",
      "Epoch 1486/40000, Loss: 0.001790365669876337, Learning Rate: 0.001673\n",
      "Epoch 1487/40000, Loss: 0.001819039462134242, Learning Rate: 0.001673\n",
      "Epoch 1488/40000, Loss: 0.0018238313496112823, Learning Rate: 0.001673\n",
      "Epoch 1489/40000, Loss: 0.0018019563285633922, Learning Rate: 0.001673\n",
      "Epoch 1490/40000, Loss: 0.0016988646239042282, Learning Rate: 0.001673\n",
      "Epoch 1491/40000, Loss: 0.0018530359957367182, Learning Rate: 0.001672\n",
      "Epoch 1492/40000, Loss: 0.0018662724178284407, Learning Rate: 0.001672\n",
      "Epoch 1493/40000, Loss: 0.0017903706757351756, Learning Rate: 0.001672\n",
      "Epoch 1494/40000, Loss: 0.0018754794728010893, Learning Rate: 0.001672\n",
      "Epoch 1495/40000, Loss: 0.0018428893527016044, Learning Rate: 0.001672\n",
      "Epoch 1496/40000, Loss: 0.0018821194535121322, Learning Rate: 0.001671\n",
      "Epoch 1497/40000, Loss: 0.001900225761346519, Learning Rate: 0.001671\n",
      "Epoch 1498/40000, Loss: 0.001804795116186142, Learning Rate: 0.001671\n",
      "Epoch 1499/40000, Loss: 0.00180661806371063, Learning Rate: 0.001671\n",
      "Epoch 1500/40000, Loss: 0.0019040040206164122, Learning Rate: 0.001671\n",
      "Epoch 1501/40000, Loss: 0.001764815067872405, Learning Rate: 0.001670\n",
      "Epoch 1502/40000, Loss: 0.0018857410177588463, Learning Rate: 0.001670\n",
      "Epoch 1503/40000, Loss: 0.0017062968108803034, Learning Rate: 0.001670\n",
      "Epoch 1504/40000, Loss: 0.0016811685636639595, Learning Rate: 0.001670\n",
      "Epoch 1505/40000, Loss: 0.0018897282425314188, Learning Rate: 0.001670\n",
      "Epoch 1506/40000, Loss: 0.0017363526858389378, Learning Rate: 0.001669\n",
      "Epoch 1507/40000, Loss: 0.001748997950926423, Learning Rate: 0.001669\n",
      "Epoch 1508/40000, Loss: 0.001803694642148912, Learning Rate: 0.001669\n",
      "Epoch 1509/40000, Loss: 0.0017736403970047832, Learning Rate: 0.001669\n",
      "Epoch 1510/40000, Loss: 0.0017444172408431768, Learning Rate: 0.001669\n",
      "Epoch 1511/40000, Loss: 0.001732450444251299, Learning Rate: 0.001668\n",
      "Epoch 1512/40000, Loss: 0.001779124024324119, Learning Rate: 0.001668\n",
      "Epoch 1513/40000, Loss: 0.001721316366456449, Learning Rate: 0.001668\n",
      "Epoch 1514/40000, Loss: 0.0018096640706062317, Learning Rate: 0.001668\n",
      "Epoch 1515/40000, Loss: 0.0018157809972763062, Learning Rate: 0.001668\n",
      "Epoch 1516/40000, Loss: 0.0017732669366523623, Learning Rate: 0.001667\n",
      "Epoch 1517/40000, Loss: 0.0019514095038175583, Learning Rate: 0.001667\n",
      "Epoch 1518/40000, Loss: 0.0017802375368773937, Learning Rate: 0.001667\n",
      "Epoch 1519/40000, Loss: 0.001859067240729928, Learning Rate: 0.001667\n",
      "Epoch 1520/40000, Loss: 0.001853518420830369, Learning Rate: 0.001667\n",
      "Epoch 1521/40000, Loss: 0.0020143920555710793, Learning Rate: 0.001666\n",
      "Epoch 1522/40000, Loss: 0.0020527117885649204, Learning Rate: 0.001666\n",
      "Epoch 1523/40000, Loss: 0.0017639686120674014, Learning Rate: 0.001666\n",
      "Epoch 1524/40000, Loss: 0.001767215202562511, Learning Rate: 0.001666\n",
      "Epoch 1525/40000, Loss: 0.0018178917234763503, Learning Rate: 0.001666\n",
      "Epoch 1526/40000, Loss: 0.0018538858275860548, Learning Rate: 0.001665\n",
      "Epoch 1527/40000, Loss: 0.0020935507491230965, Learning Rate: 0.001665\n",
      "Epoch 1528/40000, Loss: 0.0018282260280102491, Learning Rate: 0.001665\n",
      "Epoch 1529/40000, Loss: 0.0018485834589228034, Learning Rate: 0.001665\n",
      "Epoch 1530/40000, Loss: 0.0019141777884215117, Learning Rate: 0.001665\n",
      "Epoch 1531/40000, Loss: 0.0018379674293100834, Learning Rate: 0.001664\n",
      "Epoch 1532/40000, Loss: 0.001739289378747344, Learning Rate: 0.001664\n",
      "Epoch 1533/40000, Loss: 0.001813661539927125, Learning Rate: 0.001664\n",
      "Epoch 1534/40000, Loss: 0.0017036888748407364, Learning Rate: 0.001664\n",
      "Epoch 1535/40000, Loss: 0.0019171314779669046, Learning Rate: 0.001664\n",
      "Epoch 1536/40000, Loss: 0.0018115176353603601, Learning Rate: 0.001663\n",
      "Epoch 1537/40000, Loss: 0.0018273196183145046, Learning Rate: 0.001663\n",
      "Epoch 1538/40000, Loss: 0.0017999609699472785, Learning Rate: 0.001663\n",
      "Epoch 1539/40000, Loss: 0.001616913708858192, Learning Rate: 0.001663\n",
      "Epoch 1540/40000, Loss: 0.0017164042219519615, Learning Rate: 0.001663\n",
      "Epoch 1541/40000, Loss: 0.0018369541503489017, Learning Rate: 0.001662\n",
      "Epoch 1542/40000, Loss: 0.0018210116541013122, Learning Rate: 0.001662\n",
      "Epoch 1543/40000, Loss: 0.0017430705484002829, Learning Rate: 0.001662\n",
      "Epoch 1544/40000, Loss: 0.0017913531046360731, Learning Rate: 0.001662\n",
      "Epoch 1545/40000, Loss: 0.0018047536723315716, Learning Rate: 0.001662\n",
      "Epoch 1546/40000, Loss: 0.0018954719416797161, Learning Rate: 0.001661\n",
      "Epoch 1547/40000, Loss: 0.0018487328197807074, Learning Rate: 0.001661\n",
      "Epoch 1548/40000, Loss: 0.0020092318300157785, Learning Rate: 0.001661\n",
      "Epoch 1549/40000, Loss: 0.0018909969367086887, Learning Rate: 0.001661\n",
      "Epoch 1550/40000, Loss: 0.0017629109788686037, Learning Rate: 0.001661\n",
      "Epoch 1551/40000, Loss: 0.0017650788649916649, Learning Rate: 0.001660\n",
      "Epoch 1552/40000, Loss: 0.0018158199964091182, Learning Rate: 0.001660\n",
      "Epoch 1553/40000, Loss: 0.001780322170816362, Learning Rate: 0.001660\n",
      "Epoch 1554/40000, Loss: 0.0022289655171334743, Learning Rate: 0.001660\n",
      "Epoch 1555/40000, Loss: 0.0018728498835116625, Learning Rate: 0.001660\n",
      "Epoch 1556/40000, Loss: 0.0017568855546414852, Learning Rate: 0.001659\n",
      "Epoch 1557/40000, Loss: 0.001809633569791913, Learning Rate: 0.001659\n",
      "Epoch 1558/40000, Loss: 0.0016818796284496784, Learning Rate: 0.001659\n",
      "Epoch 1559/40000, Loss: 0.0017604962922632694, Learning Rate: 0.001659\n",
      "Epoch 1560/40000, Loss: 0.0017753602005541325, Learning Rate: 0.001659\n",
      "Epoch 1561/40000, Loss: 0.0017374394228681922, Learning Rate: 0.001658\n",
      "Epoch 1562/40000, Loss: 0.0016959597123786807, Learning Rate: 0.001658\n",
      "Epoch 1563/40000, Loss: 0.0016673959325999022, Learning Rate: 0.001658\n",
      "Epoch 1564/40000, Loss: 0.00181497260928154, Learning Rate: 0.001658\n",
      "Epoch 1565/40000, Loss: 0.0017978771356865764, Learning Rate: 0.001658\n",
      "Epoch 1566/40000, Loss: 0.0017056830693036318, Learning Rate: 0.001657\n",
      "Epoch 1567/40000, Loss: 0.0016962590161710978, Learning Rate: 0.001657\n",
      "Epoch 1568/40000, Loss: 0.0016160558443516493, Learning Rate: 0.001657\n",
      "Epoch 1569/40000, Loss: 0.001831039204262197, Learning Rate: 0.001657\n",
      "Epoch 1570/40000, Loss: 0.0017034889897331595, Learning Rate: 0.001657\n",
      "Epoch 1571/40000, Loss: 0.0018150336109101772, Learning Rate: 0.001656\n",
      "Epoch 1572/40000, Loss: 0.0017921291291713715, Learning Rate: 0.001656\n",
      "Epoch 1573/40000, Loss: 0.0017793707083910704, Learning Rate: 0.001656\n",
      "Epoch 1574/40000, Loss: 0.0020146220922470093, Learning Rate: 0.001656\n",
      "Epoch 1575/40000, Loss: 0.0018477598205208778, Learning Rate: 0.001656\n",
      "Epoch 1576/40000, Loss: 0.00193129968829453, Learning Rate: 0.001655\n",
      "Epoch 1577/40000, Loss: 0.0018020009156316519, Learning Rate: 0.001655\n",
      "Epoch 1578/40000, Loss: 0.0024936674162745476, Learning Rate: 0.001655\n",
      "Epoch 1579/40000, Loss: 0.0018701357766985893, Learning Rate: 0.001655\n",
      "Epoch 1580/40000, Loss: 0.0018747941358014941, Learning Rate: 0.001655\n",
      "Epoch 1581/40000, Loss: 0.0018671535653993487, Learning Rate: 0.001654\n",
      "Epoch 1582/40000, Loss: 0.0018689542775973678, Learning Rate: 0.001654\n",
      "Epoch 1583/40000, Loss: 0.0025792624801397324, Learning Rate: 0.001654\n",
      "Epoch 1584/40000, Loss: 0.0019134772010147572, Learning Rate: 0.001654\n",
      "Epoch 1585/40000, Loss: 0.002169293351471424, Learning Rate: 0.001654\n",
      "Epoch 1586/40000, Loss: 0.0020286000799387693, Learning Rate: 0.001653\n",
      "Epoch 1587/40000, Loss: 0.0019598996732383966, Learning Rate: 0.001653\n",
      "Epoch 1588/40000, Loss: 0.0018665636889636517, Learning Rate: 0.001653\n",
      "Epoch 1589/40000, Loss: 0.0019843755289912224, Learning Rate: 0.001653\n",
      "Epoch 1590/40000, Loss: 0.0018862857250496745, Learning Rate: 0.001653\n",
      "Epoch 1591/40000, Loss: 0.0022914197761565447, Learning Rate: 0.001652\n",
      "Epoch 1592/40000, Loss: 0.0022908991668373346, Learning Rate: 0.001652\n",
      "Epoch 1593/40000, Loss: 0.0019811545498669147, Learning Rate: 0.001652\n",
      "Epoch 1594/40000, Loss: 0.0018452422227710485, Learning Rate: 0.001652\n",
      "Epoch 1595/40000, Loss: 0.002129613421857357, Learning Rate: 0.001652\n",
      "Epoch 1596/40000, Loss: 0.001797357457689941, Learning Rate: 0.001651\n",
      "Epoch 1597/40000, Loss: 0.001735449186526239, Learning Rate: 0.001651\n",
      "Epoch 1598/40000, Loss: 0.0019173293840140104, Learning Rate: 0.001651\n",
      "Epoch 1599/40000, Loss: 0.0017727022059261799, Learning Rate: 0.001651\n",
      "Epoch 1600/40000, Loss: 0.001815251773223281, Learning Rate: 0.001651\n",
      "Epoch 1601/40000, Loss: 0.0017637204146012664, Learning Rate: 0.001650\n",
      "Epoch 1602/40000, Loss: 0.0017846585251390934, Learning Rate: 0.001650\n",
      "Epoch 1603/40000, Loss: 0.0017492310144007206, Learning Rate: 0.001650\n",
      "Epoch 1604/40000, Loss: 0.0018967960495501757, Learning Rate: 0.001650\n",
      "Epoch 1605/40000, Loss: 0.0017802254296839237, Learning Rate: 0.001650\n",
      "Epoch 1606/40000, Loss: 0.0016542802331969142, Learning Rate: 0.001649\n",
      "Epoch 1607/40000, Loss: 0.0018013410735875368, Learning Rate: 0.001649\n",
      "Epoch 1608/40000, Loss: 0.0018390195909887552, Learning Rate: 0.001649\n",
      "Epoch 1609/40000, Loss: 0.0016808434156700969, Learning Rate: 0.001649\n",
      "Epoch 1610/40000, Loss: 0.0016640755347907543, Learning Rate: 0.001649\n",
      "Epoch 1611/40000, Loss: 0.0016918762121349573, Learning Rate: 0.001648\n",
      "Epoch 1612/40000, Loss: 0.0018091222736984491, Learning Rate: 0.001648\n",
      "Epoch 1613/40000, Loss: 0.0017684258054941893, Learning Rate: 0.001648\n",
      "Epoch 1614/40000, Loss: 0.0017033539479598403, Learning Rate: 0.001648\n",
      "Epoch 1615/40000, Loss: 0.0018068336648866534, Learning Rate: 0.001648\n",
      "Epoch 1616/40000, Loss: 0.0018029252532869577, Learning Rate: 0.001647\n",
      "Epoch 1617/40000, Loss: 0.0017605989705771208, Learning Rate: 0.001647\n",
      "Epoch 1618/40000, Loss: 0.0017263181507587433, Learning Rate: 0.001647\n",
      "Epoch 1619/40000, Loss: 0.0017851315205916762, Learning Rate: 0.001647\n",
      "Epoch 1620/40000, Loss: 0.001724247820675373, Learning Rate: 0.001647\n",
      "Epoch 1621/40000, Loss: 0.0017120259581133723, Learning Rate: 0.001646\n",
      "Epoch 1622/40000, Loss: 0.0017078261589631438, Learning Rate: 0.001646\n",
      "Epoch 1623/40000, Loss: 0.0017595964018255472, Learning Rate: 0.001646\n",
      "Epoch 1624/40000, Loss: 0.001649186247959733, Learning Rate: 0.001646\n",
      "Epoch 1625/40000, Loss: 0.001703962218016386, Learning Rate: 0.001646\n",
      "Epoch 1626/40000, Loss: 0.0016901439521461725, Learning Rate: 0.001645\n",
      "Epoch 1627/40000, Loss: 0.0016315699322149158, Learning Rate: 0.001645\n",
      "Epoch 1628/40000, Loss: 0.0017065771389752626, Learning Rate: 0.001645\n",
      "Epoch 1629/40000, Loss: 0.0015991036780178547, Learning Rate: 0.001645\n",
      "Epoch 1630/40000, Loss: 0.0016261886339634657, Learning Rate: 0.001645\n",
      "Epoch 1631/40000, Loss: 0.001681050518527627, Learning Rate: 0.001644\n",
      "Epoch 1632/40000, Loss: 0.0017088421154767275, Learning Rate: 0.001644\n",
      "Epoch 1633/40000, Loss: 0.0016702323919162154, Learning Rate: 0.001644\n",
      "Epoch 1634/40000, Loss: 0.0016904273070394993, Learning Rate: 0.001644\n",
      "Epoch 1635/40000, Loss: 0.0016298460541293025, Learning Rate: 0.001644\n",
      "Epoch 1636/40000, Loss: 0.0016346046468243003, Learning Rate: 0.001643\n",
      "Epoch 1637/40000, Loss: 0.001706994022242725, Learning Rate: 0.001643\n",
      "Epoch 1638/40000, Loss: 0.0016835912829264998, Learning Rate: 0.001643\n",
      "Epoch 1639/40000, Loss: 0.0017304972279816866, Learning Rate: 0.001643\n",
      "Epoch 1640/40000, Loss: 0.0017771574202924967, Learning Rate: 0.001643\n",
      "Epoch 1641/40000, Loss: 0.0017587239854037762, Learning Rate: 0.001643\n",
      "Epoch 1642/40000, Loss: 0.0017426012782379985, Learning Rate: 0.001642\n",
      "Epoch 1643/40000, Loss: 0.0017165704630315304, Learning Rate: 0.001642\n",
      "Epoch 1644/40000, Loss: 0.001648087752982974, Learning Rate: 0.001642\n",
      "Epoch 1645/40000, Loss: 0.0017174223903566599, Learning Rate: 0.001642\n",
      "Epoch 1646/40000, Loss: 0.0018161453772336245, Learning Rate: 0.001642\n",
      "Epoch 1647/40000, Loss: 0.0017149827908724546, Learning Rate: 0.001641\n",
      "Epoch 1648/40000, Loss: 0.0016504996456205845, Learning Rate: 0.001641\n",
      "Epoch 1649/40000, Loss: 0.0017083155689761043, Learning Rate: 0.001641\n",
      "Epoch 1650/40000, Loss: 0.0017538240645080805, Learning Rate: 0.001641\n",
      "Epoch 1651/40000, Loss: 0.0017004068940877914, Learning Rate: 0.001641\n",
      "Epoch 1652/40000, Loss: 0.0017458703368902206, Learning Rate: 0.001640\n",
      "Epoch 1653/40000, Loss: 0.001703888876363635, Learning Rate: 0.001640\n",
      "Epoch 1654/40000, Loss: 0.0017396848415955901, Learning Rate: 0.001640\n",
      "Epoch 1655/40000, Loss: 0.0016879363683983684, Learning Rate: 0.001640\n",
      "Epoch 1656/40000, Loss: 0.0016904007643461227, Learning Rate: 0.001640\n",
      "Epoch 1657/40000, Loss: 0.0016852959524840117, Learning Rate: 0.001639\n",
      "Epoch 1658/40000, Loss: 0.0016842050245031714, Learning Rate: 0.001639\n",
      "Epoch 1659/40000, Loss: 0.0016248603351414204, Learning Rate: 0.001639\n",
      "Epoch 1660/40000, Loss: 0.0016271194908767939, Learning Rate: 0.001639\n",
      "Epoch 1661/40000, Loss: 0.0015960484743118286, Learning Rate: 0.001639\n",
      "Epoch 1662/40000, Loss: 0.0015672037843614817, Learning Rate: 0.001638\n",
      "Epoch 1663/40000, Loss: 0.0018796350341290236, Learning Rate: 0.001638\n",
      "Epoch 1664/40000, Loss: 0.0017035435885190964, Learning Rate: 0.001638\n",
      "Epoch 1665/40000, Loss: 0.0016858489252626896, Learning Rate: 0.001638\n",
      "Epoch 1666/40000, Loss: 0.002141945529729128, Learning Rate: 0.001638\n",
      "Epoch 1667/40000, Loss: 0.0019070835551247, Learning Rate: 0.001637\n",
      "Epoch 1668/40000, Loss: 0.001898221904411912, Learning Rate: 0.001637\n",
      "Epoch 1669/40000, Loss: 0.0018921198789030313, Learning Rate: 0.001637\n",
      "Epoch 1670/40000, Loss: 0.001971963094547391, Learning Rate: 0.001637\n",
      "Epoch 1671/40000, Loss: 0.001957432832568884, Learning Rate: 0.001637\n",
      "Epoch 1672/40000, Loss: 0.0017366897081956267, Learning Rate: 0.001636\n",
      "Epoch 1673/40000, Loss: 0.002005215734243393, Learning Rate: 0.001636\n",
      "Epoch 1674/40000, Loss: 0.0019302007276564837, Learning Rate: 0.001636\n",
      "Epoch 1675/40000, Loss: 0.0018904003081843257, Learning Rate: 0.001636\n",
      "Epoch 1676/40000, Loss: 0.0019434329587966204, Learning Rate: 0.001636\n",
      "Epoch 1677/40000, Loss: 0.001810643239878118, Learning Rate: 0.001635\n",
      "Epoch 1678/40000, Loss: 0.002149671083316207, Learning Rate: 0.001635\n",
      "Epoch 1679/40000, Loss: 0.0017700290773063898, Learning Rate: 0.001635\n",
      "Epoch 1680/40000, Loss: 0.002081687329337001, Learning Rate: 0.001635\n",
      "Epoch 1681/40000, Loss: 0.0019725782331079245, Learning Rate: 0.001635\n",
      "Epoch 1682/40000, Loss: 0.001988233532756567, Learning Rate: 0.001634\n",
      "Epoch 1683/40000, Loss: 0.0020946040749549866, Learning Rate: 0.001634\n",
      "Epoch 1684/40000, Loss: 0.0019617793150246143, Learning Rate: 0.001634\n",
      "Epoch 1685/40000, Loss: 0.00193596794269979, Learning Rate: 0.001634\n",
      "Epoch 1686/40000, Loss: 0.0018596998415887356, Learning Rate: 0.001634\n",
      "Epoch 1687/40000, Loss: 0.0021530110388994217, Learning Rate: 0.001633\n",
      "Epoch 1688/40000, Loss: 0.0020208568312227726, Learning Rate: 0.001633\n",
      "Epoch 1689/40000, Loss: 0.002103258389979601, Learning Rate: 0.001633\n",
      "Epoch 1690/40000, Loss: 0.0024511218070983887, Learning Rate: 0.001633\n",
      "Epoch 1691/40000, Loss: 0.0020795362070202827, Learning Rate: 0.001633\n",
      "Epoch 1692/40000, Loss: 0.0019196087960153818, Learning Rate: 0.001632\n",
      "Epoch 1693/40000, Loss: 0.0017877116333693266, Learning Rate: 0.001632\n",
      "Epoch 1694/40000, Loss: 0.001805651350878179, Learning Rate: 0.001632\n",
      "Epoch 1695/40000, Loss: 0.0018963267793878913, Learning Rate: 0.001632\n",
      "Epoch 1696/40000, Loss: 0.0017237658612430096, Learning Rate: 0.001632\n",
      "Epoch 1697/40000, Loss: 0.0017027584835886955, Learning Rate: 0.001632\n",
      "Epoch 1698/40000, Loss: 0.0018372036283835769, Learning Rate: 0.001631\n",
      "Epoch 1699/40000, Loss: 0.001655740081332624, Learning Rate: 0.001631\n",
      "Epoch 1700/40000, Loss: 0.0016749580390751362, Learning Rate: 0.001631\n",
      "Epoch 1701/40000, Loss: 0.0016966296825557947, Learning Rate: 0.001631\n",
      "Epoch 1702/40000, Loss: 0.0016497590113431215, Learning Rate: 0.001631\n",
      "Epoch 1703/40000, Loss: 0.0016284174052998424, Learning Rate: 0.001630\n",
      "Epoch 1704/40000, Loss: 0.0015603287611156702, Learning Rate: 0.001630\n",
      "Epoch 1705/40000, Loss: 0.0016651115147396922, Learning Rate: 0.001630\n",
      "Epoch 1706/40000, Loss: 0.0016108507988974452, Learning Rate: 0.001630\n",
      "Epoch 1707/40000, Loss: 0.0016804763581603765, Learning Rate: 0.001630\n",
      "Epoch 1708/40000, Loss: 0.0016895700246095657, Learning Rate: 0.001629\n",
      "Epoch 1709/40000, Loss: 0.0017169564962387085, Learning Rate: 0.001629\n",
      "Epoch 1710/40000, Loss: 0.0019020293839275837, Learning Rate: 0.001629\n",
      "Epoch 1711/40000, Loss: 0.0016273988876491785, Learning Rate: 0.001629\n",
      "Epoch 1712/40000, Loss: 0.0016287592006847262, Learning Rate: 0.001629\n",
      "Epoch 1713/40000, Loss: 0.00158284034114331, Learning Rate: 0.001628\n",
      "Epoch 1714/40000, Loss: 0.0016555580077692866, Learning Rate: 0.001628\n",
      "Epoch 1715/40000, Loss: 0.0016282116994261742, Learning Rate: 0.001628\n",
      "Epoch 1716/40000, Loss: 0.0017786959651857615, Learning Rate: 0.001628\n",
      "Epoch 1717/40000, Loss: 0.001694822101853788, Learning Rate: 0.001628\n",
      "Epoch 1718/40000, Loss: 0.0017583540175110102, Learning Rate: 0.001627\n",
      "Epoch 1719/40000, Loss: 0.0017274043057113886, Learning Rate: 0.001627\n",
      "Epoch 1720/40000, Loss: 0.001736287958920002, Learning Rate: 0.001627\n",
      "Epoch 1721/40000, Loss: 0.0016435312572866678, Learning Rate: 0.001627\n",
      "Epoch 1722/40000, Loss: 0.001708890195004642, Learning Rate: 0.001627\n",
      "Epoch 1723/40000, Loss: 0.001620068564079702, Learning Rate: 0.001626\n",
      "Epoch 1724/40000, Loss: 0.0016595274209976196, Learning Rate: 0.001626\n",
      "Epoch 1725/40000, Loss: 0.0015548685332760215, Learning Rate: 0.001626\n",
      "Epoch 1726/40000, Loss: 0.0016000489704310894, Learning Rate: 0.001626\n",
      "Epoch 1727/40000, Loss: 0.0016464854124933481, Learning Rate: 0.001626\n",
      "Epoch 1728/40000, Loss: 0.001603954005986452, Learning Rate: 0.001625\n",
      "Epoch 1729/40000, Loss: 0.0016516379546374083, Learning Rate: 0.001625\n",
      "Epoch 1730/40000, Loss: 0.0016167459543794394, Learning Rate: 0.001625\n",
      "Epoch 1731/40000, Loss: 0.001516509335488081, Learning Rate: 0.001625\n",
      "Epoch 1732/40000, Loss: 0.0016199806705117226, Learning Rate: 0.001625\n",
      "Epoch 1733/40000, Loss: 0.0016939942725002766, Learning Rate: 0.001624\n",
      "Epoch 1734/40000, Loss: 0.0016681186389178038, Learning Rate: 0.001624\n",
      "Epoch 1735/40000, Loss: 0.0016174172051250935, Learning Rate: 0.001624\n",
      "Epoch 1736/40000, Loss: 0.001693384489044547, Learning Rate: 0.001624\n",
      "Epoch 1737/40000, Loss: 0.0015546707436442375, Learning Rate: 0.001624\n",
      "Epoch 1738/40000, Loss: 0.0015908635687083006, Learning Rate: 0.001624\n",
      "Epoch 1739/40000, Loss: 0.0016179181402549148, Learning Rate: 0.001623\n",
      "Epoch 1740/40000, Loss: 0.001687940675765276, Learning Rate: 0.001623\n",
      "Epoch 1741/40000, Loss: 0.0017548989271745086, Learning Rate: 0.001623\n",
      "Epoch 1742/40000, Loss: 0.0017202612943947315, Learning Rate: 0.001623\n",
      "Epoch 1743/40000, Loss: 0.001615804503671825, Learning Rate: 0.001623\n",
      "Epoch 1744/40000, Loss: 0.0016043633222579956, Learning Rate: 0.001622\n",
      "Epoch 1745/40000, Loss: 0.0016173989279195666, Learning Rate: 0.001622\n",
      "Epoch 1746/40000, Loss: 0.0017505873693153262, Learning Rate: 0.001622\n",
      "Epoch 1747/40000, Loss: 0.0016412334516644478, Learning Rate: 0.001622\n",
      "Epoch 1748/40000, Loss: 0.001648829784244299, Learning Rate: 0.001622\n",
      "Epoch 1749/40000, Loss: 0.0016577346250414848, Learning Rate: 0.001621\n",
      "Epoch 1750/40000, Loss: 0.0016813887050375342, Learning Rate: 0.001621\n",
      "Epoch 1751/40000, Loss: 0.0015937220305204391, Learning Rate: 0.001621\n",
      "Epoch 1752/40000, Loss: 0.001598207512870431, Learning Rate: 0.001621\n",
      "Epoch 1753/40000, Loss: 0.0015463433228433132, Learning Rate: 0.001621\n",
      "Epoch 1754/40000, Loss: 0.0016458842437714338, Learning Rate: 0.001620\n",
      "Epoch 1755/40000, Loss: 0.0015575112774968147, Learning Rate: 0.001620\n",
      "Epoch 1756/40000, Loss: 0.0015511744422838092, Learning Rate: 0.001620\n",
      "Epoch 1757/40000, Loss: 0.0016250935150310397, Learning Rate: 0.001620\n",
      "Epoch 1758/40000, Loss: 0.0016514586750417948, Learning Rate: 0.001620\n",
      "Epoch 1759/40000, Loss: 0.0016334539977833629, Learning Rate: 0.001619\n",
      "Epoch 1760/40000, Loss: 0.0016906799282878637, Learning Rate: 0.001619\n",
      "Epoch 1761/40000, Loss: 0.0016498420154675841, Learning Rate: 0.001619\n",
      "Epoch 1762/40000, Loss: 0.001614064327441156, Learning Rate: 0.001619\n",
      "Epoch 1763/40000, Loss: 0.0016746218316257, Learning Rate: 0.001619\n",
      "Epoch 1764/40000, Loss: 0.001645995769649744, Learning Rate: 0.001618\n",
      "Epoch 1765/40000, Loss: 0.001577569404616952, Learning Rate: 0.001618\n",
      "Epoch 1766/40000, Loss: 0.0016405973583459854, Learning Rate: 0.001618\n",
      "Epoch 1767/40000, Loss: 0.0015829808544367552, Learning Rate: 0.001618\n",
      "Epoch 1768/40000, Loss: 0.0016708637122064829, Learning Rate: 0.001618\n",
      "Epoch 1769/40000, Loss: 0.0016395075945183635, Learning Rate: 0.001617\n",
      "Epoch 1770/40000, Loss: 0.0017018544021993876, Learning Rate: 0.001617\n",
      "Epoch 1771/40000, Loss: 0.0015839182306081057, Learning Rate: 0.001617\n",
      "Epoch 1772/40000, Loss: 0.0017107258317992091, Learning Rate: 0.001617\n",
      "Epoch 1773/40000, Loss: 0.00171079661231488, Learning Rate: 0.001617\n",
      "Epoch 1774/40000, Loss: 0.0016356263076886535, Learning Rate: 0.001617\n",
      "Epoch 1775/40000, Loss: 0.00190363684669137, Learning Rate: 0.001616\n",
      "Epoch 1776/40000, Loss: 0.001772299874573946, Learning Rate: 0.001616\n",
      "Epoch 1777/40000, Loss: 0.0016106266994029284, Learning Rate: 0.001616\n",
      "Epoch 1778/40000, Loss: 0.0016863199416548014, Learning Rate: 0.001616\n",
      "Epoch 1779/40000, Loss: 0.0018664849922060966, Learning Rate: 0.001616\n",
      "Epoch 1780/40000, Loss: 0.001779084443114698, Learning Rate: 0.001615\n",
      "Epoch 1781/40000, Loss: 0.0016543813981115818, Learning Rate: 0.001615\n",
      "Epoch 1782/40000, Loss: 0.0018825388979166746, Learning Rate: 0.001615\n",
      "Epoch 1783/40000, Loss: 0.0016061452915892005, Learning Rate: 0.001615\n",
      "Epoch 1784/40000, Loss: 0.0017977147363126278, Learning Rate: 0.001615\n",
      "Epoch 1785/40000, Loss: 0.001778700272552669, Learning Rate: 0.001614\n",
      "Epoch 1786/40000, Loss: 0.0017638918943703175, Learning Rate: 0.001614\n",
      "Epoch 1787/40000, Loss: 0.001937410794198513, Learning Rate: 0.001614\n",
      "Epoch 1788/40000, Loss: 0.0017156916437670588, Learning Rate: 0.001614\n",
      "Epoch 1789/40000, Loss: 0.001721831038594246, Learning Rate: 0.001614\n",
      "Epoch 1790/40000, Loss: 0.0018792736809700727, Learning Rate: 0.001613\n",
      "Epoch 1791/40000, Loss: 0.0016343219904229045, Learning Rate: 0.001613\n",
      "Epoch 1792/40000, Loss: 0.001664048875682056, Learning Rate: 0.001613\n",
      "Epoch 1793/40000, Loss: 0.0018449177732691169, Learning Rate: 0.001613\n",
      "Epoch 1794/40000, Loss: 0.0017253805417567492, Learning Rate: 0.001613\n",
      "Epoch 1795/40000, Loss: 0.0016204540152102709, Learning Rate: 0.001612\n",
      "Epoch 1796/40000, Loss: 0.0016040496993809938, Learning Rate: 0.001612\n",
      "Epoch 1797/40000, Loss: 0.0017228493234142661, Learning Rate: 0.001612\n",
      "Epoch 1798/40000, Loss: 0.0017047816654667258, Learning Rate: 0.001612\n",
      "Epoch 1799/40000, Loss: 0.0016334489919245243, Learning Rate: 0.001612\n",
      "Epoch 1800/40000, Loss: 0.0016143724787980318, Learning Rate: 0.001611\n",
      "Epoch 1801/40000, Loss: 0.001716898987069726, Learning Rate: 0.001611\n",
      "Epoch 1802/40000, Loss: 0.0016911639831960201, Learning Rate: 0.001611\n",
      "Epoch 1803/40000, Loss: 0.0015538712032139301, Learning Rate: 0.001611\n",
      "Epoch 1804/40000, Loss: 0.001595974899828434, Learning Rate: 0.001611\n",
      "Epoch 1805/40000, Loss: 0.0016346974298357964, Learning Rate: 0.001611\n",
      "Epoch 1806/40000, Loss: 0.0015806627925485373, Learning Rate: 0.001610\n",
      "Epoch 1807/40000, Loss: 0.0015965481288731098, Learning Rate: 0.001610\n",
      "Epoch 1808/40000, Loss: 0.0016856788424775004, Learning Rate: 0.001610\n",
      "Epoch 1809/40000, Loss: 0.0016970633296296, Learning Rate: 0.001610\n",
      "Epoch 1810/40000, Loss: 0.001676874584518373, Learning Rate: 0.001610\n",
      "Epoch 1811/40000, Loss: 0.0016827669460326433, Learning Rate: 0.001609\n",
      "Epoch 1812/40000, Loss: 0.0016461418708786368, Learning Rate: 0.001609\n",
      "Epoch 1813/40000, Loss: 0.0016463487409055233, Learning Rate: 0.001609\n",
      "Epoch 1814/40000, Loss: 0.0015734428307041526, Learning Rate: 0.001609\n",
      "Epoch 1815/40000, Loss: 0.001560088130645454, Learning Rate: 0.001609\n",
      "Epoch 1816/40000, Loss: 0.0015702820383012295, Learning Rate: 0.001608\n",
      "Epoch 1817/40000, Loss: 0.0016557116759940982, Learning Rate: 0.001608\n",
      "Epoch 1818/40000, Loss: 0.0015079736476764083, Learning Rate: 0.001608\n",
      "Epoch 1819/40000, Loss: 0.0015657896874472499, Learning Rate: 0.001608\n",
      "Epoch 1820/40000, Loss: 0.0015145678771659732, Learning Rate: 0.001608\n",
      "Epoch 1821/40000, Loss: 0.0016338190762326121, Learning Rate: 0.001607\n",
      "Epoch 1822/40000, Loss: 0.001631921506486833, Learning Rate: 0.001607\n",
      "Epoch 1823/40000, Loss: 0.0016476528253406286, Learning Rate: 0.001607\n",
      "Epoch 1824/40000, Loss: 0.0015565271023660898, Learning Rate: 0.001607\n",
      "Epoch 1825/40000, Loss: 0.001488126115873456, Learning Rate: 0.001607\n",
      "Epoch 1826/40000, Loss: 0.0016334423562511802, Learning Rate: 0.001606\n",
      "Epoch 1827/40000, Loss: 0.001591688022017479, Learning Rate: 0.001606\n",
      "Epoch 1828/40000, Loss: 0.0015407779719680548, Learning Rate: 0.001606\n",
      "Epoch 1829/40000, Loss: 0.0016755409305915236, Learning Rate: 0.001606\n",
      "Epoch 1830/40000, Loss: 0.0016229309840127826, Learning Rate: 0.001606\n",
      "Epoch 1831/40000, Loss: 0.0015847075264900923, Learning Rate: 0.001605\n",
      "Epoch 1832/40000, Loss: 0.0015686966944485903, Learning Rate: 0.001605\n",
      "Epoch 1833/40000, Loss: 0.0016552528832107782, Learning Rate: 0.001605\n",
      "Epoch 1834/40000, Loss: 0.0016775443218648434, Learning Rate: 0.001605\n",
      "Epoch 1835/40000, Loss: 0.0016586750280112028, Learning Rate: 0.001605\n",
      "Epoch 1836/40000, Loss: 0.001548761734738946, Learning Rate: 0.001605\n",
      "Epoch 1837/40000, Loss: 0.0015226963441818953, Learning Rate: 0.001604\n",
      "Epoch 1838/40000, Loss: 0.0015485708136111498, Learning Rate: 0.001604\n",
      "Epoch 1839/40000, Loss: 0.0016055649612098932, Learning Rate: 0.001604\n",
      "Epoch 1840/40000, Loss: 0.0015032615046948195, Learning Rate: 0.001604\n",
      "Epoch 1841/40000, Loss: 0.0016333627281710505, Learning Rate: 0.001604\n",
      "Epoch 1842/40000, Loss: 0.0015575721627101302, Learning Rate: 0.001603\n",
      "Epoch 1843/40000, Loss: 0.0016901774797588587, Learning Rate: 0.001603\n",
      "Epoch 1844/40000, Loss: 0.0016857099253684282, Learning Rate: 0.001603\n",
      "Epoch 1845/40000, Loss: 0.0016165650449693203, Learning Rate: 0.001603\n",
      "Epoch 1846/40000, Loss: 0.001580128795467317, Learning Rate: 0.001603\n",
      "Epoch 1847/40000, Loss: 0.0017491965554654598, Learning Rate: 0.001602\n",
      "Epoch 1848/40000, Loss: 0.0016547234263271093, Learning Rate: 0.001602\n",
      "Epoch 1849/40000, Loss: 0.0016152983298525214, Learning Rate: 0.001602\n",
      "Epoch 1850/40000, Loss: 0.001585933263413608, Learning Rate: 0.001602\n",
      "Epoch 1851/40000, Loss: 0.0016456732992082834, Learning Rate: 0.001602\n",
      "Epoch 1852/40000, Loss: 0.0016715720994397998, Learning Rate: 0.001601\n",
      "Epoch 1853/40000, Loss: 0.0015693544410169125, Learning Rate: 0.001601\n",
      "Epoch 1854/40000, Loss: 0.0016028268728405237, Learning Rate: 0.001601\n",
      "Epoch 1855/40000, Loss: 0.0017617314588278532, Learning Rate: 0.001601\n",
      "Epoch 1856/40000, Loss: 0.001652374630793929, Learning Rate: 0.001601\n",
      "Epoch 1857/40000, Loss: 0.001625702134333551, Learning Rate: 0.001600\n",
      "Epoch 1858/40000, Loss: 0.0015940594021230936, Learning Rate: 0.001600\n",
      "Epoch 1859/40000, Loss: 0.0015365484869107604, Learning Rate: 0.001600\n",
      "Epoch 1860/40000, Loss: 0.001684974180534482, Learning Rate: 0.001600\n",
      "Epoch 1861/40000, Loss: 0.0016129440627992153, Learning Rate: 0.001600\n",
      "Epoch 1862/40000, Loss: 0.001632810803130269, Learning Rate: 0.001600\n",
      "Epoch 1863/40000, Loss: 0.0015501457965001464, Learning Rate: 0.001599\n",
      "Epoch 1864/40000, Loss: 0.0016316899564117193, Learning Rate: 0.001599\n",
      "Epoch 1865/40000, Loss: 0.0016695880331099033, Learning Rate: 0.001599\n",
      "Epoch 1866/40000, Loss: 0.0015671171713620424, Learning Rate: 0.001599\n",
      "Epoch 1867/40000, Loss: 0.0016876216977834702, Learning Rate: 0.001599\n",
      "Epoch 1868/40000, Loss: 0.0016149126458913088, Learning Rate: 0.001598\n",
      "Epoch 1869/40000, Loss: 0.0015843285946175456, Learning Rate: 0.001598\n",
      "Epoch 1870/40000, Loss: 0.0016206633299589157, Learning Rate: 0.001598\n",
      "Epoch 1871/40000, Loss: 0.0015273068565875292, Learning Rate: 0.001598\n",
      "Epoch 1872/40000, Loss: 0.001616923836991191, Learning Rate: 0.001598\n",
      "Epoch 1873/40000, Loss: 0.0015207291580736637, Learning Rate: 0.001597\n",
      "Epoch 1874/40000, Loss: 0.0015199319459497929, Learning Rate: 0.001597\n",
      "Epoch 1875/40000, Loss: 0.0016219494864344597, Learning Rate: 0.001597\n",
      "Epoch 1876/40000, Loss: 0.0015792384510859847, Learning Rate: 0.001597\n",
      "Epoch 1877/40000, Loss: 0.001566797262057662, Learning Rate: 0.001597\n",
      "Epoch 1878/40000, Loss: 0.0015086004277691245, Learning Rate: 0.001596\n",
      "Epoch 1879/40000, Loss: 0.0014798222109675407, Learning Rate: 0.001596\n",
      "Epoch 1880/40000, Loss: 0.0016109796706587076, Learning Rate: 0.001596\n",
      "Epoch 1881/40000, Loss: 0.0015258416533470154, Learning Rate: 0.001596\n",
      "Epoch 1882/40000, Loss: 0.0014158864505589008, Learning Rate: 0.001596\n",
      "Epoch 1883/40000, Loss: 0.0016079432098194957, Learning Rate: 0.001595\n",
      "Epoch 1884/40000, Loss: 0.0015653291484341025, Learning Rate: 0.001595\n",
      "Epoch 1885/40000, Loss: 0.0015232861042022705, Learning Rate: 0.001595\n",
      "Epoch 1886/40000, Loss: 0.0016879048198461533, Learning Rate: 0.001595\n",
      "Epoch 1887/40000, Loss: 0.001567576895467937, Learning Rate: 0.001595\n",
      "Epoch 1888/40000, Loss: 0.0020365207456052303, Learning Rate: 0.001595\n",
      "Epoch 1889/40000, Loss: 0.0014880676753818989, Learning Rate: 0.001594\n",
      "Epoch 1890/40000, Loss: 0.0018107563955709338, Learning Rate: 0.001594\n",
      "Epoch 1891/40000, Loss: 0.0015901968581601977, Learning Rate: 0.001594\n",
      "Epoch 1892/40000, Loss: 0.0016531165456399322, Learning Rate: 0.001594\n",
      "Epoch 1893/40000, Loss: 0.0015638908371329308, Learning Rate: 0.001594\n",
      "Epoch 1894/40000, Loss: 0.001557110226713121, Learning Rate: 0.001593\n",
      "Epoch 1895/40000, Loss: 0.0016747097251936793, Learning Rate: 0.001593\n",
      "Epoch 1896/40000, Loss: 0.001587943988852203, Learning Rate: 0.001593\n",
      "Epoch 1897/40000, Loss: 0.00151780154556036, Learning Rate: 0.001593\n",
      "Epoch 1898/40000, Loss: 0.001511125359684229, Learning Rate: 0.001593\n",
      "Epoch 1899/40000, Loss: 0.0015232495497912169, Learning Rate: 0.001592\n",
      "Epoch 1900/40000, Loss: 0.0015889591304585338, Learning Rate: 0.001592\n",
      "Epoch 1901/40000, Loss: 0.0015154248103499413, Learning Rate: 0.001592\n",
      "Epoch 1902/40000, Loss: 0.0015377993695437908, Learning Rate: 0.001592\n",
      "Epoch 1903/40000, Loss: 0.0014672259567305446, Learning Rate: 0.001592\n",
      "Epoch 1904/40000, Loss: 0.001592810032889247, Learning Rate: 0.001591\n",
      "Epoch 1905/40000, Loss: 0.001554118818603456, Learning Rate: 0.001591\n",
      "Epoch 1906/40000, Loss: 0.0014773472212255, Learning Rate: 0.001591\n",
      "Epoch 1907/40000, Loss: 0.0015089611988514662, Learning Rate: 0.001591\n",
      "Epoch 1908/40000, Loss: 0.0015938354190438986, Learning Rate: 0.001591\n",
      "Epoch 1909/40000, Loss: 0.001620412222109735, Learning Rate: 0.001591\n",
      "Epoch 1910/40000, Loss: 0.0015261354856193066, Learning Rate: 0.001590\n",
      "Epoch 1911/40000, Loss: 0.0015824072761461139, Learning Rate: 0.001590\n",
      "Epoch 1912/40000, Loss: 0.0015324753476306796, Learning Rate: 0.001590\n",
      "Epoch 1913/40000, Loss: 0.001558130607008934, Learning Rate: 0.001590\n",
      "Epoch 1914/40000, Loss: 0.0015115358401089907, Learning Rate: 0.001590\n",
      "Epoch 1915/40000, Loss: 0.0014755240408703685, Learning Rate: 0.001589\n",
      "Epoch 1916/40000, Loss: 0.001526799751445651, Learning Rate: 0.001589\n",
      "Epoch 1917/40000, Loss: 0.0015900745056569576, Learning Rate: 0.001589\n",
      "Epoch 1918/40000, Loss: 0.001632152358070016, Learning Rate: 0.001589\n",
      "Epoch 1919/40000, Loss: 0.001531620742753148, Learning Rate: 0.001589\n",
      "Epoch 1920/40000, Loss: 0.0015553830889984965, Learning Rate: 0.001588\n",
      "Epoch 1921/40000, Loss: 0.0014611841179430485, Learning Rate: 0.001588\n",
      "Epoch 1922/40000, Loss: 0.0015830168267711997, Learning Rate: 0.001588\n",
      "Epoch 1923/40000, Loss: 0.0015465328469872475, Learning Rate: 0.001588\n",
      "Epoch 1924/40000, Loss: 0.001549877692013979, Learning Rate: 0.001588\n",
      "Epoch 1925/40000, Loss: 0.0015523093752563, Learning Rate: 0.001587\n",
      "Epoch 1926/40000, Loss: 0.0016017353627830744, Learning Rate: 0.001587\n",
      "Epoch 1927/40000, Loss: 0.0015285920817404985, Learning Rate: 0.001587\n",
      "Epoch 1928/40000, Loss: 0.0015382959973067045, Learning Rate: 0.001587\n",
      "Epoch 1929/40000, Loss: 0.0015322440303862095, Learning Rate: 0.001587\n",
      "Epoch 1930/40000, Loss: 0.001600737450644374, Learning Rate: 0.001587\n",
      "Epoch 1931/40000, Loss: 0.0015035208780318499, Learning Rate: 0.001586\n",
      "Epoch 1932/40000, Loss: 0.0015035368269309402, Learning Rate: 0.001586\n",
      "Epoch 1933/40000, Loss: 0.0015040459111332893, Learning Rate: 0.001586\n",
      "Epoch 1934/40000, Loss: 0.0014823036035522819, Learning Rate: 0.001586\n",
      "Epoch 1935/40000, Loss: 0.0014637636486440897, Learning Rate: 0.001586\n",
      "Epoch 1936/40000, Loss: 0.001625024015083909, Learning Rate: 0.001585\n",
      "Epoch 1937/40000, Loss: 0.0015161922201514244, Learning Rate: 0.001585\n",
      "Epoch 1938/40000, Loss: 0.0015428875340148807, Learning Rate: 0.001585\n",
      "Epoch 1939/40000, Loss: 0.0017699962481856346, Learning Rate: 0.001585\n",
      "Epoch 1940/40000, Loss: 0.0016847536899149418, Learning Rate: 0.001585\n",
      "Epoch 1941/40000, Loss: 0.0016574969049543142, Learning Rate: 0.001584\n",
      "Epoch 1942/40000, Loss: 0.001594983390532434, Learning Rate: 0.001584\n",
      "Epoch 1943/40000, Loss: 0.0015764038544148207, Learning Rate: 0.001584\n",
      "Epoch 1944/40000, Loss: 0.0017407512059435248, Learning Rate: 0.001584\n",
      "Epoch 1945/40000, Loss: 0.0016878589522093534, Learning Rate: 0.001584\n",
      "Epoch 1946/40000, Loss: 0.0020316867157816887, Learning Rate: 0.001583\n",
      "Epoch 1947/40000, Loss: 0.0016715851379558444, Learning Rate: 0.001583\n",
      "Epoch 1948/40000, Loss: 0.0015950759407132864, Learning Rate: 0.001583\n",
      "Epoch 1949/40000, Loss: 0.001518235425464809, Learning Rate: 0.001583\n",
      "Epoch 1950/40000, Loss: 0.0016210002359002829, Learning Rate: 0.001583\n",
      "Epoch 1951/40000, Loss: 0.0015513174002990127, Learning Rate: 0.001583\n",
      "Epoch 1952/40000, Loss: 0.0014720044564455748, Learning Rate: 0.001582\n",
      "Epoch 1953/40000, Loss: 0.0015260272193700075, Learning Rate: 0.001582\n",
      "Epoch 1954/40000, Loss: 0.001639959285967052, Learning Rate: 0.001582\n",
      "Epoch 1955/40000, Loss: 0.0015411684289574623, Learning Rate: 0.001582\n",
      "Epoch 1956/40000, Loss: 0.0015259755309671164, Learning Rate: 0.001582\n",
      "Epoch 1957/40000, Loss: 0.0014940423425287008, Learning Rate: 0.001581\n",
      "Epoch 1958/40000, Loss: 0.0015815142542123795, Learning Rate: 0.001581\n",
      "Epoch 1959/40000, Loss: 0.0015069901710376143, Learning Rate: 0.001581\n",
      "Epoch 1960/40000, Loss: 0.001445055240765214, Learning Rate: 0.001581\n",
      "Epoch 1961/40000, Loss: 0.00156378501560539, Learning Rate: 0.001581\n",
      "Epoch 1962/40000, Loss: 0.0015381264965981245, Learning Rate: 0.001580\n",
      "Epoch 1963/40000, Loss: 0.0014560914132744074, Learning Rate: 0.001580\n",
      "Epoch 1964/40000, Loss: 0.0015586146619170904, Learning Rate: 0.001580\n",
      "Epoch 1965/40000, Loss: 0.001519117271527648, Learning Rate: 0.001580\n",
      "Epoch 1966/40000, Loss: 0.0014594639651477337, Learning Rate: 0.001580\n",
      "Epoch 1967/40000, Loss: 0.0014794146409258246, Learning Rate: 0.001579\n",
      "Epoch 1968/40000, Loss: 0.001411871868185699, Learning Rate: 0.001579\n",
      "Epoch 1969/40000, Loss: 0.0014573503285646439, Learning Rate: 0.001579\n",
      "Epoch 1970/40000, Loss: 0.001466762158088386, Learning Rate: 0.001579\n",
      "Epoch 1971/40000, Loss: 0.0015551699325442314, Learning Rate: 0.001579\n",
      "Epoch 1972/40000, Loss: 0.0015052617527544498, Learning Rate: 0.001579\n",
      "Epoch 1973/40000, Loss: 0.0015631694113835692, Learning Rate: 0.001578\n",
      "Epoch 1974/40000, Loss: 0.0015588398091495037, Learning Rate: 0.001578\n",
      "Epoch 1975/40000, Loss: 0.0016377647407352924, Learning Rate: 0.001578\n",
      "Epoch 1976/40000, Loss: 0.0017026651185005903, Learning Rate: 0.001578\n",
      "Epoch 1977/40000, Loss: 0.0014793674927204847, Learning Rate: 0.001578\n",
      "Epoch 1978/40000, Loss: 0.001628815196454525, Learning Rate: 0.001577\n",
      "Epoch 1979/40000, Loss: 0.001671593519859016, Learning Rate: 0.001577\n",
      "Epoch 1980/40000, Loss: 0.0016352941747754812, Learning Rate: 0.001577\n",
      "Epoch 1981/40000, Loss: 0.001547725172713399, Learning Rate: 0.001577\n",
      "Epoch 1982/40000, Loss: 0.0016695342492312193, Learning Rate: 0.001577\n",
      "Epoch 1983/40000, Loss: 0.0014879511436447501, Learning Rate: 0.001576\n",
      "Epoch 1984/40000, Loss: 0.001569585409015417, Learning Rate: 0.001576\n",
      "Epoch 1985/40000, Loss: 0.0014787957770749927, Learning Rate: 0.001576\n",
      "Epoch 1986/40000, Loss: 0.0015681980876252055, Learning Rate: 0.001576\n",
      "Epoch 1987/40000, Loss: 0.0015595037257298827, Learning Rate: 0.001576\n",
      "Epoch 1988/40000, Loss: 0.001536541967652738, Learning Rate: 0.001576\n",
      "Epoch 1989/40000, Loss: 0.0022060414776206017, Learning Rate: 0.001575\n",
      "Epoch 1990/40000, Loss: 0.0016580520896241069, Learning Rate: 0.001575\n",
      "Epoch 1991/40000, Loss: 0.0017197185661643744, Learning Rate: 0.001575\n",
      "Epoch 1992/40000, Loss: 0.0015348666347563267, Learning Rate: 0.001575\n",
      "Epoch 1993/40000, Loss: 0.0017968686297535896, Learning Rate: 0.001575\n",
      "Epoch 1994/40000, Loss: 0.0016190659953281283, Learning Rate: 0.001574\n",
      "Epoch 1995/40000, Loss: 0.0015899844001978636, Learning Rate: 0.001574\n",
      "Epoch 1996/40000, Loss: 0.00148577184882015, Learning Rate: 0.001574\n",
      "Epoch 1997/40000, Loss: 0.0016059487825259566, Learning Rate: 0.001574\n",
      "Epoch 1998/40000, Loss: 0.0015161773189902306, Learning Rate: 0.001574\n",
      "Epoch 1999/40000, Loss: 0.001525827101431787, Learning Rate: 0.001573\n",
      "Epoch 2000/40000, Loss: 0.0014900944661349058, Learning Rate: 0.001573\n",
      "Epoch 2001/40000, Loss: 0.0014278021408244967, Learning Rate: 0.001573\n",
      "Epoch 2002/40000, Loss: 0.0014246492646634579, Learning Rate: 0.001573\n",
      "Epoch 2003/40000, Loss: 0.0014678365550935268, Learning Rate: 0.001573\n",
      "Epoch 2004/40000, Loss: 0.001495769596658647, Learning Rate: 0.001572\n",
      "Epoch 2005/40000, Loss: 0.0014808381674811244, Learning Rate: 0.001572\n",
      "Epoch 2006/40000, Loss: 0.0014705363428220153, Learning Rate: 0.001572\n",
      "Epoch 2007/40000, Loss: 0.0014631188241764903, Learning Rate: 0.001572\n",
      "Epoch 2008/40000, Loss: 0.0015383511781692505, Learning Rate: 0.001572\n",
      "Epoch 2009/40000, Loss: 0.0014886868884786963, Learning Rate: 0.001572\n",
      "Epoch 2010/40000, Loss: 0.001541675766929984, Learning Rate: 0.001571\n",
      "Epoch 2011/40000, Loss: 0.0014227267820388079, Learning Rate: 0.001571\n",
      "Epoch 2012/40000, Loss: 0.0014369264245033264, Learning Rate: 0.001571\n",
      "Epoch 2013/40000, Loss: 0.0014611671213060617, Learning Rate: 0.001571\n",
      "Epoch 2014/40000, Loss: 0.0014962188433855772, Learning Rate: 0.001571\n",
      "Epoch 2015/40000, Loss: 0.0015781656838953495, Learning Rate: 0.001570\n",
      "Epoch 2016/40000, Loss: 0.0015363700222223997, Learning Rate: 0.001570\n",
      "Epoch 2017/40000, Loss: 0.0014775911113247275, Learning Rate: 0.001570\n",
      "Epoch 2018/40000, Loss: 0.001567887025885284, Learning Rate: 0.001570\n",
      "Epoch 2019/40000, Loss: 0.0016091949073597789, Learning Rate: 0.001570\n",
      "Epoch 2020/40000, Loss: 0.0015548160299658775, Learning Rate: 0.001569\n",
      "Epoch 2021/40000, Loss: 0.001498522120527923, Learning Rate: 0.001569\n",
      "Epoch 2022/40000, Loss: 0.0014443685067817569, Learning Rate: 0.001569\n",
      "Epoch 2023/40000, Loss: 0.001419615000486374, Learning Rate: 0.001569\n",
      "Epoch 2024/40000, Loss: 0.0014670398086309433, Learning Rate: 0.001569\n",
      "Epoch 2025/40000, Loss: 0.001479934435337782, Learning Rate: 0.001569\n",
      "Epoch 2026/40000, Loss: 0.001555953174829483, Learning Rate: 0.001568\n",
      "Epoch 2027/40000, Loss: 0.0015291590243577957, Learning Rate: 0.001568\n",
      "Epoch 2028/40000, Loss: 0.0014926563017070293, Learning Rate: 0.001568\n",
      "Epoch 2029/40000, Loss: 0.0015551319811493158, Learning Rate: 0.001568\n",
      "Epoch 2030/40000, Loss: 0.0014865535777062178, Learning Rate: 0.001568\n",
      "Epoch 2031/40000, Loss: 0.001461960724554956, Learning Rate: 0.001567\n",
      "Epoch 2032/40000, Loss: 0.001455420395359397, Learning Rate: 0.001567\n",
      "Epoch 2033/40000, Loss: 0.0014505095314234495, Learning Rate: 0.001567\n",
      "Epoch 2034/40000, Loss: 0.0014494364149868488, Learning Rate: 0.001567\n",
      "Epoch 2035/40000, Loss: 0.001462886342778802, Learning Rate: 0.001567\n",
      "Epoch 2036/40000, Loss: 0.0013815725687891245, Learning Rate: 0.001566\n",
      "Epoch 2037/40000, Loss: 0.0014831279404461384, Learning Rate: 0.001566\n",
      "Epoch 2038/40000, Loss: 0.0014060399262234569, Learning Rate: 0.001566\n",
      "Epoch 2039/40000, Loss: 0.0015413162764161825, Learning Rate: 0.001566\n",
      "Epoch 2040/40000, Loss: 0.0016740541905164719, Learning Rate: 0.001566\n",
      "Epoch 2041/40000, Loss: 0.0017919455422088504, Learning Rate: 0.001566\n",
      "Epoch 2042/40000, Loss: 0.0015374942449852824, Learning Rate: 0.001565\n",
      "Epoch 2043/40000, Loss: 0.001545229461044073, Learning Rate: 0.001565\n",
      "Epoch 2044/40000, Loss: 0.0015541149768978357, Learning Rate: 0.001565\n",
      "Epoch 2045/40000, Loss: 0.0015643506776541471, Learning Rate: 0.001565\n",
      "Epoch 2046/40000, Loss: 0.0014733335701748729, Learning Rate: 0.001565\n",
      "Epoch 2047/40000, Loss: 0.0014790298882871866, Learning Rate: 0.001564\n",
      "Epoch 2048/40000, Loss: 0.0015459003625437617, Learning Rate: 0.001564\n",
      "Epoch 2049/40000, Loss: 0.0014507275773212314, Learning Rate: 0.001564\n",
      "Epoch 2050/40000, Loss: 0.0015223764348775148, Learning Rate: 0.001564\n",
      "Epoch 2051/40000, Loss: 0.0015136455185711384, Learning Rate: 0.001564\n",
      "Epoch 2052/40000, Loss: 0.0015776854706928134, Learning Rate: 0.001563\n",
      "Epoch 2053/40000, Loss: 0.0014856546185910702, Learning Rate: 0.001563\n",
      "Epoch 2054/40000, Loss: 0.0017386721447110176, Learning Rate: 0.001563\n",
      "Epoch 2055/40000, Loss: 0.0015692326705902815, Learning Rate: 0.001563\n",
      "Epoch 2056/40000, Loss: 0.0015540866879746318, Learning Rate: 0.001563\n",
      "Epoch 2057/40000, Loss: 0.0015054306713864207, Learning Rate: 0.001563\n",
      "Epoch 2058/40000, Loss: 0.0017789197154343128, Learning Rate: 0.001562\n",
      "Epoch 2059/40000, Loss: 0.0016563673270866275, Learning Rate: 0.001562\n",
      "Epoch 2060/40000, Loss: 0.0017060802783817053, Learning Rate: 0.001562\n",
      "Epoch 2061/40000, Loss: 0.001514608971774578, Learning Rate: 0.001562\n",
      "Epoch 2062/40000, Loss: 0.0015161843039095402, Learning Rate: 0.001562\n",
      "Epoch 2063/40000, Loss: 0.0014295412693172693, Learning Rate: 0.001561\n",
      "Epoch 2064/40000, Loss: 0.0014760580379515886, Learning Rate: 0.001561\n",
      "Epoch 2065/40000, Loss: 0.0014760587364435196, Learning Rate: 0.001561\n",
      "Epoch 2066/40000, Loss: 0.001691181561909616, Learning Rate: 0.001561\n",
      "Epoch 2067/40000, Loss: 0.0015758208464831114, Learning Rate: 0.001561\n",
      "Epoch 2068/40000, Loss: 0.0014962702989578247, Learning Rate: 0.001560\n",
      "Epoch 2069/40000, Loss: 0.001614088425412774, Learning Rate: 0.001560\n",
      "Epoch 2070/40000, Loss: 0.0014780249912291765, Learning Rate: 0.001560\n",
      "Epoch 2071/40000, Loss: 0.0015221325447782874, Learning Rate: 0.001560\n",
      "Epoch 2072/40000, Loss: 0.0014709801180288196, Learning Rate: 0.001560\n",
      "Epoch 2073/40000, Loss: 0.0015027576591819525, Learning Rate: 0.001560\n",
      "Epoch 2074/40000, Loss: 0.0014734354335814714, Learning Rate: 0.001559\n",
      "Epoch 2075/40000, Loss: 0.0015444359742105007, Learning Rate: 0.001559\n",
      "Epoch 2076/40000, Loss: 0.0015123233897611499, Learning Rate: 0.001559\n",
      "Epoch 2077/40000, Loss: 0.0015739426016807556, Learning Rate: 0.001559\n",
      "Epoch 2078/40000, Loss: 0.001425385009497404, Learning Rate: 0.001559\n",
      "Epoch 2079/40000, Loss: 0.0015122354961931705, Learning Rate: 0.001558\n",
      "Epoch 2080/40000, Loss: 0.0014144362648949027, Learning Rate: 0.001558\n",
      "Epoch 2081/40000, Loss: 0.0014420084189623594, Learning Rate: 0.001558\n",
      "Epoch 2082/40000, Loss: 0.0014771337155252695, Learning Rate: 0.001558\n",
      "Epoch 2083/40000, Loss: 0.0014023699332028627, Learning Rate: 0.001558\n",
      "Epoch 2084/40000, Loss: 0.0013493759324774146, Learning Rate: 0.001557\n",
      "Epoch 2085/40000, Loss: 0.0015342566184699535, Learning Rate: 0.001557\n",
      "Epoch 2086/40000, Loss: 0.0014395941980183125, Learning Rate: 0.001557\n",
      "Epoch 2087/40000, Loss: 0.0013901793863624334, Learning Rate: 0.001557\n",
      "Epoch 2088/40000, Loss: 0.001387609401717782, Learning Rate: 0.001557\n",
      "Epoch 2089/40000, Loss: 0.0013960401993244886, Learning Rate: 0.001557\n",
      "Epoch 2090/40000, Loss: 0.0015061055310070515, Learning Rate: 0.001556\n",
      "Epoch 2091/40000, Loss: 0.0014725218061357737, Learning Rate: 0.001556\n",
      "Epoch 2092/40000, Loss: 0.001555606140755117, Learning Rate: 0.001556\n",
      "Epoch 2093/40000, Loss: 0.001539365155622363, Learning Rate: 0.001556\n",
      "Epoch 2094/40000, Loss: 0.0014587517362087965, Learning Rate: 0.001556\n",
      "Epoch 2095/40000, Loss: 0.0014820642536506057, Learning Rate: 0.001555\n",
      "Epoch 2096/40000, Loss: 0.0016130793374031782, Learning Rate: 0.001555\n",
      "Epoch 2097/40000, Loss: 0.0014962726272642612, Learning Rate: 0.001555\n",
      "Epoch 2098/40000, Loss: 0.0016252356581389904, Learning Rate: 0.001555\n",
      "Epoch 2099/40000, Loss: 0.0015371846966445446, Learning Rate: 0.001555\n",
      "Epoch 2100/40000, Loss: 0.0014115520752966404, Learning Rate: 0.001554\n",
      "Epoch 2101/40000, Loss: 0.0014261123724281788, Learning Rate: 0.001554\n",
      "Epoch 2102/40000, Loss: 0.0014317784225568175, Learning Rate: 0.001554\n",
      "Epoch 2103/40000, Loss: 0.001384992036037147, Learning Rate: 0.001554\n",
      "Epoch 2104/40000, Loss: 0.0014408240094780922, Learning Rate: 0.001554\n",
      "Epoch 2105/40000, Loss: 0.0015835515223443508, Learning Rate: 0.001554\n",
      "Epoch 2106/40000, Loss: 0.001551846507936716, Learning Rate: 0.001553\n",
      "Epoch 2107/40000, Loss: 0.0013840128667652607, Learning Rate: 0.001553\n",
      "Epoch 2108/40000, Loss: 0.0014705148059874773, Learning Rate: 0.001553\n",
      "Epoch 2109/40000, Loss: 0.001563298748806119, Learning Rate: 0.001553\n",
      "Epoch 2110/40000, Loss: 0.0015585942892357707, Learning Rate: 0.001553\n",
      "Epoch 2111/40000, Loss: 0.001429606112651527, Learning Rate: 0.001552\n",
      "Epoch 2112/40000, Loss: 0.0018057606648653746, Learning Rate: 0.001552\n",
      "Epoch 2113/40000, Loss: 0.0014674901030957699, Learning Rate: 0.001552\n",
      "Epoch 2114/40000, Loss: 0.0014378067571669817, Learning Rate: 0.001552\n",
      "Epoch 2115/40000, Loss: 0.0014473253395408392, Learning Rate: 0.001552\n",
      "Epoch 2116/40000, Loss: 0.0014430307783186436, Learning Rate: 0.001552\n",
      "Epoch 2117/40000, Loss: 0.0015562874032184482, Learning Rate: 0.001551\n",
      "Epoch 2118/40000, Loss: 0.0015448108315467834, Learning Rate: 0.001551\n",
      "Epoch 2119/40000, Loss: 0.0017197031993418932, Learning Rate: 0.001551\n",
      "Epoch 2120/40000, Loss: 0.0015747745055705309, Learning Rate: 0.001551\n",
      "Epoch 2121/40000, Loss: 0.001464197295717895, Learning Rate: 0.001551\n",
      "Epoch 2122/40000, Loss: 0.0014422598760575056, Learning Rate: 0.001550\n",
      "Epoch 2123/40000, Loss: 0.001426992821507156, Learning Rate: 0.001550\n",
      "Epoch 2124/40000, Loss: 0.0014967283932492137, Learning Rate: 0.001550\n",
      "Epoch 2125/40000, Loss: 0.001436686608940363, Learning Rate: 0.001550\n",
      "Epoch 2126/40000, Loss: 0.0013930886052548885, Learning Rate: 0.001550\n",
      "Epoch 2127/40000, Loss: 0.001462637330405414, Learning Rate: 0.001549\n",
      "Epoch 2128/40000, Loss: 0.00146430020686239, Learning Rate: 0.001549\n",
      "Epoch 2129/40000, Loss: 0.0014255413552746177, Learning Rate: 0.001549\n",
      "Epoch 2130/40000, Loss: 0.0014202340971678495, Learning Rate: 0.001549\n",
      "Epoch 2131/40000, Loss: 0.0014329520054161549, Learning Rate: 0.001549\n",
      "Epoch 2132/40000, Loss: 0.001484573818743229, Learning Rate: 0.001549\n",
      "Epoch 2133/40000, Loss: 0.0014134933007881045, Learning Rate: 0.001548\n",
      "Epoch 2134/40000, Loss: 0.0013719159178435802, Learning Rate: 0.001548\n",
      "Epoch 2135/40000, Loss: 0.0013898572651669383, Learning Rate: 0.001548\n",
      "Epoch 2136/40000, Loss: 0.0015148724196478724, Learning Rate: 0.001548\n",
      "Epoch 2137/40000, Loss: 0.0014216824201866984, Learning Rate: 0.001548\n",
      "Epoch 2138/40000, Loss: 0.0013677806127816439, Learning Rate: 0.001547\n",
      "Epoch 2139/40000, Loss: 0.0013984109973534942, Learning Rate: 0.001547\n",
      "Epoch 2140/40000, Loss: 0.0014122537104412913, Learning Rate: 0.001547\n",
      "Epoch 2141/40000, Loss: 0.0014890958555042744, Learning Rate: 0.001547\n",
      "Epoch 2142/40000, Loss: 0.0014196861302480102, Learning Rate: 0.001547\n",
      "Epoch 2143/40000, Loss: 0.001408228068612516, Learning Rate: 0.001546\n",
      "Epoch 2144/40000, Loss: 0.0014292469713836908, Learning Rate: 0.001546\n",
      "Epoch 2145/40000, Loss: 0.001417299616150558, Learning Rate: 0.001546\n",
      "Epoch 2146/40000, Loss: 0.0013540461659431458, Learning Rate: 0.001546\n",
      "Epoch 2147/40000, Loss: 0.0015562227927148342, Learning Rate: 0.001546\n",
      "Epoch 2148/40000, Loss: 0.0014910617610439658, Learning Rate: 0.001546\n",
      "Epoch 2149/40000, Loss: 0.00202524708583951, Learning Rate: 0.001545\n",
      "Epoch 2150/40000, Loss: 0.0014020547969266772, Learning Rate: 0.001545\n",
      "Epoch 2151/40000, Loss: 0.0015491305384784937, Learning Rate: 0.001545\n",
      "Epoch 2152/40000, Loss: 0.0015830236952751875, Learning Rate: 0.001545\n",
      "Epoch 2153/40000, Loss: 0.0015870935749262571, Learning Rate: 0.001545\n",
      "Epoch 2154/40000, Loss: 0.001462953514419496, Learning Rate: 0.001544\n",
      "Epoch 2155/40000, Loss: 0.0015771532198414207, Learning Rate: 0.001544\n",
      "Epoch 2156/40000, Loss: 0.0015361899277195334, Learning Rate: 0.001544\n",
      "Epoch 2157/40000, Loss: 0.0014382463414222002, Learning Rate: 0.001544\n",
      "Epoch 2158/40000, Loss: 0.001557663083076477, Learning Rate: 0.001544\n",
      "Epoch 2159/40000, Loss: 0.0015078575816005468, Learning Rate: 0.001544\n",
      "Epoch 2160/40000, Loss: 0.001433756435289979, Learning Rate: 0.001543\n",
      "Epoch 2161/40000, Loss: 0.0014553754590451717, Learning Rate: 0.001543\n",
      "Epoch 2162/40000, Loss: 0.0014640807639807463, Learning Rate: 0.001543\n",
      "Epoch 2163/40000, Loss: 0.0014019086956977844, Learning Rate: 0.001543\n",
      "Epoch 2164/40000, Loss: 0.0014358400367200375, Learning Rate: 0.001543\n",
      "Epoch 2165/40000, Loss: 0.0014675529673695564, Learning Rate: 0.001542\n",
      "Epoch 2166/40000, Loss: 0.0014546902384608984, Learning Rate: 0.001542\n",
      "Epoch 2167/40000, Loss: 0.0014661941677331924, Learning Rate: 0.001542\n",
      "Epoch 2168/40000, Loss: 0.0014600816648453474, Learning Rate: 0.001542\n",
      "Epoch 2169/40000, Loss: 0.0014346451498568058, Learning Rate: 0.001542\n",
      "Epoch 2170/40000, Loss: 0.001425440306775272, Learning Rate: 0.001541\n",
      "Epoch 2171/40000, Loss: 0.001353530678898096, Learning Rate: 0.001541\n",
      "Epoch 2172/40000, Loss: 0.0014257198199629784, Learning Rate: 0.001541\n",
      "Epoch 2173/40000, Loss: 0.0014449309092015028, Learning Rate: 0.001541\n",
      "Epoch 2174/40000, Loss: 0.00139137904625386, Learning Rate: 0.001541\n",
      "Epoch 2175/40000, Loss: 0.001454498851671815, Learning Rate: 0.001541\n",
      "Epoch 2176/40000, Loss: 0.0014094560174271464, Learning Rate: 0.001540\n",
      "Epoch 2177/40000, Loss: 0.001427413895726204, Learning Rate: 0.001540\n",
      "Epoch 2178/40000, Loss: 0.0014209354994818568, Learning Rate: 0.001540\n",
      "Epoch 2179/40000, Loss: 0.0014172870432958007, Learning Rate: 0.001540\n",
      "Epoch 2180/40000, Loss: 0.0014762821374461055, Learning Rate: 0.001540\n",
      "Epoch 2181/40000, Loss: 0.001358244102448225, Learning Rate: 0.001539\n",
      "Epoch 2182/40000, Loss: 0.0013610655441880226, Learning Rate: 0.001539\n",
      "Epoch 2183/40000, Loss: 0.0014342388603836298, Learning Rate: 0.001539\n",
      "Epoch 2184/40000, Loss: 0.0013871246483176947, Learning Rate: 0.001539\n",
      "Epoch 2185/40000, Loss: 0.0013928527478128672, Learning Rate: 0.001539\n",
      "Epoch 2186/40000, Loss: 0.0013649406610056758, Learning Rate: 0.001539\n",
      "Epoch 2187/40000, Loss: 0.0013411081163212657, Learning Rate: 0.001538\n",
      "Epoch 2188/40000, Loss: 0.0014003010001033545, Learning Rate: 0.001538\n",
      "Epoch 2189/40000, Loss: 0.0014468985609710217, Learning Rate: 0.001538\n",
      "Epoch 2190/40000, Loss: 0.00139584019780159, Learning Rate: 0.001538\n",
      "Epoch 2191/40000, Loss: 0.0014147068141028285, Learning Rate: 0.001538\n",
      "Epoch 2192/40000, Loss: 0.0013716635294258595, Learning Rate: 0.001537\n",
      "Epoch 2193/40000, Loss: 0.0014174573589116335, Learning Rate: 0.001537\n",
      "Epoch 2194/40000, Loss: 0.0013930703280493617, Learning Rate: 0.001537\n",
      "Epoch 2195/40000, Loss: 0.0014727928210049868, Learning Rate: 0.001537\n",
      "Epoch 2196/40000, Loss: 0.0014779344201087952, Learning Rate: 0.001537\n",
      "Epoch 2197/40000, Loss: 0.001456424011848867, Learning Rate: 0.001536\n",
      "Epoch 2198/40000, Loss: 0.002496116328984499, Learning Rate: 0.001536\n",
      "Epoch 2199/40000, Loss: 0.004965358879417181, Learning Rate: 0.001536\n",
      "Epoch 2200/40000, Loss: 0.0030561320018023252, Learning Rate: 0.001536\n",
      "Epoch 2201/40000, Loss: 0.002717365510761738, Learning Rate: 0.001536\n",
      "Epoch 2202/40000, Loss: 0.0022809826768934727, Learning Rate: 0.001536\n",
      "Epoch 2203/40000, Loss: 0.0026471037417650223, Learning Rate: 0.001535\n",
      "Epoch 2204/40000, Loss: 0.0023051658645272255, Learning Rate: 0.001535\n",
      "Epoch 2205/40000, Loss: 0.002296208404004574, Learning Rate: 0.001535\n",
      "Epoch 2206/40000, Loss: 0.002089856658130884, Learning Rate: 0.001535\n",
      "Epoch 2207/40000, Loss: 0.0018223184160888195, Learning Rate: 0.001535\n",
      "Epoch 2208/40000, Loss: 0.0021745250560343266, Learning Rate: 0.001534\n",
      "Epoch 2209/40000, Loss: 0.0016732331132516265, Learning Rate: 0.001534\n",
      "Epoch 2210/40000, Loss: 0.0022974153980612755, Learning Rate: 0.001534\n",
      "Epoch 2211/40000, Loss: 0.0019303521839901805, Learning Rate: 0.001534\n",
      "Epoch 2212/40000, Loss: 0.0017451647436246276, Learning Rate: 0.001534\n",
      "Epoch 2213/40000, Loss: 0.0017043243860825896, Learning Rate: 0.001534\n",
      "Epoch 2214/40000, Loss: 0.0017173107480630279, Learning Rate: 0.001533\n",
      "Epoch 2215/40000, Loss: 0.0018258241470903158, Learning Rate: 0.001533\n",
      "Epoch 2216/40000, Loss: 0.0016439607134088874, Learning Rate: 0.001533\n",
      "Epoch 2217/40000, Loss: 0.0021672092843800783, Learning Rate: 0.001533\n",
      "Epoch 2218/40000, Loss: 0.0016916979802772403, Learning Rate: 0.001533\n",
      "Epoch 2219/40000, Loss: 0.0017300143372267485, Learning Rate: 0.001532\n",
      "Epoch 2220/40000, Loss: 0.001691191690042615, Learning Rate: 0.001532\n",
      "Epoch 2221/40000, Loss: 0.001759892562404275, Learning Rate: 0.001532\n",
      "Epoch 2222/40000, Loss: 0.0016474331496283412, Learning Rate: 0.001532\n",
      "Epoch 2223/40000, Loss: 0.0016269058687612414, Learning Rate: 0.001532\n",
      "Epoch 2224/40000, Loss: 0.001617739675566554, Learning Rate: 0.001532\n",
      "Epoch 2225/40000, Loss: 0.0015859121922403574, Learning Rate: 0.001531\n",
      "Epoch 2226/40000, Loss: 0.001485016429796815, Learning Rate: 0.001531\n",
      "Epoch 2227/40000, Loss: 0.0016755908727645874, Learning Rate: 0.001531\n",
      "Epoch 2228/40000, Loss: 0.0015824728179723024, Learning Rate: 0.001531\n",
      "Epoch 2229/40000, Loss: 0.0015649679116904736, Learning Rate: 0.001531\n",
      "Epoch 2230/40000, Loss: 0.00157637195661664, Learning Rate: 0.001530\n",
      "Epoch 2231/40000, Loss: 0.001538238488137722, Learning Rate: 0.001530\n",
      "Epoch 2232/40000, Loss: 0.001500542275607586, Learning Rate: 0.001530\n",
      "Epoch 2233/40000, Loss: 0.001546943443827331, Learning Rate: 0.001530\n",
      "Epoch 2234/40000, Loss: 0.0014877369394525886, Learning Rate: 0.001530\n",
      "Epoch 2235/40000, Loss: 0.001575908507220447, Learning Rate: 0.001530\n",
      "Epoch 2236/40000, Loss: 0.0014918267261236906, Learning Rate: 0.001529\n",
      "Epoch 2237/40000, Loss: 0.0015320287784561515, Learning Rate: 0.001529\n",
      "Epoch 2238/40000, Loss: 0.0015670345164835453, Learning Rate: 0.001529\n",
      "Epoch 2239/40000, Loss: 0.0015739798545837402, Learning Rate: 0.001529\n",
      "Epoch 2240/40000, Loss: 0.0015249866992235184, Learning Rate: 0.001529\n",
      "Epoch 2241/40000, Loss: 0.0015318715013563633, Learning Rate: 0.001528\n",
      "Epoch 2242/40000, Loss: 0.001523996121250093, Learning Rate: 0.001528\n",
      "Epoch 2243/40000, Loss: 0.0014656567946076393, Learning Rate: 0.001528\n",
      "Epoch 2244/40000, Loss: 0.0015374518698081374, Learning Rate: 0.001528\n",
      "Epoch 2245/40000, Loss: 0.0014512378256767988, Learning Rate: 0.001528\n",
      "Epoch 2246/40000, Loss: 0.0015447402838617563, Learning Rate: 0.001527\n",
      "Epoch 2247/40000, Loss: 0.0015180008485913277, Learning Rate: 0.001527\n",
      "Epoch 2248/40000, Loss: 0.0014750887639820576, Learning Rate: 0.001527\n",
      "Epoch 2249/40000, Loss: 0.0014745448715984821, Learning Rate: 0.001527\n",
      "Epoch 2250/40000, Loss: 0.0014521977864205837, Learning Rate: 0.001527\n",
      "Epoch 2251/40000, Loss: 0.0015089040389284492, Learning Rate: 0.001527\n",
      "Epoch 2252/40000, Loss: 0.0015283783432096243, Learning Rate: 0.001526\n",
      "Epoch 2253/40000, Loss: 0.0015419542323797941, Learning Rate: 0.001526\n",
      "Epoch 2254/40000, Loss: 0.0014428379945456982, Learning Rate: 0.001526\n",
      "Epoch 2255/40000, Loss: 0.00149218225851655, Learning Rate: 0.001526\n",
      "Epoch 2256/40000, Loss: 0.0015150269027799368, Learning Rate: 0.001526\n",
      "Epoch 2257/40000, Loss: 0.0014733015559613705, Learning Rate: 0.001525\n",
      "Epoch 2258/40000, Loss: 0.0014698560116812587, Learning Rate: 0.001525\n",
      "Epoch 2259/40000, Loss: 0.0015100257005542517, Learning Rate: 0.001525\n",
      "Epoch 2260/40000, Loss: 0.0015013052616268396, Learning Rate: 0.001525\n",
      "Epoch 2261/40000, Loss: 0.0016427109949290752, Learning Rate: 0.001525\n",
      "Epoch 2262/40000, Loss: 0.0014708570670336485, Learning Rate: 0.001525\n",
      "Epoch 2263/40000, Loss: 0.0015549137024208903, Learning Rate: 0.001524\n",
      "Epoch 2264/40000, Loss: 0.001523259561508894, Learning Rate: 0.001524\n",
      "Epoch 2265/40000, Loss: 0.001722593791782856, Learning Rate: 0.001524\n",
      "Epoch 2266/40000, Loss: 0.001524950610473752, Learning Rate: 0.001524\n",
      "Epoch 2267/40000, Loss: 0.0014544104924425483, Learning Rate: 0.001524\n",
      "Epoch 2268/40000, Loss: 0.0015146605437621474, Learning Rate: 0.001523\n",
      "Epoch 2269/40000, Loss: 0.0015380916884168983, Learning Rate: 0.001523\n",
      "Epoch 2270/40000, Loss: 0.0014713229611515999, Learning Rate: 0.001523\n",
      "Epoch 2271/40000, Loss: 0.0014631431549787521, Learning Rate: 0.001523\n",
      "Epoch 2272/40000, Loss: 0.0015667168190702796, Learning Rate: 0.001523\n",
      "Epoch 2273/40000, Loss: 0.0014509949833154678, Learning Rate: 0.001523\n",
      "Epoch 2274/40000, Loss: 0.0015308463480323553, Learning Rate: 0.001522\n",
      "Epoch 2275/40000, Loss: 0.0015237464103847742, Learning Rate: 0.001522\n",
      "Epoch 2276/40000, Loss: 0.0015095937997102737, Learning Rate: 0.001522\n",
      "Epoch 2277/40000, Loss: 0.0015781489200890064, Learning Rate: 0.001522\n",
      "Epoch 2278/40000, Loss: 0.00150786223821342, Learning Rate: 0.001522\n",
      "Epoch 2279/40000, Loss: 0.0016041522612795234, Learning Rate: 0.001521\n",
      "Epoch 2280/40000, Loss: 0.0015847928589209914, Learning Rate: 0.001521\n",
      "Epoch 2281/40000, Loss: 0.0014883752446621656, Learning Rate: 0.001521\n",
      "Epoch 2282/40000, Loss: 0.0016359922010451555, Learning Rate: 0.001521\n",
      "Epoch 2283/40000, Loss: 0.001542394980788231, Learning Rate: 0.001521\n",
      "Epoch 2284/40000, Loss: 0.0014782568905502558, Learning Rate: 0.001521\n",
      "Epoch 2285/40000, Loss: 0.0014873601030558348, Learning Rate: 0.001520\n",
      "Epoch 2286/40000, Loss: 0.0014355538878589869, Learning Rate: 0.001520\n",
      "Epoch 2287/40000, Loss: 0.001496748300269246, Learning Rate: 0.001520\n",
      "Epoch 2288/40000, Loss: 0.0014977024402469397, Learning Rate: 0.001520\n",
      "Epoch 2289/40000, Loss: 0.001447158632799983, Learning Rate: 0.001520\n",
      "Epoch 2290/40000, Loss: 0.0014669381780549884, Learning Rate: 0.001519\n",
      "Epoch 2291/40000, Loss: 0.001455116318538785, Learning Rate: 0.001519\n",
      "Epoch 2292/40000, Loss: 0.0014365359675139189, Learning Rate: 0.001519\n",
      "Epoch 2293/40000, Loss: 0.0015447544865310192, Learning Rate: 0.001519\n",
      "Epoch 2294/40000, Loss: 0.001444827183149755, Learning Rate: 0.001519\n",
      "Epoch 2295/40000, Loss: 0.0013747502816841006, Learning Rate: 0.001519\n",
      "Epoch 2296/40000, Loss: 0.0015079773729667068, Learning Rate: 0.001518\n",
      "Epoch 2297/40000, Loss: 0.00147185567766428, Learning Rate: 0.001518\n",
      "Epoch 2298/40000, Loss: 0.0014599829446524382, Learning Rate: 0.001518\n",
      "Epoch 2299/40000, Loss: 0.0014890485908836126, Learning Rate: 0.001518\n",
      "Epoch 2300/40000, Loss: 0.001423973822966218, Learning Rate: 0.001518\n",
      "Epoch 2301/40000, Loss: 0.001474546268582344, Learning Rate: 0.001517\n",
      "Epoch 2302/40000, Loss: 0.0014282625634223223, Learning Rate: 0.001517\n",
      "Epoch 2303/40000, Loss: 0.001432375400327146, Learning Rate: 0.001517\n",
      "Epoch 2304/40000, Loss: 0.001351265120320022, Learning Rate: 0.001517\n",
      "Epoch 2305/40000, Loss: 0.0014004919212311506, Learning Rate: 0.001517\n",
      "Epoch 2306/40000, Loss: 0.0013828761875629425, Learning Rate: 0.001517\n",
      "Epoch 2307/40000, Loss: 0.001357469242066145, Learning Rate: 0.001516\n",
      "Epoch 2308/40000, Loss: 0.0013656339142471552, Learning Rate: 0.001516\n",
      "Epoch 2309/40000, Loss: 0.0014248499646782875, Learning Rate: 0.001516\n",
      "Epoch 2310/40000, Loss: 0.0014445417327806354, Learning Rate: 0.001516\n",
      "Epoch 2311/40000, Loss: 0.0014152578078210354, Learning Rate: 0.001516\n",
      "Epoch 2312/40000, Loss: 0.0014123589498922229, Learning Rate: 0.001515\n",
      "Epoch 2313/40000, Loss: 0.0014593573287129402, Learning Rate: 0.001515\n",
      "Epoch 2314/40000, Loss: 0.0013822740875184536, Learning Rate: 0.001515\n",
      "Epoch 2315/40000, Loss: 0.0014037201181054115, Learning Rate: 0.001515\n",
      "Epoch 2316/40000, Loss: 0.001378149725496769, Learning Rate: 0.001515\n",
      "Epoch 2317/40000, Loss: 0.0013894743751734495, Learning Rate: 0.001515\n",
      "Epoch 2318/40000, Loss: 0.0014132334617897868, Learning Rate: 0.001514\n",
      "Epoch 2319/40000, Loss: 0.0013646840816363692, Learning Rate: 0.001514\n",
      "Epoch 2320/40000, Loss: 0.0013941292418166995, Learning Rate: 0.001514\n",
      "Epoch 2321/40000, Loss: 0.0014411630108952522, Learning Rate: 0.001514\n",
      "Epoch 2322/40000, Loss: 0.0013714572414755821, Learning Rate: 0.001514\n",
      "Epoch 2323/40000, Loss: 0.0014467653818428516, Learning Rate: 0.001513\n",
      "Epoch 2324/40000, Loss: 0.0014107387978583574, Learning Rate: 0.001513\n",
      "Epoch 2325/40000, Loss: 0.0014806997496634722, Learning Rate: 0.001513\n",
      "Epoch 2326/40000, Loss: 0.0014171231305226684, Learning Rate: 0.001513\n",
      "Epoch 2327/40000, Loss: 0.0014009105507284403, Learning Rate: 0.001513\n",
      "Epoch 2328/40000, Loss: 0.0014423595275729895, Learning Rate: 0.001513\n",
      "Epoch 2329/40000, Loss: 0.0015222292859107256, Learning Rate: 0.001512\n",
      "Epoch 2330/40000, Loss: 0.0013753455132246017, Learning Rate: 0.001512\n",
      "Epoch 2331/40000, Loss: 0.0014699532184749842, Learning Rate: 0.001512\n",
      "Epoch 2332/40000, Loss: 0.0014861277304589748, Learning Rate: 0.001512\n",
      "Epoch 2333/40000, Loss: 0.0014096343657001853, Learning Rate: 0.001512\n",
      "Epoch 2334/40000, Loss: 0.0015275315381586552, Learning Rate: 0.001511\n",
      "Epoch 2335/40000, Loss: 0.0014564048033207655, Learning Rate: 0.001511\n",
      "Epoch 2336/40000, Loss: 0.0015238596824929118, Learning Rate: 0.001511\n",
      "Epoch 2337/40000, Loss: 0.001420554588548839, Learning Rate: 0.001511\n",
      "Epoch 2338/40000, Loss: 0.00151919387280941, Learning Rate: 0.001511\n",
      "Epoch 2339/40000, Loss: 0.001461884006857872, Learning Rate: 0.001511\n",
      "Epoch 2340/40000, Loss: 0.0014085558941587806, Learning Rate: 0.001510\n",
      "Epoch 2341/40000, Loss: 0.0015640229685232043, Learning Rate: 0.001510\n",
      "Epoch 2342/40000, Loss: 0.001407494186423719, Learning Rate: 0.001510\n",
      "Epoch 2343/40000, Loss: 0.0014100039843469858, Learning Rate: 0.001510\n",
      "Epoch 2344/40000, Loss: 0.0013784333132207394, Learning Rate: 0.001510\n",
      "Epoch 2345/40000, Loss: 0.001438944018445909, Learning Rate: 0.001509\n",
      "Epoch 2346/40000, Loss: 0.0013040545163676143, Learning Rate: 0.001509\n",
      "Epoch 2347/40000, Loss: 0.0014545534504577518, Learning Rate: 0.001509\n",
      "Epoch 2348/40000, Loss: 0.00141044519841671, Learning Rate: 0.001509\n",
      "Epoch 2349/40000, Loss: 0.0014149214839562774, Learning Rate: 0.001509\n",
      "Epoch 2350/40000, Loss: 0.0014811062719672918, Learning Rate: 0.001509\n",
      "Epoch 2351/40000, Loss: 0.0014022252289578319, Learning Rate: 0.001508\n",
      "Epoch 2352/40000, Loss: 0.0013808542862534523, Learning Rate: 0.001508\n",
      "Epoch 2353/40000, Loss: 0.0014071702025830746, Learning Rate: 0.001508\n",
      "Epoch 2354/40000, Loss: 0.0013544680550694466, Learning Rate: 0.001508\n",
      "Epoch 2355/40000, Loss: 0.00142063875682652, Learning Rate: 0.001508\n",
      "Epoch 2356/40000, Loss: 0.0014212032547220588, Learning Rate: 0.001507\n",
      "Epoch 2357/40000, Loss: 0.0014120241394266486, Learning Rate: 0.001507\n",
      "Epoch 2358/40000, Loss: 0.0013917703181505203, Learning Rate: 0.001507\n",
      "Epoch 2359/40000, Loss: 0.0013772398233413696, Learning Rate: 0.001507\n",
      "Epoch 2360/40000, Loss: 0.0013464582152664661, Learning Rate: 0.001507\n",
      "Epoch 2361/40000, Loss: 0.0013941687066107988, Learning Rate: 0.001507\n",
      "Epoch 2362/40000, Loss: 0.0013348895590752363, Learning Rate: 0.001506\n",
      "Epoch 2363/40000, Loss: 0.0013117905473336577, Learning Rate: 0.001506\n",
      "Epoch 2364/40000, Loss: 0.001386009156703949, Learning Rate: 0.001506\n",
      "Epoch 2365/40000, Loss: 0.0013582034735009074, Learning Rate: 0.001506\n",
      "Epoch 2366/40000, Loss: 0.00141709647141397, Learning Rate: 0.001506\n",
      "Epoch 2367/40000, Loss: 0.0014185527106747031, Learning Rate: 0.001505\n",
      "Epoch 2368/40000, Loss: 0.0013557912316173315, Learning Rate: 0.001505\n",
      "Epoch 2369/40000, Loss: 0.001340770861133933, Learning Rate: 0.001505\n",
      "Epoch 2370/40000, Loss: 0.001328623155131936, Learning Rate: 0.001505\n",
      "Epoch 2371/40000, Loss: 0.0013796770945191383, Learning Rate: 0.001505\n",
      "Epoch 2372/40000, Loss: 0.001464863191358745, Learning Rate: 0.001505\n",
      "Epoch 2373/40000, Loss: 0.0013681859709322453, Learning Rate: 0.001504\n",
      "Epoch 2374/40000, Loss: 0.0013890521368011832, Learning Rate: 0.001504\n",
      "Epoch 2375/40000, Loss: 0.0014352379366755486, Learning Rate: 0.001504\n",
      "Epoch 2376/40000, Loss: 0.0013778897700831294, Learning Rate: 0.001504\n",
      "Epoch 2377/40000, Loss: 0.0014443411491811275, Learning Rate: 0.001504\n",
      "Epoch 2378/40000, Loss: 0.0014476494397968054, Learning Rate: 0.001503\n",
      "Epoch 2379/40000, Loss: 0.001308145234361291, Learning Rate: 0.001503\n",
      "Epoch 2380/40000, Loss: 0.001320424024015665, Learning Rate: 0.001503\n",
      "Epoch 2381/40000, Loss: 0.0013130300212651491, Learning Rate: 0.001503\n",
      "Epoch 2382/40000, Loss: 0.001345113618299365, Learning Rate: 0.001503\n",
      "Epoch 2383/40000, Loss: 0.0013361838646233082, Learning Rate: 0.001503\n",
      "Epoch 2384/40000, Loss: 0.0013428512029349804, Learning Rate: 0.001502\n",
      "Epoch 2385/40000, Loss: 0.0014061855617910624, Learning Rate: 0.001502\n",
      "Epoch 2386/40000, Loss: 0.0013282358413562179, Learning Rate: 0.001502\n",
      "Epoch 2387/40000, Loss: 0.0013500414788722992, Learning Rate: 0.001502\n",
      "Epoch 2388/40000, Loss: 0.0014376232866197824, Learning Rate: 0.001502\n",
      "Epoch 2389/40000, Loss: 0.0014334279112517834, Learning Rate: 0.001501\n",
      "Epoch 2390/40000, Loss: 0.001417178544215858, Learning Rate: 0.001501\n",
      "Epoch 2391/40000, Loss: 0.0013860802864655852, Learning Rate: 0.001501\n",
      "Epoch 2392/40000, Loss: 0.0013566188281401992, Learning Rate: 0.001501\n",
      "Epoch 2393/40000, Loss: 0.0014860860537737608, Learning Rate: 0.001501\n",
      "Epoch 2394/40000, Loss: 0.001394194783642888, Learning Rate: 0.001501\n",
      "Epoch 2395/40000, Loss: 0.0013503474183380604, Learning Rate: 0.001500\n",
      "Epoch 2396/40000, Loss: 0.0013771874364465475, Learning Rate: 0.001500\n",
      "Epoch 2397/40000, Loss: 0.0014350824058055878, Learning Rate: 0.001500\n",
      "Epoch 2398/40000, Loss: 0.0013987355632707477, Learning Rate: 0.001500\n",
      "Epoch 2399/40000, Loss: 0.001329566235654056, Learning Rate: 0.001500\n",
      "Epoch 2400/40000, Loss: 0.0012902133166790009, Learning Rate: 0.001500\n",
      "Epoch 2401/40000, Loss: 0.0013772912789136171, Learning Rate: 0.001499\n",
      "Epoch 2402/40000, Loss: 0.0015634808223694563, Learning Rate: 0.001499\n",
      "Epoch 2403/40000, Loss: 0.001423257403075695, Learning Rate: 0.001499\n",
      "Epoch 2404/40000, Loss: 0.001440304215066135, Learning Rate: 0.001499\n",
      "Epoch 2405/40000, Loss: 0.0013849507085978985, Learning Rate: 0.001499\n",
      "Epoch 2406/40000, Loss: 0.001434207777492702, Learning Rate: 0.001498\n",
      "Epoch 2407/40000, Loss: 0.0014179509598761797, Learning Rate: 0.001498\n",
      "Epoch 2408/40000, Loss: 0.001363285118713975, Learning Rate: 0.001498\n",
      "Epoch 2409/40000, Loss: 0.0013415208086371422, Learning Rate: 0.001498\n",
      "Epoch 2410/40000, Loss: 0.0012716628843918443, Learning Rate: 0.001498\n",
      "Epoch 2411/40000, Loss: 0.001368749188259244, Learning Rate: 0.001498\n",
      "Epoch 2412/40000, Loss: 0.0012926168274134398, Learning Rate: 0.001497\n",
      "Epoch 2413/40000, Loss: 0.0013398838927969337, Learning Rate: 0.001497\n",
      "Epoch 2414/40000, Loss: 0.0012842940632253885, Learning Rate: 0.001497\n",
      "Epoch 2415/40000, Loss: 0.0013789734803140163, Learning Rate: 0.001497\n",
      "Epoch 2416/40000, Loss: 0.0013345027109608054, Learning Rate: 0.001497\n",
      "Epoch 2417/40000, Loss: 0.001355545362457633, Learning Rate: 0.001496\n",
      "Epoch 2418/40000, Loss: 0.0012581306509673595, Learning Rate: 0.001496\n",
      "Epoch 2419/40000, Loss: 0.0013380780583247542, Learning Rate: 0.001496\n",
      "Epoch 2420/40000, Loss: 0.0013100309297442436, Learning Rate: 0.001496\n",
      "Epoch 2421/40000, Loss: 0.001280446769669652, Learning Rate: 0.001496\n",
      "Epoch 2422/40000, Loss: 0.0013108013663440943, Learning Rate: 0.001496\n",
      "Epoch 2423/40000, Loss: 0.0013030669651925564, Learning Rate: 0.001495\n",
      "Epoch 2424/40000, Loss: 0.0013806705828756094, Learning Rate: 0.001495\n",
      "Epoch 2425/40000, Loss: 0.0013911600690335035, Learning Rate: 0.001495\n",
      "Epoch 2426/40000, Loss: 0.001399510889314115, Learning Rate: 0.001495\n",
      "Epoch 2427/40000, Loss: 0.0013466867385432124, Learning Rate: 0.001495\n",
      "Epoch 2428/40000, Loss: 0.001391061581671238, Learning Rate: 0.001494\n",
      "Epoch 2429/40000, Loss: 0.0012829711195081472, Learning Rate: 0.001494\n",
      "Epoch 2430/40000, Loss: 0.0013160437811166048, Learning Rate: 0.001494\n",
      "Epoch 2431/40000, Loss: 0.0013562222011387348, Learning Rate: 0.001494\n",
      "Epoch 2432/40000, Loss: 0.0013450647238641977, Learning Rate: 0.001494\n",
      "Epoch 2433/40000, Loss: 0.001408924232237041, Learning Rate: 0.001494\n",
      "Epoch 2434/40000, Loss: 0.00142344506457448, Learning Rate: 0.001493\n",
      "Epoch 2435/40000, Loss: 0.0016471868148073554, Learning Rate: 0.001493\n",
      "Epoch 2436/40000, Loss: 0.0015154171269387007, Learning Rate: 0.001493\n",
      "Epoch 2437/40000, Loss: 0.0014785858802497387, Learning Rate: 0.001493\n",
      "Epoch 2438/40000, Loss: 0.0014908702578395605, Learning Rate: 0.001493\n",
      "Epoch 2439/40000, Loss: 0.0015514837577939034, Learning Rate: 0.001493\n",
      "Epoch 2440/40000, Loss: 0.0014036521315574646, Learning Rate: 0.001492\n",
      "Epoch 2441/40000, Loss: 0.0014097671955823898, Learning Rate: 0.001492\n",
      "Epoch 2442/40000, Loss: 0.0013589379377663136, Learning Rate: 0.001492\n",
      "Epoch 2443/40000, Loss: 0.001395170809701085, Learning Rate: 0.001492\n",
      "Epoch 2444/40000, Loss: 0.0013236775994300842, Learning Rate: 0.001492\n",
      "Epoch 2445/40000, Loss: 0.0013422004412859678, Learning Rate: 0.001491\n",
      "Epoch 2446/40000, Loss: 0.0013625987339764833, Learning Rate: 0.001491\n",
      "Epoch 2447/40000, Loss: 0.0013864821521565318, Learning Rate: 0.001491\n",
      "Epoch 2448/40000, Loss: 0.001345979981124401, Learning Rate: 0.001491\n",
      "Epoch 2449/40000, Loss: 0.0013611018657684326, Learning Rate: 0.001491\n",
      "Epoch 2450/40000, Loss: 0.001287499675527215, Learning Rate: 0.001491\n",
      "Epoch 2451/40000, Loss: 0.001290330197662115, Learning Rate: 0.001490\n",
      "Epoch 2452/40000, Loss: 0.0012892448576167226, Learning Rate: 0.001490\n",
      "Epoch 2453/40000, Loss: 0.0012664238456636667, Learning Rate: 0.001490\n",
      "Epoch 2454/40000, Loss: 0.0012692109448835254, Learning Rate: 0.001490\n",
      "Epoch 2455/40000, Loss: 0.0012841252610087395, Learning Rate: 0.001490\n",
      "Epoch 2456/40000, Loss: 0.001330147497355938, Learning Rate: 0.001489\n",
      "Epoch 2457/40000, Loss: 0.0013117282651364803, Learning Rate: 0.001489\n",
      "Epoch 2458/40000, Loss: 0.0013259684201329947, Learning Rate: 0.001489\n",
      "Epoch 2459/40000, Loss: 0.0012958274455741048, Learning Rate: 0.001489\n",
      "Epoch 2460/40000, Loss: 0.0012237753253430128, Learning Rate: 0.001489\n",
      "Epoch 2461/40000, Loss: 0.001254533533938229, Learning Rate: 0.001489\n",
      "Epoch 2462/40000, Loss: 0.0013073165901005268, Learning Rate: 0.001488\n",
      "Epoch 2463/40000, Loss: 0.0013577191857621074, Learning Rate: 0.001488\n",
      "Epoch 2464/40000, Loss: 0.0012903449824079871, Learning Rate: 0.001488\n",
      "Epoch 2465/40000, Loss: 0.0012887687189504504, Learning Rate: 0.001488\n",
      "Epoch 2466/40000, Loss: 0.0012331950711086392, Learning Rate: 0.001488\n",
      "Epoch 2467/40000, Loss: 0.0012591396225616336, Learning Rate: 0.001488\n",
      "Epoch 2468/40000, Loss: 0.0013191523030400276, Learning Rate: 0.001487\n",
      "Epoch 2469/40000, Loss: 0.0012512203538790345, Learning Rate: 0.001487\n",
      "Epoch 2470/40000, Loss: 0.0012790028704330325, Learning Rate: 0.001487\n",
      "Epoch 2471/40000, Loss: 0.0012605825904756784, Learning Rate: 0.001487\n",
      "Epoch 2472/40000, Loss: 0.0013713203370571136, Learning Rate: 0.001487\n",
      "Epoch 2473/40000, Loss: 0.0013746346812695265, Learning Rate: 0.001486\n",
      "Epoch 2474/40000, Loss: 0.0013522774679586291, Learning Rate: 0.001486\n",
      "Epoch 2475/40000, Loss: 0.0012554604327306151, Learning Rate: 0.001486\n",
      "Epoch 2476/40000, Loss: 0.001286469167098403, Learning Rate: 0.001486\n",
      "Epoch 2477/40000, Loss: 0.0012598622124642134, Learning Rate: 0.001486\n",
      "Epoch 2478/40000, Loss: 0.0013072349829599261, Learning Rate: 0.001486\n",
      "Epoch 2479/40000, Loss: 0.0013083512894809246, Learning Rate: 0.001485\n",
      "Epoch 2480/40000, Loss: 0.0013496348401531577, Learning Rate: 0.001485\n",
      "Epoch 2481/40000, Loss: 0.0013255779631435871, Learning Rate: 0.001485\n",
      "Epoch 2482/40000, Loss: 0.0013035358861088753, Learning Rate: 0.001485\n",
      "Epoch 2483/40000, Loss: 0.0012503319885581732, Learning Rate: 0.001485\n",
      "Epoch 2484/40000, Loss: 0.001354270614683628, Learning Rate: 0.001484\n",
      "Epoch 2485/40000, Loss: 0.0014005943667143583, Learning Rate: 0.001484\n",
      "Epoch 2486/40000, Loss: 0.0013556837802752852, Learning Rate: 0.001484\n",
      "Epoch 2487/40000, Loss: 0.0013455336447805166, Learning Rate: 0.001484\n",
      "Epoch 2488/40000, Loss: 0.0013464700896292925, Learning Rate: 0.001484\n",
      "Epoch 2489/40000, Loss: 0.0012412185315042734, Learning Rate: 0.001484\n",
      "Epoch 2490/40000, Loss: 0.0012861934956163168, Learning Rate: 0.001483\n",
      "Epoch 2491/40000, Loss: 0.0012742113322019577, Learning Rate: 0.001483\n",
      "Epoch 2492/40000, Loss: 0.001314476365223527, Learning Rate: 0.001483\n",
      "Epoch 2493/40000, Loss: 0.0012818188406527042, Learning Rate: 0.001483\n",
      "Epoch 2494/40000, Loss: 0.0012775060022249818, Learning Rate: 0.001483\n",
      "Epoch 2495/40000, Loss: 0.0013461665948852897, Learning Rate: 0.001483\n",
      "Epoch 2496/40000, Loss: 0.0012832431821152568, Learning Rate: 0.001482\n",
      "Epoch 2497/40000, Loss: 0.0013021350605413318, Learning Rate: 0.001482\n",
      "Epoch 2498/40000, Loss: 0.0013913919683545828, Learning Rate: 0.001482\n",
      "Epoch 2499/40000, Loss: 0.0012276972411200404, Learning Rate: 0.001482\n",
      "Epoch 2500/40000, Loss: 0.0012957691214978695, Learning Rate: 0.001482\n",
      "Epoch 2501/40000, Loss: 0.001271902583539486, Learning Rate: 0.001481\n",
      "Epoch 2502/40000, Loss: 0.001460663741454482, Learning Rate: 0.001481\n",
      "Epoch 2503/40000, Loss: 0.0013057207688689232, Learning Rate: 0.001481\n",
      "Epoch 2504/40000, Loss: 0.0013837807346135378, Learning Rate: 0.001481\n",
      "Epoch 2505/40000, Loss: 0.001360772061161697, Learning Rate: 0.001481\n",
      "Epoch 2506/40000, Loss: 0.0015858871629461646, Learning Rate: 0.001481\n",
      "Epoch 2507/40000, Loss: 0.0013101259246468544, Learning Rate: 0.001480\n",
      "Epoch 2508/40000, Loss: 0.0013928741682320833, Learning Rate: 0.001480\n",
      "Epoch 2509/40000, Loss: 0.0012897694250568748, Learning Rate: 0.001480\n",
      "Epoch 2510/40000, Loss: 0.0012732665054500103, Learning Rate: 0.001480\n",
      "Epoch 2511/40000, Loss: 0.0013039353070780635, Learning Rate: 0.001480\n",
      "Epoch 2512/40000, Loss: 0.0013148020952939987, Learning Rate: 0.001479\n",
      "Epoch 2513/40000, Loss: 0.001353548839688301, Learning Rate: 0.001479\n",
      "Epoch 2514/40000, Loss: 0.0012279218062758446, Learning Rate: 0.001479\n",
      "Epoch 2515/40000, Loss: 0.0013113742461428046, Learning Rate: 0.001479\n",
      "Epoch 2516/40000, Loss: 0.0012926170602440834, Learning Rate: 0.001479\n",
      "Epoch 2517/40000, Loss: 0.001288566505536437, Learning Rate: 0.001479\n",
      "Epoch 2518/40000, Loss: 0.0013653095811605453, Learning Rate: 0.001478\n",
      "Epoch 2519/40000, Loss: 0.0012311635073274374, Learning Rate: 0.001478\n",
      "Epoch 2520/40000, Loss: 0.001361248316243291, Learning Rate: 0.001478\n",
      "Epoch 2521/40000, Loss: 0.00124788424000144, Learning Rate: 0.001478\n",
      "Epoch 2522/40000, Loss: 0.0012806884478777647, Learning Rate: 0.001478\n",
      "Epoch 2523/40000, Loss: 0.001221735030412674, Learning Rate: 0.001478\n",
      "Epoch 2524/40000, Loss: 0.0012514515547081828, Learning Rate: 0.001477\n",
      "Epoch 2525/40000, Loss: 0.0012580393813550472, Learning Rate: 0.001477\n",
      "Epoch 2526/40000, Loss: 0.0012421072460711002, Learning Rate: 0.001477\n",
      "Epoch 2527/40000, Loss: 0.0012534315465018153, Learning Rate: 0.001477\n",
      "Epoch 2528/40000, Loss: 0.0012363148853182793, Learning Rate: 0.001477\n",
      "Epoch 2529/40000, Loss: 0.0012660541106015444, Learning Rate: 0.001476\n",
      "Epoch 2530/40000, Loss: 0.00122088473290205, Learning Rate: 0.001476\n",
      "Epoch 2531/40000, Loss: 0.0012554957065731287, Learning Rate: 0.001476\n",
      "Epoch 2532/40000, Loss: 0.0012944125337526202, Learning Rate: 0.001476\n",
      "Epoch 2533/40000, Loss: 0.0012048555072396994, Learning Rate: 0.001476\n",
      "Epoch 2534/40000, Loss: 0.0012236066395416856, Learning Rate: 0.001476\n",
      "Epoch 2535/40000, Loss: 0.0013357255375012755, Learning Rate: 0.001475\n",
      "Epoch 2536/40000, Loss: 0.001268964959308505, Learning Rate: 0.001475\n",
      "Epoch 2537/40000, Loss: 0.0012788716703653336, Learning Rate: 0.001475\n",
      "Epoch 2538/40000, Loss: 0.0011772969737648964, Learning Rate: 0.001475\n",
      "Epoch 2539/40000, Loss: 0.0012986103538423777, Learning Rate: 0.001475\n",
      "Epoch 2540/40000, Loss: 0.0012117349542677402, Learning Rate: 0.001475\n",
      "Epoch 2541/40000, Loss: 0.0012353205820545554, Learning Rate: 0.001474\n",
      "Epoch 2542/40000, Loss: 0.0013318078126758337, Learning Rate: 0.001474\n",
      "Epoch 2543/40000, Loss: 0.001317881979048252, Learning Rate: 0.001474\n",
      "Epoch 2544/40000, Loss: 0.0012389273615553975, Learning Rate: 0.001474\n",
      "Epoch 2545/40000, Loss: 0.001242679776623845, Learning Rate: 0.001474\n",
      "Epoch 2546/40000, Loss: 0.0012072930112481117, Learning Rate: 0.001473\n",
      "Epoch 2547/40000, Loss: 0.0013195041101425886, Learning Rate: 0.001473\n",
      "Epoch 2548/40000, Loss: 0.00124918925575912, Learning Rate: 0.001473\n",
      "Epoch 2549/40000, Loss: 0.001235604053363204, Learning Rate: 0.001473\n",
      "Epoch 2550/40000, Loss: 0.0011835535988211632, Learning Rate: 0.001473\n",
      "Epoch 2551/40000, Loss: 0.0012055702973157167, Learning Rate: 0.001473\n",
      "Epoch 2552/40000, Loss: 0.001276309136301279, Learning Rate: 0.001472\n",
      "Epoch 2553/40000, Loss: 0.0013228582683950663, Learning Rate: 0.001472\n",
      "Epoch 2554/40000, Loss: 0.0012680462095886469, Learning Rate: 0.001472\n",
      "Epoch 2555/40000, Loss: 0.0012468944769352674, Learning Rate: 0.001472\n",
      "Epoch 2556/40000, Loss: 0.0012069903314113617, Learning Rate: 0.001472\n",
      "Epoch 2557/40000, Loss: 0.001248702290467918, Learning Rate: 0.001472\n",
      "Epoch 2558/40000, Loss: 0.0012761284597218037, Learning Rate: 0.001471\n",
      "Epoch 2559/40000, Loss: 0.0012312647886574268, Learning Rate: 0.001471\n",
      "Epoch 2560/40000, Loss: 0.0012165125226601958, Learning Rate: 0.001471\n",
      "Epoch 2561/40000, Loss: 0.001216247328557074, Learning Rate: 0.001471\n",
      "Epoch 2562/40000, Loss: 0.0012603639625012875, Learning Rate: 0.001471\n",
      "Epoch 2563/40000, Loss: 0.0012499382719397545, Learning Rate: 0.001470\n",
      "Epoch 2564/40000, Loss: 0.0012473827227950096, Learning Rate: 0.001470\n",
      "Epoch 2565/40000, Loss: 0.0013444696087390184, Learning Rate: 0.001470\n",
      "Epoch 2566/40000, Loss: 0.001246688887476921, Learning Rate: 0.001470\n",
      "Epoch 2567/40000, Loss: 0.0012844946468248963, Learning Rate: 0.001470\n",
      "Epoch 2568/40000, Loss: 0.0013167666038498282, Learning Rate: 0.001470\n",
      "Epoch 2569/40000, Loss: 0.0013177514774724841, Learning Rate: 0.001469\n",
      "Epoch 2570/40000, Loss: 0.001279342919588089, Learning Rate: 0.001469\n",
      "Epoch 2571/40000, Loss: 0.001291730790399015, Learning Rate: 0.001469\n",
      "Epoch 2572/40000, Loss: 0.0012256246991455555, Learning Rate: 0.001469\n",
      "Epoch 2573/40000, Loss: 0.001192866824567318, Learning Rate: 0.001469\n",
      "Epoch 2574/40000, Loss: 0.001156201702542603, Learning Rate: 0.001469\n",
      "Epoch 2575/40000, Loss: 0.0012347442097961903, Learning Rate: 0.001468\n",
      "Epoch 2576/40000, Loss: 0.001204140018671751, Learning Rate: 0.001468\n",
      "Epoch 2577/40000, Loss: 0.0012281739618629217, Learning Rate: 0.001468\n",
      "Epoch 2578/40000, Loss: 0.001286473823711276, Learning Rate: 0.001468\n",
      "Epoch 2579/40000, Loss: 0.00126964645460248, Learning Rate: 0.001468\n",
      "Epoch 2580/40000, Loss: 0.0012520620366558433, Learning Rate: 0.001467\n",
      "Epoch 2581/40000, Loss: 0.0012580101611092687, Learning Rate: 0.001467\n",
      "Epoch 2582/40000, Loss: 0.0012178753968328238, Learning Rate: 0.001467\n",
      "Epoch 2583/40000, Loss: 0.0012964948546141386, Learning Rate: 0.001467\n",
      "Epoch 2584/40000, Loss: 0.0012591918930411339, Learning Rate: 0.001467\n",
      "Epoch 2585/40000, Loss: 0.001259035081602633, Learning Rate: 0.001467\n",
      "Epoch 2586/40000, Loss: 0.0012617302127182484, Learning Rate: 0.001466\n",
      "Epoch 2587/40000, Loss: 0.001186460256576538, Learning Rate: 0.001466\n",
      "Epoch 2588/40000, Loss: 0.0012604455696418881, Learning Rate: 0.001466\n",
      "Epoch 2589/40000, Loss: 0.0011518805986270308, Learning Rate: 0.001466\n",
      "Epoch 2590/40000, Loss: 0.0012263390235602856, Learning Rate: 0.001466\n",
      "Epoch 2591/40000, Loss: 0.0012325331335887313, Learning Rate: 0.001466\n",
      "Epoch 2592/40000, Loss: 0.0012574794236570597, Learning Rate: 0.001465\n",
      "Epoch 2593/40000, Loss: 0.0012606538366526365, Learning Rate: 0.001465\n",
      "Epoch 2594/40000, Loss: 0.0012457291595637798, Learning Rate: 0.001465\n",
      "Epoch 2595/40000, Loss: 0.0012762196129187942, Learning Rate: 0.001465\n",
      "Epoch 2596/40000, Loss: 0.0012576606823131442, Learning Rate: 0.001465\n",
      "Epoch 2597/40000, Loss: 0.0012897730339318514, Learning Rate: 0.001464\n",
      "Epoch 2598/40000, Loss: 0.001255988609045744, Learning Rate: 0.001464\n",
      "Epoch 2599/40000, Loss: 0.0013553444296121597, Learning Rate: 0.001464\n",
      "Epoch 2600/40000, Loss: 0.0012894186656922102, Learning Rate: 0.001464\n",
      "Epoch 2601/40000, Loss: 0.0013207371812313795, Learning Rate: 0.001464\n",
      "Epoch 2602/40000, Loss: 0.001278891577385366, Learning Rate: 0.001464\n",
      "Epoch 2603/40000, Loss: 0.001274182926863432, Learning Rate: 0.001463\n",
      "Epoch 2604/40000, Loss: 0.0013599402736872435, Learning Rate: 0.001463\n",
      "Epoch 2605/40000, Loss: 0.0012638670159503818, Learning Rate: 0.001463\n",
      "Epoch 2606/40000, Loss: 0.0013328834902495146, Learning Rate: 0.001463\n",
      "Epoch 2607/40000, Loss: 0.0013224224094301462, Learning Rate: 0.001463\n",
      "Epoch 2608/40000, Loss: 0.0013506917748600245, Learning Rate: 0.001463\n",
      "Epoch 2609/40000, Loss: 0.0013425091747194529, Learning Rate: 0.001462\n",
      "Epoch 2610/40000, Loss: 0.0013534014578908682, Learning Rate: 0.001462\n",
      "Epoch 2611/40000, Loss: 0.0015897001139819622, Learning Rate: 0.001462\n",
      "Epoch 2612/40000, Loss: 0.001339597045443952, Learning Rate: 0.001462\n",
      "Epoch 2613/40000, Loss: 0.0013343833852559328, Learning Rate: 0.001462\n",
      "Epoch 2614/40000, Loss: 0.0012303827097639441, Learning Rate: 0.001462\n",
      "Epoch 2615/40000, Loss: 0.0015062361489981413, Learning Rate: 0.001461\n",
      "Epoch 2616/40000, Loss: 0.0014396477490663528, Learning Rate: 0.001461\n",
      "Epoch 2617/40000, Loss: 0.0012753137852996588, Learning Rate: 0.001461\n",
      "Epoch 2618/40000, Loss: 0.0012897865381091833, Learning Rate: 0.001461\n",
      "Epoch 2619/40000, Loss: 0.0013836915604770184, Learning Rate: 0.001461\n",
      "Epoch 2620/40000, Loss: 0.0014567453181371093, Learning Rate: 0.001460\n",
      "Epoch 2621/40000, Loss: 0.0013195945648476481, Learning Rate: 0.001460\n",
      "Epoch 2622/40000, Loss: 0.0012647302355617285, Learning Rate: 0.001460\n",
      "Epoch 2623/40000, Loss: 0.001257252530194819, Learning Rate: 0.001460\n",
      "Epoch 2624/40000, Loss: 0.0012627560645341873, Learning Rate: 0.001460\n",
      "Epoch 2625/40000, Loss: 0.0011974961962550879, Learning Rate: 0.001460\n",
      "Epoch 2626/40000, Loss: 0.0012791358167305589, Learning Rate: 0.001459\n",
      "Epoch 2627/40000, Loss: 0.0012491198722273111, Learning Rate: 0.001459\n",
      "Epoch 2628/40000, Loss: 0.001180044375360012, Learning Rate: 0.001459\n",
      "Epoch 2629/40000, Loss: 0.0012617278844118118, Learning Rate: 0.001459\n",
      "Epoch 2630/40000, Loss: 0.001208397326990962, Learning Rate: 0.001459\n",
      "Epoch 2631/40000, Loss: 0.001204687636345625, Learning Rate: 0.001459\n",
      "Epoch 2632/40000, Loss: 0.0012342537520453334, Learning Rate: 0.001458\n",
      "Epoch 2633/40000, Loss: 0.0012359386309981346, Learning Rate: 0.001458\n",
      "Epoch 2634/40000, Loss: 0.0013270769268274307, Learning Rate: 0.001458\n",
      "Epoch 2635/40000, Loss: 0.0012132131960242987, Learning Rate: 0.001458\n",
      "Epoch 2636/40000, Loss: 0.001156548853032291, Learning Rate: 0.001458\n",
      "Epoch 2637/40000, Loss: 0.001326279598288238, Learning Rate: 0.001457\n",
      "Epoch 2638/40000, Loss: 0.0011910931207239628, Learning Rate: 0.001457\n",
      "Epoch 2639/40000, Loss: 0.00122576963622123, Learning Rate: 0.001457\n",
      "Epoch 2640/40000, Loss: 0.0012017562985420227, Learning Rate: 0.001457\n",
      "Epoch 2641/40000, Loss: 0.0012419213308021426, Learning Rate: 0.001457\n",
      "Epoch 2642/40000, Loss: 0.0011509882751852274, Learning Rate: 0.001457\n",
      "Epoch 2643/40000, Loss: 0.0012115046847611666, Learning Rate: 0.001456\n",
      "Epoch 2644/40000, Loss: 0.0012178946053609252, Learning Rate: 0.001456\n",
      "Epoch 2645/40000, Loss: 0.0012103937333449721, Learning Rate: 0.001456\n",
      "Epoch 2646/40000, Loss: 0.0012474983232095838, Learning Rate: 0.001456\n",
      "Epoch 2647/40000, Loss: 0.0012351759942248464, Learning Rate: 0.001456\n",
      "Epoch 2648/40000, Loss: 0.0012135263532400131, Learning Rate: 0.001456\n",
      "Epoch 2649/40000, Loss: 0.0012513897381722927, Learning Rate: 0.001455\n",
      "Epoch 2650/40000, Loss: 0.0012064557522535324, Learning Rate: 0.001455\n",
      "Epoch 2651/40000, Loss: 0.0011503888526931405, Learning Rate: 0.001455\n",
      "Epoch 2652/40000, Loss: 0.0011828076094388962, Learning Rate: 0.001455\n",
      "Epoch 2653/40000, Loss: 0.001221098005771637, Learning Rate: 0.001455\n",
      "Epoch 2654/40000, Loss: 0.001196043798699975, Learning Rate: 0.001455\n",
      "Epoch 2655/40000, Loss: 0.0011639819713309407, Learning Rate: 0.001454\n",
      "Epoch 2656/40000, Loss: 0.0012727347202599049, Learning Rate: 0.001454\n",
      "Epoch 2657/40000, Loss: 0.0012730421731248498, Learning Rate: 0.001454\n",
      "Epoch 2658/40000, Loss: 0.0012026057811453938, Learning Rate: 0.001454\n",
      "Epoch 2659/40000, Loss: 0.0012107144575566053, Learning Rate: 0.001454\n",
      "Epoch 2660/40000, Loss: 0.0012012987863272429, Learning Rate: 0.001453\n",
      "Epoch 2661/40000, Loss: 0.0012340066023170948, Learning Rate: 0.001453\n",
      "Epoch 2662/40000, Loss: 0.0011717546731233597, Learning Rate: 0.001453\n",
      "Epoch 2663/40000, Loss: 0.0012201991630718112, Learning Rate: 0.001453\n",
      "Epoch 2664/40000, Loss: 0.0011738662142306566, Learning Rate: 0.001453\n",
      "Epoch 2665/40000, Loss: 0.0011809939751401544, Learning Rate: 0.001453\n",
      "Epoch 2666/40000, Loss: 0.0012248074635863304, Learning Rate: 0.001452\n",
      "Epoch 2667/40000, Loss: 0.0011891629546880722, Learning Rate: 0.001452\n",
      "Epoch 2668/40000, Loss: 0.0011963271535933018, Learning Rate: 0.001452\n",
      "Epoch 2669/40000, Loss: 0.0011855107732117176, Learning Rate: 0.001452\n",
      "Epoch 2670/40000, Loss: 0.0011878921650350094, Learning Rate: 0.001452\n",
      "Epoch 2671/40000, Loss: 0.0013131247833371162, Learning Rate: 0.001452\n",
      "Epoch 2672/40000, Loss: 0.0011981818825006485, Learning Rate: 0.001451\n",
      "Epoch 2673/40000, Loss: 0.001235974719747901, Learning Rate: 0.001451\n",
      "Epoch 2674/40000, Loss: 0.0012368446914479136, Learning Rate: 0.001451\n",
      "Epoch 2675/40000, Loss: 0.0011910051107406616, Learning Rate: 0.001451\n",
      "Epoch 2676/40000, Loss: 0.001201820676214993, Learning Rate: 0.001451\n",
      "Epoch 2677/40000, Loss: 0.0012548109516501427, Learning Rate: 0.001450\n",
      "Epoch 2678/40000, Loss: 0.0011662705801427364, Learning Rate: 0.001450\n",
      "Epoch 2679/40000, Loss: 0.0012608686229214072, Learning Rate: 0.001450\n",
      "Epoch 2680/40000, Loss: 0.001350330887362361, Learning Rate: 0.001450\n",
      "Epoch 2681/40000, Loss: 0.0013265175512060523, Learning Rate: 0.001450\n",
      "Epoch 2682/40000, Loss: 0.0012308829464018345, Learning Rate: 0.001450\n",
      "Epoch 2683/40000, Loss: 0.0012212806614115834, Learning Rate: 0.001449\n",
      "Epoch 2684/40000, Loss: 0.00120067922398448, Learning Rate: 0.001449\n",
      "Epoch 2685/40000, Loss: 0.0012302703689783812, Learning Rate: 0.001449\n",
      "Epoch 2686/40000, Loss: 0.0014411776792258024, Learning Rate: 0.001449\n",
      "Epoch 2687/40000, Loss: 0.0011692141415551305, Learning Rate: 0.001449\n",
      "Epoch 2688/40000, Loss: 0.0012070976663380861, Learning Rate: 0.001449\n",
      "Epoch 2689/40000, Loss: 0.001223999192006886, Learning Rate: 0.001448\n",
      "Epoch 2690/40000, Loss: 0.0011571687646210194, Learning Rate: 0.001448\n",
      "Epoch 2691/40000, Loss: 0.0012845858000218868, Learning Rate: 0.001448\n",
      "Epoch 2692/40000, Loss: 0.0015092873945832253, Learning Rate: 0.001448\n",
      "Epoch 2693/40000, Loss: 0.001280071446672082, Learning Rate: 0.001448\n",
      "Epoch 2694/40000, Loss: 0.0012344641145318747, Learning Rate: 0.001448\n",
      "Epoch 2695/40000, Loss: 0.0012931720120832324, Learning Rate: 0.001447\n",
      "Epoch 2696/40000, Loss: 0.0012765151914209127, Learning Rate: 0.001447\n",
      "Epoch 2697/40000, Loss: 0.0012570288963615894, Learning Rate: 0.001447\n",
      "Epoch 2698/40000, Loss: 0.0012214542366564274, Learning Rate: 0.001447\n",
      "Epoch 2699/40000, Loss: 0.0011879330268129706, Learning Rate: 0.001447\n",
      "Epoch 2700/40000, Loss: 0.0011863168329000473, Learning Rate: 0.001446\n",
      "Epoch 2701/40000, Loss: 0.0012074396945536137, Learning Rate: 0.001446\n",
      "Epoch 2702/40000, Loss: 0.0012233750894665718, Learning Rate: 0.001446\n",
      "Epoch 2703/40000, Loss: 0.0011712483828887343, Learning Rate: 0.001446\n",
      "Epoch 2704/40000, Loss: 0.001265011727809906, Learning Rate: 0.001446\n",
      "Epoch 2705/40000, Loss: 0.0011924052378162742, Learning Rate: 0.001446\n",
      "Epoch 2706/40000, Loss: 0.0012110231909900904, Learning Rate: 0.001445\n",
      "Epoch 2707/40000, Loss: 0.0011835141340270638, Learning Rate: 0.001445\n",
      "Epoch 2708/40000, Loss: 0.0011739860055968165, Learning Rate: 0.001445\n",
      "Epoch 2709/40000, Loss: 0.0011877890210598707, Learning Rate: 0.001445\n",
      "Epoch 2710/40000, Loss: 0.0012091374956071377, Learning Rate: 0.001445\n",
      "Epoch 2711/40000, Loss: 0.0011605945182964206, Learning Rate: 0.001445\n",
      "Epoch 2712/40000, Loss: 0.001234802301041782, Learning Rate: 0.001444\n",
      "Epoch 2713/40000, Loss: 0.0011807008413597941, Learning Rate: 0.001444\n",
      "Epoch 2714/40000, Loss: 0.0012217105831950903, Learning Rate: 0.001444\n",
      "Epoch 2715/40000, Loss: 0.001210953458212316, Learning Rate: 0.001444\n",
      "Epoch 2716/40000, Loss: 0.0012629746925085783, Learning Rate: 0.001444\n",
      "Epoch 2717/40000, Loss: 0.00124286615755409, Learning Rate: 0.001444\n",
      "Epoch 2718/40000, Loss: 0.0012197663309052587, Learning Rate: 0.001443\n",
      "Epoch 2719/40000, Loss: 0.0012866770848631859, Learning Rate: 0.001443\n",
      "Epoch 2720/40000, Loss: 0.0012079495936632156, Learning Rate: 0.001443\n",
      "Epoch 2721/40000, Loss: 0.0012155747972428799, Learning Rate: 0.001443\n",
      "Epoch 2722/40000, Loss: 0.001158262137323618, Learning Rate: 0.001443\n",
      "Epoch 2723/40000, Loss: 0.0011346818646416068, Learning Rate: 0.001443\n",
      "Epoch 2724/40000, Loss: 0.0011510278563946486, Learning Rate: 0.001442\n",
      "Epoch 2725/40000, Loss: 0.0011395205510780215, Learning Rate: 0.001442\n",
      "Epoch 2726/40000, Loss: 0.0012259789509698749, Learning Rate: 0.001442\n",
      "Epoch 2727/40000, Loss: 0.0012544776545837522, Learning Rate: 0.001442\n",
      "Epoch 2728/40000, Loss: 0.001271443790756166, Learning Rate: 0.001442\n",
      "Epoch 2729/40000, Loss: 0.0012315409258008003, Learning Rate: 0.001441\n",
      "Epoch 2730/40000, Loss: 0.0011719923932105303, Learning Rate: 0.001441\n",
      "Epoch 2731/40000, Loss: 0.001196174998767674, Learning Rate: 0.001441\n",
      "Epoch 2732/40000, Loss: 0.0012613715371116996, Learning Rate: 0.001441\n",
      "Epoch 2733/40000, Loss: 0.0012595481239259243, Learning Rate: 0.001441\n",
      "Epoch 2734/40000, Loss: 0.0012193284928798676, Learning Rate: 0.001441\n",
      "Epoch 2735/40000, Loss: 0.0012389755574986339, Learning Rate: 0.001440\n",
      "Epoch 2736/40000, Loss: 0.0012637219624593854, Learning Rate: 0.001440\n",
      "Epoch 2737/40000, Loss: 0.0011554294032976031, Learning Rate: 0.001440\n",
      "Epoch 2738/40000, Loss: 0.001214975374750793, Learning Rate: 0.001440\n",
      "Epoch 2739/40000, Loss: 0.0012682955712080002, Learning Rate: 0.001440\n",
      "Epoch 2740/40000, Loss: 0.001222742022946477, Learning Rate: 0.001440\n",
      "Epoch 2741/40000, Loss: 0.0011795542668551207, Learning Rate: 0.001439\n",
      "Epoch 2742/40000, Loss: 0.001182394102215767, Learning Rate: 0.001439\n",
      "Epoch 2743/40000, Loss: 0.0012293427716940641, Learning Rate: 0.001439\n",
      "Epoch 2744/40000, Loss: 0.0012128556845709682, Learning Rate: 0.001439\n",
      "Epoch 2745/40000, Loss: 0.0012272701133042574, Learning Rate: 0.001439\n",
      "Epoch 2746/40000, Loss: 0.001220939215272665, Learning Rate: 0.001439\n",
      "Epoch 2747/40000, Loss: 0.0011834101751446724, Learning Rate: 0.001438\n",
      "Epoch 2748/40000, Loss: 0.0013128349091857672, Learning Rate: 0.001438\n",
      "Epoch 2749/40000, Loss: 0.0011848683934658766, Learning Rate: 0.001438\n",
      "Epoch 2750/40000, Loss: 0.0012437165714800358, Learning Rate: 0.001438\n",
      "Epoch 2751/40000, Loss: 0.0012191827408969402, Learning Rate: 0.001438\n",
      "Epoch 2752/40000, Loss: 0.001169180846773088, Learning Rate: 0.001437\n",
      "Epoch 2753/40000, Loss: 0.0012510903179645538, Learning Rate: 0.001437\n",
      "Epoch 2754/40000, Loss: 0.0012297584908083081, Learning Rate: 0.001437\n",
      "Epoch 2755/40000, Loss: 0.0011881354730576277, Learning Rate: 0.001437\n",
      "Epoch 2756/40000, Loss: 0.001153752557002008, Learning Rate: 0.001437\n",
      "Epoch 2757/40000, Loss: 0.0012405408779159188, Learning Rate: 0.001437\n",
      "Epoch 2758/40000, Loss: 0.0011548197362571955, Learning Rate: 0.001436\n",
      "Epoch 2759/40000, Loss: 0.0011538994731381536, Learning Rate: 0.001436\n",
      "Epoch 2760/40000, Loss: 0.0011619891738519073, Learning Rate: 0.001436\n",
      "Epoch 2761/40000, Loss: 0.0011563515290617943, Learning Rate: 0.001436\n",
      "Epoch 2762/40000, Loss: 0.0012063871836289763, Learning Rate: 0.001436\n",
      "Epoch 2763/40000, Loss: 0.0011778231710195541, Learning Rate: 0.001436\n",
      "Epoch 2764/40000, Loss: 0.0011595505056902766, Learning Rate: 0.001435\n",
      "Epoch 2765/40000, Loss: 0.0011853546602651477, Learning Rate: 0.001435\n",
      "Epoch 2766/40000, Loss: 0.0011676943395286798, Learning Rate: 0.001435\n",
      "Epoch 2767/40000, Loss: 0.0012156115844845772, Learning Rate: 0.001435\n",
      "Epoch 2768/40000, Loss: 0.0011794115416705608, Learning Rate: 0.001435\n",
      "Epoch 2769/40000, Loss: 0.0011824313551187515, Learning Rate: 0.001435\n",
      "Epoch 2770/40000, Loss: 0.0013246170710772276, Learning Rate: 0.001434\n",
      "Epoch 2771/40000, Loss: 0.0012215562164783478, Learning Rate: 0.001434\n",
      "Epoch 2772/40000, Loss: 0.0017569338670000434, Learning Rate: 0.001434\n",
      "Epoch 2773/40000, Loss: 0.0012215757742524147, Learning Rate: 0.001434\n",
      "Epoch 2774/40000, Loss: 0.0013055922463536263, Learning Rate: 0.001434\n",
      "Epoch 2775/40000, Loss: 0.001232090755365789, Learning Rate: 0.001434\n",
      "Epoch 2776/40000, Loss: 0.0012454921379685402, Learning Rate: 0.001433\n",
      "Epoch 2777/40000, Loss: 0.0014086775481700897, Learning Rate: 0.001433\n",
      "Epoch 2778/40000, Loss: 0.0014936223160475492, Learning Rate: 0.001433\n",
      "Epoch 2779/40000, Loss: 0.0011602708837017417, Learning Rate: 0.001433\n",
      "Epoch 2780/40000, Loss: 0.0011897108051925898, Learning Rate: 0.001433\n",
      "Epoch 2781/40000, Loss: 0.001231597037985921, Learning Rate: 0.001433\n",
      "Epoch 2782/40000, Loss: 0.0013083626981824636, Learning Rate: 0.001432\n",
      "Epoch 2783/40000, Loss: 0.001283056102693081, Learning Rate: 0.001432\n",
      "Epoch 2784/40000, Loss: 0.0014437417266890407, Learning Rate: 0.001432\n",
      "Epoch 2785/40000, Loss: 0.0012303334660828114, Learning Rate: 0.001432\n",
      "Epoch 2786/40000, Loss: 0.0012823255965486169, Learning Rate: 0.001432\n",
      "Epoch 2787/40000, Loss: 0.0012151144910603762, Learning Rate: 0.001431\n",
      "Epoch 2788/40000, Loss: 0.0011973879300057888, Learning Rate: 0.001431\n",
      "Epoch 2789/40000, Loss: 0.0011415781918913126, Learning Rate: 0.001431\n",
      "Epoch 2790/40000, Loss: 0.0011605258332565427, Learning Rate: 0.001431\n",
      "Epoch 2791/40000, Loss: 0.0012120670871809125, Learning Rate: 0.001431\n",
      "Epoch 2792/40000, Loss: 0.0012657262850552797, Learning Rate: 0.001431\n",
      "Epoch 2793/40000, Loss: 0.0012593172723427415, Learning Rate: 0.001430\n",
      "Epoch 2794/40000, Loss: 0.0011870920425280929, Learning Rate: 0.001430\n",
      "Epoch 2795/40000, Loss: 0.0012120595201849937, Learning Rate: 0.001430\n",
      "Epoch 2796/40000, Loss: 0.0012074278201907873, Learning Rate: 0.001430\n",
      "Epoch 2797/40000, Loss: 0.0012561846524477005, Learning Rate: 0.001430\n",
      "Epoch 2798/40000, Loss: 0.00118432124145329, Learning Rate: 0.001430\n",
      "Epoch 2799/40000, Loss: 0.0013357782736420631, Learning Rate: 0.001429\n",
      "Epoch 2800/40000, Loss: 0.0012027447810396552, Learning Rate: 0.001429\n",
      "Epoch 2801/40000, Loss: 0.0012628441909328103, Learning Rate: 0.001429\n",
      "Epoch 2802/40000, Loss: 0.0013059163466095924, Learning Rate: 0.001429\n",
      "Epoch 2803/40000, Loss: 0.001259222743101418, Learning Rate: 0.001429\n",
      "Epoch 2804/40000, Loss: 0.0011850837618112564, Learning Rate: 0.001429\n",
      "Epoch 2805/40000, Loss: 0.001154947909526527, Learning Rate: 0.001428\n",
      "Epoch 2806/40000, Loss: 0.0011738594621419907, Learning Rate: 0.001428\n",
      "Epoch 2807/40000, Loss: 0.0011849739821627736, Learning Rate: 0.001428\n",
      "Epoch 2808/40000, Loss: 0.0011230417294427752, Learning Rate: 0.001428\n",
      "Epoch 2809/40000, Loss: 0.0011226459173485637, Learning Rate: 0.001428\n",
      "Epoch 2810/40000, Loss: 0.0011137302499264479, Learning Rate: 0.001428\n",
      "Epoch 2811/40000, Loss: 0.0011037645163014531, Learning Rate: 0.001427\n",
      "Epoch 2812/40000, Loss: 0.001162027707323432, Learning Rate: 0.001427\n",
      "Epoch 2813/40000, Loss: 0.0011710806284099817, Learning Rate: 0.001427\n",
      "Epoch 2814/40000, Loss: 0.0011652351822704077, Learning Rate: 0.001427\n",
      "Epoch 2815/40000, Loss: 0.0012217909097671509, Learning Rate: 0.001427\n",
      "Epoch 2816/40000, Loss: 0.0011476384242996573, Learning Rate: 0.001426\n",
      "Epoch 2817/40000, Loss: 0.0011618126882240176, Learning Rate: 0.001426\n",
      "Epoch 2818/40000, Loss: 0.0011689799139276147, Learning Rate: 0.001426\n",
      "Epoch 2819/40000, Loss: 0.001152886776253581, Learning Rate: 0.001426\n",
      "Epoch 2820/40000, Loss: 0.0011385175166651607, Learning Rate: 0.001426\n",
      "Epoch 2821/40000, Loss: 0.0012133095879107714, Learning Rate: 0.001426\n",
      "Epoch 2822/40000, Loss: 0.0011930081527680159, Learning Rate: 0.001425\n",
      "Epoch 2823/40000, Loss: 0.001231819624081254, Learning Rate: 0.001425\n",
      "Epoch 2824/40000, Loss: 0.0011994612868875265, Learning Rate: 0.001425\n",
      "Epoch 2825/40000, Loss: 0.0011144457384943962, Learning Rate: 0.001425\n",
      "Epoch 2826/40000, Loss: 0.0011811706935986876, Learning Rate: 0.001425\n",
      "Epoch 2827/40000, Loss: 0.0011942093260586262, Learning Rate: 0.001425\n",
      "Epoch 2828/40000, Loss: 0.0011489306343719363, Learning Rate: 0.001424\n",
      "Epoch 2829/40000, Loss: 0.0011980910785496235, Learning Rate: 0.001424\n",
      "Epoch 2830/40000, Loss: 0.0011567396577447653, Learning Rate: 0.001424\n",
      "Epoch 2831/40000, Loss: 0.0012116783764213324, Learning Rate: 0.001424\n",
      "Epoch 2832/40000, Loss: 0.0011627456406131387, Learning Rate: 0.001424\n",
      "Epoch 2833/40000, Loss: 0.0011904118582606316, Learning Rate: 0.001424\n",
      "Epoch 2834/40000, Loss: 0.0011676936410367489, Learning Rate: 0.001423\n",
      "Epoch 2835/40000, Loss: 0.0012367508606985211, Learning Rate: 0.001423\n",
      "Epoch 2836/40000, Loss: 0.001136806677095592, Learning Rate: 0.001423\n",
      "Epoch 2837/40000, Loss: 0.001088917488232255, Learning Rate: 0.001423\n",
      "Epoch 2838/40000, Loss: 0.0010877300519496202, Learning Rate: 0.001423\n",
      "Epoch 2839/40000, Loss: 0.0010900915367528796, Learning Rate: 0.001423\n",
      "Epoch 2840/40000, Loss: 0.0010781916789710522, Learning Rate: 0.001422\n",
      "Epoch 2841/40000, Loss: 0.0010960507206618786, Learning Rate: 0.001422\n",
      "Epoch 2842/40000, Loss: 0.0010924639645963907, Learning Rate: 0.001422\n",
      "Epoch 2843/40000, Loss: 0.0011393113527446985, Learning Rate: 0.001422\n",
      "Epoch 2844/40000, Loss: 0.0011791010620072484, Learning Rate: 0.001422\n",
      "Epoch 2845/40000, Loss: 0.0011801752261817455, Learning Rate: 0.001422\n",
      "Epoch 2846/40000, Loss: 0.0011485114227980375, Learning Rate: 0.001421\n",
      "Epoch 2847/40000, Loss: 0.001183150103315711, Learning Rate: 0.001421\n",
      "Epoch 2848/40000, Loss: 0.0011935785878449678, Learning Rate: 0.001421\n",
      "Epoch 2849/40000, Loss: 0.001160111976787448, Learning Rate: 0.001421\n",
      "Epoch 2850/40000, Loss: 0.0011068752501159906, Learning Rate: 0.001421\n",
      "Epoch 2851/40000, Loss: 0.0010835668072104454, Learning Rate: 0.001421\n",
      "Epoch 2852/40000, Loss: 0.00113167823292315, Learning Rate: 0.001420\n",
      "Epoch 2853/40000, Loss: 0.0010943034430965781, Learning Rate: 0.001420\n",
      "Epoch 2854/40000, Loss: 0.0012510468950495124, Learning Rate: 0.001420\n",
      "Epoch 2855/40000, Loss: 0.001153885736130178, Learning Rate: 0.001420\n",
      "Epoch 2856/40000, Loss: 0.0011606409680098295, Learning Rate: 0.001420\n",
      "Epoch 2857/40000, Loss: 0.0011762650683522224, Learning Rate: 0.001419\n",
      "Epoch 2858/40000, Loss: 0.0011260583996772766, Learning Rate: 0.001419\n",
      "Epoch 2859/40000, Loss: 0.001198943704366684, Learning Rate: 0.001419\n",
      "Epoch 2860/40000, Loss: 0.0011747947428375483, Learning Rate: 0.001419\n",
      "Epoch 2861/40000, Loss: 0.0011712261475622654, Learning Rate: 0.001419\n",
      "Epoch 2862/40000, Loss: 0.0011764103546738625, Learning Rate: 0.001419\n",
      "Epoch 2863/40000, Loss: 0.0011852816678583622, Learning Rate: 0.001418\n",
      "Epoch 2864/40000, Loss: 0.0011596990516409278, Learning Rate: 0.001418\n",
      "Epoch 2865/40000, Loss: 0.001098086591809988, Learning Rate: 0.001418\n",
      "Epoch 2866/40000, Loss: 0.0011879620142281055, Learning Rate: 0.001418\n",
      "Epoch 2867/40000, Loss: 0.0012066884664818645, Learning Rate: 0.001418\n",
      "Epoch 2868/40000, Loss: 0.0011850142618641257, Learning Rate: 0.001418\n",
      "Epoch 2869/40000, Loss: 0.0011047697626054287, Learning Rate: 0.001417\n",
      "Epoch 2870/40000, Loss: 0.0010857736924663186, Learning Rate: 0.001417\n",
      "Epoch 2871/40000, Loss: 0.001100737601518631, Learning Rate: 0.001417\n",
      "Epoch 2872/40000, Loss: 0.0011335051385685802, Learning Rate: 0.001417\n",
      "Epoch 2873/40000, Loss: 0.0011539675761014223, Learning Rate: 0.001417\n",
      "Epoch 2874/40000, Loss: 0.0011296754237264395, Learning Rate: 0.001417\n",
      "Epoch 2875/40000, Loss: 0.0011597061529755592, Learning Rate: 0.001416\n",
      "Epoch 2876/40000, Loss: 0.0011630072258412838, Learning Rate: 0.001416\n",
      "Epoch 2877/40000, Loss: 0.0011128152254968882, Learning Rate: 0.001416\n",
      "Epoch 2878/40000, Loss: 0.0011516185477375984, Learning Rate: 0.001416\n",
      "Epoch 2879/40000, Loss: 0.0011065017897635698, Learning Rate: 0.001416\n",
      "Epoch 2880/40000, Loss: 0.0011403253301978111, Learning Rate: 0.001416\n",
      "Epoch 2881/40000, Loss: 0.0012121398467570543, Learning Rate: 0.001415\n",
      "Epoch 2882/40000, Loss: 0.0011429814621806145, Learning Rate: 0.001415\n",
      "Epoch 2883/40000, Loss: 0.0011552647920325398, Learning Rate: 0.001415\n",
      "Epoch 2884/40000, Loss: 0.0011793492594733834, Learning Rate: 0.001415\n",
      "Epoch 2885/40000, Loss: 0.0011696607107296586, Learning Rate: 0.001415\n",
      "Epoch 2886/40000, Loss: 0.001155642792582512, Learning Rate: 0.001415\n",
      "Epoch 2887/40000, Loss: 0.0011644995538517833, Learning Rate: 0.001414\n",
      "Epoch 2888/40000, Loss: 0.0011177362175658345, Learning Rate: 0.001414\n",
      "Epoch 2889/40000, Loss: 0.001226147636771202, Learning Rate: 0.001414\n",
      "Epoch 2890/40000, Loss: 0.001182463951408863, Learning Rate: 0.001414\n",
      "Epoch 2891/40000, Loss: 0.0012271995656192303, Learning Rate: 0.001414\n",
      "Epoch 2892/40000, Loss: 0.0011650763917714357, Learning Rate: 0.001414\n",
      "Epoch 2893/40000, Loss: 0.0012019076384603977, Learning Rate: 0.001413\n",
      "Epoch 2894/40000, Loss: 0.0011670568492263556, Learning Rate: 0.001413\n",
      "Epoch 2895/40000, Loss: 0.0011762437643483281, Learning Rate: 0.001413\n",
      "Epoch 2896/40000, Loss: 0.0012508539948612452, Learning Rate: 0.001413\n",
      "Epoch 2897/40000, Loss: 0.0011247200891375542, Learning Rate: 0.001413\n",
      "Epoch 2898/40000, Loss: 0.0011514127254486084, Learning Rate: 0.001413\n",
      "Epoch 2899/40000, Loss: 0.001143263652920723, Learning Rate: 0.001412\n",
      "Epoch 2900/40000, Loss: 0.001257111318409443, Learning Rate: 0.001412\n",
      "Epoch 2901/40000, Loss: 0.0012509441003203392, Learning Rate: 0.001412\n",
      "Epoch 2902/40000, Loss: 0.0012008837657049298, Learning Rate: 0.001412\n",
      "Epoch 2903/40000, Loss: 0.0011818917701020837, Learning Rate: 0.001412\n",
      "Epoch 2904/40000, Loss: 0.0011448145378381014, Learning Rate: 0.001412\n",
      "Epoch 2905/40000, Loss: 0.0011643338948488235, Learning Rate: 0.001411\n",
      "Epoch 2906/40000, Loss: 0.0011782970977947116, Learning Rate: 0.001411\n",
      "Epoch 2907/40000, Loss: 0.001349585596472025, Learning Rate: 0.001411\n",
      "Epoch 2908/40000, Loss: 0.001202909043058753, Learning Rate: 0.001411\n",
      "Epoch 2909/40000, Loss: 0.001232408918440342, Learning Rate: 0.001411\n",
      "Epoch 2910/40000, Loss: 0.0012902210000902414, Learning Rate: 0.001410\n",
      "Epoch 2911/40000, Loss: 0.0012439086567610502, Learning Rate: 0.001410\n",
      "Epoch 2912/40000, Loss: 0.0012585045769810677, Learning Rate: 0.001410\n",
      "Epoch 2913/40000, Loss: 0.0011592042865231633, Learning Rate: 0.001410\n",
      "Epoch 2914/40000, Loss: 0.0011552401119843125, Learning Rate: 0.001410\n",
      "Epoch 2915/40000, Loss: 0.0011263720225542784, Learning Rate: 0.001410\n",
      "Epoch 2916/40000, Loss: 0.0011471470352262259, Learning Rate: 0.001409\n",
      "Epoch 2917/40000, Loss: 0.0011930650798603892, Learning Rate: 0.001409\n",
      "Epoch 2918/40000, Loss: 0.0011935129296034575, Learning Rate: 0.001409\n",
      "Epoch 2919/40000, Loss: 0.0012642554938793182, Learning Rate: 0.001409\n",
      "Epoch 2920/40000, Loss: 0.0011546957539394498, Learning Rate: 0.001409\n",
      "Epoch 2921/40000, Loss: 0.0015275799669325352, Learning Rate: 0.001409\n",
      "Epoch 2922/40000, Loss: 0.0014013249892741442, Learning Rate: 0.001408\n",
      "Epoch 2923/40000, Loss: 0.0013336329720914364, Learning Rate: 0.001408\n",
      "Epoch 2924/40000, Loss: 0.0012683214154094458, Learning Rate: 0.001408\n",
      "Epoch 2925/40000, Loss: 0.0012576412409543991, Learning Rate: 0.001408\n",
      "Epoch 2926/40000, Loss: 0.0012900620931759477, Learning Rate: 0.001408\n",
      "Epoch 2927/40000, Loss: 0.0012659861240535975, Learning Rate: 0.001408\n",
      "Epoch 2928/40000, Loss: 0.0012250329600647092, Learning Rate: 0.001407\n",
      "Epoch 2929/40000, Loss: 0.0012315436033532023, Learning Rate: 0.001407\n",
      "Epoch 2930/40000, Loss: 0.0011902854312211275, Learning Rate: 0.001407\n",
      "Epoch 2931/40000, Loss: 0.0011790411081165075, Learning Rate: 0.001407\n",
      "Epoch 2932/40000, Loss: 0.0012396855745464563, Learning Rate: 0.001407\n",
      "Epoch 2933/40000, Loss: 0.0011867163702845573, Learning Rate: 0.001407\n",
      "Epoch 2934/40000, Loss: 0.001166601781733334, Learning Rate: 0.001406\n",
      "Epoch 2935/40000, Loss: 0.001107016229070723, Learning Rate: 0.001406\n",
      "Epoch 2936/40000, Loss: 0.001215033233165741, Learning Rate: 0.001406\n",
      "Epoch 2937/40000, Loss: 0.001089284080080688, Learning Rate: 0.001406\n",
      "Epoch 2938/40000, Loss: 0.0011862784158438444, Learning Rate: 0.001406\n",
      "Epoch 2939/40000, Loss: 0.001207313616760075, Learning Rate: 0.001406\n",
      "Epoch 2940/40000, Loss: 0.0012544546043500304, Learning Rate: 0.001405\n",
      "Epoch 2941/40000, Loss: 0.0011787437833845615, Learning Rate: 0.001405\n",
      "Epoch 2942/40000, Loss: 0.0012007385957986116, Learning Rate: 0.001405\n",
      "Epoch 2943/40000, Loss: 0.0011152310762554407, Learning Rate: 0.001405\n",
      "Epoch 2944/40000, Loss: 0.0011891708709299564, Learning Rate: 0.001405\n",
      "Epoch 2945/40000, Loss: 0.001100991852581501, Learning Rate: 0.001405\n",
      "Epoch 2946/40000, Loss: 0.001107921707443893, Learning Rate: 0.001404\n",
      "Epoch 2947/40000, Loss: 0.0011130983475595713, Learning Rate: 0.001404\n",
      "Epoch 2948/40000, Loss: 0.0011260727187618613, Learning Rate: 0.001404\n",
      "Epoch 2949/40000, Loss: 0.0011355263413861394, Learning Rate: 0.001404\n",
      "Epoch 2950/40000, Loss: 0.0010820862371474504, Learning Rate: 0.001404\n",
      "Epoch 2951/40000, Loss: 0.0011593216331675649, Learning Rate: 0.001404\n",
      "Epoch 2952/40000, Loss: 0.0011221825843676925, Learning Rate: 0.001403\n",
      "Epoch 2953/40000, Loss: 0.0011429967125877738, Learning Rate: 0.001403\n",
      "Epoch 2954/40000, Loss: 0.0012022883165627718, Learning Rate: 0.001403\n",
      "Epoch 2955/40000, Loss: 0.0011310635600239038, Learning Rate: 0.001403\n",
      "Epoch 2956/40000, Loss: 0.0011533740907907486, Learning Rate: 0.001403\n",
      "Epoch 2957/40000, Loss: 0.0011395211331546307, Learning Rate: 0.001403\n",
      "Epoch 2958/40000, Loss: 0.001044657314196229, Learning Rate: 0.001402\n",
      "Epoch 2959/40000, Loss: 0.0010779044823721051, Learning Rate: 0.001402\n",
      "Epoch 2960/40000, Loss: 0.0011565204476937652, Learning Rate: 0.001402\n",
      "Epoch 2961/40000, Loss: 0.0011261667823418975, Learning Rate: 0.001402\n",
      "Epoch 2962/40000, Loss: 0.0010974083561450243, Learning Rate: 0.001402\n",
      "Epoch 2963/40000, Loss: 0.0010770090157166123, Learning Rate: 0.001402\n",
      "Epoch 2964/40000, Loss: 0.0010909494012594223, Learning Rate: 0.001401\n",
      "Epoch 2965/40000, Loss: 0.0011069085448980331, Learning Rate: 0.001401\n",
      "Epoch 2966/40000, Loss: 0.0010754436952993274, Learning Rate: 0.001401\n",
      "Epoch 2967/40000, Loss: 0.0010914155282080173, Learning Rate: 0.001401\n",
      "Epoch 2968/40000, Loss: 0.0010741881560534239, Learning Rate: 0.001401\n",
      "Epoch 2969/40000, Loss: 0.0010921853827312589, Learning Rate: 0.001401\n",
      "Epoch 2970/40000, Loss: 0.0011013454059138894, Learning Rate: 0.001400\n",
      "Epoch 2971/40000, Loss: 0.0010822985786944628, Learning Rate: 0.001400\n",
      "Epoch 2972/40000, Loss: 0.001157274004071951, Learning Rate: 0.001400\n",
      "Epoch 2973/40000, Loss: 0.001136267208494246, Learning Rate: 0.001400\n",
      "Epoch 2974/40000, Loss: 0.001117633655667305, Learning Rate: 0.001400\n",
      "Epoch 2975/40000, Loss: 0.001146090915426612, Learning Rate: 0.001400\n",
      "Epoch 2976/40000, Loss: 0.001073639141395688, Learning Rate: 0.001399\n",
      "Epoch 2977/40000, Loss: 0.0010604638373479247, Learning Rate: 0.001399\n",
      "Epoch 2978/40000, Loss: 0.0011180253932252526, Learning Rate: 0.001399\n",
      "Epoch 2979/40000, Loss: 0.0010948714334517717, Learning Rate: 0.001399\n",
      "Epoch 2980/40000, Loss: 0.0011160746216773987, Learning Rate: 0.001399\n",
      "Epoch 2981/40000, Loss: 0.0011338882613927126, Learning Rate: 0.001399\n",
      "Epoch 2982/40000, Loss: 0.0011460863752290606, Learning Rate: 0.001398\n",
      "Epoch 2983/40000, Loss: 0.0011238951701670885, Learning Rate: 0.001398\n",
      "Epoch 2984/40000, Loss: 0.0010981641244143248, Learning Rate: 0.001398\n",
      "Epoch 2985/40000, Loss: 0.0011173139791935682, Learning Rate: 0.001398\n",
      "Epoch 2986/40000, Loss: 0.001086549018509686, Learning Rate: 0.001398\n",
      "Epoch 2987/40000, Loss: 0.001120370114222169, Learning Rate: 0.001398\n",
      "Epoch 2988/40000, Loss: 0.0011258390732109547, Learning Rate: 0.001397\n",
      "Epoch 2989/40000, Loss: 0.0011231739772483706, Learning Rate: 0.001397\n",
      "Epoch 2990/40000, Loss: 0.0011249680537730455, Learning Rate: 0.001397\n",
      "Epoch 2991/40000, Loss: 0.0011223414912819862, Learning Rate: 0.001397\n",
      "Epoch 2992/40000, Loss: 0.0011242295149713755, Learning Rate: 0.001397\n",
      "Epoch 2993/40000, Loss: 0.0011103281285613775, Learning Rate: 0.001397\n",
      "Epoch 2994/40000, Loss: 0.0011136587709188461, Learning Rate: 0.001396\n",
      "Epoch 2995/40000, Loss: 0.0011210220400243998, Learning Rate: 0.001396\n",
      "Epoch 2996/40000, Loss: 0.0011448910227045417, Learning Rate: 0.001396\n",
      "Epoch 2997/40000, Loss: 0.0010953409364446998, Learning Rate: 0.001396\n",
      "Epoch 2998/40000, Loss: 0.0013944183010607958, Learning Rate: 0.001396\n",
      "Epoch 2999/40000, Loss: 0.0012098399456590414, Learning Rate: 0.001396\n",
      "Epoch 3000/40000, Loss: 0.001188574475236237, Learning Rate: 0.001395\n",
      "Epoch 3001/40000, Loss: 0.0004890376003459096, Learning Rate: 0.001395\n",
      "Epoch 3002/40000, Loss: 0.0009619028423912823, Learning Rate: 0.001395\n",
      "Epoch 3003/40000, Loss: 0.0005298921605572104, Learning Rate: 0.001395\n",
      "Epoch 3004/40000, Loss: 0.0009659595671109855, Learning Rate: 0.001395\n",
      "Epoch 3005/40000, Loss: 0.002949583111330867, Learning Rate: 0.001395\n",
      "Epoch 3006/40000, Loss: 0.0012171313865110278, Learning Rate: 0.001394\n",
      "Epoch 3007/40000, Loss: 0.0009061243035830557, Learning Rate: 0.001394\n",
      "Epoch 3008/40000, Loss: 0.0011541437124833465, Learning Rate: 0.001394\n",
      "Epoch 3009/40000, Loss: 0.0011132403742522001, Learning Rate: 0.001394\n",
      "Epoch 3010/40000, Loss: 0.0007002535276114941, Learning Rate: 0.001394\n",
      "Epoch 3011/40000, Loss: 0.000420532189309597, Learning Rate: 0.001394\n",
      "Epoch 3012/40000, Loss: 0.00033247756073251367, Learning Rate: 0.001393\n",
      "Epoch 3013/40000, Loss: 0.0004451857239473611, Learning Rate: 0.001393\n",
      "Epoch 3014/40000, Loss: 0.00032951892353594303, Learning Rate: 0.001393\n",
      "Epoch 3015/40000, Loss: 0.0005382472882047296, Learning Rate: 0.001393\n",
      "Epoch 3016/40000, Loss: 0.0003804903826676309, Learning Rate: 0.001393\n",
      "Epoch 3017/40000, Loss: 0.00026442346279509366, Learning Rate: 0.001393\n",
      "Epoch 3018/40000, Loss: 0.0005761400680057704, Learning Rate: 0.001392\n",
      "Epoch 3019/40000, Loss: 0.000608241418376565, Learning Rate: 0.001392\n",
      "Epoch 3020/40000, Loss: 0.00034782386501319706, Learning Rate: 0.001392\n",
      "Epoch 3021/40000, Loss: 0.00020022275566589087, Learning Rate: 0.001392\n",
      "Epoch 3022/40000, Loss: 0.00018720449588727206, Learning Rate: 0.001392\n",
      "Epoch 3023/40000, Loss: 0.0004084384418092668, Learning Rate: 0.001392\n",
      "Epoch 3024/40000, Loss: 0.00039635528810322285, Learning Rate: 0.001391\n",
      "Epoch 3025/40000, Loss: 0.00040857831481844187, Learning Rate: 0.001391\n",
      "Epoch 3026/40000, Loss: 0.0002535354869905859, Learning Rate: 0.001391\n",
      "Epoch 3027/40000, Loss: 0.0003485350462142378, Learning Rate: 0.001391\n",
      "Epoch 3028/40000, Loss: 0.00039292816654779017, Learning Rate: 0.001391\n",
      "Epoch 3029/40000, Loss: 0.0001984043774427846, Learning Rate: 0.001391\n",
      "Epoch 3030/40000, Loss: 0.00021134382404852659, Learning Rate: 0.001390\n",
      "Epoch 3031/40000, Loss: 0.0004836667503695935, Learning Rate: 0.001390\n",
      "Epoch 3032/40000, Loss: 0.0001748402719385922, Learning Rate: 0.001390\n",
      "Epoch 3033/40000, Loss: 0.0006941713509149849, Learning Rate: 0.001390\n",
      "Epoch 3034/40000, Loss: 0.0006647150730714202, Learning Rate: 0.001390\n",
      "Epoch 3035/40000, Loss: 0.0003909955848939717, Learning Rate: 0.001389\n",
      "Epoch 3036/40000, Loss: 0.00045797464554198086, Learning Rate: 0.001389\n",
      "Epoch 3037/40000, Loss: 0.00024251666036434472, Learning Rate: 0.001389\n",
      "Epoch 3038/40000, Loss: 0.00024954494438134134, Learning Rate: 0.001389\n",
      "Epoch 3039/40000, Loss: 0.00011861003440571949, Learning Rate: 0.001389\n",
      "Epoch 3040/40000, Loss: 0.00015101897588465363, Learning Rate: 0.001389\n",
      "Epoch 3041/40000, Loss: 0.00013681668497156352, Learning Rate: 0.001388\n",
      "Epoch 3042/40000, Loss: 0.0001353056577499956, Learning Rate: 0.001388\n",
      "Epoch 3043/40000, Loss: 0.0001263048907276243, Learning Rate: 0.001388\n",
      "Epoch 3044/40000, Loss: 0.00012614367005880922, Learning Rate: 0.001388\n",
      "Epoch 3045/40000, Loss: 0.00019834224076475948, Learning Rate: 0.001388\n",
      "Epoch 3046/40000, Loss: 0.000146310732816346, Learning Rate: 0.001388\n",
      "Epoch 3047/40000, Loss: 8.517470268998295e-05, Learning Rate: 0.001387\n",
      "Epoch 3048/40000, Loss: 0.00012228573905304074, Learning Rate: 0.001387\n",
      "Epoch 3049/40000, Loss: 0.00019278844411019236, Learning Rate: 0.001387\n",
      "Epoch 3050/40000, Loss: 0.00018461876607034355, Learning Rate: 0.001387\n",
      "Epoch 3051/40000, Loss: 9.89713444141671e-05, Learning Rate: 0.001387\n",
      "Epoch 3052/40000, Loss: 0.00014575530076399446, Learning Rate: 0.001387\n",
      "Epoch 3053/40000, Loss: 0.00021677272161468863, Learning Rate: 0.001387\n",
      "Epoch 3054/40000, Loss: 0.00017148417828138918, Learning Rate: 0.001386\n",
      "Epoch 3055/40000, Loss: 0.00021051082876510918, Learning Rate: 0.001386\n",
      "Epoch 3056/40000, Loss: 0.00018167048983741552, Learning Rate: 0.001386\n",
      "Epoch 3057/40000, Loss: 0.0001902690710267052, Learning Rate: 0.001386\n",
      "Epoch 3058/40000, Loss: 0.00017410820873919874, Learning Rate: 0.001386\n",
      "Epoch 3059/40000, Loss: 0.00014836499758530408, Learning Rate: 0.001386\n",
      "Epoch 3060/40000, Loss: 0.0001385703799314797, Learning Rate: 0.001385\n",
      "Epoch 3061/40000, Loss: 0.00017051566101145, Learning Rate: 0.001385\n",
      "Epoch 3062/40000, Loss: 9.246633271686733e-05, Learning Rate: 0.001385\n",
      "Epoch 3063/40000, Loss: 0.00013880843471270055, Learning Rate: 0.001385\n",
      "Epoch 3064/40000, Loss: 0.0001874133013188839, Learning Rate: 0.001385\n",
      "Epoch 3065/40000, Loss: 9.790130570763722e-05, Learning Rate: 0.001385\n",
      "Epoch 3066/40000, Loss: 0.00018539521261118352, Learning Rate: 0.001384\n",
      "Epoch 3067/40000, Loss: 0.00019391125533729792, Learning Rate: 0.001384\n",
      "Epoch 3068/40000, Loss: 0.00037210292066447437, Learning Rate: 0.001384\n",
      "Epoch 3069/40000, Loss: 0.00010481208300916478, Learning Rate: 0.001384\n",
      "Epoch 3070/40000, Loss: 0.0001842905767261982, Learning Rate: 0.001384\n",
      "Epoch 3071/40000, Loss: 0.00012502461322583258, Learning Rate: 0.001384\n",
      "Epoch 3072/40000, Loss: 0.00014172961527947336, Learning Rate: 0.001383\n",
      "Epoch 3073/40000, Loss: 0.0001420368062099442, Learning Rate: 0.001383\n",
      "Epoch 3074/40000, Loss: 0.0001511084847152233, Learning Rate: 0.001383\n",
      "Epoch 3075/40000, Loss: 0.000204133553779684, Learning Rate: 0.001383\n",
      "Epoch 3076/40000, Loss: 0.00019700711709447205, Learning Rate: 0.001383\n",
      "Epoch 3077/40000, Loss: 0.0001902763469843194, Learning Rate: 0.001383\n",
      "Epoch 3078/40000, Loss: 0.00015704342513345182, Learning Rate: 0.001382\n",
      "Epoch 3079/40000, Loss: 0.0002285212103743106, Learning Rate: 0.001382\n",
      "Epoch 3080/40000, Loss: 0.00021354574710130692, Learning Rate: 0.001382\n",
      "Epoch 3081/40000, Loss: 0.00010975964687531814, Learning Rate: 0.001382\n",
      "Epoch 3082/40000, Loss: 0.0003447490744292736, Learning Rate: 0.001382\n",
      "Epoch 3083/40000, Loss: 0.00016269524348899722, Learning Rate: 0.001382\n",
      "Epoch 3084/40000, Loss: 0.0002661283069755882, Learning Rate: 0.001381\n",
      "Epoch 3085/40000, Loss: 0.00015777922817505896, Learning Rate: 0.001381\n",
      "Epoch 3086/40000, Loss: 0.00023466961283702403, Learning Rate: 0.001381\n",
      "Epoch 3087/40000, Loss: 0.00011082640412496403, Learning Rate: 0.001381\n",
      "Epoch 3088/40000, Loss: 0.00013859316823072731, Learning Rate: 0.001381\n",
      "Epoch 3089/40000, Loss: 0.00026923808036372066, Learning Rate: 0.001381\n",
      "Epoch 3090/40000, Loss: 0.00021099256991874427, Learning Rate: 0.001380\n",
      "Epoch 3091/40000, Loss: 0.00011865081614814699, Learning Rate: 0.001380\n",
      "Epoch 3092/40000, Loss: 8.364865789189935e-05, Learning Rate: 0.001380\n",
      "Epoch 3093/40000, Loss: 0.00010511681466596201, Learning Rate: 0.001380\n",
      "Epoch 3094/40000, Loss: 9.20185775612481e-05, Learning Rate: 0.001380\n",
      "Epoch 3095/40000, Loss: 0.0001315204135607928, Learning Rate: 0.001380\n",
      "Epoch 3096/40000, Loss: 0.00016691465862095356, Learning Rate: 0.001379\n",
      "Epoch 3097/40000, Loss: 0.00016382461762987077, Learning Rate: 0.001379\n",
      "Epoch 3098/40000, Loss: 8.293164137285203e-05, Learning Rate: 0.001379\n",
      "Epoch 3099/40000, Loss: 7.483427179977298e-05, Learning Rate: 0.001379\n",
      "Epoch 3100/40000, Loss: 0.00014808961714152247, Learning Rate: 0.001379\n",
      "Epoch 3101/40000, Loss: 0.00016332941595464945, Learning Rate: 0.001379\n",
      "Epoch 3102/40000, Loss: 0.0001742924505379051, Learning Rate: 0.001378\n",
      "Epoch 3103/40000, Loss: 0.0001622659619897604, Learning Rate: 0.001378\n",
      "Epoch 3104/40000, Loss: 0.00015868297487031668, Learning Rate: 0.001378\n",
      "Epoch 3105/40000, Loss: 0.00012067424540873617, Learning Rate: 0.001378\n",
      "Epoch 3106/40000, Loss: 8.373932359972969e-05, Learning Rate: 0.001378\n",
      "Epoch 3107/40000, Loss: 6.815680535510182e-05, Learning Rate: 0.001378\n",
      "Epoch 3108/40000, Loss: 0.00010123245738213882, Learning Rate: 0.001377\n",
      "Epoch 3109/40000, Loss: 7.065084355417639e-05, Learning Rate: 0.001377\n",
      "Epoch 3110/40000, Loss: 0.0001278350973734632, Learning Rate: 0.001377\n",
      "Epoch 3111/40000, Loss: 0.00018633296713232994, Learning Rate: 0.001377\n",
      "Epoch 3112/40000, Loss: 8.001546666491777e-05, Learning Rate: 0.001377\n",
      "Epoch 3113/40000, Loss: 0.0001600781688466668, Learning Rate: 0.001377\n",
      "Epoch 3114/40000, Loss: 9.25648128031753e-05, Learning Rate: 0.001376\n",
      "Epoch 3115/40000, Loss: 0.00018136143626179546, Learning Rate: 0.001376\n",
      "Epoch 3116/40000, Loss: 0.00016325870819855481, Learning Rate: 0.001376\n",
      "Epoch 3117/40000, Loss: 0.00011483929847599939, Learning Rate: 0.001376\n",
      "Epoch 3118/40000, Loss: 0.00012607264216057956, Learning Rate: 0.001376\n",
      "Epoch 3119/40000, Loss: 9.668878192314878e-05, Learning Rate: 0.001376\n",
      "Epoch 3120/40000, Loss: 0.00019268954929430038, Learning Rate: 0.001375\n",
      "Epoch 3121/40000, Loss: 0.0001147889852290973, Learning Rate: 0.001375\n",
      "Epoch 3122/40000, Loss: 0.0001567910221638158, Learning Rate: 0.001375\n",
      "Epoch 3123/40000, Loss: 0.00011947618622798473, Learning Rate: 0.001375\n",
      "Epoch 3124/40000, Loss: 0.00020388085977174342, Learning Rate: 0.001375\n",
      "Epoch 3125/40000, Loss: 0.0001001232594717294, Learning Rate: 0.001375\n",
      "Epoch 3126/40000, Loss: 0.0001825440995162353, Learning Rate: 0.001374\n",
      "Epoch 3127/40000, Loss: 8.772077853791416e-05, Learning Rate: 0.001374\n",
      "Epoch 3128/40000, Loss: 0.0002234882122138515, Learning Rate: 0.001374\n",
      "Epoch 3129/40000, Loss: 0.00014429485599976033, Learning Rate: 0.001374\n",
      "Epoch 3130/40000, Loss: 0.00021900505817029625, Learning Rate: 0.001374\n",
      "Epoch 3131/40000, Loss: 0.00014403513341676444, Learning Rate: 0.001374\n",
      "Epoch 3132/40000, Loss: 0.0002513925137463957, Learning Rate: 0.001373\n",
      "Epoch 3133/40000, Loss: 0.0001434029109077528, Learning Rate: 0.001373\n",
      "Epoch 3134/40000, Loss: 0.00024121948808897287, Learning Rate: 0.001373\n",
      "Epoch 3135/40000, Loss: 0.00026759537286125124, Learning Rate: 0.001373\n",
      "Epoch 3136/40000, Loss: 0.00020125712035223842, Learning Rate: 0.001373\n",
      "Epoch 3137/40000, Loss: 0.00015656572941225022, Learning Rate: 0.001373\n",
      "Epoch 3138/40000, Loss: 0.00030035621603019536, Learning Rate: 0.001372\n",
      "Epoch 3139/40000, Loss: 0.000144901656312868, Learning Rate: 0.001372\n",
      "Epoch 3140/40000, Loss: 0.0001370726095046848, Learning Rate: 0.001372\n",
      "Epoch 3141/40000, Loss: 0.00015930597146507353, Learning Rate: 0.001372\n",
      "Epoch 3142/40000, Loss: 0.00021603272762149572, Learning Rate: 0.001372\n",
      "Epoch 3143/40000, Loss: 0.0002765143581200391, Learning Rate: 0.001372\n",
      "Epoch 3144/40000, Loss: 0.00023617286933586001, Learning Rate: 0.001371\n",
      "Epoch 3145/40000, Loss: 0.00012338509259279817, Learning Rate: 0.001371\n",
      "Epoch 3146/40000, Loss: 0.0004049894050695002, Learning Rate: 0.001371\n",
      "Epoch 3147/40000, Loss: 0.0003546745574567467, Learning Rate: 0.001371\n",
      "Epoch 3148/40000, Loss: 0.0001446409005438909, Learning Rate: 0.001371\n",
      "Epoch 3149/40000, Loss: 0.00010951294098049402, Learning Rate: 0.001371\n",
      "Epoch 3150/40000, Loss: 8.070892363321036e-05, Learning Rate: 0.001370\n",
      "Epoch 3151/40000, Loss: 0.0001450598647352308, Learning Rate: 0.001370\n",
      "Epoch 3152/40000, Loss: 0.00017424847465008497, Learning Rate: 0.001370\n",
      "Epoch 3153/40000, Loss: 8.848437573760748e-05, Learning Rate: 0.001370\n",
      "Epoch 3154/40000, Loss: 6.674754695268348e-05, Learning Rate: 0.001370\n",
      "Epoch 3155/40000, Loss: 0.00014123124128673226, Learning Rate: 0.001370\n",
      "Epoch 3156/40000, Loss: 0.00010961248335661367, Learning Rate: 0.001369\n",
      "Epoch 3157/40000, Loss: 7.467795512638986e-05, Learning Rate: 0.001369\n",
      "Epoch 3158/40000, Loss: 0.00015473258099518716, Learning Rate: 0.001369\n",
      "Epoch 3159/40000, Loss: 0.00015279313083738089, Learning Rate: 0.001369\n",
      "Epoch 3160/40000, Loss: 8.08183613116853e-05, Learning Rate: 0.001369\n",
      "Epoch 3161/40000, Loss: 6.746381404809654e-05, Learning Rate: 0.001369\n",
      "Epoch 3162/40000, Loss: 6.744505662936717e-05, Learning Rate: 0.001368\n",
      "Epoch 3163/40000, Loss: 8.670114766573533e-05, Learning Rate: 0.001368\n",
      "Epoch 3164/40000, Loss: 0.00012076000712113455, Learning Rate: 0.001368\n",
      "Epoch 3165/40000, Loss: 0.00021744811965618283, Learning Rate: 0.001368\n",
      "Epoch 3166/40000, Loss: 0.00021261641813907772, Learning Rate: 0.001368\n",
      "Epoch 3167/40000, Loss: 0.0001410263794241473, Learning Rate: 0.001368\n",
      "Epoch 3168/40000, Loss: 0.00013120223593432456, Learning Rate: 0.001367\n",
      "Epoch 3169/40000, Loss: 0.00023687331122346222, Learning Rate: 0.001367\n",
      "Epoch 3170/40000, Loss: 0.0002356571494601667, Learning Rate: 0.001367\n",
      "Epoch 3171/40000, Loss: 0.00013682518329005688, Learning Rate: 0.001367\n",
      "Epoch 3172/40000, Loss: 0.00014388006820809096, Learning Rate: 0.001367\n",
      "Epoch 3173/40000, Loss: 0.0001343585754511878, Learning Rate: 0.001367\n",
      "Epoch 3174/40000, Loss: 8.150465146172792e-05, Learning Rate: 0.001367\n",
      "Epoch 3175/40000, Loss: 0.00012194475857540965, Learning Rate: 0.001366\n",
      "Epoch 3176/40000, Loss: 0.00015022791922092438, Learning Rate: 0.001366\n",
      "Epoch 3177/40000, Loss: 0.00015324860578402877, Learning Rate: 0.001366\n",
      "Epoch 3178/40000, Loss: 0.0001176266378024593, Learning Rate: 0.001366\n",
      "Epoch 3179/40000, Loss: 0.00011617157724685967, Learning Rate: 0.001366\n",
      "Epoch 3180/40000, Loss: 0.00013951593427918851, Learning Rate: 0.001366\n",
      "Epoch 3181/40000, Loss: 0.00016281823627650738, Learning Rate: 0.001365\n",
      "Epoch 3182/40000, Loss: 0.00011924232239834964, Learning Rate: 0.001365\n",
      "Epoch 3183/40000, Loss: 0.00022757686383556575, Learning Rate: 0.001365\n",
      "Epoch 3184/40000, Loss: 0.00013701131683774292, Learning Rate: 0.001365\n",
      "Epoch 3185/40000, Loss: 0.0005163975292816758, Learning Rate: 0.001365\n",
      "Epoch 3186/40000, Loss: 0.00016103716916404665, Learning Rate: 0.001365\n",
      "Epoch 3187/40000, Loss: 0.0002042185515165329, Learning Rate: 0.001364\n",
      "Epoch 3188/40000, Loss: 0.00038945188862271607, Learning Rate: 0.001364\n",
      "Epoch 3189/40000, Loss: 0.0001546300045447424, Learning Rate: 0.001364\n",
      "Epoch 3190/40000, Loss: 0.00021548203949350864, Learning Rate: 0.001364\n",
      "Epoch 3191/40000, Loss: 0.00010029263648902997, Learning Rate: 0.001364\n",
      "Epoch 3192/40000, Loss: 0.00018966766947414726, Learning Rate: 0.001364\n",
      "Epoch 3193/40000, Loss: 0.00016839697491377592, Learning Rate: 0.001363\n",
      "Epoch 3194/40000, Loss: 0.00012713733303826302, Learning Rate: 0.001363\n",
      "Epoch 3195/40000, Loss: 0.00011239728337386623, Learning Rate: 0.001363\n",
      "Epoch 3196/40000, Loss: 0.00011724070645868778, Learning Rate: 0.001363\n",
      "Epoch 3197/40000, Loss: 0.00017217017011716962, Learning Rate: 0.001363\n",
      "Epoch 3198/40000, Loss: 0.00017369285342283547, Learning Rate: 0.001363\n",
      "Epoch 3199/40000, Loss: 0.0001288010971620679, Learning Rate: 0.001362\n",
      "Epoch 3200/40000, Loss: 0.00020476292411331087, Learning Rate: 0.001362\n",
      "Epoch 3201/40000, Loss: 0.00012108799273846671, Learning Rate: 0.001362\n",
      "Epoch 3202/40000, Loss: 0.00018931194790638983, Learning Rate: 0.001362\n",
      "Epoch 3203/40000, Loss: 9.403414878761396e-05, Learning Rate: 0.001362\n",
      "Epoch 3204/40000, Loss: 0.00036830236786045134, Learning Rate: 0.001362\n",
      "Epoch 3205/40000, Loss: 9.511739335721359e-05, Learning Rate: 0.001361\n",
      "Epoch 3206/40000, Loss: 0.00018228632688988, Learning Rate: 0.001361\n",
      "Epoch 3207/40000, Loss: 0.0001366926881019026, Learning Rate: 0.001361\n",
      "Epoch 3208/40000, Loss: 0.00028274802025407553, Learning Rate: 0.001361\n",
      "Epoch 3209/40000, Loss: 0.00013889194815419614, Learning Rate: 0.001361\n",
      "Epoch 3210/40000, Loss: 0.00011695526336552575, Learning Rate: 0.001361\n",
      "Epoch 3211/40000, Loss: 0.0001858115429058671, Learning Rate: 0.001360\n",
      "Epoch 3212/40000, Loss: 9.456279803998768e-05, Learning Rate: 0.001360\n",
      "Epoch 3213/40000, Loss: 0.00021771228057332337, Learning Rate: 0.001360\n",
      "Epoch 3214/40000, Loss: 0.0001626720913918689, Learning Rate: 0.001360\n",
      "Epoch 3215/40000, Loss: 0.0001769605150911957, Learning Rate: 0.001360\n",
      "Epoch 3216/40000, Loss: 0.0001962722890311852, Learning Rate: 0.001360\n",
      "Epoch 3217/40000, Loss: 8.969853661255911e-05, Learning Rate: 0.001359\n",
      "Epoch 3218/40000, Loss: 0.00018370785983279347, Learning Rate: 0.001359\n",
      "Epoch 3219/40000, Loss: 0.00017624982865527272, Learning Rate: 0.001359\n",
      "Epoch 3220/40000, Loss: 0.0001669795747147873, Learning Rate: 0.001359\n",
      "Epoch 3221/40000, Loss: 0.0001723266759654507, Learning Rate: 0.001359\n",
      "Epoch 3222/40000, Loss: 0.00021694468159694225, Learning Rate: 0.001359\n",
      "Epoch 3223/40000, Loss: 0.00022884676582179964, Learning Rate: 0.001359\n",
      "Epoch 3224/40000, Loss: 0.0001818279706640169, Learning Rate: 0.001358\n",
      "Epoch 3225/40000, Loss: 0.00010053294681711122, Learning Rate: 0.001358\n",
      "Epoch 3226/40000, Loss: 0.00019962069927714765, Learning Rate: 0.001358\n",
      "Epoch 3227/40000, Loss: 0.0002365392429055646, Learning Rate: 0.001358\n",
      "Epoch 3228/40000, Loss: 0.00023367404355667531, Learning Rate: 0.001358\n",
      "Epoch 3229/40000, Loss: 0.00022981844085734338, Learning Rate: 0.001358\n",
      "Epoch 3230/40000, Loss: 0.0002095842792186886, Learning Rate: 0.001357\n",
      "Epoch 3231/40000, Loss: 0.0001867298415163532, Learning Rate: 0.001357\n",
      "Epoch 3232/40000, Loss: 0.00017761571507435292, Learning Rate: 0.001357\n",
      "Epoch 3233/40000, Loss: 0.00020667845092248172, Learning Rate: 0.001357\n",
      "Epoch 3234/40000, Loss: 0.00010736795229604468, Learning Rate: 0.001357\n",
      "Epoch 3235/40000, Loss: 0.0001761503517627716, Learning Rate: 0.001357\n",
      "Epoch 3236/40000, Loss: 0.00017178722191601992, Learning Rate: 0.001356\n",
      "Epoch 3237/40000, Loss: 0.0001634169020690024, Learning Rate: 0.001356\n",
      "Epoch 3238/40000, Loss: 0.00011487019219202921, Learning Rate: 0.001356\n",
      "Epoch 3239/40000, Loss: 0.0001310766820097342, Learning Rate: 0.001356\n",
      "Epoch 3240/40000, Loss: 0.00016731755749788135, Learning Rate: 0.001356\n",
      "Epoch 3241/40000, Loss: 0.00018152959819417447, Learning Rate: 0.001356\n",
      "Epoch 3242/40000, Loss: 0.00021356107026804239, Learning Rate: 0.001355\n",
      "Epoch 3243/40000, Loss: 0.00030134417465887964, Learning Rate: 0.001355\n",
      "Epoch 3244/40000, Loss: 0.00014835625188425183, Learning Rate: 0.001355\n",
      "Epoch 3245/40000, Loss: 0.0003438883868511766, Learning Rate: 0.001355\n",
      "Epoch 3246/40000, Loss: 0.0002391091111348942, Learning Rate: 0.001355\n",
      "Epoch 3247/40000, Loss: 0.00014067591109778732, Learning Rate: 0.001355\n",
      "Epoch 3248/40000, Loss: 0.00016005632642190903, Learning Rate: 0.001354\n",
      "Epoch 3249/40000, Loss: 0.0002602395252324641, Learning Rate: 0.001354\n",
      "Epoch 3250/40000, Loss: 0.0001266540348296985, Learning Rate: 0.001354\n",
      "Epoch 3251/40000, Loss: 0.00011311215348541737, Learning Rate: 0.001354\n",
      "Epoch 3252/40000, Loss: 0.0004619714163709432, Learning Rate: 0.001354\n",
      "Epoch 3253/40000, Loss: 0.00031307648168876767, Learning Rate: 0.001354\n",
      "Epoch 3254/40000, Loss: 0.00018507000640965998, Learning Rate: 0.001353\n",
      "Epoch 3255/40000, Loss: 0.0002110818022629246, Learning Rate: 0.001353\n",
      "Epoch 3256/40000, Loss: 0.00012440264981705695, Learning Rate: 0.001353\n",
      "Epoch 3257/40000, Loss: 0.0002760650822892785, Learning Rate: 0.001353\n",
      "Epoch 3258/40000, Loss: 0.00015765413991175592, Learning Rate: 0.001353\n",
      "Epoch 3259/40000, Loss: 0.00017624731117393821, Learning Rate: 0.001353\n",
      "Epoch 3260/40000, Loss: 0.00039350613951683044, Learning Rate: 0.001352\n",
      "Epoch 3261/40000, Loss: 0.0002187772624893114, Learning Rate: 0.001352\n",
      "Epoch 3262/40000, Loss: 0.00011116084351669997, Learning Rate: 0.001352\n",
      "Epoch 3263/40000, Loss: 0.00010027306416304782, Learning Rate: 0.001352\n",
      "Epoch 3264/40000, Loss: 0.00032323249615728855, Learning Rate: 0.001352\n",
      "Epoch 3265/40000, Loss: 0.00013359723379835486, Learning Rate: 0.001352\n",
      "Epoch 3266/40000, Loss: 0.0001844579674070701, Learning Rate: 0.001352\n",
      "Epoch 3267/40000, Loss: 0.00012615739251486957, Learning Rate: 0.001351\n",
      "Epoch 3268/40000, Loss: 8.390525908907875e-05, Learning Rate: 0.001351\n",
      "Epoch 3269/40000, Loss: 0.00012293549661990255, Learning Rate: 0.001351\n",
      "Epoch 3270/40000, Loss: 0.00011201012239325792, Learning Rate: 0.001351\n",
      "Epoch 3271/40000, Loss: 0.0001080938964150846, Learning Rate: 0.001351\n",
      "Epoch 3272/40000, Loss: 0.0001596487854840234, Learning Rate: 0.001351\n",
      "Epoch 3273/40000, Loss: 8.249546226579696e-05, Learning Rate: 0.001350\n",
      "Epoch 3274/40000, Loss: 7.806104986229911e-05, Learning Rate: 0.001350\n",
      "Epoch 3275/40000, Loss: 8.463342965114862e-05, Learning Rate: 0.001350\n",
      "Epoch 3276/40000, Loss: 0.00013430786202661693, Learning Rate: 0.001350\n",
      "Epoch 3277/40000, Loss: 0.00015738427464384586, Learning Rate: 0.001350\n",
      "Epoch 3278/40000, Loss: 0.00015403068391606212, Learning Rate: 0.001350\n",
      "Epoch 3279/40000, Loss: 0.0001021076095639728, Learning Rate: 0.001349\n",
      "Epoch 3280/40000, Loss: 0.00016904581570997834, Learning Rate: 0.001349\n",
      "Epoch 3281/40000, Loss: 0.00010466374078532681, Learning Rate: 0.001349\n",
      "Epoch 3282/40000, Loss: 0.00010104308603331447, Learning Rate: 0.001349\n",
      "Epoch 3283/40000, Loss: 7.649374310858548e-05, Learning Rate: 0.001349\n",
      "Epoch 3284/40000, Loss: 7.793869008310139e-05, Learning Rate: 0.001349\n",
      "Epoch 3285/40000, Loss: 0.00011175357212778181, Learning Rate: 0.001348\n",
      "Epoch 3286/40000, Loss: 0.0001619418035261333, Learning Rate: 0.001348\n",
      "Epoch 3287/40000, Loss: 9.837194374995306e-05, Learning Rate: 0.001348\n",
      "Epoch 3288/40000, Loss: 0.00010823402408277616, Learning Rate: 0.001348\n",
      "Epoch 3289/40000, Loss: 8.08625845820643e-05, Learning Rate: 0.001348\n",
      "Epoch 3290/40000, Loss: 0.00013676981325261295, Learning Rate: 0.001348\n",
      "Epoch 3291/40000, Loss: 0.00015937350690364838, Learning Rate: 0.001347\n",
      "Epoch 3292/40000, Loss: 0.00010364007175667211, Learning Rate: 0.001347\n",
      "Epoch 3293/40000, Loss: 0.0001269844506168738, Learning Rate: 0.001347\n",
      "Epoch 3294/40000, Loss: 0.00012942921603098512, Learning Rate: 0.001347\n",
      "Epoch 3295/40000, Loss: 0.00015704735415056348, Learning Rate: 0.001347\n",
      "Epoch 3296/40000, Loss: 0.000189401296665892, Learning Rate: 0.001347\n",
      "Epoch 3297/40000, Loss: 0.0001456053287256509, Learning Rate: 0.001346\n",
      "Epoch 3298/40000, Loss: 0.00012233882443979383, Learning Rate: 0.001346\n",
      "Epoch 3299/40000, Loss: 8.890464960131794e-05, Learning Rate: 0.001346\n",
      "Epoch 3300/40000, Loss: 0.00016329475329257548, Learning Rate: 0.001346\n",
      "Epoch 3301/40000, Loss: 0.00015461083967238665, Learning Rate: 0.001346\n",
      "Epoch 3302/40000, Loss: 7.105081749614328e-05, Learning Rate: 0.001346\n",
      "Epoch 3303/40000, Loss: 7.356380228884518e-05, Learning Rate: 0.001346\n",
      "Epoch 3304/40000, Loss: 0.0001559304364491254, Learning Rate: 0.001345\n",
      "Epoch 3305/40000, Loss: 9.483098256168887e-05, Learning Rate: 0.001345\n",
      "Epoch 3306/40000, Loss: 0.00012940671877004206, Learning Rate: 0.001345\n",
      "Epoch 3307/40000, Loss: 0.00012493011308833957, Learning Rate: 0.001345\n",
      "Epoch 3308/40000, Loss: 0.00021765798737760633, Learning Rate: 0.001345\n",
      "Epoch 3309/40000, Loss: 0.00013434361608233303, Learning Rate: 0.001345\n",
      "Epoch 3310/40000, Loss: 0.0002071568014798686, Learning Rate: 0.001344\n",
      "Epoch 3311/40000, Loss: 0.00011790502321673557, Learning Rate: 0.001344\n",
      "Epoch 3312/40000, Loss: 0.0003947037912439555, Learning Rate: 0.001344\n",
      "Epoch 3313/40000, Loss: 0.00020385706739034504, Learning Rate: 0.001344\n",
      "Epoch 3314/40000, Loss: 0.0001511879381723702, Learning Rate: 0.001344\n",
      "Epoch 3315/40000, Loss: 0.00022214748605620116, Learning Rate: 0.001344\n",
      "Epoch 3316/40000, Loss: 0.00021642934007104486, Learning Rate: 0.001343\n",
      "Epoch 3317/40000, Loss: 0.0002643823972903192, Learning Rate: 0.001343\n",
      "Epoch 3318/40000, Loss: 0.0002062860585283488, Learning Rate: 0.001343\n",
      "Epoch 3319/40000, Loss: 0.00014194521645549685, Learning Rate: 0.001343\n",
      "Epoch 3320/40000, Loss: 0.000130035710753873, Learning Rate: 0.001343\n",
      "Epoch 3321/40000, Loss: 0.00018506712513044477, Learning Rate: 0.001343\n",
      "Epoch 3322/40000, Loss: 0.0001793661358533427, Learning Rate: 0.001342\n",
      "Epoch 3323/40000, Loss: 0.0001511224254500121, Learning Rate: 0.001342\n",
      "Epoch 3324/40000, Loss: 0.00013296464749146253, Learning Rate: 0.001342\n",
      "Epoch 3325/40000, Loss: 0.0001420028565917164, Learning Rate: 0.001342\n",
      "Epoch 3326/40000, Loss: 0.0001843859936343506, Learning Rate: 0.001342\n",
      "Epoch 3327/40000, Loss: 0.0002502277202438563, Learning Rate: 0.001342\n",
      "Epoch 3328/40000, Loss: 0.0001761974854161963, Learning Rate: 0.001341\n",
      "Epoch 3329/40000, Loss: 0.00014080390974413604, Learning Rate: 0.001341\n",
      "Epoch 3330/40000, Loss: 9.901514567900449e-05, Learning Rate: 0.001341\n",
      "Epoch 3331/40000, Loss: 0.00024731384473852813, Learning Rate: 0.001341\n",
      "Epoch 3332/40000, Loss: 0.00018112114048562944, Learning Rate: 0.001341\n",
      "Epoch 3333/40000, Loss: 8.876086212694645e-05, Learning Rate: 0.001341\n",
      "Epoch 3334/40000, Loss: 0.00012767314910888672, Learning Rate: 0.001341\n",
      "Epoch 3335/40000, Loss: 0.00012146121298428625, Learning Rate: 0.001340\n",
      "Epoch 3336/40000, Loss: 0.00016205260180868208, Learning Rate: 0.001340\n",
      "Epoch 3337/40000, Loss: 8.384107059100643e-05, Learning Rate: 0.001340\n",
      "Epoch 3338/40000, Loss: 0.00011813648598035797, Learning Rate: 0.001340\n",
      "Epoch 3339/40000, Loss: 0.00018877399270422757, Learning Rate: 0.001340\n",
      "Epoch 3340/40000, Loss: 0.00018702383385971189, Learning Rate: 0.001340\n",
      "Epoch 3341/40000, Loss: 0.00015949472435750067, Learning Rate: 0.001339\n",
      "Epoch 3342/40000, Loss: 7.081896183080971e-05, Learning Rate: 0.001339\n",
      "Epoch 3343/40000, Loss: 0.00011209225340280682, Learning Rate: 0.001339\n",
      "Epoch 3344/40000, Loss: 0.0001722514716675505, Learning Rate: 0.001339\n",
      "Epoch 3345/40000, Loss: 0.00015775772044435143, Learning Rate: 0.001339\n",
      "Epoch 3346/40000, Loss: 0.00010607055446598679, Learning Rate: 0.001339\n",
      "Epoch 3347/40000, Loss: 0.0002988360938616097, Learning Rate: 0.001338\n",
      "Epoch 3348/40000, Loss: 0.00013065777602605522, Learning Rate: 0.001338\n",
      "Epoch 3349/40000, Loss: 0.0001809045934351161, Learning Rate: 0.001338\n",
      "Epoch 3350/40000, Loss: 8.094986696960405e-05, Learning Rate: 0.001338\n",
      "Epoch 3351/40000, Loss: 0.00011074577196268365, Learning Rate: 0.001338\n",
      "Epoch 3352/40000, Loss: 0.00015980115858837962, Learning Rate: 0.001338\n",
      "Epoch 3353/40000, Loss: 6.383707659551874e-05, Learning Rate: 0.001337\n",
      "Epoch 3354/40000, Loss: 0.00015649983834009618, Learning Rate: 0.001337\n",
      "Epoch 3355/40000, Loss: 0.0001094643012038432, Learning Rate: 0.001337\n",
      "Epoch 3356/40000, Loss: 0.00022687471937388182, Learning Rate: 0.001337\n",
      "Epoch 3357/40000, Loss: 0.0001360738679068163, Learning Rate: 0.001337\n",
      "Epoch 3358/40000, Loss: 0.00010983196261804551, Learning Rate: 0.001337\n",
      "Epoch 3359/40000, Loss: 7.168253796407953e-05, Learning Rate: 0.001337\n",
      "Epoch 3360/40000, Loss: 0.0002019957610173151, Learning Rate: 0.001336\n",
      "Epoch 3361/40000, Loss: 7.890100096119568e-05, Learning Rate: 0.001336\n",
      "Epoch 3362/40000, Loss: 7.663256110390648e-05, Learning Rate: 0.001336\n",
      "Epoch 3363/40000, Loss: 5.7555706007406116e-05, Learning Rate: 0.001336\n",
      "Epoch 3364/40000, Loss: 0.00010534949251450598, Learning Rate: 0.001336\n",
      "Epoch 3365/40000, Loss: 9.901558951241896e-05, Learning Rate: 0.001336\n",
      "Epoch 3366/40000, Loss: 0.00014438401558436453, Learning Rate: 0.001335\n",
      "Epoch 3367/40000, Loss: 9.736570063978434e-05, Learning Rate: 0.001335\n",
      "Epoch 3368/40000, Loss: 0.00010334454418625683, Learning Rate: 0.001335\n",
      "Epoch 3369/40000, Loss: 6.501957977889106e-05, Learning Rate: 0.001335\n",
      "Epoch 3370/40000, Loss: 0.00011540085688466206, Learning Rate: 0.001335\n",
      "Epoch 3371/40000, Loss: 0.0001508276181994006, Learning Rate: 0.001335\n",
      "Epoch 3372/40000, Loss: 6.31324146525003e-05, Learning Rate: 0.001334\n",
      "Epoch 3373/40000, Loss: 0.0001871821441454813, Learning Rate: 0.001334\n",
      "Epoch 3374/40000, Loss: 0.00013497320469468832, Learning Rate: 0.001334\n",
      "Epoch 3375/40000, Loss: 9.088777733268216e-05, Learning Rate: 0.001334\n",
      "Epoch 3376/40000, Loss: 0.00017785669479053468, Learning Rate: 0.001334\n",
      "Epoch 3377/40000, Loss: 7.49395985621959e-05, Learning Rate: 0.001334\n",
      "Epoch 3378/40000, Loss: 0.00014359093620441854, Learning Rate: 0.001333\n",
      "Epoch 3379/40000, Loss: 0.00014324428047984838, Learning Rate: 0.001333\n",
      "Epoch 3380/40000, Loss: 0.00019639609672594815, Learning Rate: 0.001333\n",
      "Epoch 3381/40000, Loss: 9.108838276006281e-05, Learning Rate: 0.001333\n",
      "Epoch 3382/40000, Loss: 0.000320151710184291, Learning Rate: 0.001333\n",
      "Epoch 3383/40000, Loss: 0.00015093186812009662, Learning Rate: 0.001333\n",
      "Epoch 3384/40000, Loss: 0.00010708435002015904, Learning Rate: 0.001333\n",
      "Epoch 3385/40000, Loss: 0.0001912367733893916, Learning Rate: 0.001332\n",
      "Epoch 3386/40000, Loss: 0.0001290383079322055, Learning Rate: 0.001332\n",
      "Epoch 3387/40000, Loss: 0.00017392999143339694, Learning Rate: 0.001332\n",
      "Epoch 3388/40000, Loss: 0.00020481263345573097, Learning Rate: 0.001332\n",
      "Epoch 3389/40000, Loss: 0.0003216871991753578, Learning Rate: 0.001332\n",
      "Epoch 3390/40000, Loss: 0.00019661612168420106, Learning Rate: 0.001332\n",
      "Epoch 3391/40000, Loss: 8.739254553802311e-05, Learning Rate: 0.001331\n",
      "Epoch 3392/40000, Loss: 0.00014407226990442723, Learning Rate: 0.001331\n",
      "Epoch 3393/40000, Loss: 0.00023311511904466897, Learning Rate: 0.001331\n",
      "Epoch 3394/40000, Loss: 0.0001861217460827902, Learning Rate: 0.001331\n",
      "Epoch 3395/40000, Loss: 0.00017025507986545563, Learning Rate: 0.001331\n",
      "Epoch 3396/40000, Loss: 0.00011912725312868133, Learning Rate: 0.001331\n",
      "Epoch 3397/40000, Loss: 0.00010726519394665956, Learning Rate: 0.001330\n",
      "Epoch 3398/40000, Loss: 7.872878632042557e-05, Learning Rate: 0.001330\n",
      "Epoch 3399/40000, Loss: 0.00024757065693847835, Learning Rate: 0.001330\n",
      "Epoch 3400/40000, Loss: 0.00014829522115178406, Learning Rate: 0.001330\n",
      "Epoch 3401/40000, Loss: 0.00023029885778669268, Learning Rate: 0.001330\n",
      "Epoch 3402/40000, Loss: 0.00024069448409136385, Learning Rate: 0.001330\n",
      "Epoch 3403/40000, Loss: 0.0001579625968588516, Learning Rate: 0.001329\n",
      "Epoch 3404/40000, Loss: 0.0005760471685789526, Learning Rate: 0.001329\n",
      "Epoch 3405/40000, Loss: 0.00031344889430329204, Learning Rate: 0.001329\n",
      "Epoch 3406/40000, Loss: 0.00014891158207319677, Learning Rate: 0.001329\n",
      "Epoch 3407/40000, Loss: 0.00021249290148261935, Learning Rate: 0.001329\n",
      "Epoch 3408/40000, Loss: 0.00018622296920511872, Learning Rate: 0.001329\n",
      "Epoch 3409/40000, Loss: 0.00012249818246345967, Learning Rate: 0.001329\n",
      "Epoch 3410/40000, Loss: 0.00020218017743900418, Learning Rate: 0.001328\n",
      "Epoch 3411/40000, Loss: 0.00010742682934505865, Learning Rate: 0.001328\n",
      "Epoch 3412/40000, Loss: 0.00013860466424375772, Learning Rate: 0.001328\n",
      "Epoch 3413/40000, Loss: 0.0003056477289646864, Learning Rate: 0.001328\n",
      "Epoch 3414/40000, Loss: 0.00012356236402411014, Learning Rate: 0.001328\n",
      "Epoch 3415/40000, Loss: 0.00013960138312540948, Learning Rate: 0.001328\n",
      "Epoch 3416/40000, Loss: 0.000106466279248707, Learning Rate: 0.001327\n",
      "Epoch 3417/40000, Loss: 0.0002357133198529482, Learning Rate: 0.001327\n",
      "Epoch 3418/40000, Loss: 0.00016765437612775713, Learning Rate: 0.001327\n",
      "Epoch 3419/40000, Loss: 0.00010510325955692679, Learning Rate: 0.001327\n",
      "Epoch 3420/40000, Loss: 0.0001044852178893052, Learning Rate: 0.001327\n",
      "Epoch 3421/40000, Loss: 7.515534525737166e-05, Learning Rate: 0.001327\n",
      "Epoch 3422/40000, Loss: 0.00016948206757660955, Learning Rate: 0.001326\n",
      "Epoch 3423/40000, Loss: 0.00010995593765983358, Learning Rate: 0.001326\n",
      "Epoch 3424/40000, Loss: 0.00010200651740888134, Learning Rate: 0.001326\n",
      "Epoch 3425/40000, Loss: 0.00017763876530807465, Learning Rate: 0.001326\n",
      "Epoch 3426/40000, Loss: 0.00010908536205533892, Learning Rate: 0.001326\n",
      "Epoch 3427/40000, Loss: 0.00017294412828050554, Learning Rate: 0.001326\n",
      "Epoch 3428/40000, Loss: 7.863011705921963e-05, Learning Rate: 0.001325\n",
      "Epoch 3429/40000, Loss: 0.00011757301399484277, Learning Rate: 0.001325\n",
      "Epoch 3430/40000, Loss: 0.00014638417633250356, Learning Rate: 0.001325\n",
      "Epoch 3431/40000, Loss: 0.0001100025256164372, Learning Rate: 0.001325\n",
      "Epoch 3432/40000, Loss: 0.00021191156702116132, Learning Rate: 0.001325\n",
      "Epoch 3433/40000, Loss: 0.00018776026263367385, Learning Rate: 0.001325\n",
      "Epoch 3434/40000, Loss: 0.00010117329657077789, Learning Rate: 0.001325\n",
      "Epoch 3435/40000, Loss: 8.973031799541786e-05, Learning Rate: 0.001324\n",
      "Epoch 3436/40000, Loss: 0.00016802460595499724, Learning Rate: 0.001324\n",
      "Epoch 3437/40000, Loss: 7.618610106874257e-05, Learning Rate: 0.001324\n",
      "Epoch 3438/40000, Loss: 0.00012142208288423717, Learning Rate: 0.001324\n",
      "Epoch 3439/40000, Loss: 8.71733864187263e-05, Learning Rate: 0.001324\n",
      "Epoch 3440/40000, Loss: 0.00015930675726849586, Learning Rate: 0.001324\n",
      "Epoch 3441/40000, Loss: 0.000154814581037499, Learning Rate: 0.001323\n",
      "Epoch 3442/40000, Loss: 8.630682714283466e-05, Learning Rate: 0.001323\n",
      "Epoch 3443/40000, Loss: 0.00015977431030478328, Learning Rate: 0.001323\n",
      "Epoch 3444/40000, Loss: 0.00021180308249313384, Learning Rate: 0.001323\n",
      "Epoch 3445/40000, Loss: 0.00019473150314297527, Learning Rate: 0.001323\n",
      "Epoch 3446/40000, Loss: 0.00025401302264072, Learning Rate: 0.001323\n",
      "Epoch 3447/40000, Loss: 0.0001358947774861008, Learning Rate: 0.001322\n",
      "Epoch 3448/40000, Loss: 0.0003284738922957331, Learning Rate: 0.001322\n",
      "Epoch 3449/40000, Loss: 0.00027130998205393553, Learning Rate: 0.001322\n",
      "Epoch 3450/40000, Loss: 0.00021605781512334943, Learning Rate: 0.001322\n",
      "Epoch 3451/40000, Loss: 0.0001782310428097844, Learning Rate: 0.001322\n",
      "Epoch 3452/40000, Loss: 0.00028834794647991657, Learning Rate: 0.001322\n",
      "Epoch 3453/40000, Loss: 0.000312849908368662, Learning Rate: 0.001322\n",
      "Epoch 3454/40000, Loss: 0.0006477470160461962, Learning Rate: 0.001321\n",
      "Epoch 3455/40000, Loss: 0.0002811555750668049, Learning Rate: 0.001321\n",
      "Epoch 3456/40000, Loss: 0.00021973809634801, Learning Rate: 0.001321\n",
      "Epoch 3457/40000, Loss: 0.0002255530998809263, Learning Rate: 0.001321\n",
      "Epoch 3458/40000, Loss: 0.00022081045608501881, Learning Rate: 0.001321\n",
      "Epoch 3459/40000, Loss: 0.00028136002947576344, Learning Rate: 0.001321\n",
      "Epoch 3460/40000, Loss: 0.00018452371296007186, Learning Rate: 0.001320\n",
      "Epoch 3461/40000, Loss: 9.76249502855353e-05, Learning Rate: 0.001320\n",
      "Epoch 3462/40000, Loss: 0.00021131565154064447, Learning Rate: 0.001320\n",
      "Epoch 3463/40000, Loss: 0.0001554736663820222, Learning Rate: 0.001320\n",
      "Epoch 3464/40000, Loss: 0.00014851354353595525, Learning Rate: 0.001320\n",
      "Epoch 3465/40000, Loss: 0.00022198421356733888, Learning Rate: 0.001320\n",
      "Epoch 3466/40000, Loss: 0.0001169831957668066, Learning Rate: 0.001319\n",
      "Epoch 3467/40000, Loss: 0.00012303800031077117, Learning Rate: 0.001319\n",
      "Epoch 3468/40000, Loss: 0.00010939886851701885, Learning Rate: 0.001319\n",
      "Epoch 3469/40000, Loss: 0.00011452478793216869, Learning Rate: 0.001319\n",
      "Epoch 3470/40000, Loss: 0.00020143811707384884, Learning Rate: 0.001319\n",
      "Epoch 3471/40000, Loss: 0.00026307840016670525, Learning Rate: 0.001319\n",
      "Epoch 3472/40000, Loss: 0.0001263103331439197, Learning Rate: 0.001319\n",
      "Epoch 3473/40000, Loss: 0.00010380752792116255, Learning Rate: 0.001318\n",
      "Epoch 3474/40000, Loss: 9.429801866644993e-05, Learning Rate: 0.001318\n",
      "Epoch 3475/40000, Loss: 0.00017357998876832426, Learning Rate: 0.001318\n",
      "Epoch 3476/40000, Loss: 8.663292828714475e-05, Learning Rate: 0.001318\n",
      "Epoch 3477/40000, Loss: 0.00011150739737786353, Learning Rate: 0.001318\n",
      "Epoch 3478/40000, Loss: 0.00011264505883445963, Learning Rate: 0.001318\n",
      "Epoch 3479/40000, Loss: 0.0002498793473932892, Learning Rate: 0.001317\n",
      "Epoch 3480/40000, Loss: 8.852524479152635e-05, Learning Rate: 0.001317\n",
      "Epoch 3481/40000, Loss: 8.440247620455921e-05, Learning Rate: 0.001317\n",
      "Epoch 3482/40000, Loss: 0.0001948313001776114, Learning Rate: 0.001317\n",
      "Epoch 3483/40000, Loss: 7.207815360743552e-05, Learning Rate: 0.001317\n",
      "Epoch 3484/40000, Loss: 6.075432975194417e-05, Learning Rate: 0.001317\n",
      "Epoch 3485/40000, Loss: 6.334867794066668e-05, Learning Rate: 0.001316\n",
      "Epoch 3486/40000, Loss: 0.00015805996372364461, Learning Rate: 0.001316\n",
      "Epoch 3487/40000, Loss: 8.445634739473462e-05, Learning Rate: 0.001316\n",
      "Epoch 3488/40000, Loss: 6.738900992786512e-05, Learning Rate: 0.001316\n",
      "Epoch 3489/40000, Loss: 6.650266732322052e-05, Learning Rate: 0.001316\n",
      "Epoch 3490/40000, Loss: 0.0001779526937752962, Learning Rate: 0.001316\n",
      "Epoch 3491/40000, Loss: 9.472308738622814e-05, Learning Rate: 0.001316\n",
      "Epoch 3492/40000, Loss: 0.0001806034124456346, Learning Rate: 0.001315\n",
      "Epoch 3493/40000, Loss: 0.00019505067029967904, Learning Rate: 0.001315\n",
      "Epoch 3494/40000, Loss: 0.00010949525312753394, Learning Rate: 0.001315\n",
      "Epoch 3495/40000, Loss: 0.00010441509948577732, Learning Rate: 0.001315\n",
      "Epoch 3496/40000, Loss: 0.0002358181809540838, Learning Rate: 0.001315\n",
      "Epoch 3497/40000, Loss: 0.00025311880744993687, Learning Rate: 0.001315\n",
      "Epoch 3498/40000, Loss: 0.0001858157047536224, Learning Rate: 0.001314\n",
      "Epoch 3499/40000, Loss: 0.0003499099111650139, Learning Rate: 0.001314\n",
      "Epoch 3500/40000, Loss: 0.00020253747061360627, Learning Rate: 0.001314\n",
      "Epoch 3501/40000, Loss: 0.00021710175496991724, Learning Rate: 0.001314\n",
      "Epoch 3502/40000, Loss: 0.000255109480349347, Learning Rate: 0.001314\n",
      "Epoch 3503/40000, Loss: 0.0001509229332441464, Learning Rate: 0.001314\n",
      "Epoch 3504/40000, Loss: 0.0002965903258882463, Learning Rate: 0.001313\n",
      "Epoch 3505/40000, Loss: 0.0003527380176819861, Learning Rate: 0.001313\n",
      "Epoch 3506/40000, Loss: 0.00011380271462257951, Learning Rate: 0.001313\n",
      "Epoch 3507/40000, Loss: 0.00016899258480407298, Learning Rate: 0.001313\n",
      "Epoch 3508/40000, Loss: 0.0001236215903190896, Learning Rate: 0.001313\n",
      "Epoch 3509/40000, Loss: 0.00016724647139199078, Learning Rate: 0.001313\n",
      "Epoch 3510/40000, Loss: 0.00015542653272859752, Learning Rate: 0.001313\n",
      "Epoch 3511/40000, Loss: 0.00014203977480065078, Learning Rate: 0.001312\n",
      "Epoch 3512/40000, Loss: 0.00020035995112266392, Learning Rate: 0.001312\n",
      "Epoch 3513/40000, Loss: 0.0001712191296974197, Learning Rate: 0.001312\n",
      "Epoch 3514/40000, Loss: 9.640170173952356e-05, Learning Rate: 0.001312\n",
      "Epoch 3515/40000, Loss: 7.094212924130261e-05, Learning Rate: 0.001312\n",
      "Epoch 3516/40000, Loss: 8.490475011058152e-05, Learning Rate: 0.001312\n",
      "Epoch 3517/40000, Loss: 0.00013803370529785752, Learning Rate: 0.001311\n",
      "Epoch 3518/40000, Loss: 0.00016599800437688828, Learning Rate: 0.001311\n",
      "Epoch 3519/40000, Loss: 0.00017873148317448795, Learning Rate: 0.001311\n",
      "Epoch 3520/40000, Loss: 0.0001271276705665514, Learning Rate: 0.001311\n",
      "Epoch 3521/40000, Loss: 0.0002243660419480875, Learning Rate: 0.001311\n",
      "Epoch 3522/40000, Loss: 0.0001532496535219252, Learning Rate: 0.001311\n",
      "Epoch 3523/40000, Loss: 8.871984027791768e-05, Learning Rate: 0.001310\n",
      "Epoch 3524/40000, Loss: 0.00013287602632772177, Learning Rate: 0.001310\n",
      "Epoch 3525/40000, Loss: 0.00014720679610036314, Learning Rate: 0.001310\n",
      "Epoch 3526/40000, Loss: 0.00010217981616733596, Learning Rate: 0.001310\n",
      "Epoch 3527/40000, Loss: 0.00010173684131586924, Learning Rate: 0.001310\n",
      "Epoch 3528/40000, Loss: 9.921329910866916e-05, Learning Rate: 0.001310\n",
      "Epoch 3529/40000, Loss: 0.00016622051771264523, Learning Rate: 0.001310\n",
      "Epoch 3530/40000, Loss: 0.0001609430182725191, Learning Rate: 0.001309\n",
      "Epoch 3531/40000, Loss: 0.0001448462571715936, Learning Rate: 0.001309\n",
      "Epoch 3532/40000, Loss: 0.00014294676657300442, Learning Rate: 0.001309\n",
      "Epoch 3533/40000, Loss: 0.00013895283336751163, Learning Rate: 0.001309\n",
      "Epoch 3534/40000, Loss: 9.874581155600026e-05, Learning Rate: 0.001309\n",
      "Epoch 3535/40000, Loss: 0.0001458667975384742, Learning Rate: 0.001309\n",
      "Epoch 3536/40000, Loss: 9.474362741457298e-05, Learning Rate: 0.001308\n",
      "Epoch 3537/40000, Loss: 9.912023961078376e-05, Learning Rate: 0.001308\n",
      "Epoch 3538/40000, Loss: 0.0001386434305459261, Learning Rate: 0.001308\n",
      "Epoch 3539/40000, Loss: 0.00013567622227128595, Learning Rate: 0.001308\n",
      "Epoch 3540/40000, Loss: 0.00013850080722477287, Learning Rate: 0.001308\n",
      "Epoch 3541/40000, Loss: 5.290027547744103e-05, Learning Rate: 0.001308\n",
      "Epoch 3542/40000, Loss: 9.915826376527548e-05, Learning Rate: 0.001307\n",
      "Epoch 3543/40000, Loss: 9.287215652875602e-05, Learning Rate: 0.001307\n",
      "Epoch 3544/40000, Loss: 5.154587415745482e-05, Learning Rate: 0.001307\n",
      "Epoch 3545/40000, Loss: 9.502519242232665e-05, Learning Rate: 0.001307\n",
      "Epoch 3546/40000, Loss: 7.19692325219512e-05, Learning Rate: 0.001307\n",
      "Epoch 3547/40000, Loss: 5.391863305703737e-05, Learning Rate: 0.001307\n",
      "Epoch 3548/40000, Loss: 0.00010747083433670923, Learning Rate: 0.001307\n",
      "Epoch 3549/40000, Loss: 0.00013747450429946184, Learning Rate: 0.001306\n",
      "Epoch 3550/40000, Loss: 0.00013801618479192257, Learning Rate: 0.001306\n",
      "Epoch 3551/40000, Loss: 0.00013511668657884002, Learning Rate: 0.001306\n",
      "Epoch 3552/40000, Loss: 9.388501348439604e-05, Learning Rate: 0.001306\n",
      "Epoch 3553/40000, Loss: 0.00013954535825178027, Learning Rate: 0.001306\n",
      "Epoch 3554/40000, Loss: 0.00013433799904305488, Learning Rate: 0.001306\n",
      "Epoch 3555/40000, Loss: 0.00010118004865944386, Learning Rate: 0.001305\n",
      "Epoch 3556/40000, Loss: 9.392437641508877e-05, Learning Rate: 0.001305\n",
      "Epoch 3557/40000, Loss: 0.00013914576265960932, Learning Rate: 0.001305\n",
      "Epoch 3558/40000, Loss: 0.00013311752991285175, Learning Rate: 0.001305\n",
      "Epoch 3559/40000, Loss: 9.85603837762028e-05, Learning Rate: 0.001305\n",
      "Epoch 3560/40000, Loss: 9.810939081944525e-05, Learning Rate: 0.001305\n",
      "Epoch 3561/40000, Loss: 0.00013330271758604795, Learning Rate: 0.001305\n",
      "Epoch 3562/40000, Loss: 5.629619045066647e-05, Learning Rate: 0.001304\n",
      "Epoch 3563/40000, Loss: 0.00013717875117436051, Learning Rate: 0.001304\n",
      "Epoch 3564/40000, Loss: 0.00010104684042744339, Learning Rate: 0.001304\n",
      "Epoch 3565/40000, Loss: 9.918306022882462e-05, Learning Rate: 0.001304\n",
      "Epoch 3566/40000, Loss: 0.00013423072232399136, Learning Rate: 0.001304\n",
      "Epoch 3567/40000, Loss: 0.000140935109811835, Learning Rate: 0.001304\n",
      "Epoch 3568/40000, Loss: 9.670240251580253e-05, Learning Rate: 0.001303\n",
      "Epoch 3569/40000, Loss: 7.53730273572728e-05, Learning Rate: 0.001303\n",
      "Epoch 3570/40000, Loss: 6.871893856441602e-05, Learning Rate: 0.001303\n",
      "Epoch 3571/40000, Loss: 0.00012024661555187777, Learning Rate: 0.001303\n",
      "Epoch 3572/40000, Loss: 0.00010576990462141111, Learning Rate: 0.001303\n",
      "Epoch 3573/40000, Loss: 0.00015461043221876025, Learning Rate: 0.001303\n",
      "Epoch 3574/40000, Loss: 0.00021423507132567465, Learning Rate: 0.001302\n",
      "Epoch 3575/40000, Loss: 9.80743279797025e-05, Learning Rate: 0.001302\n",
      "Epoch 3576/40000, Loss: 0.00020571600180119276, Learning Rate: 0.001302\n",
      "Epoch 3577/40000, Loss: 0.00028978323098272085, Learning Rate: 0.001302\n",
      "Epoch 3578/40000, Loss: 0.0004712500376626849, Learning Rate: 0.001302\n",
      "Epoch 3579/40000, Loss: 0.0002537276886869222, Learning Rate: 0.001302\n",
      "Epoch 3580/40000, Loss: 0.00033647805685177445, Learning Rate: 0.001302\n",
      "Epoch 3581/40000, Loss: 0.00027286141994409263, Learning Rate: 0.001301\n",
      "Epoch 3582/40000, Loss: 0.00031077105086296797, Learning Rate: 0.001301\n",
      "Epoch 3583/40000, Loss: 0.00024043383018579334, Learning Rate: 0.001301\n",
      "Epoch 3584/40000, Loss: 9.911818779073656e-05, Learning Rate: 0.001301\n",
      "Epoch 3585/40000, Loss: 0.00040336084202863276, Learning Rate: 0.001301\n",
      "Epoch 3586/40000, Loss: 0.00016508364933542907, Learning Rate: 0.001301\n",
      "Epoch 3587/40000, Loss: 0.00020349906117189676, Learning Rate: 0.001300\n",
      "Epoch 3588/40000, Loss: 0.0002073179348371923, Learning Rate: 0.001300\n",
      "Epoch 3589/40000, Loss: 0.00013023721112404019, Learning Rate: 0.001300\n",
      "Epoch 3590/40000, Loss: 0.00018740302766673267, Learning Rate: 0.001300\n",
      "Epoch 3591/40000, Loss: 0.00014531888882629573, Learning Rate: 0.001300\n",
      "Epoch 3592/40000, Loss: 0.00011439529043855146, Learning Rate: 0.001300\n",
      "Epoch 3593/40000, Loss: 0.00010156159260077402, Learning Rate: 0.001300\n",
      "Epoch 3594/40000, Loss: 0.00011140849528601393, Learning Rate: 0.001299\n",
      "Epoch 3595/40000, Loss: 0.00017285531794186682, Learning Rate: 0.001299\n",
      "Epoch 3596/40000, Loss: 0.00014109243056736887, Learning Rate: 0.001299\n",
      "Epoch 3597/40000, Loss: 0.00013336552365217358, Learning Rate: 0.001299\n",
      "Epoch 3598/40000, Loss: 0.00010789676889544353, Learning Rate: 0.001299\n",
      "Epoch 3599/40000, Loss: 9.938806761056185e-05, Learning Rate: 0.001299\n",
      "Epoch 3600/40000, Loss: 9.619371121516451e-05, Learning Rate: 0.001298\n",
      "Epoch 3601/40000, Loss: 8.390331640839577e-05, Learning Rate: 0.001298\n",
      "Epoch 3602/40000, Loss: 6.749076419509947e-05, Learning Rate: 0.001298\n",
      "Epoch 3603/40000, Loss: 0.00011847235873574391, Learning Rate: 0.001298\n",
      "Epoch 3604/40000, Loss: 0.00015285379777196795, Learning Rate: 0.001298\n",
      "Epoch 3605/40000, Loss: 0.0001531519810669124, Learning Rate: 0.001298\n",
      "Epoch 3606/40000, Loss: 0.00015708996215835214, Learning Rate: 0.001297\n",
      "Epoch 3607/40000, Loss: 0.00010834619024535641, Learning Rate: 0.001297\n",
      "Epoch 3608/40000, Loss: 0.00012258098286110908, Learning Rate: 0.001297\n",
      "Epoch 3609/40000, Loss: 0.00011267403169767931, Learning Rate: 0.001297\n",
      "Epoch 3610/40000, Loss: 0.00018478292622603476, Learning Rate: 0.001297\n",
      "Epoch 3611/40000, Loss: 0.00010550502338446677, Learning Rate: 0.001297\n",
      "Epoch 3612/40000, Loss: 0.00015590214752592146, Learning Rate: 0.001297\n",
      "Epoch 3613/40000, Loss: 9.565184154780582e-05, Learning Rate: 0.001296\n",
      "Epoch 3614/40000, Loss: 0.0001701933069853112, Learning Rate: 0.001296\n",
      "Epoch 3615/40000, Loss: 0.00012698958744294941, Learning Rate: 0.001296\n",
      "Epoch 3616/40000, Loss: 0.00020262539328541607, Learning Rate: 0.001296\n",
      "Epoch 3617/40000, Loss: 0.00017506774747744203, Learning Rate: 0.001296\n",
      "Epoch 3618/40000, Loss: 0.00010888053657254204, Learning Rate: 0.001296\n",
      "Epoch 3619/40000, Loss: 0.0002981656580232084, Learning Rate: 0.001295\n",
      "Epoch 3620/40000, Loss: 0.00020126349409110844, Learning Rate: 0.001295\n",
      "Epoch 3621/40000, Loss: 0.00013844302156940103, Learning Rate: 0.001295\n",
      "Epoch 3622/40000, Loss: 0.00018027344776783139, Learning Rate: 0.001295\n",
      "Epoch 3623/40000, Loss: 0.00014556075620930642, Learning Rate: 0.001295\n",
      "Epoch 3624/40000, Loss: 0.00014885416021570563, Learning Rate: 0.001295\n",
      "Epoch 3625/40000, Loss: 0.0002262847265228629, Learning Rate: 0.001295\n",
      "Epoch 3626/40000, Loss: 0.00020360534836072475, Learning Rate: 0.001294\n",
      "Epoch 3627/40000, Loss: 0.0001927238336065784, Learning Rate: 0.001294\n",
      "Epoch 3628/40000, Loss: 0.0001405477523803711, Learning Rate: 0.001294\n",
      "Epoch 3629/40000, Loss: 0.0001761197781888768, Learning Rate: 0.001294\n",
      "Epoch 3630/40000, Loss: 8.978862024378031e-05, Learning Rate: 0.001294\n",
      "Epoch 3631/40000, Loss: 7.523856038460508e-05, Learning Rate: 0.001294\n",
      "Epoch 3632/40000, Loss: 8.430510206380859e-05, Learning Rate: 0.001293\n",
      "Epoch 3633/40000, Loss: 6.73206159262918e-05, Learning Rate: 0.001293\n",
      "Epoch 3634/40000, Loss: 0.0001842622586991638, Learning Rate: 0.001293\n",
      "Epoch 3635/40000, Loss: 0.00011209832155145705, Learning Rate: 0.001293\n",
      "Epoch 3636/40000, Loss: 0.00012338983651716262, Learning Rate: 0.001293\n",
      "Epoch 3637/40000, Loss: 8.268052624771371e-05, Learning Rate: 0.001293\n",
      "Epoch 3638/40000, Loss: 0.00015247381816152483, Learning Rate: 0.001293\n",
      "Epoch 3639/40000, Loss: 0.00016594221233390272, Learning Rate: 0.001292\n",
      "Epoch 3640/40000, Loss: 0.0002657139557413757, Learning Rate: 0.001292\n",
      "Epoch 3641/40000, Loss: 0.00014925179129932076, Learning Rate: 0.001292\n",
      "Epoch 3642/40000, Loss: 0.0001587562874192372, Learning Rate: 0.001292\n",
      "Epoch 3643/40000, Loss: 0.0001031094288919121, Learning Rate: 0.001292\n",
      "Epoch 3644/40000, Loss: 9.149765537586063e-05, Learning Rate: 0.001292\n",
      "Epoch 3645/40000, Loss: 0.00010062284127343446, Learning Rate: 0.001291\n",
      "Epoch 3646/40000, Loss: 0.00022591104789171368, Learning Rate: 0.001291\n",
      "Epoch 3647/40000, Loss: 0.00011924572754651308, Learning Rate: 0.001291\n",
      "Epoch 3648/40000, Loss: 9.830808266997337e-05, Learning Rate: 0.001291\n",
      "Epoch 3649/40000, Loss: 0.00014025077689439058, Learning Rate: 0.001291\n",
      "Epoch 3650/40000, Loss: 0.0004250640340615064, Learning Rate: 0.001291\n",
      "Epoch 3651/40000, Loss: 0.00024106938508339226, Learning Rate: 0.001290\n",
      "Epoch 3652/40000, Loss: 0.00018439149425830692, Learning Rate: 0.001290\n",
      "Epoch 3653/40000, Loss: 0.0001594102941453457, Learning Rate: 0.001290\n",
      "Epoch 3654/40000, Loss: 0.00011729662219295278, Learning Rate: 0.001290\n",
      "Epoch 3655/40000, Loss: 0.0001846400846261531, Learning Rate: 0.001290\n",
      "Epoch 3656/40000, Loss: 9.68088279478252e-05, Learning Rate: 0.001290\n",
      "Epoch 3657/40000, Loss: 0.00014547925093211234, Learning Rate: 0.001290\n",
      "Epoch 3658/40000, Loss: 0.0002663537161424756, Learning Rate: 0.001289\n",
      "Epoch 3659/40000, Loss: 0.00012965385394636542, Learning Rate: 0.001289\n",
      "Epoch 3660/40000, Loss: 0.00023351125128101557, Learning Rate: 0.001289\n",
      "Epoch 3661/40000, Loss: 0.0001473691954743117, Learning Rate: 0.001289\n",
      "Epoch 3662/40000, Loss: 0.00012066598719684407, Learning Rate: 0.001289\n",
      "Epoch 3663/40000, Loss: 8.558561967220157e-05, Learning Rate: 0.001289\n",
      "Epoch 3664/40000, Loss: 0.00015380702097900212, Learning Rate: 0.001288\n",
      "Epoch 3665/40000, Loss: 0.0002894266799557954, Learning Rate: 0.001288\n",
      "Epoch 3666/40000, Loss: 0.00010242215648759156, Learning Rate: 0.001288\n",
      "Epoch 3667/40000, Loss: 8.153936505550519e-05, Learning Rate: 0.001288\n",
      "Epoch 3668/40000, Loss: 0.00016628779121674597, Learning Rate: 0.001288\n",
      "Epoch 3669/40000, Loss: 0.00011203671601833776, Learning Rate: 0.001288\n",
      "Epoch 3670/40000, Loss: 6.876651605125517e-05, Learning Rate: 0.001288\n",
      "Epoch 3671/40000, Loss: 0.00018476559489499778, Learning Rate: 0.001287\n",
      "Epoch 3672/40000, Loss: 0.0001321839663432911, Learning Rate: 0.001287\n",
      "Epoch 3673/40000, Loss: 0.00010485062375664711, Learning Rate: 0.001287\n",
      "Epoch 3674/40000, Loss: 0.00010797751019708812, Learning Rate: 0.001287\n",
      "Epoch 3675/40000, Loss: 0.00011201674351468682, Learning Rate: 0.001287\n",
      "Epoch 3676/40000, Loss: 0.00022368850477505475, Learning Rate: 0.001287\n",
      "Epoch 3677/40000, Loss: 6.441985169658437e-05, Learning Rate: 0.001286\n",
      "Epoch 3678/40000, Loss: 8.796516340225935e-05, Learning Rate: 0.001286\n",
      "Epoch 3679/40000, Loss: 7.912075670901686e-05, Learning Rate: 0.001286\n",
      "Epoch 3680/40000, Loss: 6.13325901213102e-05, Learning Rate: 0.001286\n",
      "Epoch 3681/40000, Loss: 7.237115642055869e-05, Learning Rate: 0.001286\n",
      "Epoch 3682/40000, Loss: 0.00019708620675373822, Learning Rate: 0.001286\n",
      "Epoch 3683/40000, Loss: 9.094413690036163e-05, Learning Rate: 0.001286\n",
      "Epoch 3684/40000, Loss: 0.00011877264478243887, Learning Rate: 0.001285\n",
      "Epoch 3685/40000, Loss: 0.00010703840234782547, Learning Rate: 0.001285\n",
      "Epoch 3686/40000, Loss: 0.00011749918485293165, Learning Rate: 0.001285\n",
      "Epoch 3687/40000, Loss: 0.0002102642902173102, Learning Rate: 0.001285\n",
      "Epoch 3688/40000, Loss: 0.00013479389599524438, Learning Rate: 0.001285\n",
      "Epoch 3689/40000, Loss: 9.070392115972936e-05, Learning Rate: 0.001285\n",
      "Epoch 3690/40000, Loss: 0.00017907297296915203, Learning Rate: 0.001284\n",
      "Epoch 3691/40000, Loss: 0.00011906048894161358, Learning Rate: 0.001284\n",
      "Epoch 3692/40000, Loss: 0.00014245399506762624, Learning Rate: 0.001284\n",
      "Epoch 3693/40000, Loss: 0.0002212449471699074, Learning Rate: 0.001284\n",
      "Epoch 3694/40000, Loss: 0.00018434671801514924, Learning Rate: 0.001284\n",
      "Epoch 3695/40000, Loss: 0.00015895019168965518, Learning Rate: 0.001284\n",
      "Epoch 3696/40000, Loss: 9.913263784255832e-05, Learning Rate: 0.001284\n",
      "Epoch 3697/40000, Loss: 0.0002587996132206172, Learning Rate: 0.001283\n",
      "Epoch 3698/40000, Loss: 0.00013421580661088228, Learning Rate: 0.001283\n",
      "Epoch 3699/40000, Loss: 0.00011742617061827332, Learning Rate: 0.001283\n",
      "Epoch 3700/40000, Loss: 0.00019943449296988547, Learning Rate: 0.001283\n",
      "Epoch 3701/40000, Loss: 0.0002028275339398533, Learning Rate: 0.001283\n",
      "Epoch 3702/40000, Loss: 0.00026035666815005243, Learning Rate: 0.001283\n",
      "Epoch 3703/40000, Loss: 0.00024859182303771377, Learning Rate: 0.001282\n",
      "Epoch 3704/40000, Loss: 0.00012414914090186357, Learning Rate: 0.001282\n",
      "Epoch 3705/40000, Loss: 0.00014195372932590544, Learning Rate: 0.001282\n",
      "Epoch 3706/40000, Loss: 0.00020916122593916953, Learning Rate: 0.001282\n",
      "Epoch 3707/40000, Loss: 0.00017394816677551717, Learning Rate: 0.001282\n",
      "Epoch 3708/40000, Loss: 0.00011399286449886858, Learning Rate: 0.001282\n",
      "Epoch 3709/40000, Loss: 0.00018018244008999318, Learning Rate: 0.001282\n",
      "Epoch 3710/40000, Loss: 0.000231429236009717, Learning Rate: 0.001281\n",
      "Epoch 3711/40000, Loss: 0.00013364982441999018, Learning Rate: 0.001281\n",
      "Epoch 3712/40000, Loss: 0.00016754414536990225, Learning Rate: 0.001281\n",
      "Epoch 3713/40000, Loss: 0.0001659095723880455, Learning Rate: 0.001281\n",
      "Epoch 3714/40000, Loss: 9.566397056914866e-05, Learning Rate: 0.001281\n",
      "Epoch 3715/40000, Loss: 9.658681665314361e-05, Learning Rate: 0.001281\n",
      "Epoch 3716/40000, Loss: 0.0001801055477699265, Learning Rate: 0.001280\n",
      "Epoch 3717/40000, Loss: 0.00010328971256967634, Learning Rate: 0.001280\n",
      "Epoch 3718/40000, Loss: 0.0001203408173751086, Learning Rate: 0.001280\n",
      "Epoch 3719/40000, Loss: 8.485867874696851e-05, Learning Rate: 0.001280\n",
      "Epoch 3720/40000, Loss: 0.00019574715406633914, Learning Rate: 0.001280\n",
      "Epoch 3721/40000, Loss: 0.00018638088658917695, Learning Rate: 0.001280\n",
      "Epoch 3722/40000, Loss: 0.00017580944404471666, Learning Rate: 0.001280\n",
      "Epoch 3723/40000, Loss: 0.00018213690782431513, Learning Rate: 0.001279\n",
      "Epoch 3724/40000, Loss: 0.00013483957445714623, Learning Rate: 0.001279\n",
      "Epoch 3725/40000, Loss: 0.00023870797303970903, Learning Rate: 0.001279\n",
      "Epoch 3726/40000, Loss: 7.426057709380984e-05, Learning Rate: 0.001279\n",
      "Epoch 3727/40000, Loss: 6.496626156149432e-05, Learning Rate: 0.001279\n",
      "Epoch 3728/40000, Loss: 0.0001625527802389115, Learning Rate: 0.001279\n",
      "Epoch 3729/40000, Loss: 0.00018411120981909335, Learning Rate: 0.001278\n",
      "Epoch 3730/40000, Loss: 0.00010376821592217311, Learning Rate: 0.001278\n",
      "Epoch 3731/40000, Loss: 0.00010565789125394076, Learning Rate: 0.001278\n",
      "Epoch 3732/40000, Loss: 0.0003479893784970045, Learning Rate: 0.001278\n",
      "Epoch 3733/40000, Loss: 6.894025136716664e-05, Learning Rate: 0.001278\n",
      "Epoch 3734/40000, Loss: 0.0001548568980069831, Learning Rate: 0.001278\n",
      "Epoch 3735/40000, Loss: 0.00011032715701730922, Learning Rate: 0.001278\n",
      "Epoch 3736/40000, Loss: 0.00010100895451614633, Learning Rate: 0.001277\n",
      "Epoch 3737/40000, Loss: 0.0001808031229302287, Learning Rate: 0.001277\n",
      "Epoch 3738/40000, Loss: 0.0001084497052943334, Learning Rate: 0.001277\n",
      "Epoch 3739/40000, Loss: 8.300571789732203e-05, Learning Rate: 0.001277\n",
      "Epoch 3740/40000, Loss: 0.0001801913313101977, Learning Rate: 0.001277\n",
      "Epoch 3741/40000, Loss: 0.00020438627689145505, Learning Rate: 0.001277\n",
      "Epoch 3742/40000, Loss: 0.0001764446496963501, Learning Rate: 0.001276\n",
      "Epoch 3743/40000, Loss: 0.00011776007158914581, Learning Rate: 0.001276\n",
      "Epoch 3744/40000, Loss: 0.00011593531962716952, Learning Rate: 0.001276\n",
      "Epoch 3745/40000, Loss: 0.000334250828018412, Learning Rate: 0.001276\n",
      "Epoch 3746/40000, Loss: 0.00022016702860128134, Learning Rate: 0.001276\n",
      "Epoch 3747/40000, Loss: 0.0002740832860581577, Learning Rate: 0.001276\n",
      "Epoch 3748/40000, Loss: 0.0001458784972783178, Learning Rate: 0.001276\n",
      "Epoch 3749/40000, Loss: 0.00017572926299180835, Learning Rate: 0.001275\n",
      "Epoch 3750/40000, Loss: 0.00010924912930931896, Learning Rate: 0.001275\n",
      "Epoch 3751/40000, Loss: 0.00014781244681216776, Learning Rate: 0.001275\n",
      "Epoch 3752/40000, Loss: 9.808528557186946e-05, Learning Rate: 0.001275\n",
      "Epoch 3753/40000, Loss: 0.00016176544886548072, Learning Rate: 0.001275\n",
      "Epoch 3754/40000, Loss: 0.00019463214266579598, Learning Rate: 0.001275\n",
      "Epoch 3755/40000, Loss: 0.00015681801596656442, Learning Rate: 0.001274\n",
      "Epoch 3756/40000, Loss: 0.00015772996994201094, Learning Rate: 0.001274\n",
      "Epoch 3757/40000, Loss: 0.00022426084615290165, Learning Rate: 0.001274\n",
      "Epoch 3758/40000, Loss: 6.542877235915512e-05, Learning Rate: 0.001274\n",
      "Epoch 3759/40000, Loss: 9.585251973476261e-05, Learning Rate: 0.001274\n",
      "Epoch 3760/40000, Loss: 0.0001864478108473122, Learning Rate: 0.001274\n",
      "Epoch 3761/40000, Loss: 0.00016155684716068208, Learning Rate: 0.001274\n",
      "Epoch 3762/40000, Loss: 0.00012166312808403745, Learning Rate: 0.001273\n",
      "Epoch 3763/40000, Loss: 7.574553455924615e-05, Learning Rate: 0.001273\n",
      "Epoch 3764/40000, Loss: 0.00011844297841889784, Learning Rate: 0.001273\n",
      "Epoch 3765/40000, Loss: 0.00016207578300964087, Learning Rate: 0.001273\n",
      "Epoch 3766/40000, Loss: 7.588826701976359e-05, Learning Rate: 0.001273\n",
      "Epoch 3767/40000, Loss: 7.194843055913225e-05, Learning Rate: 0.001273\n",
      "Epoch 3768/40000, Loss: 9.523204062134027e-05, Learning Rate: 0.001272\n",
      "Epoch 3769/40000, Loss: 0.00013511542056221515, Learning Rate: 0.001272\n",
      "Epoch 3770/40000, Loss: 9.308121661888435e-05, Learning Rate: 0.001272\n",
      "Epoch 3771/40000, Loss: 6.892618694109842e-05, Learning Rate: 0.001272\n",
      "Epoch 3772/40000, Loss: 0.0001327511708950624, Learning Rate: 0.001272\n",
      "Epoch 3773/40000, Loss: 9.45484425756149e-05, Learning Rate: 0.001272\n",
      "Epoch 3774/40000, Loss: 5.0894810556201264e-05, Learning Rate: 0.001272\n",
      "Epoch 3775/40000, Loss: 9.118695743381977e-05, Learning Rate: 0.001271\n",
      "Epoch 3776/40000, Loss: 6.86890707584098e-05, Learning Rate: 0.001271\n",
      "Epoch 3777/40000, Loss: 8.901036198949441e-05, Learning Rate: 0.001271\n",
      "Epoch 3778/40000, Loss: 9.103011689148843e-05, Learning Rate: 0.001271\n",
      "Epoch 3779/40000, Loss: 9.045458864420652e-05, Learning Rate: 0.001271\n",
      "Epoch 3780/40000, Loss: 6.900987500557676e-05, Learning Rate: 0.001271\n",
      "Epoch 3781/40000, Loss: 0.00013211159966886044, Learning Rate: 0.001271\n",
      "Epoch 3782/40000, Loss: 9.024605242302641e-05, Learning Rate: 0.001270\n",
      "Epoch 3783/40000, Loss: 0.0001318066060775891, Learning Rate: 0.001270\n",
      "Epoch 3784/40000, Loss: 9.309898450737819e-05, Learning Rate: 0.001270\n",
      "Epoch 3785/40000, Loss: 7.03378245816566e-05, Learning Rate: 0.001270\n",
      "Epoch 3786/40000, Loss: 0.00010715863754739985, Learning Rate: 0.001270\n",
      "Epoch 3787/40000, Loss: 5.885708742425777e-05, Learning Rate: 0.001270\n",
      "Epoch 3788/40000, Loss: 0.00010194048081757501, Learning Rate: 0.001269\n",
      "Epoch 3789/40000, Loss: 6.773929635528475e-05, Learning Rate: 0.001269\n",
      "Epoch 3790/40000, Loss: 0.0001667277974775061, Learning Rate: 0.001269\n",
      "Epoch 3791/40000, Loss: 0.00012149406393291429, Learning Rate: 0.001269\n",
      "Epoch 3792/40000, Loss: 0.0001353574771201238, Learning Rate: 0.001269\n",
      "Epoch 3793/40000, Loss: 0.00010950611613225192, Learning Rate: 0.001269\n",
      "Epoch 3794/40000, Loss: 7.81479975557886e-05, Learning Rate: 0.001269\n",
      "Epoch 3795/40000, Loss: 0.0001452910655643791, Learning Rate: 0.001268\n",
      "Epoch 3796/40000, Loss: 0.00011168068886036053, Learning Rate: 0.001268\n",
      "Epoch 3797/40000, Loss: 0.0001717963896226138, Learning Rate: 0.001268\n",
      "Epoch 3798/40000, Loss: 0.0001542541867820546, Learning Rate: 0.001268\n",
      "Epoch 3799/40000, Loss: 0.00024359303642995656, Learning Rate: 0.001268\n",
      "Epoch 3800/40000, Loss: 0.0002038477687165141, Learning Rate: 0.001268\n",
      "Epoch 3801/40000, Loss: 0.00016951093857642263, Learning Rate: 0.001267\n",
      "Epoch 3802/40000, Loss: 0.00015500020526815206, Learning Rate: 0.001267\n",
      "Epoch 3803/40000, Loss: 0.00016880055773071945, Learning Rate: 0.001267\n",
      "Epoch 3804/40000, Loss: 0.00021565578936133534, Learning Rate: 0.001267\n",
      "Epoch 3805/40000, Loss: 0.00028575462056323886, Learning Rate: 0.001267\n",
      "Epoch 3806/40000, Loss: 0.00017686378851067275, Learning Rate: 0.001267\n",
      "Epoch 3807/40000, Loss: 0.00012760503159370273, Learning Rate: 0.001267\n",
      "Epoch 3808/40000, Loss: 0.00011047737643821165, Learning Rate: 0.001266\n",
      "Epoch 3809/40000, Loss: 0.0001049467027769424, Learning Rate: 0.001266\n",
      "Epoch 3810/40000, Loss: 0.00021256640320643783, Learning Rate: 0.001266\n",
      "Epoch 3811/40000, Loss: 0.00019696803065016866, Learning Rate: 0.001266\n",
      "Epoch 3812/40000, Loss: 0.00023208980564959347, Learning Rate: 0.001266\n",
      "Epoch 3813/40000, Loss: 0.0001605467841727659, Learning Rate: 0.001266\n",
      "Epoch 3814/40000, Loss: 0.00016022224735934287, Learning Rate: 0.001265\n",
      "Epoch 3815/40000, Loss: 8.170677756424993e-05, Learning Rate: 0.001265\n",
      "Epoch 3816/40000, Loss: 6.538272282341495e-05, Learning Rate: 0.001265\n",
      "Epoch 3817/40000, Loss: 0.00016133260214701295, Learning Rate: 0.001265\n",
      "Epoch 3818/40000, Loss: 0.00018463675223756582, Learning Rate: 0.001265\n",
      "Epoch 3819/40000, Loss: 8.265187352662906e-05, Learning Rate: 0.001265\n",
      "Epoch 3820/40000, Loss: 0.0001709274365566671, Learning Rate: 0.001265\n",
      "Epoch 3821/40000, Loss: 0.000203938310733065, Learning Rate: 0.001264\n",
      "Epoch 3822/40000, Loss: 0.0001888473198050633, Learning Rate: 0.001264\n",
      "Epoch 3823/40000, Loss: 0.00021872299839742482, Learning Rate: 0.001264\n",
      "Epoch 3824/40000, Loss: 0.0004285894683562219, Learning Rate: 0.001264\n",
      "Epoch 3825/40000, Loss: 0.00025637299404479563, Learning Rate: 0.001264\n",
      "Epoch 3826/40000, Loss: 0.0003354687651153654, Learning Rate: 0.001264\n",
      "Epoch 3827/40000, Loss: 0.0009193962323479354, Learning Rate: 0.001264\n",
      "Epoch 3828/40000, Loss: 0.00044140423415228724, Learning Rate: 0.001263\n",
      "Epoch 3829/40000, Loss: 0.00023600233544129878, Learning Rate: 0.001263\n",
      "Epoch 3830/40000, Loss: 0.0007315358379855752, Learning Rate: 0.001263\n",
      "Epoch 3831/40000, Loss: 0.0004332910757511854, Learning Rate: 0.001263\n",
      "Epoch 3832/40000, Loss: 0.0008663884946145117, Learning Rate: 0.001263\n",
      "Epoch 3833/40000, Loss: 0.0005914430948905647, Learning Rate: 0.001263\n",
      "Epoch 3834/40000, Loss: 0.0003900997980963439, Learning Rate: 0.001262\n",
      "Epoch 3835/40000, Loss: 0.00040906519279815257, Learning Rate: 0.001262\n",
      "Epoch 3836/40000, Loss: 0.00021523344912566245, Learning Rate: 0.001262\n",
      "Epoch 3837/40000, Loss: 0.00031231663888320327, Learning Rate: 0.001262\n",
      "Epoch 3838/40000, Loss: 0.0002561997389420867, Learning Rate: 0.001262\n",
      "Epoch 3839/40000, Loss: 0.00013725770986638963, Learning Rate: 0.001262\n",
      "Epoch 3840/40000, Loss: 0.00012039845751132816, Learning Rate: 0.001262\n",
      "Epoch 3841/40000, Loss: 0.00016200759273488075, Learning Rate: 0.001261\n",
      "Epoch 3842/40000, Loss: 0.00017770149861462414, Learning Rate: 0.001261\n",
      "Epoch 3843/40000, Loss: 0.00017696982831694186, Learning Rate: 0.001261\n",
      "Epoch 3844/40000, Loss: 0.00011555172386579216, Learning Rate: 0.001261\n",
      "Epoch 3845/40000, Loss: 0.0002159871655749157, Learning Rate: 0.001261\n",
      "Epoch 3846/40000, Loss: 0.00010539770300965756, Learning Rate: 0.001261\n",
      "Epoch 3847/40000, Loss: 0.0001388142554787919, Learning Rate: 0.001260\n",
      "Epoch 3848/40000, Loss: 0.00017893493350129575, Learning Rate: 0.001260\n",
      "Epoch 3849/40000, Loss: 0.00016973431047517806, Learning Rate: 0.001260\n",
      "Epoch 3850/40000, Loss: 0.0001766072673490271, Learning Rate: 0.001260\n",
      "Epoch 3851/40000, Loss: 0.00013947096886113286, Learning Rate: 0.001260\n",
      "Epoch 3852/40000, Loss: 0.0001263494195882231, Learning Rate: 0.001260\n",
      "Epoch 3853/40000, Loss: 0.00017046561697497964, Learning Rate: 0.001260\n",
      "Epoch 3854/40000, Loss: 0.00010984105028910562, Learning Rate: 0.001259\n",
      "Epoch 3855/40000, Loss: 0.00018490463844500482, Learning Rate: 0.001259\n",
      "Epoch 3856/40000, Loss: 0.00011712182458722964, Learning Rate: 0.001259\n",
      "Epoch 3857/40000, Loss: 0.00012120632891310379, Learning Rate: 0.001259\n",
      "Epoch 3858/40000, Loss: 0.00011529242328833789, Learning Rate: 0.001259\n",
      "Epoch 3859/40000, Loss: 0.00015357813390437514, Learning Rate: 0.001259\n",
      "Epoch 3860/40000, Loss: 0.00011416090273996815, Learning Rate: 0.001259\n",
      "Epoch 3861/40000, Loss: 0.00020116203813813627, Learning Rate: 0.001258\n",
      "Epoch 3862/40000, Loss: 0.00011442820687079802, Learning Rate: 0.001258\n",
      "Epoch 3863/40000, Loss: 0.0002709525579120964, Learning Rate: 0.001258\n",
      "Epoch 3864/40000, Loss: 0.00010847986413864419, Learning Rate: 0.001258\n",
      "Epoch 3865/40000, Loss: 0.00023488605802413076, Learning Rate: 0.001258\n",
      "Epoch 3866/40000, Loss: 0.00015917759446892887, Learning Rate: 0.001258\n",
      "Epoch 3867/40000, Loss: 0.00017314912111032754, Learning Rate: 0.001257\n",
      "Epoch 3868/40000, Loss: 0.00014656694838777184, Learning Rate: 0.001257\n",
      "Epoch 3869/40000, Loss: 0.00014623193419538438, Learning Rate: 0.001257\n",
      "Epoch 3870/40000, Loss: 0.00010086162365041673, Learning Rate: 0.001257\n",
      "Epoch 3871/40000, Loss: 0.00013510439021047205, Learning Rate: 0.001257\n",
      "Epoch 3872/40000, Loss: 0.00011931585322599858, Learning Rate: 0.001257\n",
      "Epoch 3873/40000, Loss: 0.0001056511391652748, Learning Rate: 0.001257\n",
      "Epoch 3874/40000, Loss: 0.00018150167306885123, Learning Rate: 0.001256\n",
      "Epoch 3875/40000, Loss: 9.715093619888648e-05, Learning Rate: 0.001256\n",
      "Epoch 3876/40000, Loss: 0.00012409736518748105, Learning Rate: 0.001256\n",
      "Epoch 3877/40000, Loss: 0.0001441279164282605, Learning Rate: 0.001256\n",
      "Epoch 3878/40000, Loss: 0.00016800774028524756, Learning Rate: 0.001256\n",
      "Epoch 3879/40000, Loss: 0.00012033699022140354, Learning Rate: 0.001256\n",
      "Epoch 3880/40000, Loss: 7.374957931460813e-05, Learning Rate: 0.001256\n",
      "Epoch 3881/40000, Loss: 0.00017253901751246303, Learning Rate: 0.001255\n",
      "Epoch 3882/40000, Loss: 6.38219207758084e-05, Learning Rate: 0.001255\n",
      "Epoch 3883/40000, Loss: 6.763878627680242e-05, Learning Rate: 0.001255\n",
      "Epoch 3884/40000, Loss: 0.00011480887042125687, Learning Rate: 0.001255\n",
      "Epoch 3885/40000, Loss: 9.884896280709654e-05, Learning Rate: 0.001255\n",
      "Epoch 3886/40000, Loss: 0.00010589180601527914, Learning Rate: 0.001255\n",
      "Epoch 3887/40000, Loss: 9.86092709354125e-05, Learning Rate: 0.001254\n",
      "Epoch 3888/40000, Loss: 8.816492481855676e-05, Learning Rate: 0.001254\n",
      "Epoch 3889/40000, Loss: 6.673023017356172e-05, Learning Rate: 0.001254\n",
      "Epoch 3890/40000, Loss: 9.91011256701313e-05, Learning Rate: 0.001254\n",
      "Epoch 3891/40000, Loss: 6.473686516983435e-05, Learning Rate: 0.001254\n",
      "Epoch 3892/40000, Loss: 0.00010583183029666543, Learning Rate: 0.001254\n",
      "Epoch 3893/40000, Loss: 0.00013974511239212006, Learning Rate: 0.001254\n",
      "Epoch 3894/40000, Loss: 0.00013464689254760742, Learning Rate: 0.001253\n",
      "Epoch 3895/40000, Loss: 0.0001334899861831218, Learning Rate: 0.001253\n",
      "Epoch 3896/40000, Loss: 0.00015723476826678962, Learning Rate: 0.001253\n",
      "Epoch 3897/40000, Loss: 8.936332596931607e-05, Learning Rate: 0.001253\n",
      "Epoch 3898/40000, Loss: 6.0057594964746386e-05, Learning Rate: 0.001253\n",
      "Epoch 3899/40000, Loss: 0.00010810689855134115, Learning Rate: 0.001253\n",
      "Epoch 3900/40000, Loss: 0.000143664117786102, Learning Rate: 0.001253\n",
      "Epoch 3901/40000, Loss: 9.034609684022143e-05, Learning Rate: 0.001252\n",
      "Epoch 3902/40000, Loss: 0.00013399218732956797, Learning Rate: 0.001252\n",
      "Epoch 3903/40000, Loss: 0.00012748334847856313, Learning Rate: 0.001252\n",
      "Epoch 3904/40000, Loss: 5.634475746774115e-05, Learning Rate: 0.001252\n",
      "Epoch 3905/40000, Loss: 9.680228686193004e-05, Learning Rate: 0.001252\n",
      "Epoch 3906/40000, Loss: 0.00013820805179420859, Learning Rate: 0.001252\n",
      "Epoch 3907/40000, Loss: 0.00010433868737891316, Learning Rate: 0.001251\n",
      "Epoch 3908/40000, Loss: 5.485367000801489e-05, Learning Rate: 0.001251\n",
      "Epoch 3909/40000, Loss: 0.00013773117098025978, Learning Rate: 0.001251\n",
      "Epoch 3910/40000, Loss: 8.251937833847478e-05, Learning Rate: 0.001251\n",
      "Epoch 3911/40000, Loss: 9.47182415984571e-05, Learning Rate: 0.001251\n",
      "Epoch 3912/40000, Loss: 0.0002031913463724777, Learning Rate: 0.001251\n",
      "Epoch 3913/40000, Loss: 0.00010168297740165144, Learning Rate: 0.001251\n",
      "Epoch 3914/40000, Loss: 9.415012755198404e-05, Learning Rate: 0.001250\n",
      "Epoch 3915/40000, Loss: 0.00015590089606121182, Learning Rate: 0.001250\n",
      "Epoch 3916/40000, Loss: 0.00014641869347542524, Learning Rate: 0.001250\n",
      "Epoch 3917/40000, Loss: 9.867297194432467e-05, Learning Rate: 0.001250\n",
      "Epoch 3918/40000, Loss: 9.489816875429824e-05, Learning Rate: 0.001250\n",
      "Epoch 3919/40000, Loss: 0.00022720862762071192, Learning Rate: 0.001250\n",
      "Epoch 3920/40000, Loss: 0.00015487227938137949, Learning Rate: 0.001249\n",
      "Epoch 3921/40000, Loss: 6.892953388160095e-05, Learning Rate: 0.001249\n",
      "Epoch 3922/40000, Loss: 0.00016791143571026623, Learning Rate: 0.001249\n",
      "Epoch 3923/40000, Loss: 0.0001705666072666645, Learning Rate: 0.001249\n",
      "Epoch 3924/40000, Loss: 0.00014959070540498942, Learning Rate: 0.001249\n",
      "Epoch 3925/40000, Loss: 0.00011099094990640879, Learning Rate: 0.001249\n",
      "Epoch 3926/40000, Loss: 0.00019555976905394346, Learning Rate: 0.001249\n",
      "Epoch 3927/40000, Loss: 0.00011337821342749521, Learning Rate: 0.001248\n",
      "Epoch 3928/40000, Loss: 0.00010437351738801226, Learning Rate: 0.001248\n",
      "Epoch 3929/40000, Loss: 9.873497037915513e-05, Learning Rate: 0.001248\n",
      "Epoch 3930/40000, Loss: 0.00017626595217734575, Learning Rate: 0.001248\n",
      "Epoch 3931/40000, Loss: 0.0001029395280056633, Learning Rate: 0.001248\n",
      "Epoch 3932/40000, Loss: 0.00015966533101163805, Learning Rate: 0.001248\n",
      "Epoch 3933/40000, Loss: 0.00014974111400078982, Learning Rate: 0.001248\n",
      "Epoch 3934/40000, Loss: 0.00015900604194030166, Learning Rate: 0.001247\n",
      "Epoch 3935/40000, Loss: 5.742109351558611e-05, Learning Rate: 0.001247\n",
      "Epoch 3936/40000, Loss: 8.771329157752916e-05, Learning Rate: 0.001247\n",
      "Epoch 3937/40000, Loss: 0.00016885297372937202, Learning Rate: 0.001247\n",
      "Epoch 3938/40000, Loss: 0.0001541344536235556, Learning Rate: 0.001247\n",
      "Epoch 3939/40000, Loss: 0.0001097114582080394, Learning Rate: 0.001247\n",
      "Epoch 3940/40000, Loss: 0.00016929415869526565, Learning Rate: 0.001247\n",
      "Epoch 3941/40000, Loss: 9.748109732754529e-05, Learning Rate: 0.001246\n",
      "Epoch 3942/40000, Loss: 0.0001437164464732632, Learning Rate: 0.001246\n",
      "Epoch 3943/40000, Loss: 0.00013367526116780937, Learning Rate: 0.001246\n",
      "Epoch 3944/40000, Loss: 8.000375237315893e-05, Learning Rate: 0.001246\n",
      "Epoch 3945/40000, Loss: 0.00014893844490870833, Learning Rate: 0.001246\n",
      "Epoch 3946/40000, Loss: 9.554727876093239e-05, Learning Rate: 0.001246\n",
      "Epoch 3947/40000, Loss: 9.849556226981804e-05, Learning Rate: 0.001245\n",
      "Epoch 3948/40000, Loss: 0.00013802987814415246, Learning Rate: 0.001245\n",
      "Epoch 3949/40000, Loss: 9.00578306755051e-05, Learning Rate: 0.001245\n",
      "Epoch 3950/40000, Loss: 0.00013170162856113166, Learning Rate: 0.001245\n",
      "Epoch 3951/40000, Loss: 0.0001308452629018575, Learning Rate: 0.001245\n",
      "Epoch 3952/40000, Loss: 9.065911581274122e-05, Learning Rate: 0.001245\n",
      "Epoch 3953/40000, Loss: 9.304390550823882e-05, Learning Rate: 0.001245\n",
      "Epoch 3954/40000, Loss: 9.287175635108724e-05, Learning Rate: 0.001244\n",
      "Epoch 3955/40000, Loss: 9.211945871356875e-05, Learning Rate: 0.001244\n",
      "Epoch 3956/40000, Loss: 5.067564779892564e-05, Learning Rate: 0.001244\n",
      "Epoch 3957/40000, Loss: 5.0452403229428455e-05, Learning Rate: 0.001244\n",
      "Epoch 3958/40000, Loss: 7.605129212606698e-05, Learning Rate: 0.001244\n",
      "Epoch 3959/40000, Loss: 0.00012688711285591125, Learning Rate: 0.001244\n",
      "Epoch 3960/40000, Loss: 7.911309512564912e-05, Learning Rate: 0.001244\n",
      "Epoch 3961/40000, Loss: 9.31523900362663e-05, Learning Rate: 0.001243\n",
      "Epoch 3962/40000, Loss: 7.529266440542415e-05, Learning Rate: 0.001243\n",
      "Epoch 3963/40000, Loss: 0.00013678085815627128, Learning Rate: 0.001243\n",
      "Epoch 3964/40000, Loss: 5.461381806526333e-05, Learning Rate: 0.001243\n",
      "Epoch 3965/40000, Loss: 9.503070032224059e-05, Learning Rate: 0.001243\n",
      "Epoch 3966/40000, Loss: 8.674978016642854e-05, Learning Rate: 0.001243\n",
      "Epoch 3967/40000, Loss: 0.0002198049915023148, Learning Rate: 0.001242\n",
      "Epoch 3968/40000, Loss: 0.00010474075679667294, Learning Rate: 0.001242\n",
      "Epoch 3969/40000, Loss: 0.00015707396960351616, Learning Rate: 0.001242\n",
      "Epoch 3970/40000, Loss: 0.00013663072604686022, Learning Rate: 0.001242\n",
      "Epoch 3971/40000, Loss: 0.00023934833006933331, Learning Rate: 0.001242\n",
      "Epoch 3972/40000, Loss: 0.00023019588843453676, Learning Rate: 0.001242\n",
      "Epoch 3973/40000, Loss: 0.0002515775559004396, Learning Rate: 0.001242\n",
      "Epoch 3974/40000, Loss: 0.00016770690854173154, Learning Rate: 0.001241\n",
      "Epoch 3975/40000, Loss: 0.00040734349749982357, Learning Rate: 0.001241\n",
      "Epoch 3976/40000, Loss: 0.00033937825355678797, Learning Rate: 0.001241\n",
      "Epoch 3977/40000, Loss: 0.00024881024728529155, Learning Rate: 0.001241\n",
      "Epoch 3978/40000, Loss: 0.00017560466949362308, Learning Rate: 0.001241\n",
      "Epoch 3979/40000, Loss: 0.0001034085507853888, Learning Rate: 0.001241\n",
      "Epoch 3980/40000, Loss: 0.0001491437724325806, Learning Rate: 0.001241\n",
      "Epoch 3981/40000, Loss: 0.0002695137227419764, Learning Rate: 0.001240\n",
      "Epoch 3982/40000, Loss: 0.0001474907185183838, Learning Rate: 0.001240\n",
      "Epoch 3983/40000, Loss: 0.00010179215314565226, Learning Rate: 0.001240\n",
      "Epoch 3984/40000, Loss: 8.878785592969507e-05, Learning Rate: 0.001240\n",
      "Epoch 3985/40000, Loss: 0.0002019690873567015, Learning Rate: 0.001240\n",
      "Epoch 3986/40000, Loss: 0.00013465261145029217, Learning Rate: 0.001240\n",
      "Epoch 3987/40000, Loss: 0.00017803047376219183, Learning Rate: 0.001239\n",
      "Epoch 3988/40000, Loss: 0.00016593890904914588, Learning Rate: 0.001239\n",
      "Epoch 3989/40000, Loss: 0.00010385065252194181, Learning Rate: 0.001239\n",
      "Epoch 3990/40000, Loss: 0.00020285524078644812, Learning Rate: 0.001239\n",
      "Epoch 3991/40000, Loss: 0.0001359837915515527, Learning Rate: 0.001239\n",
      "Epoch 3992/40000, Loss: 9.753303311299533e-05, Learning Rate: 0.001239\n",
      "Epoch 3993/40000, Loss: 0.00020926054276060313, Learning Rate: 0.001239\n",
      "Epoch 3994/40000, Loss: 0.0002129090134985745, Learning Rate: 0.001238\n",
      "Epoch 3995/40000, Loss: 0.00013551143638323992, Learning Rate: 0.001238\n",
      "Epoch 3996/40000, Loss: 0.00010308293713023886, Learning Rate: 0.001238\n",
      "Epoch 3997/40000, Loss: 0.00017991651839111, Learning Rate: 0.001238\n",
      "Epoch 3998/40000, Loss: 0.00010998141078744084, Learning Rate: 0.001238\n",
      "Epoch 3999/40000, Loss: 9.273709292756394e-05, Learning Rate: 0.001238\n",
      "Epoch 4000/40000, Loss: 0.00016871292609721422, Learning Rate: 0.001238\n",
      "Epoch 4001/40000, Loss: 8.524679287802428e-05, Learning Rate: 0.001237\n",
      "Epoch 4002/40000, Loss: 0.00010427928646095097, Learning Rate: 0.001237\n",
      "Epoch 4003/40000, Loss: 0.00017487199511379004, Learning Rate: 0.001237\n",
      "Epoch 4004/40000, Loss: 0.000226432821364142, Learning Rate: 0.001237\n",
      "Epoch 4005/40000, Loss: 0.0001766897621564567, Learning Rate: 0.001237\n",
      "Epoch 4006/40000, Loss: 0.00014490696776192635, Learning Rate: 0.001237\n",
      "Epoch 4007/40000, Loss: 0.00010766369086923078, Learning Rate: 0.001237\n",
      "Epoch 4008/40000, Loss: 9.573673742124811e-05, Learning Rate: 0.001236\n",
      "Epoch 4009/40000, Loss: 0.00015089304361026734, Learning Rate: 0.001236\n",
      "Epoch 4010/40000, Loss: 0.00014236423885449767, Learning Rate: 0.001236\n",
      "Epoch 4011/40000, Loss: 0.0001376491563860327, Learning Rate: 0.001236\n",
      "Epoch 4012/40000, Loss: 0.00012879015412181616, Learning Rate: 0.001236\n",
      "Epoch 4013/40000, Loss: 0.00018209371773991734, Learning Rate: 0.001236\n",
      "Epoch 4014/40000, Loss: 0.00010390370152890682, Learning Rate: 0.001235\n",
      "Epoch 4015/40000, Loss: 7.880289194872603e-05, Learning Rate: 0.001235\n",
      "Epoch 4016/40000, Loss: 0.00018588009697850794, Learning Rate: 0.001235\n",
      "Epoch 4017/40000, Loss: 0.00016349388170056045, Learning Rate: 0.001235\n",
      "Epoch 4018/40000, Loss: 0.000143396231578663, Learning Rate: 0.001235\n",
      "Epoch 4019/40000, Loss: 7.193184865172952e-05, Learning Rate: 0.001235\n",
      "Epoch 4020/40000, Loss: 0.00016173577751033008, Learning Rate: 0.001235\n",
      "Epoch 4021/40000, Loss: 7.68576210248284e-05, Learning Rate: 0.001234\n",
      "Epoch 4022/40000, Loss: 8.77159254741855e-05, Learning Rate: 0.001234\n",
      "Epoch 4023/40000, Loss: 0.00016229131142608821, Learning Rate: 0.001234\n",
      "Epoch 4024/40000, Loss: 0.00015953555703163147, Learning Rate: 0.001234\n",
      "Epoch 4025/40000, Loss: 0.00027715900796465576, Learning Rate: 0.001234\n",
      "Epoch 4026/40000, Loss: 0.0001292077940888703, Learning Rate: 0.001234\n",
      "Epoch 4027/40000, Loss: 0.00016708628390915692, Learning Rate: 0.001234\n",
      "Epoch 4028/40000, Loss: 0.0001404821960022673, Learning Rate: 0.001233\n",
      "Epoch 4029/40000, Loss: 0.00015292948228307068, Learning Rate: 0.001233\n",
      "Epoch 4030/40000, Loss: 0.00014801982615608722, Learning Rate: 0.001233\n",
      "Epoch 4031/40000, Loss: 7.916382310213521e-05, Learning Rate: 0.001233\n",
      "Epoch 4032/40000, Loss: 0.00015280299703590572, Learning Rate: 0.001233\n",
      "Epoch 4033/40000, Loss: 9.468685311730951e-05, Learning Rate: 0.001233\n",
      "Epoch 4034/40000, Loss: 0.00017213473620358855, Learning Rate: 0.001233\n",
      "Epoch 4035/40000, Loss: 0.00017555279191583395, Learning Rate: 0.001232\n",
      "Epoch 4036/40000, Loss: 0.0002630643139127642, Learning Rate: 0.001232\n",
      "Epoch 4037/40000, Loss: 0.000265340058831498, Learning Rate: 0.001232\n",
      "Epoch 4038/40000, Loss: 0.00013427704107016325, Learning Rate: 0.001232\n",
      "Epoch 4039/40000, Loss: 0.00025427734362892807, Learning Rate: 0.001232\n",
      "Epoch 4040/40000, Loss: 0.00014305768127087504, Learning Rate: 0.001232\n",
      "Epoch 4041/40000, Loss: 0.00018094237020704895, Learning Rate: 0.001231\n",
      "Epoch 4042/40000, Loss: 0.0003897333808708936, Learning Rate: 0.001231\n",
      "Epoch 4043/40000, Loss: 0.0002466522273607552, Learning Rate: 0.001231\n",
      "Epoch 4044/40000, Loss: 0.00014639183063991368, Learning Rate: 0.001231\n",
      "Epoch 4045/40000, Loss: 0.000321727420669049, Learning Rate: 0.001231\n",
      "Epoch 4046/40000, Loss: 0.0002625751367304474, Learning Rate: 0.001231\n",
      "Epoch 4047/40000, Loss: 0.00022076963796280324, Learning Rate: 0.001231\n",
      "Epoch 4048/40000, Loss: 0.00035072147147729993, Learning Rate: 0.001230\n",
      "Epoch 4049/40000, Loss: 0.00017372230649925768, Learning Rate: 0.001230\n",
      "Epoch 4050/40000, Loss: 0.0001289567881030962, Learning Rate: 0.001230\n",
      "Epoch 4051/40000, Loss: 0.00013223753194324672, Learning Rate: 0.001230\n",
      "Epoch 4052/40000, Loss: 9.301469981437549e-05, Learning Rate: 0.001230\n",
      "Epoch 4053/40000, Loss: 0.0002190618251916021, Learning Rate: 0.001230\n",
      "Epoch 4054/40000, Loss: 0.0001737620768835768, Learning Rate: 0.001230\n",
      "Epoch 4055/40000, Loss: 0.00014462711988016963, Learning Rate: 0.001229\n",
      "Epoch 4056/40000, Loss: 0.00010059928899863735, Learning Rate: 0.001229\n",
      "Epoch 4057/40000, Loss: 0.00020579913689289242, Learning Rate: 0.001229\n",
      "Epoch 4058/40000, Loss: 0.00013021743507124484, Learning Rate: 0.001229\n",
      "Epoch 4059/40000, Loss: 9.511234384262934e-05, Learning Rate: 0.001229\n",
      "Epoch 4060/40000, Loss: 0.0001308757928200066, Learning Rate: 0.001229\n",
      "Epoch 4061/40000, Loss: 0.0001936259213835001, Learning Rate: 0.001229\n",
      "Epoch 4062/40000, Loss: 0.00014067126903682947, Learning Rate: 0.001228\n",
      "Epoch 4063/40000, Loss: 0.00018803664715960622, Learning Rate: 0.001228\n",
      "Epoch 4064/40000, Loss: 7.415690924972296e-05, Learning Rate: 0.001228\n",
      "Epoch 4065/40000, Loss: 9.779908577911556e-05, Learning Rate: 0.001228\n",
      "Epoch 4066/40000, Loss: 0.0001341212773695588, Learning Rate: 0.001228\n",
      "Epoch 4067/40000, Loss: 0.0001189696486108005, Learning Rate: 0.001228\n",
      "Epoch 4068/40000, Loss: 8.370669092983007e-05, Learning Rate: 0.001228\n",
      "Epoch 4069/40000, Loss: 0.00010842466872418299, Learning Rate: 0.001227\n",
      "Epoch 4070/40000, Loss: 0.000142354765557684, Learning Rate: 0.001227\n",
      "Epoch 4071/40000, Loss: 6.462092278525233e-05, Learning Rate: 0.001227\n",
      "Epoch 4072/40000, Loss: 8.341849024873227e-05, Learning Rate: 0.001227\n",
      "Epoch 4073/40000, Loss: 0.00013872246199753135, Learning Rate: 0.001227\n",
      "Epoch 4074/40000, Loss: 5.739458356401883e-05, Learning Rate: 0.001227\n",
      "Epoch 4075/40000, Loss: 9.437970584258437e-05, Learning Rate: 0.001226\n",
      "Epoch 4076/40000, Loss: 9.167123062070459e-05, Learning Rate: 0.001226\n",
      "Epoch 4077/40000, Loss: 0.00012671355216298252, Learning Rate: 0.001226\n",
      "Epoch 4078/40000, Loss: 5.2373150538187474e-05, Learning Rate: 0.001226\n",
      "Epoch 4079/40000, Loss: 7.298334094230086e-05, Learning Rate: 0.001226\n",
      "Epoch 4080/40000, Loss: 5.971443897578865e-05, Learning Rate: 0.001226\n",
      "Epoch 4081/40000, Loss: 5.553603114094585e-05, Learning Rate: 0.001226\n",
      "Epoch 4082/40000, Loss: 0.00012818194227293134, Learning Rate: 0.001225\n",
      "Epoch 4083/40000, Loss: 0.00012560865434352309, Learning Rate: 0.001225\n",
      "Epoch 4084/40000, Loss: 0.00012620369670912623, Learning Rate: 0.001225\n",
      "Epoch 4085/40000, Loss: 9.112450061365962e-05, Learning Rate: 0.001225\n",
      "Epoch 4086/40000, Loss: 9.008758934214711e-05, Learning Rate: 0.001225\n",
      "Epoch 4087/40000, Loss: 4.9632712034508586e-05, Learning Rate: 0.001225\n",
      "Epoch 4088/40000, Loss: 0.00012403079017531127, Learning Rate: 0.001225\n",
      "Epoch 4089/40000, Loss: 0.00013155094347894192, Learning Rate: 0.001224\n",
      "Epoch 4090/40000, Loss: 8.979409176390618e-05, Learning Rate: 0.001224\n",
      "Epoch 4091/40000, Loss: 0.00013133617176208645, Learning Rate: 0.001224\n",
      "Epoch 4092/40000, Loss: 9.05509150470607e-05, Learning Rate: 0.001224\n",
      "Epoch 4093/40000, Loss: 8.932001219363883e-05, Learning Rate: 0.001224\n",
      "Epoch 4094/40000, Loss: 9.202557703247294e-05, Learning Rate: 0.001224\n",
      "Epoch 4095/40000, Loss: 9.58318414632231e-05, Learning Rate: 0.001224\n",
      "Epoch 4096/40000, Loss: 9.36020296649076e-05, Learning Rate: 0.001223\n",
      "Epoch 4097/40000, Loss: 0.00012238218914717436, Learning Rate: 0.001223\n",
      "Epoch 4098/40000, Loss: 0.00013168217265047133, Learning Rate: 0.001223\n",
      "Epoch 4099/40000, Loss: 9.232672891812399e-05, Learning Rate: 0.001223\n",
      "Epoch 4100/40000, Loss: 8.76924823387526e-05, Learning Rate: 0.001223\n",
      "Epoch 4101/40000, Loss: 9.022344602271914e-05, Learning Rate: 0.001223\n",
      "Epoch 4102/40000, Loss: 7.006295345490798e-05, Learning Rate: 0.001223\n",
      "Epoch 4103/40000, Loss: 4.720635115518235e-05, Learning Rate: 0.001222\n",
      "Epoch 4104/40000, Loss: 0.00013114503235556185, Learning Rate: 0.001222\n",
      "Epoch 4105/40000, Loss: 9.109484381042421e-05, Learning Rate: 0.001222\n",
      "Epoch 4106/40000, Loss: 4.6507338993251324e-05, Learning Rate: 0.001222\n",
      "Epoch 4107/40000, Loss: 0.0001214878138853237, Learning Rate: 0.001222\n",
      "Epoch 4108/40000, Loss: 8.97469071787782e-05, Learning Rate: 0.001222\n",
      "Epoch 4109/40000, Loss: 0.0001307824277319014, Learning Rate: 0.001221\n",
      "Epoch 4110/40000, Loss: 0.0001316518901148811, Learning Rate: 0.001221\n",
      "Epoch 4111/40000, Loss: 8.821439405437559e-05, Learning Rate: 0.001221\n",
      "Epoch 4112/40000, Loss: 0.0001324623153777793, Learning Rate: 0.001221\n",
      "Epoch 4113/40000, Loss: 4.8610378144076094e-05, Learning Rate: 0.001221\n",
      "Epoch 4114/40000, Loss: 7.091557199601084e-05, Learning Rate: 0.001221\n",
      "Epoch 4115/40000, Loss: 4.9752576160244644e-05, Learning Rate: 0.001221\n",
      "Epoch 4116/40000, Loss: 9.198573388857767e-05, Learning Rate: 0.001220\n",
      "Epoch 4117/40000, Loss: 0.00013362574100028723, Learning Rate: 0.001220\n",
      "Epoch 4118/40000, Loss: 0.0001014392837532796, Learning Rate: 0.001220\n",
      "Epoch 4119/40000, Loss: 0.0001313640968874097, Learning Rate: 0.001220\n",
      "Epoch 4120/40000, Loss: 9.483526810072362e-05, Learning Rate: 0.001220\n",
      "Epoch 4121/40000, Loss: 7.475477468688041e-05, Learning Rate: 0.001220\n",
      "Epoch 4122/40000, Loss: 0.0001498977217124775, Learning Rate: 0.001220\n",
      "Epoch 4123/40000, Loss: 0.00015773691120557487, Learning Rate: 0.001219\n",
      "Epoch 4124/40000, Loss: 0.0001565008278703317, Learning Rate: 0.001219\n",
      "Epoch 4125/40000, Loss: 0.00013850505638401955, Learning Rate: 0.001219\n",
      "Epoch 4126/40000, Loss: 0.00013770410441793501, Learning Rate: 0.001219\n",
      "Epoch 4127/40000, Loss: 6.40559010207653e-05, Learning Rate: 0.001219\n",
      "Epoch 4128/40000, Loss: 8.781136421021074e-05, Learning Rate: 0.001219\n",
      "Epoch 4129/40000, Loss: 0.00015056505799293518, Learning Rate: 0.001219\n",
      "Epoch 4130/40000, Loss: 7.916404865682125e-05, Learning Rate: 0.001218\n",
      "Epoch 4131/40000, Loss: 0.00018212536815553904, Learning Rate: 0.001218\n",
      "Epoch 4132/40000, Loss: 0.00014738661411684006, Learning Rate: 0.001218\n",
      "Epoch 4133/40000, Loss: 0.0001631563063710928, Learning Rate: 0.001218\n",
      "Epoch 4134/40000, Loss: 0.00010984721302520484, Learning Rate: 0.001218\n",
      "Epoch 4135/40000, Loss: 0.0001125437265727669, Learning Rate: 0.001218\n",
      "Epoch 4136/40000, Loss: 0.00014497428492177278, Learning Rate: 0.001218\n",
      "Epoch 4137/40000, Loss: 0.00015229362179525197, Learning Rate: 0.001217\n",
      "Epoch 4138/40000, Loss: 0.00022071177954785526, Learning Rate: 0.001217\n",
      "Epoch 4139/40000, Loss: 0.00011879546218551695, Learning Rate: 0.001217\n",
      "Epoch 4140/40000, Loss: 0.00033458179677836597, Learning Rate: 0.001217\n",
      "Epoch 4141/40000, Loss: 0.00018206407548859715, Learning Rate: 0.001217\n",
      "Epoch 4142/40000, Loss: 0.00012109210365451872, Learning Rate: 0.001217\n",
      "Epoch 4143/40000, Loss: 0.0001412098208675161, Learning Rate: 0.001217\n",
      "Epoch 4144/40000, Loss: 0.0003685197443701327, Learning Rate: 0.001216\n",
      "Epoch 4145/40000, Loss: 0.00026214818353764713, Learning Rate: 0.001216\n",
      "Epoch 4146/40000, Loss: 0.0001888040715130046, Learning Rate: 0.001216\n",
      "Epoch 4147/40000, Loss: 0.00021267026022542268, Learning Rate: 0.001216\n",
      "Epoch 4148/40000, Loss: 0.00014938812819309533, Learning Rate: 0.001216\n",
      "Epoch 4149/40000, Loss: 0.00017652510723564774, Learning Rate: 0.001216\n",
      "Epoch 4150/40000, Loss: 0.00024386026780121028, Learning Rate: 0.001215\n",
      "Epoch 4151/40000, Loss: 0.0001689872151473537, Learning Rate: 0.001215\n",
      "Epoch 4152/40000, Loss: 0.00020256458083167672, Learning Rate: 0.001215\n",
      "Epoch 4153/40000, Loss: 0.00012766860891133547, Learning Rate: 0.001215\n",
      "Epoch 4154/40000, Loss: 0.00019263141439296305, Learning Rate: 0.001215\n",
      "Epoch 4155/40000, Loss: 0.00013080015196464956, Learning Rate: 0.001215\n",
      "Epoch 4156/40000, Loss: 0.00011376786278560758, Learning Rate: 0.001215\n",
      "Epoch 4157/40000, Loss: 0.0001686627510935068, Learning Rate: 0.001214\n",
      "Epoch 4158/40000, Loss: 0.0001717747945804149, Learning Rate: 0.001214\n",
      "Epoch 4159/40000, Loss: 0.00020711503748316318, Learning Rate: 0.001214\n",
      "Epoch 4160/40000, Loss: 0.00013686870806850493, Learning Rate: 0.001214\n",
      "Epoch 4161/40000, Loss: 9.778016101336107e-05, Learning Rate: 0.001214\n",
      "Epoch 4162/40000, Loss: 8.696335862623528e-05, Learning Rate: 0.001214\n",
      "Epoch 4163/40000, Loss: 0.00012488479842431843, Learning Rate: 0.001214\n",
      "Epoch 4164/40000, Loss: 0.00010868233948713169, Learning Rate: 0.001213\n",
      "Epoch 4165/40000, Loss: 0.00015202234499156475, Learning Rate: 0.001213\n",
      "Epoch 4166/40000, Loss: 8.417292701778933e-05, Learning Rate: 0.001213\n",
      "Epoch 4167/40000, Loss: 0.00011319550685584545, Learning Rate: 0.001213\n",
      "Epoch 4168/40000, Loss: 0.00010631141776684672, Learning Rate: 0.001213\n",
      "Epoch 4169/40000, Loss: 0.00010653560457285494, Learning Rate: 0.001213\n",
      "Epoch 4170/40000, Loss: 0.00014739726611878723, Learning Rate: 0.001213\n",
      "Epoch 4171/40000, Loss: 0.00012981730105821043, Learning Rate: 0.001212\n",
      "Epoch 4172/40000, Loss: 0.00015418347902595997, Learning Rate: 0.001212\n",
      "Epoch 4173/40000, Loss: 5.585369581240229e-05, Learning Rate: 0.001212\n",
      "Epoch 4174/40000, Loss: 5.7137531257467344e-05, Learning Rate: 0.001212\n",
      "Epoch 4175/40000, Loss: 7.898877811385319e-05, Learning Rate: 0.001212\n",
      "Epoch 4176/40000, Loss: 9.759871318237856e-05, Learning Rate: 0.001212\n",
      "Epoch 4177/40000, Loss: 9.557464363751933e-05, Learning Rate: 0.001212\n",
      "Epoch 4178/40000, Loss: 0.0001346551871392876, Learning Rate: 0.001211\n",
      "Epoch 4179/40000, Loss: 8.989971684059128e-05, Learning Rate: 0.001211\n",
      "Epoch 4180/40000, Loss: 7.301972800632939e-05, Learning Rate: 0.001211\n",
      "Epoch 4181/40000, Loss: 0.0001312302629230544, Learning Rate: 0.001211\n",
      "Epoch 4182/40000, Loss: 7.221471605589613e-05, Learning Rate: 0.001211\n",
      "Epoch 4183/40000, Loss: 0.00013745835167355835, Learning Rate: 0.001211\n",
      "Epoch 4184/40000, Loss: 0.00012374744983389974, Learning Rate: 0.001211\n",
      "Epoch 4185/40000, Loss: 0.00013416432193480432, Learning Rate: 0.001210\n",
      "Epoch 4186/40000, Loss: 8.857760985847563e-05, Learning Rate: 0.001210\n",
      "Epoch 4187/40000, Loss: 0.00015967719082254916, Learning Rate: 0.001210\n",
      "Epoch 4188/40000, Loss: 0.00014217091666068882, Learning Rate: 0.001210\n",
      "Epoch 4189/40000, Loss: 8.927622548071668e-05, Learning Rate: 0.001210\n",
      "Epoch 4190/40000, Loss: 7.306363841053098e-05, Learning Rate: 0.001210\n",
      "Epoch 4191/40000, Loss: 0.00012907720520161092, Learning Rate: 0.001210\n",
      "Epoch 4192/40000, Loss: 5.288646570988931e-05, Learning Rate: 0.001209\n",
      "Epoch 4193/40000, Loss: 8.980814163805917e-05, Learning Rate: 0.001209\n",
      "Epoch 4194/40000, Loss: 9.090339881367981e-05, Learning Rate: 0.001209\n",
      "Epoch 4195/40000, Loss: 9.13554904400371e-05, Learning Rate: 0.001209\n",
      "Epoch 4196/40000, Loss: 0.00013159785885363817, Learning Rate: 0.001209\n",
      "Epoch 4197/40000, Loss: 7.259891572175547e-05, Learning Rate: 0.001209\n",
      "Epoch 4198/40000, Loss: 7.213091885205358e-05, Learning Rate: 0.001209\n",
      "Epoch 4199/40000, Loss: 7.151441241148859e-05, Learning Rate: 0.001208\n",
      "Epoch 4200/40000, Loss: 0.0001393382844980806, Learning Rate: 0.001208\n",
      "Epoch 4201/40000, Loss: 9.044630132848397e-05, Learning Rate: 0.001208\n",
      "Epoch 4202/40000, Loss: 5.343168231775053e-05, Learning Rate: 0.001208\n",
      "Epoch 4203/40000, Loss: 0.00014671617827843875, Learning Rate: 0.001208\n",
      "Epoch 4204/40000, Loss: 7.60855691623874e-05, Learning Rate: 0.001208\n",
      "Epoch 4205/40000, Loss: 0.00010675997327780351, Learning Rate: 0.001207\n",
      "Epoch 4206/40000, Loss: 0.00018914729298558086, Learning Rate: 0.001207\n",
      "Epoch 4207/40000, Loss: 7.637246744707227e-05, Learning Rate: 0.001207\n",
      "Epoch 4208/40000, Loss: 0.00015009200433269143, Learning Rate: 0.001207\n",
      "Epoch 4209/40000, Loss: 0.00012105544010410085, Learning Rate: 0.001207\n",
      "Epoch 4210/40000, Loss: 0.000109130960481707, Learning Rate: 0.001207\n",
      "Epoch 4211/40000, Loss: 0.00010460348130436614, Learning Rate: 0.001207\n",
      "Epoch 4212/40000, Loss: 0.00010253579239360988, Learning Rate: 0.001206\n",
      "Epoch 4213/40000, Loss: 0.00016273865185212344, Learning Rate: 0.001206\n",
      "Epoch 4214/40000, Loss: 0.00015005901514086872, Learning Rate: 0.001206\n",
      "Epoch 4215/40000, Loss: 9.568320820108056e-05, Learning Rate: 0.001206\n",
      "Epoch 4216/40000, Loss: 0.00015920447185635567, Learning Rate: 0.001206\n",
      "Epoch 4217/40000, Loss: 0.0001617386151337996, Learning Rate: 0.001206\n",
      "Epoch 4218/40000, Loss: 0.00014858416398055851, Learning Rate: 0.001206\n",
      "Epoch 4219/40000, Loss: 0.00019018413149751723, Learning Rate: 0.001205\n",
      "Epoch 4220/40000, Loss: 0.00021689313871320337, Learning Rate: 0.001205\n",
      "Epoch 4221/40000, Loss: 0.0001109202130464837, Learning Rate: 0.001205\n",
      "Epoch 4222/40000, Loss: 0.00010012568236561492, Learning Rate: 0.001205\n",
      "Epoch 4223/40000, Loss: 7.049083797028288e-05, Learning Rate: 0.001205\n",
      "Epoch 4224/40000, Loss: 6.13275624345988e-05, Learning Rate: 0.001205\n",
      "Epoch 4225/40000, Loss: 0.00012001767026958987, Learning Rate: 0.001205\n",
      "Epoch 4226/40000, Loss: 9.36408614506945e-05, Learning Rate: 0.001204\n",
      "Epoch 4227/40000, Loss: 8.60482978168875e-05, Learning Rate: 0.001204\n",
      "Epoch 4228/40000, Loss: 0.00013836762809660286, Learning Rate: 0.001204\n",
      "Epoch 4229/40000, Loss: 0.00015357541269622743, Learning Rate: 0.001204\n",
      "Epoch 4230/40000, Loss: 0.00010450965783093125, Learning Rate: 0.001204\n",
      "Epoch 4231/40000, Loss: 0.00019526189134921879, Learning Rate: 0.001204\n",
      "Epoch 4232/40000, Loss: 0.0001309462677454576, Learning Rate: 0.001204\n",
      "Epoch 4233/40000, Loss: 0.0001205518128699623, Learning Rate: 0.001203\n",
      "Epoch 4234/40000, Loss: 0.0002891295007430017, Learning Rate: 0.001203\n",
      "Epoch 4235/40000, Loss: 0.0002216589346062392, Learning Rate: 0.001203\n",
      "Epoch 4236/40000, Loss: 0.00011695452849380672, Learning Rate: 0.001203\n",
      "Epoch 4237/40000, Loss: 0.0001396199077134952, Learning Rate: 0.001203\n",
      "Epoch 4238/40000, Loss: 0.0002466605219524354, Learning Rate: 0.001203\n",
      "Epoch 4239/40000, Loss: 0.00027766573475673795, Learning Rate: 0.001203\n",
      "Epoch 4240/40000, Loss: 0.00029225434991531074, Learning Rate: 0.001202\n",
      "Epoch 4241/40000, Loss: 0.00021457221009768546, Learning Rate: 0.001202\n",
      "Epoch 4242/40000, Loss: 0.00017213575483765453, Learning Rate: 0.001202\n",
      "Epoch 4243/40000, Loss: 0.00025652325712144375, Learning Rate: 0.001202\n",
      "Epoch 4244/40000, Loss: 0.00020707346266135573, Learning Rate: 0.001202\n",
      "Epoch 4245/40000, Loss: 0.0002404304686933756, Learning Rate: 0.001202\n",
      "Epoch 4246/40000, Loss: 0.00045869959285482764, Learning Rate: 0.001202\n",
      "Epoch 4247/40000, Loss: 0.00036623916821554303, Learning Rate: 0.001201\n",
      "Epoch 4248/40000, Loss: 0.00024251980357803404, Learning Rate: 0.001201\n",
      "Epoch 4249/40000, Loss: 0.0002554400125518441, Learning Rate: 0.001201\n",
      "Epoch 4250/40000, Loss: 9.9961667729076e-05, Learning Rate: 0.001201\n",
      "Epoch 4251/40000, Loss: 7.774690311634913e-05, Learning Rate: 0.001201\n",
      "Epoch 4252/40000, Loss: 0.00011143317533424124, Learning Rate: 0.001201\n",
      "Epoch 4253/40000, Loss: 0.0001817418378777802, Learning Rate: 0.001201\n",
      "Epoch 4254/40000, Loss: 0.00025750594795681536, Learning Rate: 0.001200\n",
      "Epoch 4255/40000, Loss: 0.0002259523025713861, Learning Rate: 0.001200\n",
      "Epoch 4256/40000, Loss: 0.0002465566503815353, Learning Rate: 0.001200\n",
      "Epoch 4257/40000, Loss: 0.00026344668003730476, Learning Rate: 0.001200\n",
      "Epoch 4258/40000, Loss: 0.00020050919556524605, Learning Rate: 0.001200\n",
      "Epoch 4259/40000, Loss: 0.00010442391794640571, Learning Rate: 0.001200\n",
      "Epoch 4260/40000, Loss: 0.00014578951231669635, Learning Rate: 0.001200\n",
      "Epoch 4261/40000, Loss: 0.00020816881442442536, Learning Rate: 0.001199\n",
      "Epoch 4262/40000, Loss: 0.00023947509180288762, Learning Rate: 0.001199\n",
      "Epoch 4263/40000, Loss: 0.00014191912487149239, Learning Rate: 0.001199\n",
      "Epoch 4264/40000, Loss: 0.00014808033301960677, Learning Rate: 0.001199\n",
      "Epoch 4265/40000, Loss: 0.0002955803938675672, Learning Rate: 0.001199\n",
      "Epoch 4266/40000, Loss: 0.00030282276566140354, Learning Rate: 0.001199\n",
      "Epoch 4267/40000, Loss: 0.00021316480706445873, Learning Rate: 0.001199\n",
      "Epoch 4268/40000, Loss: 0.0001265817554667592, Learning Rate: 0.001198\n",
      "Epoch 4269/40000, Loss: 0.00010431335249450058, Learning Rate: 0.001198\n",
      "Epoch 4270/40000, Loss: 9.709732694318518e-05, Learning Rate: 0.001198\n",
      "Epoch 4271/40000, Loss: 0.0001634505606489256, Learning Rate: 0.001198\n",
      "Epoch 4272/40000, Loss: 0.0001227848115377128, Learning Rate: 0.001198\n",
      "Epoch 4273/40000, Loss: 0.0001539755321573466, Learning Rate: 0.001198\n",
      "Epoch 4274/40000, Loss: 9.723480616230518e-05, Learning Rate: 0.001198\n",
      "Epoch 4275/40000, Loss: 0.00013428748934529722, Learning Rate: 0.001197\n",
      "Epoch 4276/40000, Loss: 8.035151404328644e-05, Learning Rate: 0.001197\n",
      "Epoch 4277/40000, Loss: 0.00010109128925250843, Learning Rate: 0.001197\n",
      "Epoch 4278/40000, Loss: 0.00014346421812660992, Learning Rate: 0.001197\n",
      "Epoch 4279/40000, Loss: 0.00016982652596198022, Learning Rate: 0.001197\n",
      "Epoch 4280/40000, Loss: 0.00023350771516561508, Learning Rate: 0.001197\n",
      "Epoch 4281/40000, Loss: 0.00010894786100834608, Learning Rate: 0.001197\n",
      "Epoch 4282/40000, Loss: 0.0001640942064113915, Learning Rate: 0.001196\n",
      "Epoch 4283/40000, Loss: 0.00010257604299113154, Learning Rate: 0.001196\n",
      "Epoch 4284/40000, Loss: 6.759334064554423e-05, Learning Rate: 0.001196\n",
      "Epoch 4285/40000, Loss: 0.00014889778685756028, Learning Rate: 0.001196\n",
      "Epoch 4286/40000, Loss: 8.357497426914051e-05, Learning Rate: 0.001196\n",
      "Epoch 4287/40000, Loss: 0.00010405149805592373, Learning Rate: 0.001196\n",
      "Epoch 4288/40000, Loss: 9.743976988829672e-05, Learning Rate: 0.001196\n",
      "Epoch 4289/40000, Loss: 9.874808165477589e-05, Learning Rate: 0.001195\n",
      "Epoch 4290/40000, Loss: 5.8604498917702585e-05, Learning Rate: 0.001195\n",
      "Epoch 4291/40000, Loss: 6.644447421422228e-05, Learning Rate: 0.001195\n",
      "Epoch 4292/40000, Loss: 6.0440659581217915e-05, Learning Rate: 0.001195\n",
      "Epoch 4293/40000, Loss: 5.0337483116891235e-05, Learning Rate: 0.001195\n",
      "Epoch 4294/40000, Loss: 0.00010865799413295463, Learning Rate: 0.001195\n",
      "Epoch 4295/40000, Loss: 0.00010517535702092573, Learning Rate: 0.001195\n",
      "Epoch 4296/40000, Loss: 6.948575173737481e-05, Learning Rate: 0.001194\n",
      "Epoch 4297/40000, Loss: 0.00010212392953690141, Learning Rate: 0.001194\n",
      "Epoch 4298/40000, Loss: 0.00011063426063628867, Learning Rate: 0.001194\n",
      "Epoch 4299/40000, Loss: 0.0001420671760570258, Learning Rate: 0.001194\n",
      "Epoch 4300/40000, Loss: 0.00013025960652157664, Learning Rate: 0.001194\n",
      "Epoch 4301/40000, Loss: 0.00011761390487663448, Learning Rate: 0.001194\n",
      "Epoch 4302/40000, Loss: 9.020057041198015e-05, Learning Rate: 0.001194\n",
      "Epoch 4303/40000, Loss: 0.00013803673209622502, Learning Rate: 0.001193\n",
      "Epoch 4304/40000, Loss: 0.00015331388567574322, Learning Rate: 0.001193\n",
      "Epoch 4305/40000, Loss: 0.00011985667515546083, Learning Rate: 0.001193\n",
      "Epoch 4306/40000, Loss: 6.000931534799747e-05, Learning Rate: 0.001193\n",
      "Epoch 4307/40000, Loss: 0.00014408768038265407, Learning Rate: 0.001193\n",
      "Epoch 4308/40000, Loss: 0.00011135742533951998, Learning Rate: 0.001193\n",
      "Epoch 4309/40000, Loss: 7.85262236604467e-05, Learning Rate: 0.001193\n",
      "Epoch 4310/40000, Loss: 0.00017033691983669996, Learning Rate: 0.001192\n",
      "Epoch 4311/40000, Loss: 0.00015060165605973452, Learning Rate: 0.001192\n",
      "Epoch 4312/40000, Loss: 0.0001032500877045095, Learning Rate: 0.001192\n",
      "Epoch 4313/40000, Loss: 9.793532080948353e-05, Learning Rate: 0.001192\n",
      "Epoch 4314/40000, Loss: 8.085477747954428e-05, Learning Rate: 0.001192\n",
      "Epoch 4315/40000, Loss: 0.00012091473763575777, Learning Rate: 0.001192\n",
      "Epoch 4316/40000, Loss: 6.442592712119222e-05, Learning Rate: 0.001192\n",
      "Epoch 4317/40000, Loss: 9.321186371380463e-05, Learning Rate: 0.001191\n",
      "Epoch 4318/40000, Loss: 7.262911094585434e-05, Learning Rate: 0.001191\n",
      "Epoch 4319/40000, Loss: 0.00011944173456868157, Learning Rate: 0.001191\n",
      "Epoch 4320/40000, Loss: 8.672814146848395e-05, Learning Rate: 0.001191\n",
      "Epoch 4321/40000, Loss: 0.00010104300599778071, Learning Rate: 0.001191\n",
      "Epoch 4322/40000, Loss: 5.679094829247333e-05, Learning Rate: 0.001191\n",
      "Epoch 4323/40000, Loss: 7.534882752224803e-05, Learning Rate: 0.001191\n",
      "Epoch 4324/40000, Loss: 0.00010345747432438657, Learning Rate: 0.001190\n",
      "Epoch 4325/40000, Loss: 0.00013741155271418393, Learning Rate: 0.001190\n",
      "Epoch 4326/40000, Loss: 5.7010543969227e-05, Learning Rate: 0.001190\n",
      "Epoch 4327/40000, Loss: 9.722446702653542e-05, Learning Rate: 0.001190\n",
      "Epoch 4328/40000, Loss: 0.00014991615898907185, Learning Rate: 0.001190\n",
      "Epoch 4329/40000, Loss: 0.00010175040370086208, Learning Rate: 0.001190\n",
      "Epoch 4330/40000, Loss: 9.881370351649821e-05, Learning Rate: 0.001190\n",
      "Epoch 4331/40000, Loss: 8.338493353221565e-05, Learning Rate: 0.001189\n",
      "Epoch 4332/40000, Loss: 8.686156797921285e-05, Learning Rate: 0.001189\n",
      "Epoch 4333/40000, Loss: 8.276435983134434e-05, Learning Rate: 0.001189\n",
      "Epoch 4334/40000, Loss: 7.692731742281467e-05, Learning Rate: 0.001189\n",
      "Epoch 4335/40000, Loss: 0.00015518561122007668, Learning Rate: 0.001189\n",
      "Epoch 4336/40000, Loss: 7.078574708430097e-05, Learning Rate: 0.001189\n",
      "Epoch 4337/40000, Loss: 0.00013118385686539114, Learning Rate: 0.001189\n",
      "Epoch 4338/40000, Loss: 0.00011230377276660874, Learning Rate: 0.001188\n",
      "Epoch 4339/40000, Loss: 8.000672096386552e-05, Learning Rate: 0.001188\n",
      "Epoch 4340/40000, Loss: 9.452666563447565e-05, Learning Rate: 0.001188\n",
      "Epoch 4341/40000, Loss: 0.00012856844114139676, Learning Rate: 0.001188\n",
      "Epoch 4342/40000, Loss: 9.727224096423015e-05, Learning Rate: 0.001188\n",
      "Epoch 4343/40000, Loss: 6.977231532800943e-05, Learning Rate: 0.001188\n",
      "Epoch 4344/40000, Loss: 0.0001271692744921893, Learning Rate: 0.001188\n",
      "Epoch 4345/40000, Loss: 7.517638005083427e-05, Learning Rate: 0.001187\n",
      "Epoch 4346/40000, Loss: 0.0001360028109047562, Learning Rate: 0.001187\n",
      "Epoch 4347/40000, Loss: 0.00011473731137812138, Learning Rate: 0.001187\n",
      "Epoch 4348/40000, Loss: 0.00011988032201770693, Learning Rate: 0.001187\n",
      "Epoch 4349/40000, Loss: 0.00015461513248737901, Learning Rate: 0.001187\n",
      "Epoch 4350/40000, Loss: 0.0001473732991144061, Learning Rate: 0.001187\n",
      "Epoch 4351/40000, Loss: 0.00010648705210769549, Learning Rate: 0.001187\n",
      "Epoch 4352/40000, Loss: 0.00013757483975496143, Learning Rate: 0.001186\n",
      "Epoch 4353/40000, Loss: 9.669806604506448e-05, Learning Rate: 0.001186\n",
      "Epoch 4354/40000, Loss: 0.00010199662938248366, Learning Rate: 0.001186\n",
      "Epoch 4355/40000, Loss: 5.2280858653830364e-05, Learning Rate: 0.001186\n",
      "Epoch 4356/40000, Loss: 5.1050858019152656e-05, Learning Rate: 0.001186\n",
      "Epoch 4357/40000, Loss: 0.00014225911581888795, Learning Rate: 0.001186\n",
      "Epoch 4358/40000, Loss: 6.227268022485077e-05, Learning Rate: 0.001186\n",
      "Epoch 4359/40000, Loss: 0.00014303255011327565, Learning Rate: 0.001185\n",
      "Epoch 4360/40000, Loss: 7.371097308350727e-05, Learning Rate: 0.001185\n",
      "Epoch 4361/40000, Loss: 0.00011028851440642029, Learning Rate: 0.001185\n",
      "Epoch 4362/40000, Loss: 8.422062819590792e-05, Learning Rate: 0.001185\n",
      "Epoch 4363/40000, Loss: 0.00017003480752464384, Learning Rate: 0.001185\n",
      "Epoch 4364/40000, Loss: 0.00011195888509973884, Learning Rate: 0.001185\n",
      "Epoch 4365/40000, Loss: 6.23965825070627e-05, Learning Rate: 0.001185\n",
      "Epoch 4366/40000, Loss: 0.0002265613729832694, Learning Rate: 0.001184\n",
      "Epoch 4367/40000, Loss: 0.00021441229910124093, Learning Rate: 0.001184\n",
      "Epoch 4368/40000, Loss: 0.0001749248622218147, Learning Rate: 0.001184\n",
      "Epoch 4369/40000, Loss: 0.0001524196268292144, Learning Rate: 0.001184\n",
      "Epoch 4370/40000, Loss: 0.00011520794942043722, Learning Rate: 0.001184\n",
      "Epoch 4371/40000, Loss: 0.00029204058228060603, Learning Rate: 0.001184\n",
      "Epoch 4372/40000, Loss: 0.00014011640450917184, Learning Rate: 0.001184\n",
      "Epoch 4373/40000, Loss: 0.00010945540270768106, Learning Rate: 0.001183\n",
      "Epoch 4374/40000, Loss: 0.00017222543829120696, Learning Rate: 0.001183\n",
      "Epoch 4375/40000, Loss: 0.00010612177720759064, Learning Rate: 0.001183\n",
      "Epoch 4376/40000, Loss: 0.00025358772836625576, Learning Rate: 0.001183\n",
      "Epoch 4377/40000, Loss: 0.00020239259174559265, Learning Rate: 0.001183\n",
      "Epoch 4378/40000, Loss: 0.00010837474110303447, Learning Rate: 0.001183\n",
      "Epoch 4379/40000, Loss: 0.00010599625966278836, Learning Rate: 0.001183\n",
      "Epoch 4380/40000, Loss: 0.0001257293188245967, Learning Rate: 0.001182\n",
      "Epoch 4381/40000, Loss: 0.00012311001773923635, Learning Rate: 0.001182\n",
      "Epoch 4382/40000, Loss: 7.002376514719799e-05, Learning Rate: 0.001182\n",
      "Epoch 4383/40000, Loss: 0.00016552982560824603, Learning Rate: 0.001182\n",
      "Epoch 4384/40000, Loss: 0.00015895915566943586, Learning Rate: 0.001182\n",
      "Epoch 4385/40000, Loss: 0.00015695387264713645, Learning Rate: 0.001182\n",
      "Epoch 4386/40000, Loss: 0.00012523640180006623, Learning Rate: 0.001182\n",
      "Epoch 4387/40000, Loss: 0.0001237392716575414, Learning Rate: 0.001181\n",
      "Epoch 4388/40000, Loss: 0.0002058754616882652, Learning Rate: 0.001181\n",
      "Epoch 4389/40000, Loss: 7.521388761233538e-05, Learning Rate: 0.001181\n",
      "Epoch 4390/40000, Loss: 5.921163392486051e-05, Learning Rate: 0.001181\n",
      "Epoch 4391/40000, Loss: 9.782213601283729e-05, Learning Rate: 0.001181\n",
      "Epoch 4392/40000, Loss: 7.887588435551152e-05, Learning Rate: 0.001181\n",
      "Epoch 4393/40000, Loss: 5.411696474766359e-05, Learning Rate: 0.001181\n",
      "Epoch 4394/40000, Loss: 0.0001449703995604068, Learning Rate: 0.001180\n",
      "Epoch 4395/40000, Loss: 0.00011339393677189946, Learning Rate: 0.001180\n",
      "Epoch 4396/40000, Loss: 0.0001432101271348074, Learning Rate: 0.001180\n",
      "Epoch 4397/40000, Loss: 0.0001022828437271528, Learning Rate: 0.001180\n",
      "Epoch 4398/40000, Loss: 0.00013251446944195777, Learning Rate: 0.001180\n",
      "Epoch 4399/40000, Loss: 8.841095404932275e-05, Learning Rate: 0.001180\n",
      "Epoch 4400/40000, Loss: 0.00017364909581374377, Learning Rate: 0.001180\n",
      "Epoch 4401/40000, Loss: 9.754220809554681e-05, Learning Rate: 0.001179\n",
      "Epoch 4402/40000, Loss: 0.00010229805775452405, Learning Rate: 0.001179\n",
      "Epoch 4403/40000, Loss: 0.00013260176638141274, Learning Rate: 0.001179\n",
      "Epoch 4404/40000, Loss: 0.00016849819803610444, Learning Rate: 0.001179\n",
      "Epoch 4405/40000, Loss: 0.00011327279207762331, Learning Rate: 0.001179\n",
      "Epoch 4406/40000, Loss: 0.00013117345224600285, Learning Rate: 0.001179\n",
      "Epoch 4407/40000, Loss: 0.00019084353698417544, Learning Rate: 0.001179\n",
      "Epoch 4408/40000, Loss: 0.0002255709987366572, Learning Rate: 0.001178\n",
      "Epoch 4409/40000, Loss: 0.00040062915650196373, Learning Rate: 0.001178\n",
      "Epoch 4410/40000, Loss: 0.0001388655073242262, Learning Rate: 0.001178\n",
      "Epoch 4411/40000, Loss: 0.00027071579825133085, Learning Rate: 0.001178\n",
      "Epoch 4412/40000, Loss: 0.00020533250062726438, Learning Rate: 0.001178\n",
      "Epoch 4413/40000, Loss: 0.00016324159514624625, Learning Rate: 0.001178\n",
      "Epoch 4414/40000, Loss: 0.0002886032743845135, Learning Rate: 0.001178\n",
      "Epoch 4415/40000, Loss: 0.00032943501719273627, Learning Rate: 0.001177\n",
      "Epoch 4416/40000, Loss: 0.00013611692702397704, Learning Rate: 0.001177\n",
      "Epoch 4417/40000, Loss: 0.0002004949055844918, Learning Rate: 0.001177\n",
      "Epoch 4418/40000, Loss: 8.506307494826615e-05, Learning Rate: 0.001177\n",
      "Epoch 4419/40000, Loss: 0.00017815559112932533, Learning Rate: 0.001177\n",
      "Epoch 4420/40000, Loss: 0.00011130249185953289, Learning Rate: 0.001177\n",
      "Epoch 4421/40000, Loss: 8.672729745740071e-05, Learning Rate: 0.001177\n",
      "Epoch 4422/40000, Loss: 0.00014906268916092813, Learning Rate: 0.001176\n",
      "Epoch 4423/40000, Loss: 0.00010693339572753757, Learning Rate: 0.001176\n",
      "Epoch 4424/40000, Loss: 0.00015172090206760913, Learning Rate: 0.001176\n",
      "Epoch 4425/40000, Loss: 0.00013673333160113543, Learning Rate: 0.001176\n",
      "Epoch 4426/40000, Loss: 0.00010163067781832069, Learning Rate: 0.001176\n",
      "Epoch 4427/40000, Loss: 9.901206067297608e-05, Learning Rate: 0.001176\n",
      "Epoch 4428/40000, Loss: 0.00013753739767707884, Learning Rate: 0.001176\n",
      "Epoch 4429/40000, Loss: 5.2987765229772776e-05, Learning Rate: 0.001175\n",
      "Epoch 4430/40000, Loss: 9.339466487290338e-05, Learning Rate: 0.001175\n",
      "Epoch 4431/40000, Loss: 0.00013355941337067634, Learning Rate: 0.001175\n",
      "Epoch 4432/40000, Loss: 0.00012485777551773936, Learning Rate: 0.001175\n",
      "Epoch 4433/40000, Loss: 9.726616553962231e-05, Learning Rate: 0.001175\n",
      "Epoch 4434/40000, Loss: 0.00013090396532788873, Learning Rate: 0.001175\n",
      "Epoch 4435/40000, Loss: 0.00012881969450972974, Learning Rate: 0.001175\n",
      "Epoch 4436/40000, Loss: 8.708029781701043e-05, Learning Rate: 0.001174\n",
      "Epoch 4437/40000, Loss: 0.0001327895006397739, Learning Rate: 0.001174\n",
      "Epoch 4438/40000, Loss: 5.156639963388443e-05, Learning Rate: 0.001174\n",
      "Epoch 4439/40000, Loss: 4.837396045331843e-05, Learning Rate: 0.001174\n",
      "Epoch 4440/40000, Loss: 0.00012192491703899577, Learning Rate: 0.001174\n",
      "Epoch 4441/40000, Loss: 7.23316115909256e-05, Learning Rate: 0.001174\n",
      "Epoch 4442/40000, Loss: 9.060359298018739e-05, Learning Rate: 0.001174\n",
      "Epoch 4443/40000, Loss: 4.8599442379781976e-05, Learning Rate: 0.001173\n",
      "Epoch 4444/40000, Loss: 0.0001316922134719789, Learning Rate: 0.001173\n",
      "Epoch 4445/40000, Loss: 0.00012191911082481965, Learning Rate: 0.001173\n",
      "Epoch 4446/40000, Loss: 7.096884655766189e-05, Learning Rate: 0.001173\n",
      "Epoch 4447/40000, Loss: 0.00014048015873413533, Learning Rate: 0.001173\n",
      "Epoch 4448/40000, Loss: 9.512960241409019e-05, Learning Rate: 0.001173\n",
      "Epoch 4449/40000, Loss: 9.479363507125527e-05, Learning Rate: 0.001173\n",
      "Epoch 4450/40000, Loss: 9.209859126713127e-05, Learning Rate: 0.001173\n",
      "Epoch 4451/40000, Loss: 0.0002515423111617565, Learning Rate: 0.001172\n",
      "Epoch 4452/40000, Loss: 8.694225834915414e-05, Learning Rate: 0.001172\n",
      "Epoch 4453/40000, Loss: 0.00017773243598639965, Learning Rate: 0.001172\n",
      "Epoch 4454/40000, Loss: 0.0001681738067418337, Learning Rate: 0.001172\n",
      "Epoch 4455/40000, Loss: 7.522416126448661e-05, Learning Rate: 0.001172\n",
      "Epoch 4456/40000, Loss: 7.298616401385516e-05, Learning Rate: 0.001172\n",
      "Epoch 4457/40000, Loss: 0.00016655494982842356, Learning Rate: 0.001172\n",
      "Epoch 4458/40000, Loss: 0.00021930283401161432, Learning Rate: 0.001171\n",
      "Epoch 4459/40000, Loss: 0.00019307521870359778, Learning Rate: 0.001171\n",
      "Epoch 4460/40000, Loss: 0.00015625133528374135, Learning Rate: 0.001171\n",
      "Epoch 4461/40000, Loss: 0.00013231050979811698, Learning Rate: 0.001171\n",
      "Epoch 4462/40000, Loss: 0.000145066631375812, Learning Rate: 0.001171\n",
      "Epoch 4463/40000, Loss: 0.00013557629426941276, Learning Rate: 0.001171\n",
      "Epoch 4464/40000, Loss: 0.00018948476645164192, Learning Rate: 0.001171\n",
      "Epoch 4465/40000, Loss: 0.00018304859986528754, Learning Rate: 0.001170\n",
      "Epoch 4466/40000, Loss: 0.00012587785022333264, Learning Rate: 0.001170\n",
      "Epoch 4467/40000, Loss: 9.722341201268137e-05, Learning Rate: 0.001170\n",
      "Epoch 4468/40000, Loss: 9.416562534170225e-05, Learning Rate: 0.001170\n",
      "Epoch 4469/40000, Loss: 0.00015874544624239206, Learning Rate: 0.001170\n",
      "Epoch 4470/40000, Loss: 0.00014529198233503848, Learning Rate: 0.001170\n",
      "Epoch 4471/40000, Loss: 0.00016096673789434135, Learning Rate: 0.001170\n",
      "Epoch 4472/40000, Loss: 0.00010571027814876288, Learning Rate: 0.001169\n",
      "Epoch 4473/40000, Loss: 0.00018388379248790443, Learning Rate: 0.001169\n",
      "Epoch 4474/40000, Loss: 0.0001913227024488151, Learning Rate: 0.001169\n",
      "Epoch 4475/40000, Loss: 0.00024456539540551603, Learning Rate: 0.001169\n",
      "Epoch 4476/40000, Loss: 0.00019412304391153157, Learning Rate: 0.001169\n",
      "Epoch 4477/40000, Loss: 0.0001356612629024312, Learning Rate: 0.001169\n",
      "Epoch 4478/40000, Loss: 0.0001223872823175043, Learning Rate: 0.001169\n",
      "Epoch 4479/40000, Loss: 0.00011274638382019475, Learning Rate: 0.001168\n",
      "Epoch 4480/40000, Loss: 0.0001346354401903227, Learning Rate: 0.001168\n",
      "Epoch 4481/40000, Loss: 0.00021939535508863628, Learning Rate: 0.001168\n",
      "Epoch 4482/40000, Loss: 0.00012641906505450606, Learning Rate: 0.001168\n",
      "Epoch 4483/40000, Loss: 0.00012962585606146604, Learning Rate: 0.001168\n",
      "Epoch 4484/40000, Loss: 0.00026628546766005456, Learning Rate: 0.001168\n",
      "Epoch 4485/40000, Loss: 0.00021295464830473065, Learning Rate: 0.001168\n",
      "Epoch 4486/40000, Loss: 0.00026407503173686564, Learning Rate: 0.001167\n",
      "Epoch 4487/40000, Loss: 0.00015556399011984468, Learning Rate: 0.001167\n",
      "Epoch 4488/40000, Loss: 0.00013044812658336014, Learning Rate: 0.001167\n",
      "Epoch 4489/40000, Loss: 0.0001477098121540621, Learning Rate: 0.001167\n",
      "Epoch 4490/40000, Loss: 0.0010371007956564426, Learning Rate: 0.001167\n",
      "Epoch 4491/40000, Loss: 0.0001574623747728765, Learning Rate: 0.001167\n",
      "Epoch 4492/40000, Loss: 0.00013588496949523687, Learning Rate: 0.001167\n",
      "Epoch 4493/40000, Loss: 0.00019378111755941063, Learning Rate: 0.001166\n",
      "Epoch 4494/40000, Loss: 0.0006556507432833314, Learning Rate: 0.001166\n",
      "Epoch 4495/40000, Loss: 0.00024972562096081674, Learning Rate: 0.001166\n",
      "Epoch 4496/40000, Loss: 0.00012087024515494704, Learning Rate: 0.001166\n",
      "Epoch 4497/40000, Loss: 0.00018157971499022096, Learning Rate: 0.001166\n",
      "Epoch 4498/40000, Loss: 0.00010050021228380501, Learning Rate: 0.001166\n",
      "Epoch 4499/40000, Loss: 8.53110832395032e-05, Learning Rate: 0.001166\n",
      "Epoch 4500/40000, Loss: 0.00015515963605139405, Learning Rate: 0.001165\n",
      "Epoch 4501/40000, Loss: 0.00010427064262330532, Learning Rate: 0.001165\n",
      "Epoch 4502/40000, Loss: 9.246110857930034e-05, Learning Rate: 0.001165\n",
      "Epoch 4503/40000, Loss: 0.0001998189400183037, Learning Rate: 0.001165\n",
      "Epoch 4504/40000, Loss: 0.00010049381671706215, Learning Rate: 0.001165\n",
      "Epoch 4505/40000, Loss: 0.00014862261014059186, Learning Rate: 0.001165\n",
      "Epoch 4506/40000, Loss: 0.00010606636351440102, Learning Rate: 0.001165\n",
      "Epoch 4507/40000, Loss: 0.00011019367957487702, Learning Rate: 0.001165\n",
      "Epoch 4508/40000, Loss: 0.00010440218466101214, Learning Rate: 0.001164\n",
      "Epoch 4509/40000, Loss: 0.00010635651415213943, Learning Rate: 0.001164\n",
      "Epoch 4510/40000, Loss: 9.488222713116556e-05, Learning Rate: 0.001164\n",
      "Epoch 4511/40000, Loss: 0.0001404913200531155, Learning Rate: 0.001164\n",
      "Epoch 4512/40000, Loss: 0.0001363423652946949, Learning Rate: 0.001164\n",
      "Epoch 4513/40000, Loss: 9.873473027255386e-05, Learning Rate: 0.001164\n",
      "Epoch 4514/40000, Loss: 0.00015186220116447657, Learning Rate: 0.001164\n",
      "Epoch 4515/40000, Loss: 9.744459384819493e-05, Learning Rate: 0.001163\n",
      "Epoch 4516/40000, Loss: 9.085285273613408e-05, Learning Rate: 0.001163\n",
      "Epoch 4517/40000, Loss: 0.00012883907766081393, Learning Rate: 0.001163\n",
      "Epoch 4518/40000, Loss: 0.00013590874732472003, Learning Rate: 0.001163\n",
      "Epoch 4519/40000, Loss: 0.00013482618669513613, Learning Rate: 0.001163\n",
      "Epoch 4520/40000, Loss: 9.515383135294542e-05, Learning Rate: 0.001163\n",
      "Epoch 4521/40000, Loss: 0.00013031602429691702, Learning Rate: 0.001163\n",
      "Epoch 4522/40000, Loss: 7.345923950197175e-05, Learning Rate: 0.001162\n",
      "Epoch 4523/40000, Loss: 0.00013228988973423839, Learning Rate: 0.001162\n",
      "Epoch 4524/40000, Loss: 0.00012634840095415711, Learning Rate: 0.001162\n",
      "Epoch 4525/40000, Loss: 0.00010080590436700732, Learning Rate: 0.001162\n",
      "Epoch 4526/40000, Loss: 8.268543751910329e-05, Learning Rate: 0.001162\n",
      "Epoch 4527/40000, Loss: 0.00013144445256330073, Learning Rate: 0.001162\n",
      "Epoch 4528/40000, Loss: 9.275821503251791e-05, Learning Rate: 0.001162\n",
      "Epoch 4529/40000, Loss: 5.1157338020857424e-05, Learning Rate: 0.001161\n",
      "Epoch 4530/40000, Loss: 7.506496331188828e-05, Learning Rate: 0.001161\n",
      "Epoch 4531/40000, Loss: 8.931498450692743e-05, Learning Rate: 0.001161\n",
      "Epoch 4532/40000, Loss: 9.186090755974874e-05, Learning Rate: 0.001161\n",
      "Epoch 4533/40000, Loss: 8.73963363119401e-05, Learning Rate: 0.001161\n",
      "Epoch 4534/40000, Loss: 8.654029807075858e-05, Learning Rate: 0.001161\n",
      "Epoch 4535/40000, Loss: 0.00012319900270085782, Learning Rate: 0.001161\n",
      "Epoch 4536/40000, Loss: 0.00012933631660416722, Learning Rate: 0.001160\n",
      "Epoch 4537/40000, Loss: 7.284940511453897e-05, Learning Rate: 0.001160\n",
      "Epoch 4538/40000, Loss: 0.00012883842282462865, Learning Rate: 0.001160\n",
      "Epoch 4539/40000, Loss: 7.388751691905782e-05, Learning Rate: 0.001160\n",
      "Epoch 4540/40000, Loss: 4.771472595166415e-05, Learning Rate: 0.001160\n",
      "Epoch 4541/40000, Loss: 8.636067650513723e-05, Learning Rate: 0.001160\n",
      "Epoch 4542/40000, Loss: 0.0001288139319512993, Learning Rate: 0.001160\n",
      "Epoch 4543/40000, Loss: 9.067068458534777e-05, Learning Rate: 0.001159\n",
      "Epoch 4544/40000, Loss: 9.222673543263227e-05, Learning Rate: 0.001159\n",
      "Epoch 4545/40000, Loss: 0.00012908646021969616, Learning Rate: 0.001159\n",
      "Epoch 4546/40000, Loss: 4.837434971705079e-05, Learning Rate: 0.001159\n",
      "Epoch 4547/40000, Loss: 8.963637810666114e-05, Learning Rate: 0.001159\n",
      "Epoch 4548/40000, Loss: 0.0001277750125154853, Learning Rate: 0.001159\n",
      "Epoch 4549/40000, Loss: 7.312812522286549e-05, Learning Rate: 0.001159\n",
      "Epoch 4550/40000, Loss: 7.219988765427843e-05, Learning Rate: 0.001159\n",
      "Epoch 4551/40000, Loss: 7.139838999137282e-05, Learning Rate: 0.001158\n",
      "Epoch 4552/40000, Loss: 8.946168964030221e-05, Learning Rate: 0.001158\n",
      "Epoch 4553/40000, Loss: 0.00012504347250796854, Learning Rate: 0.001158\n",
      "Epoch 4554/40000, Loss: 0.00012620160123333335, Learning Rate: 0.001158\n",
      "Epoch 4555/40000, Loss: 4.95828062412329e-05, Learning Rate: 0.001158\n",
      "Epoch 4556/40000, Loss: 9.827340545598418e-05, Learning Rate: 0.001158\n",
      "Epoch 4557/40000, Loss: 9.678156493464485e-05, Learning Rate: 0.001158\n",
      "Epoch 4558/40000, Loss: 9.95198788587004e-05, Learning Rate: 0.001157\n",
      "Epoch 4559/40000, Loss: 9.965014760382473e-05, Learning Rate: 0.001157\n",
      "Epoch 4560/40000, Loss: 9.209602285409346e-05, Learning Rate: 0.001157\n",
      "Epoch 4561/40000, Loss: 8.859376976033673e-05, Learning Rate: 0.001157\n",
      "Epoch 4562/40000, Loss: 9.296401549363509e-05, Learning Rate: 0.001157\n",
      "Epoch 4563/40000, Loss: 0.00012644917296711355, Learning Rate: 0.001157\n",
      "Epoch 4564/40000, Loss: 8.995686948765069e-05, Learning Rate: 0.001157\n",
      "Epoch 4565/40000, Loss: 0.00010415576980449259, Learning Rate: 0.001156\n",
      "Epoch 4566/40000, Loss: 0.00012885122851002961, Learning Rate: 0.001156\n",
      "Epoch 4567/40000, Loss: 6.607555405935273e-05, Learning Rate: 0.001156\n",
      "Epoch 4568/40000, Loss: 9.730691090226173e-05, Learning Rate: 0.001156\n",
      "Epoch 4569/40000, Loss: 0.00014368693518918008, Learning Rate: 0.001156\n",
      "Epoch 4570/40000, Loss: 0.0001125330018112436, Learning Rate: 0.001156\n",
      "Epoch 4571/40000, Loss: 0.00015911391528788954, Learning Rate: 0.001156\n",
      "Epoch 4572/40000, Loss: 9.746290743350983e-05, Learning Rate: 0.001155\n",
      "Epoch 4573/40000, Loss: 9.875688556348905e-05, Learning Rate: 0.001155\n",
      "Epoch 4574/40000, Loss: 0.00019160943338647485, Learning Rate: 0.001155\n",
      "Epoch 4575/40000, Loss: 7.25703066564165e-05, Learning Rate: 0.001155\n",
      "Epoch 4576/40000, Loss: 0.00010703987936722115, Learning Rate: 0.001155\n",
      "Epoch 4577/40000, Loss: 0.0001426938979420811, Learning Rate: 0.001155\n",
      "Epoch 4578/40000, Loss: 8.769689156906679e-05, Learning Rate: 0.001155\n",
      "Epoch 4579/40000, Loss: 0.0001540230878163129, Learning Rate: 0.001154\n",
      "Epoch 4580/40000, Loss: 0.00013574861804954708, Learning Rate: 0.001154\n",
      "Epoch 4581/40000, Loss: 5.5421693105017766e-05, Learning Rate: 0.001154\n",
      "Epoch 4582/40000, Loss: 9.508551011094823e-05, Learning Rate: 0.001154\n",
      "Epoch 4583/40000, Loss: 9.378713002661243e-05, Learning Rate: 0.001154\n",
      "Epoch 4584/40000, Loss: 0.00013394006236921996, Learning Rate: 0.001154\n",
      "Epoch 4585/40000, Loss: 0.00013842769840266556, Learning Rate: 0.001154\n",
      "Epoch 4586/40000, Loss: 5.97029211348854e-05, Learning Rate: 0.001154\n",
      "Epoch 4587/40000, Loss: 9.965468780137599e-05, Learning Rate: 0.001153\n",
      "Epoch 4588/40000, Loss: 0.00010359216685174033, Learning Rate: 0.001153\n",
      "Epoch 4589/40000, Loss: 9.182276698993519e-05, Learning Rate: 0.001153\n",
      "Epoch 4590/40000, Loss: 0.0001252733200090006, Learning Rate: 0.001153\n",
      "Epoch 4591/40000, Loss: 0.00012894370593130589, Learning Rate: 0.001153\n",
      "Epoch 4592/40000, Loss: 8.768905536271632e-05, Learning Rate: 0.001153\n",
      "Epoch 4593/40000, Loss: 8.577847620472312e-05, Learning Rate: 0.001153\n",
      "Epoch 4594/40000, Loss: 4.7566063585691154e-05, Learning Rate: 0.001152\n",
      "Epoch 4595/40000, Loss: 0.000121152370411437, Learning Rate: 0.001152\n",
      "Epoch 4596/40000, Loss: 8.565501047996804e-05, Learning Rate: 0.001152\n",
      "Epoch 4597/40000, Loss: 0.00012468735803849995, Learning Rate: 0.001152\n",
      "Epoch 4598/40000, Loss: 7.931226718937978e-05, Learning Rate: 0.001152\n",
      "Epoch 4599/40000, Loss: 7.621345139341429e-05, Learning Rate: 0.001152\n",
      "Epoch 4600/40000, Loss: 0.00013267385656945407, Learning Rate: 0.001152\n",
      "Epoch 4601/40000, Loss: 0.00013459697947837412, Learning Rate: 0.001151\n",
      "Epoch 4602/40000, Loss: 9.374156798003241e-05, Learning Rate: 0.001151\n",
      "Epoch 4603/40000, Loss: 9.44788844208233e-05, Learning Rate: 0.001151\n",
      "Epoch 4604/40000, Loss: 0.00010853964340640232, Learning Rate: 0.001151\n",
      "Epoch 4605/40000, Loss: 8.250381506513804e-05, Learning Rate: 0.001151\n",
      "Epoch 4606/40000, Loss: 0.00015943244216032326, Learning Rate: 0.001151\n",
      "Epoch 4607/40000, Loss: 0.00017125725571531802, Learning Rate: 0.001151\n",
      "Epoch 4608/40000, Loss: 0.00011233289842493832, Learning Rate: 0.001150\n",
      "Epoch 4609/40000, Loss: 6.410707283066586e-05, Learning Rate: 0.001150\n",
      "Epoch 4610/40000, Loss: 0.00018740180530585349, Learning Rate: 0.001150\n",
      "Epoch 4611/40000, Loss: 0.00017306716472376138, Learning Rate: 0.001150\n",
      "Epoch 4612/40000, Loss: 0.00010854974971152842, Learning Rate: 0.001150\n",
      "Epoch 4613/40000, Loss: 0.00013892061542719603, Learning Rate: 0.001150\n",
      "Epoch 4614/40000, Loss: 0.0001388335513183847, Learning Rate: 0.001150\n",
      "Epoch 4615/40000, Loss: 0.00015212679863907397, Learning Rate: 0.001150\n",
      "Epoch 4616/40000, Loss: 0.00011277367593720555, Learning Rate: 0.001149\n",
      "Epoch 4617/40000, Loss: 0.00018354516942054033, Learning Rate: 0.001149\n",
      "Epoch 4618/40000, Loss: 0.000181632160092704, Learning Rate: 0.001149\n",
      "Epoch 4619/40000, Loss: 0.00024669733829796314, Learning Rate: 0.001149\n",
      "Epoch 4620/40000, Loss: 0.00047479322529397905, Learning Rate: 0.001149\n",
      "Epoch 4621/40000, Loss: 0.0004020154010504484, Learning Rate: 0.001149\n",
      "Epoch 4622/40000, Loss: 0.00016782166494522244, Learning Rate: 0.001149\n",
      "Epoch 4623/40000, Loss: 0.0006700189551338553, Learning Rate: 0.001148\n",
      "Epoch 4624/40000, Loss: 0.00026311230612918735, Learning Rate: 0.001148\n",
      "Epoch 4625/40000, Loss: 0.0002268609096063301, Learning Rate: 0.001148\n",
      "Epoch 4626/40000, Loss: 0.0002856944629456848, Learning Rate: 0.001148\n",
      "Epoch 4627/40000, Loss: 0.00020876777125522494, Learning Rate: 0.001148\n",
      "Epoch 4628/40000, Loss: 0.00016388540097977966, Learning Rate: 0.001148\n",
      "Epoch 4629/40000, Loss: 0.00014417379861697555, Learning Rate: 0.001148\n",
      "Epoch 4630/40000, Loss: 0.00020672104437835515, Learning Rate: 0.001147\n",
      "Epoch 4631/40000, Loss: 0.00020647193014156073, Learning Rate: 0.001147\n",
      "Epoch 4632/40000, Loss: 0.00033710041316226125, Learning Rate: 0.001147\n",
      "Epoch 4633/40000, Loss: 0.0002055239019682631, Learning Rate: 0.001147\n",
      "Epoch 4634/40000, Loss: 0.00023785569646861404, Learning Rate: 0.001147\n",
      "Epoch 4635/40000, Loss: 0.0002097852702718228, Learning Rate: 0.001147\n",
      "Epoch 4636/40000, Loss: 0.00013815007696393877, Learning Rate: 0.001147\n",
      "Epoch 4637/40000, Loss: 0.0003172087308485061, Learning Rate: 0.001146\n",
      "Epoch 4638/40000, Loss: 0.00019874353893101215, Learning Rate: 0.001146\n",
      "Epoch 4639/40000, Loss: 0.00010788242070702836, Learning Rate: 0.001146\n",
      "Epoch 4640/40000, Loss: 8.971097122412175e-05, Learning Rate: 0.001146\n",
      "Epoch 4641/40000, Loss: 6.28835114184767e-05, Learning Rate: 0.001146\n",
      "Epoch 4642/40000, Loss: 0.0001759537262842059, Learning Rate: 0.001146\n",
      "Epoch 4643/40000, Loss: 0.00016215188952628523, Learning Rate: 0.001146\n",
      "Epoch 4644/40000, Loss: 8.173320384230465e-05, Learning Rate: 0.001146\n",
      "Epoch 4645/40000, Loss: 9.879131539491937e-05, Learning Rate: 0.001145\n",
      "Epoch 4646/40000, Loss: 0.00010657691746018827, Learning Rate: 0.001145\n",
      "Epoch 4647/40000, Loss: 0.00010128168651135638, Learning Rate: 0.001145\n",
      "Epoch 4648/40000, Loss: 9.696357301436365e-05, Learning Rate: 0.001145\n",
      "Epoch 4649/40000, Loss: 0.0001059882779372856, Learning Rate: 0.001145\n",
      "Epoch 4650/40000, Loss: 0.00013354368275031447, Learning Rate: 0.001145\n",
      "Epoch 4651/40000, Loss: 7.645523874089122e-05, Learning Rate: 0.001145\n",
      "Epoch 4652/40000, Loss: 7.672706851735711e-05, Learning Rate: 0.001144\n",
      "Epoch 4653/40000, Loss: 0.00012652596342377365, Learning Rate: 0.001144\n",
      "Epoch 4654/40000, Loss: 5.046336445957422e-05, Learning Rate: 0.001144\n",
      "Epoch 4655/40000, Loss: 0.00010632471821736544, Learning Rate: 0.001144\n",
      "Epoch 4656/40000, Loss: 0.00012335923383943737, Learning Rate: 0.001144\n",
      "Epoch 4657/40000, Loss: 0.0001254662056453526, Learning Rate: 0.001144\n",
      "Epoch 4658/40000, Loss: 0.00013952837616670877, Learning Rate: 0.001144\n",
      "Epoch 4659/40000, Loss: 0.00010590395686449483, Learning Rate: 0.001143\n",
      "Epoch 4660/40000, Loss: 0.00033744060783647, Learning Rate: 0.001143\n",
      "Epoch 4661/40000, Loss: 9.610941197024658e-05, Learning Rate: 0.001143\n",
      "Epoch 4662/40000, Loss: 0.0001485862594563514, Learning Rate: 0.001143\n",
      "Epoch 4663/40000, Loss: 9.579194011166692e-05, Learning Rate: 0.001143\n",
      "Epoch 4664/40000, Loss: 5.890791362617165e-05, Learning Rate: 0.001143\n",
      "Epoch 4665/40000, Loss: 5.290935587254353e-05, Learning Rate: 0.001143\n",
      "Epoch 4666/40000, Loss: 9.050373046193272e-05, Learning Rate: 0.001143\n",
      "Epoch 4667/40000, Loss: 0.00012481202429626137, Learning Rate: 0.001142\n",
      "Epoch 4668/40000, Loss: 9.82095516519621e-05, Learning Rate: 0.001142\n",
      "Epoch 4669/40000, Loss: 9.854738163994625e-05, Learning Rate: 0.001142\n",
      "Epoch 4670/40000, Loss: 0.00010026359086623415, Learning Rate: 0.001142\n",
      "Epoch 4671/40000, Loss: 9.682020754553378e-05, Learning Rate: 0.001142\n",
      "Epoch 4672/40000, Loss: 0.0001281279546674341, Learning Rate: 0.001142\n",
      "Epoch 4673/40000, Loss: 0.00011495040234876797, Learning Rate: 0.001142\n",
      "Epoch 4674/40000, Loss: 0.00010108067363034934, Learning Rate: 0.001141\n",
      "Epoch 4675/40000, Loss: 8.503327990183607e-05, Learning Rate: 0.001141\n",
      "Epoch 4676/40000, Loss: 7.599095988553017e-05, Learning Rate: 0.001141\n",
      "Epoch 4677/40000, Loss: 0.0001224586449097842, Learning Rate: 0.001141\n",
      "Epoch 4678/40000, Loss: 0.00012727406283374876, Learning Rate: 0.001141\n",
      "Epoch 4679/40000, Loss: 7.781471504131332e-05, Learning Rate: 0.001141\n",
      "Epoch 4680/40000, Loss: 0.00012564772623591125, Learning Rate: 0.001141\n",
      "Epoch 4681/40000, Loss: 9.358621900901198e-05, Learning Rate: 0.001140\n",
      "Epoch 4682/40000, Loss: 0.00014985422603785992, Learning Rate: 0.001140\n",
      "Epoch 4683/40000, Loss: 0.00014451087918132544, Learning Rate: 0.001140\n",
      "Epoch 4684/40000, Loss: 0.00011962766438955441, Learning Rate: 0.001140\n",
      "Epoch 4685/40000, Loss: 0.00010333777754567564, Learning Rate: 0.001140\n",
      "Epoch 4686/40000, Loss: 7.59975882829167e-05, Learning Rate: 0.001140\n",
      "Epoch 4687/40000, Loss: 9.127047087531537e-05, Learning Rate: 0.001140\n",
      "Epoch 4688/40000, Loss: 7.863376231398433e-05, Learning Rate: 0.001139\n",
      "Epoch 4689/40000, Loss: 0.00015905304462648928, Learning Rate: 0.001139\n",
      "Epoch 4690/40000, Loss: 0.00015408467152155936, Learning Rate: 0.001139\n",
      "Epoch 4691/40000, Loss: 0.00013703835429623723, Learning Rate: 0.001139\n",
      "Epoch 4692/40000, Loss: 5.502755448105745e-05, Learning Rate: 0.001139\n",
      "Epoch 4693/40000, Loss: 0.0001027143225655891, Learning Rate: 0.001139\n",
      "Epoch 4694/40000, Loss: 9.612348367227241e-05, Learning Rate: 0.001139\n",
      "Epoch 4695/40000, Loss: 0.00015562430780846626, Learning Rate: 0.001139\n",
      "Epoch 4696/40000, Loss: 0.0001427301758667454, Learning Rate: 0.001138\n",
      "Epoch 4697/40000, Loss: 0.0001278517593163997, Learning Rate: 0.001138\n",
      "Epoch 4698/40000, Loss: 0.00016764320025686175, Learning Rate: 0.001138\n",
      "Epoch 4699/40000, Loss: 0.0001368363300571218, Learning Rate: 0.001138\n",
      "Epoch 4700/40000, Loss: 0.00021616100275423378, Learning Rate: 0.001138\n",
      "Epoch 4701/40000, Loss: 0.0001504714455222711, Learning Rate: 0.001138\n",
      "Epoch 4702/40000, Loss: 0.00012058318679919466, Learning Rate: 0.001138\n",
      "Epoch 4703/40000, Loss: 0.00013850218965671957, Learning Rate: 0.001137\n",
      "Epoch 4704/40000, Loss: 7.829189416952431e-05, Learning Rate: 0.001137\n",
      "Epoch 4705/40000, Loss: 0.00011586386972339824, Learning Rate: 0.001137\n",
      "Epoch 4706/40000, Loss: 0.00017261361062992364, Learning Rate: 0.001137\n",
      "Epoch 4707/40000, Loss: 7.87559401942417e-05, Learning Rate: 0.001137\n",
      "Epoch 4708/40000, Loss: 0.00010587694850983098, Learning Rate: 0.001137\n",
      "Epoch 4709/40000, Loss: 0.00019211083417758346, Learning Rate: 0.001137\n",
      "Epoch 4710/40000, Loss: 0.00010793077672133222, Learning Rate: 0.001136\n",
      "Epoch 4711/40000, Loss: 0.00014565417950507253, Learning Rate: 0.001136\n",
      "Epoch 4712/40000, Loss: 0.0001391715632053092, Learning Rate: 0.001136\n",
      "Epoch 4713/40000, Loss: 7.967472629388794e-05, Learning Rate: 0.001136\n",
      "Epoch 4714/40000, Loss: 0.00016856726142577827, Learning Rate: 0.001136\n",
      "Epoch 4715/40000, Loss: 8.451957546640188e-05, Learning Rate: 0.001136\n",
      "Epoch 4716/40000, Loss: 0.00011299186007818207, Learning Rate: 0.001136\n",
      "Epoch 4717/40000, Loss: 8.938580140238628e-05, Learning Rate: 0.001136\n",
      "Epoch 4718/40000, Loss: 0.00014781417849007994, Learning Rate: 0.001135\n",
      "Epoch 4719/40000, Loss: 8.72246891958639e-05, Learning Rate: 0.001135\n",
      "Epoch 4720/40000, Loss: 0.0001687789335846901, Learning Rate: 0.001135\n",
      "Epoch 4721/40000, Loss: 0.00014996498066466302, Learning Rate: 0.001135\n",
      "Epoch 4722/40000, Loss: 9.752489859238267e-05, Learning Rate: 0.001135\n",
      "Epoch 4723/40000, Loss: 7.951796578709036e-05, Learning Rate: 0.001135\n",
      "Epoch 4724/40000, Loss: 8.32777950563468e-05, Learning Rate: 0.001135\n",
      "Epoch 4725/40000, Loss: 5.4489093599841e-05, Learning Rate: 0.001134\n",
      "Epoch 4726/40000, Loss: 9.77799209067598e-05, Learning Rate: 0.001134\n",
      "Epoch 4727/40000, Loss: 0.0001397080923197791, Learning Rate: 0.001134\n",
      "Epoch 4728/40000, Loss: 0.00013237979146651924, Learning Rate: 0.001134\n",
      "Epoch 4729/40000, Loss: 8.442599937552586e-05, Learning Rate: 0.001134\n",
      "Epoch 4730/40000, Loss: 5.077407695353031e-05, Learning Rate: 0.001134\n",
      "Epoch 4731/40000, Loss: 0.0001544379338156432, Learning Rate: 0.001134\n",
      "Epoch 4732/40000, Loss: 0.00012560436152853072, Learning Rate: 0.001133\n",
      "Epoch 4733/40000, Loss: 0.0001135232305387035, Learning Rate: 0.001133\n",
      "Epoch 4734/40000, Loss: 6.758191739208996e-05, Learning Rate: 0.001133\n",
      "Epoch 4735/40000, Loss: 5.739524931414053e-05, Learning Rate: 0.001133\n",
      "Epoch 4736/40000, Loss: 0.0001669911725912243, Learning Rate: 0.001133\n",
      "Epoch 4737/40000, Loss: 0.00015933954273350537, Learning Rate: 0.001133\n",
      "Epoch 4738/40000, Loss: 0.00013378157746046782, Learning Rate: 0.001133\n",
      "Epoch 4739/40000, Loss: 6.641852087341249e-05, Learning Rate: 0.001133\n",
      "Epoch 4740/40000, Loss: 0.00010398545418865979, Learning Rate: 0.001132\n",
      "Epoch 4741/40000, Loss: 8.249792153947055e-05, Learning Rate: 0.001132\n",
      "Epoch 4742/40000, Loss: 9.669251448940486e-05, Learning Rate: 0.001132\n",
      "Epoch 4743/40000, Loss: 0.00015296877245418727, Learning Rate: 0.001132\n",
      "Epoch 4744/40000, Loss: 0.00010301467409590259, Learning Rate: 0.001132\n",
      "Epoch 4745/40000, Loss: 8.47942428663373e-05, Learning Rate: 0.001132\n",
      "Epoch 4746/40000, Loss: 8.138167322613299e-05, Learning Rate: 0.001132\n",
      "Epoch 4747/40000, Loss: 7.519769133068621e-05, Learning Rate: 0.001131\n",
      "Epoch 4748/40000, Loss: 0.00011267736408626661, Learning Rate: 0.001131\n",
      "Epoch 4749/40000, Loss: 0.00013353560643736273, Learning Rate: 0.001131\n",
      "Epoch 4750/40000, Loss: 9.123687050305307e-05, Learning Rate: 0.001131\n",
      "Epoch 4751/40000, Loss: 0.00013875206059310585, Learning Rate: 0.001131\n",
      "Epoch 4752/40000, Loss: 0.0001560300588607788, Learning Rate: 0.001131\n",
      "Epoch 4753/40000, Loss: 0.00022278347751125693, Learning Rate: 0.001131\n",
      "Epoch 4754/40000, Loss: 0.0001483301748521626, Learning Rate: 0.001131\n",
      "Epoch 4755/40000, Loss: 0.0001453863806091249, Learning Rate: 0.001130\n",
      "Epoch 4756/40000, Loss: 0.00012711717863567173, Learning Rate: 0.001130\n",
      "Epoch 4757/40000, Loss: 0.00013325392501428723, Learning Rate: 0.001130\n",
      "Epoch 4758/40000, Loss: 0.00013056054012849927, Learning Rate: 0.001130\n",
      "Epoch 4759/40000, Loss: 0.00013002818741369992, Learning Rate: 0.001130\n",
      "Epoch 4760/40000, Loss: 0.00012568662350531667, Learning Rate: 0.001130\n",
      "Epoch 4761/40000, Loss: 0.00012626002717297524, Learning Rate: 0.001130\n",
      "Epoch 4762/40000, Loss: 0.0001256731484318152, Learning Rate: 0.001129\n",
      "Epoch 4763/40000, Loss: 4.862136847805232e-05, Learning Rate: 0.001129\n",
      "Epoch 4764/40000, Loss: 0.00016199909441638738, Learning Rate: 0.001129\n",
      "Epoch 4765/40000, Loss: 9.488868818152696e-05, Learning Rate: 0.001129\n",
      "Epoch 4766/40000, Loss: 0.00015203736256808043, Learning Rate: 0.001129\n",
      "Epoch 4767/40000, Loss: 0.00010278140689479187, Learning Rate: 0.001129\n",
      "Epoch 4768/40000, Loss: 9.506064088782296e-05, Learning Rate: 0.001129\n",
      "Epoch 4769/40000, Loss: 0.0001467587862862274, Learning Rate: 0.001128\n",
      "Epoch 4770/40000, Loss: 0.0001448488765163347, Learning Rate: 0.001128\n",
      "Epoch 4771/40000, Loss: 0.00010316076804883778, Learning Rate: 0.001128\n",
      "Epoch 4772/40000, Loss: 0.00013577713980339468, Learning Rate: 0.001128\n",
      "Epoch 4773/40000, Loss: 0.00010563494288362563, Learning Rate: 0.001128\n",
      "Epoch 4774/40000, Loss: 0.00017464911798015237, Learning Rate: 0.001128\n",
      "Epoch 4775/40000, Loss: 9.47324515436776e-05, Learning Rate: 0.001128\n",
      "Epoch 4776/40000, Loss: 0.00014246329374145716, Learning Rate: 0.001128\n",
      "Epoch 4777/40000, Loss: 9.684810356702656e-05, Learning Rate: 0.001127\n",
      "Epoch 4778/40000, Loss: 0.0001573383779032156, Learning Rate: 0.001127\n",
      "Epoch 4779/40000, Loss: 0.00011662883480312303, Learning Rate: 0.001127\n",
      "Epoch 4780/40000, Loss: 0.00013678577670361847, Learning Rate: 0.001127\n",
      "Epoch 4781/40000, Loss: 0.00010929759446298704, Learning Rate: 0.001127\n",
      "Epoch 4782/40000, Loss: 0.00015842595894355327, Learning Rate: 0.001127\n",
      "Epoch 4783/40000, Loss: 0.00016059441259130836, Learning Rate: 0.001127\n",
      "Epoch 4784/40000, Loss: 9.582252823747694e-05, Learning Rate: 0.001126\n",
      "Epoch 4785/40000, Loss: 0.00010160136298509315, Learning Rate: 0.001126\n",
      "Epoch 4786/40000, Loss: 0.0001520782388979569, Learning Rate: 0.001126\n",
      "Epoch 4787/40000, Loss: 0.0001241935242433101, Learning Rate: 0.001126\n",
      "Epoch 4788/40000, Loss: 0.0001690547214820981, Learning Rate: 0.001126\n",
      "Epoch 4789/40000, Loss: 0.00018858017574530095, Learning Rate: 0.001126\n",
      "Epoch 4790/40000, Loss: 0.0001986368151847273, Learning Rate: 0.001126\n",
      "Epoch 4791/40000, Loss: 0.00037178845377638936, Learning Rate: 0.001125\n",
      "Epoch 4792/40000, Loss: 0.00021731444576289505, Learning Rate: 0.001125\n",
      "Epoch 4793/40000, Loss: 0.0001624906581128016, Learning Rate: 0.001125\n",
      "Epoch 4794/40000, Loss: 0.00012084931222489104, Learning Rate: 0.001125\n",
      "Epoch 4795/40000, Loss: 0.00015954108675941825, Learning Rate: 0.001125\n",
      "Epoch 4796/40000, Loss: 9.747454896569252e-05, Learning Rate: 0.001125\n",
      "Epoch 4797/40000, Loss: 9.41752950893715e-05, Learning Rate: 0.001125\n",
      "Epoch 4798/40000, Loss: 0.00014570266648661345, Learning Rate: 0.001125\n",
      "Epoch 4799/40000, Loss: 0.00010154535993933678, Learning Rate: 0.001124\n",
      "Epoch 4800/40000, Loss: 8.316128514707088e-05, Learning Rate: 0.001124\n",
      "Epoch 4801/40000, Loss: 8.485428406856954e-05, Learning Rate: 0.001124\n",
      "Epoch 4802/40000, Loss: 8.169746433850378e-05, Learning Rate: 0.001124\n",
      "Epoch 4803/40000, Loss: 0.00010468030086485669, Learning Rate: 0.001124\n",
      "Epoch 4804/40000, Loss: 0.0001038584450725466, Learning Rate: 0.001124\n",
      "Epoch 4805/40000, Loss: 9.797805978450924e-05, Learning Rate: 0.001124\n",
      "Epoch 4806/40000, Loss: 5.361243165680207e-05, Learning Rate: 0.001123\n",
      "Epoch 4807/40000, Loss: 0.00013847395894117653, Learning Rate: 0.001123\n",
      "Epoch 4808/40000, Loss: 5.386520933825523e-05, Learning Rate: 0.001123\n",
      "Epoch 4809/40000, Loss: 0.0001318334834650159, Learning Rate: 0.001123\n",
      "Epoch 4810/40000, Loss: 8.831780723994598e-05, Learning Rate: 0.001123\n",
      "Epoch 4811/40000, Loss: 0.00013400649186223745, Learning Rate: 0.001123\n",
      "Epoch 4812/40000, Loss: 4.901439751847647e-05, Learning Rate: 0.001123\n",
      "Epoch 4813/40000, Loss: 9.115676948567852e-05, Learning Rate: 0.001123\n",
      "Epoch 4814/40000, Loss: 0.0001230864436365664, Learning Rate: 0.001122\n",
      "Epoch 4815/40000, Loss: 8.861083188094199e-05, Learning Rate: 0.001122\n",
      "Epoch 4816/40000, Loss: 8.77677375683561e-05, Learning Rate: 0.001122\n",
      "Epoch 4817/40000, Loss: 0.0001240035198861733, Learning Rate: 0.001122\n",
      "Epoch 4818/40000, Loss: 5.057144880993292e-05, Learning Rate: 0.001122\n",
      "Epoch 4819/40000, Loss: 0.00012592978600878268, Learning Rate: 0.001122\n",
      "Epoch 4820/40000, Loss: 8.926771261030808e-05, Learning Rate: 0.001122\n",
      "Epoch 4821/40000, Loss: 8.945250738179311e-05, Learning Rate: 0.001121\n",
      "Epoch 4822/40000, Loss: 9.081820462597534e-05, Learning Rate: 0.001121\n",
      "Epoch 4823/40000, Loss: 0.00013082065561320633, Learning Rate: 0.001121\n",
      "Epoch 4824/40000, Loss: 0.00013340366422198713, Learning Rate: 0.001121\n",
      "Epoch 4825/40000, Loss: 0.00012577448796946555, Learning Rate: 0.001121\n",
      "Epoch 4826/40000, Loss: 0.0001044576201820746, Learning Rate: 0.001121\n",
      "Epoch 4827/40000, Loss: 0.0001758155267452821, Learning Rate: 0.001121\n",
      "Epoch 4828/40000, Loss: 0.00017476877837907523, Learning Rate: 0.001121\n",
      "Epoch 4829/40000, Loss: 0.0001483607484260574, Learning Rate: 0.001120\n",
      "Epoch 4830/40000, Loss: 0.00011163568706251681, Learning Rate: 0.001120\n",
      "Epoch 4831/40000, Loss: 0.00014744361396878958, Learning Rate: 0.001120\n",
      "Epoch 4832/40000, Loss: 6.644493259955198e-05, Learning Rate: 0.001120\n",
      "Epoch 4833/40000, Loss: 0.00011072914639953524, Learning Rate: 0.001120\n",
      "Epoch 4834/40000, Loss: 0.00010410974209662527, Learning Rate: 0.001120\n",
      "Epoch 4835/40000, Loss: 0.0001464301603846252, Learning Rate: 0.001120\n",
      "Epoch 4836/40000, Loss: 0.00010421941988170147, Learning Rate: 0.001119\n",
      "Epoch 4837/40000, Loss: 0.000124708516523242, Learning Rate: 0.001119\n",
      "Epoch 4838/40000, Loss: 4.872948193224147e-05, Learning Rate: 0.001119\n",
      "Epoch 4839/40000, Loss: 9.768846939550713e-05, Learning Rate: 0.001119\n",
      "Epoch 4840/40000, Loss: 0.00012572323612403125, Learning Rate: 0.001119\n",
      "Epoch 4841/40000, Loss: 5.1990024076076224e-05, Learning Rate: 0.001119\n",
      "Epoch 4842/40000, Loss: 0.00010487742838449776, Learning Rate: 0.001119\n",
      "Epoch 4843/40000, Loss: 9.769381722435355e-05, Learning Rate: 0.001118\n",
      "Epoch 4844/40000, Loss: 0.00013321942242328078, Learning Rate: 0.001118\n",
      "Epoch 4845/40000, Loss: 5.523008439922705e-05, Learning Rate: 0.001118\n",
      "Epoch 4846/40000, Loss: 0.0001308559876633808, Learning Rate: 0.001118\n",
      "Epoch 4847/40000, Loss: 0.00013799416774418205, Learning Rate: 0.001118\n",
      "Epoch 4848/40000, Loss: 0.00010085021494887769, Learning Rate: 0.001118\n",
      "Epoch 4849/40000, Loss: 0.00014756768359802663, Learning Rate: 0.001118\n",
      "Epoch 4850/40000, Loss: 8.134204108500853e-05, Learning Rate: 0.001118\n",
      "Epoch 4851/40000, Loss: 8.25697134132497e-05, Learning Rate: 0.001117\n",
      "Epoch 4852/40000, Loss: 0.0001239472912857309, Learning Rate: 0.001117\n",
      "Epoch 4853/40000, Loss: 0.00016852015687618405, Learning Rate: 0.001117\n",
      "Epoch 4854/40000, Loss: 0.00010837474110303447, Learning Rate: 0.001117\n",
      "Epoch 4855/40000, Loss: 8.43669768073596e-05, Learning Rate: 0.001117\n",
      "Epoch 4856/40000, Loss: 0.0002247068186989054, Learning Rate: 0.001117\n",
      "Epoch 4857/40000, Loss: 0.0002117422263836488, Learning Rate: 0.001117\n",
      "Epoch 4858/40000, Loss: 0.00020019494695588946, Learning Rate: 0.001116\n",
      "Epoch 4859/40000, Loss: 0.00017027567082550377, Learning Rate: 0.001116\n",
      "Epoch 4860/40000, Loss: 0.00010687152098398656, Learning Rate: 0.001116\n",
      "Epoch 4861/40000, Loss: 0.00010235103400191292, Learning Rate: 0.001116\n",
      "Epoch 4862/40000, Loss: 0.00013005886285100132, Learning Rate: 0.001116\n",
      "Epoch 4863/40000, Loss: 0.00014850388106424361, Learning Rate: 0.001116\n",
      "Epoch 4864/40000, Loss: 0.0001455062738386914, Learning Rate: 0.001116\n",
      "Epoch 4865/40000, Loss: 0.00024311883316840976, Learning Rate: 0.001116\n",
      "Epoch 4866/40000, Loss: 0.00016617035726085305, Learning Rate: 0.001115\n",
      "Epoch 4867/40000, Loss: 8.642394823255017e-05, Learning Rate: 0.001115\n",
      "Epoch 4868/40000, Loss: 0.00015622087812516838, Learning Rate: 0.001115\n",
      "Epoch 4869/40000, Loss: 0.0002364992833463475, Learning Rate: 0.001115\n",
      "Epoch 4870/40000, Loss: 0.0002081987913697958, Learning Rate: 0.001115\n",
      "Epoch 4871/40000, Loss: 0.00010111906885867938, Learning Rate: 0.001115\n",
      "Epoch 4872/40000, Loss: 0.00021489521896000952, Learning Rate: 0.001115\n",
      "Epoch 4873/40000, Loss: 0.00015639813500456512, Learning Rate: 0.001114\n",
      "Epoch 4874/40000, Loss: 0.00015198750770650804, Learning Rate: 0.001114\n",
      "Epoch 4875/40000, Loss: 0.0001197245146613568, Learning Rate: 0.001114\n",
      "Epoch 4876/40000, Loss: 0.00019797672575805336, Learning Rate: 0.001114\n",
      "Epoch 4877/40000, Loss: 9.011634392663836e-05, Learning Rate: 0.001114\n",
      "Epoch 4878/40000, Loss: 0.00012827450700569898, Learning Rate: 0.001114\n",
      "Epoch 4879/40000, Loss: 0.00016047581448219717, Learning Rate: 0.001114\n",
      "Epoch 4880/40000, Loss: 8.401981176575646e-05, Learning Rate: 0.001114\n",
      "Epoch 4881/40000, Loss: 5.668508674716577e-05, Learning Rate: 0.001113\n",
      "Epoch 4882/40000, Loss: 0.00013118800416123122, Learning Rate: 0.001113\n",
      "Epoch 4883/40000, Loss: 0.00010727212065830827, Learning Rate: 0.001113\n",
      "Epoch 4884/40000, Loss: 6.963168561924249e-05, Learning Rate: 0.001113\n",
      "Epoch 4885/40000, Loss: 0.0001552319445181638, Learning Rate: 0.001113\n",
      "Epoch 4886/40000, Loss: 9.579486504662782e-05, Learning Rate: 0.001113\n",
      "Epoch 4887/40000, Loss: 0.00018126067880075425, Learning Rate: 0.001113\n",
      "Epoch 4888/40000, Loss: 0.00014535148511640728, Learning Rate: 0.001112\n",
      "Epoch 4889/40000, Loss: 0.00012732927280012518, Learning Rate: 0.001112\n",
      "Epoch 4890/40000, Loss: 0.00011375886970199645, Learning Rate: 0.001112\n",
      "Epoch 4891/40000, Loss: 0.00011225002526771277, Learning Rate: 0.001112\n",
      "Epoch 4892/40000, Loss: 0.00010361985914641991, Learning Rate: 0.001112\n",
      "Epoch 4893/40000, Loss: 0.00010938452032860368, Learning Rate: 0.001112\n",
      "Epoch 4894/40000, Loss: 0.00010857945017050952, Learning Rate: 0.001112\n",
      "Epoch 4895/40000, Loss: 0.00014744015061296523, Learning Rate: 0.001112\n",
      "Epoch 4896/40000, Loss: 0.00016687791503500193, Learning Rate: 0.001111\n",
      "Epoch 4897/40000, Loss: 0.0001588432351127267, Learning Rate: 0.001111\n",
      "Epoch 4898/40000, Loss: 8.402015373576432e-05, Learning Rate: 0.001111\n",
      "Epoch 4899/40000, Loss: 0.0002268991229357198, Learning Rate: 0.001111\n",
      "Epoch 4900/40000, Loss: 6.959798338357359e-05, Learning Rate: 0.001111\n",
      "Epoch 4901/40000, Loss: 0.0001850719709182158, Learning Rate: 0.001111\n",
      "Epoch 4902/40000, Loss: 6.742344703525305e-05, Learning Rate: 0.001111\n",
      "Epoch 4903/40000, Loss: 0.00013643910642713308, Learning Rate: 0.001110\n",
      "Epoch 4904/40000, Loss: 9.873602539300919e-05, Learning Rate: 0.001110\n",
      "Epoch 4905/40000, Loss: 0.00016886211233213544, Learning Rate: 0.001110\n",
      "Epoch 4906/40000, Loss: 0.00018442260625306517, Learning Rate: 0.001110\n",
      "Epoch 4907/40000, Loss: 8.968557085609064e-05, Learning Rate: 0.001110\n",
      "Epoch 4908/40000, Loss: 9.410145867150277e-05, Learning Rate: 0.001110\n",
      "Epoch 4909/40000, Loss: 0.0002108658227371052, Learning Rate: 0.001110\n",
      "Epoch 4910/40000, Loss: 0.00014788280532229692, Learning Rate: 0.001110\n",
      "Epoch 4911/40000, Loss: 0.00013176265929359943, Learning Rate: 0.001109\n",
      "Epoch 4912/40000, Loss: 6.827028119005263e-05, Learning Rate: 0.001109\n",
      "Epoch 4913/40000, Loss: 0.00013540551299229264, Learning Rate: 0.001109\n",
      "Epoch 4914/40000, Loss: 0.00012293610780034214, Learning Rate: 0.001109\n",
      "Epoch 4915/40000, Loss: 0.0001500947546446696, Learning Rate: 0.001109\n",
      "Epoch 4916/40000, Loss: 0.00011376236216165125, Learning Rate: 0.001109\n",
      "Epoch 4917/40000, Loss: 0.00010321481386199594, Learning Rate: 0.001109\n",
      "Epoch 4918/40000, Loss: 6.6396823967807e-05, Learning Rate: 0.001108\n",
      "Epoch 4919/40000, Loss: 0.00013680632400792092, Learning Rate: 0.001108\n",
      "Epoch 4920/40000, Loss: 8.07760952739045e-05, Learning Rate: 0.001108\n",
      "Epoch 4921/40000, Loss: 9.080088057089597e-05, Learning Rate: 0.001108\n",
      "Epoch 4922/40000, Loss: 0.00010898390610236675, Learning Rate: 0.001108\n",
      "Epoch 4923/40000, Loss: 0.00013194735220167786, Learning Rate: 0.001108\n",
      "Epoch 4924/40000, Loss: 6.849812052678317e-05, Learning Rate: 0.001108\n",
      "Epoch 4925/40000, Loss: 0.00010062243381980807, Learning Rate: 0.001108\n",
      "Epoch 4926/40000, Loss: 6.30977374385111e-05, Learning Rate: 0.001107\n",
      "Epoch 4927/40000, Loss: 9.897009294945747e-05, Learning Rate: 0.001107\n",
      "Epoch 4928/40000, Loss: 0.0001451715943403542, Learning Rate: 0.001107\n",
      "Epoch 4929/40000, Loss: 0.0001367516233585775, Learning Rate: 0.001107\n",
      "Epoch 4930/40000, Loss: 0.00010207755258306861, Learning Rate: 0.001107\n",
      "Epoch 4931/40000, Loss: 0.0001406698429491371, Learning Rate: 0.001107\n",
      "Epoch 4932/40000, Loss: 5.3544481488643214e-05, Learning Rate: 0.001107\n",
      "Epoch 4933/40000, Loss: 0.00014626604388467968, Learning Rate: 0.001106\n",
      "Epoch 4934/40000, Loss: 0.00013874447904527187, Learning Rate: 0.001106\n",
      "Epoch 4935/40000, Loss: 0.00016049190890043974, Learning Rate: 0.001106\n",
      "Epoch 4936/40000, Loss: 0.00014317566819954664, Learning Rate: 0.001106\n",
      "Epoch 4937/40000, Loss: 0.00013904011575505137, Learning Rate: 0.001106\n",
      "Epoch 4938/40000, Loss: 0.00023826202959753573, Learning Rate: 0.001106\n",
      "Epoch 4939/40000, Loss: 7.379370799753815e-05, Learning Rate: 0.001106\n",
      "Epoch 4940/40000, Loss: 7.218592509161681e-05, Learning Rate: 0.001106\n",
      "Epoch 4941/40000, Loss: 0.00014425894187297672, Learning Rate: 0.001105\n",
      "Epoch 4942/40000, Loss: 0.00023263720504473895, Learning Rate: 0.001105\n",
      "Epoch 4943/40000, Loss: 0.00035866026883013546, Learning Rate: 0.001105\n",
      "Epoch 4944/40000, Loss: 0.0002404617116553709, Learning Rate: 0.001105\n",
      "Epoch 4945/40000, Loss: 0.00017574364028405398, Learning Rate: 0.001105\n",
      "Epoch 4946/40000, Loss: 0.00012559772585518658, Learning Rate: 0.001105\n",
      "Epoch 4947/40000, Loss: 0.0001228377514053136, Learning Rate: 0.001105\n",
      "Epoch 4948/40000, Loss: 0.00022360883303917944, Learning Rate: 0.001104\n",
      "Epoch 4949/40000, Loss: 0.00012308820441830903, Learning Rate: 0.001104\n",
      "Epoch 4950/40000, Loss: 0.00011274513235548511, Learning Rate: 0.001104\n",
      "Epoch 4951/40000, Loss: 0.0001397858140990138, Learning Rate: 0.001104\n",
      "Epoch 4952/40000, Loss: 0.00011843600077554584, Learning Rate: 0.001104\n",
      "Epoch 4953/40000, Loss: 0.0001288977509830147, Learning Rate: 0.001104\n",
      "Epoch 4954/40000, Loss: 0.00017836653569247574, Learning Rate: 0.001104\n",
      "Epoch 4955/40000, Loss: 0.0001620133116375655, Learning Rate: 0.001104\n",
      "Epoch 4956/40000, Loss: 0.00016714134835638106, Learning Rate: 0.001103\n",
      "Epoch 4957/40000, Loss: 0.00010819478484336287, Learning Rate: 0.001103\n",
      "Epoch 4958/40000, Loss: 0.00033795792842283845, Learning Rate: 0.001103\n",
      "Epoch 4959/40000, Loss: 0.00019867639639414847, Learning Rate: 0.001103\n",
      "Epoch 4960/40000, Loss: 0.00018048931087832898, Learning Rate: 0.001103\n",
      "Epoch 4961/40000, Loss: 0.00018550480308476835, Learning Rate: 0.001103\n",
      "Epoch 4962/40000, Loss: 0.00016038281319197267, Learning Rate: 0.001103\n",
      "Epoch 4963/40000, Loss: 0.0002154237445211038, Learning Rate: 0.001103\n",
      "Epoch 4964/40000, Loss: 0.00010385787027189508, Learning Rate: 0.001102\n",
      "Epoch 4965/40000, Loss: 0.00017509744793642312, Learning Rate: 0.001102\n",
      "Epoch 4966/40000, Loss: 0.00011318409815430641, Learning Rate: 0.001102\n",
      "Epoch 4967/40000, Loss: 0.0001034589804476127, Learning Rate: 0.001102\n",
      "Epoch 4968/40000, Loss: 5.905598300159909e-05, Learning Rate: 0.001102\n",
      "Epoch 4969/40000, Loss: 0.0001382718182867393, Learning Rate: 0.001102\n",
      "Epoch 4970/40000, Loss: 5.990057979943231e-05, Learning Rate: 0.001102\n",
      "Epoch 4971/40000, Loss: 0.0001681689900578931, Learning Rate: 0.001101\n",
      "Epoch 4972/40000, Loss: 0.00012848743062932044, Learning Rate: 0.001101\n",
      "Epoch 4973/40000, Loss: 7.873566937632859e-05, Learning Rate: 0.001101\n",
      "Epoch 4974/40000, Loss: 0.00013792676327284425, Learning Rate: 0.001101\n",
      "Epoch 4975/40000, Loss: 0.00010954830941045657, Learning Rate: 0.001101\n",
      "Epoch 4976/40000, Loss: 6.460268923547119e-05, Learning Rate: 0.001101\n",
      "Epoch 4977/40000, Loss: 0.0001347823126707226, Learning Rate: 0.001101\n",
      "Epoch 4978/40000, Loss: 0.0001374042039969936, Learning Rate: 0.001101\n",
      "Epoch 4979/40000, Loss: 0.00012058006541337818, Learning Rate: 0.001100\n",
      "Epoch 4980/40000, Loss: 0.0001549209700897336, Learning Rate: 0.001100\n",
      "Epoch 4981/40000, Loss: 7.486584945581853e-05, Learning Rate: 0.001100\n",
      "Epoch 4982/40000, Loss: 0.0001160660176537931, Learning Rate: 0.001100\n",
      "Epoch 4983/40000, Loss: 0.00017203798051923513, Learning Rate: 0.001100\n",
      "Epoch 4984/40000, Loss: 0.0001555599010316655, Learning Rate: 0.001100\n",
      "Epoch 4985/40000, Loss: 0.00021154640126042068, Learning Rate: 0.001100\n",
      "Epoch 4986/40000, Loss: 8.041138062253594e-05, Learning Rate: 0.001099\n",
      "Epoch 4987/40000, Loss: 9.050332300830632e-05, Learning Rate: 0.001099\n",
      "Epoch 4988/40000, Loss: 0.00014592033403459936, Learning Rate: 0.001099\n",
      "Epoch 4989/40000, Loss: 7.940315117593855e-05, Learning Rate: 0.001099\n",
      "Epoch 4990/40000, Loss: 0.00011548566544661298, Learning Rate: 0.001099\n",
      "Epoch 4991/40000, Loss: 9.637813491281122e-05, Learning Rate: 0.001099\n",
      "Epoch 4992/40000, Loss: 9.742475958773866e-05, Learning Rate: 0.001099\n",
      "Epoch 4993/40000, Loss: 9.303556726081297e-05, Learning Rate: 0.001099\n",
      "Epoch 4994/40000, Loss: 0.0001443698856746778, Learning Rate: 0.001098\n",
      "Epoch 4995/40000, Loss: 0.00013117160415276885, Learning Rate: 0.001098\n",
      "Epoch 4996/40000, Loss: 0.00012484562466852367, Learning Rate: 0.001098\n",
      "Epoch 4997/40000, Loss: 0.00012967895600013435, Learning Rate: 0.001098\n",
      "Epoch 4998/40000, Loss: 5.673187115462497e-05, Learning Rate: 0.001098\n",
      "Epoch 4999/40000, Loss: 5.062665877630934e-05, Learning Rate: 0.001098\n",
      "Epoch 5000/40000, Loss: 9.231332660419866e-05, Learning Rate: 0.001098\n",
      "Epoch 5001/40000, Loss: 0.00013729069905821234, Learning Rate: 0.001097\n",
      "Epoch 5002/40000, Loss: 0.00014085746079217643, Learning Rate: 0.001097\n",
      "Epoch 5003/40000, Loss: 0.00013636407675221562, Learning Rate: 0.001097\n",
      "Epoch 5004/40000, Loss: 7.815637218300253e-05, Learning Rate: 0.001097\n",
      "Epoch 5005/40000, Loss: 9.827313624555245e-05, Learning Rate: 0.001097\n",
      "Epoch 5006/40000, Loss: 0.0001350805105175823, Learning Rate: 0.001097\n",
      "Epoch 5007/40000, Loss: 8.591821824666113e-05, Learning Rate: 0.001097\n",
      "Epoch 5008/40000, Loss: 7.571886817459017e-05, Learning Rate: 0.001097\n",
      "Epoch 5009/40000, Loss: 0.00014761133934371173, Learning Rate: 0.001096\n",
      "Epoch 5010/40000, Loss: 9.253071038983762e-05, Learning Rate: 0.001096\n",
      "Epoch 5011/40000, Loss: 7.843756611691788e-05, Learning Rate: 0.001096\n",
      "Epoch 5012/40000, Loss: 5.665569187840447e-05, Learning Rate: 0.001096\n",
      "Epoch 5013/40000, Loss: 8.919239189708605e-05, Learning Rate: 0.001096\n",
      "Epoch 5014/40000, Loss: 0.00012329444871284068, Learning Rate: 0.001096\n",
      "Epoch 5015/40000, Loss: 0.00012930502998642623, Learning Rate: 0.001096\n",
      "Epoch 5016/40000, Loss: 5.149711068952456e-05, Learning Rate: 0.001096\n",
      "Epoch 5017/40000, Loss: 0.00013060661149211228, Learning Rate: 0.001095\n",
      "Epoch 5018/40000, Loss: 0.00014126079622656107, Learning Rate: 0.001095\n",
      "Epoch 5019/40000, Loss: 5.5266646086238325e-05, Learning Rate: 0.001095\n",
      "Epoch 5020/40000, Loss: 9.123141353484243e-05, Learning Rate: 0.001095\n",
      "Epoch 5021/40000, Loss: 7.436422311002389e-05, Learning Rate: 0.001095\n",
      "Epoch 5022/40000, Loss: 9.915057307807729e-05, Learning Rate: 0.001095\n",
      "Epoch 5023/40000, Loss: 0.0001311321830144152, Learning Rate: 0.001095\n",
      "Epoch 5024/40000, Loss: 5.8880024880636483e-05, Learning Rate: 0.001094\n",
      "Epoch 5025/40000, Loss: 0.00012357538798823953, Learning Rate: 0.001094\n",
      "Epoch 5026/40000, Loss: 8.889997843652964e-05, Learning Rate: 0.001094\n",
      "Epoch 5027/40000, Loss: 9.674741158960387e-05, Learning Rate: 0.001094\n",
      "Epoch 5028/40000, Loss: 0.0001553808688186109, Learning Rate: 0.001094\n",
      "Epoch 5029/40000, Loss: 0.0001401156623614952, Learning Rate: 0.001094\n",
      "Epoch 5030/40000, Loss: 9.532345575280488e-05, Learning Rate: 0.001094\n",
      "Epoch 5031/40000, Loss: 0.0001490788854425773, Learning Rate: 0.001094\n",
      "Epoch 5032/40000, Loss: 8.11632999102585e-05, Learning Rate: 0.001093\n",
      "Epoch 5033/40000, Loss: 0.0001375352730974555, Learning Rate: 0.001093\n",
      "Epoch 5034/40000, Loss: 0.00012910754594486207, Learning Rate: 0.001093\n",
      "Epoch 5035/40000, Loss: 9.727425640448928e-05, Learning Rate: 0.001093\n",
      "Epoch 5036/40000, Loss: 9.491232049185783e-05, Learning Rate: 0.001093\n",
      "Epoch 5037/40000, Loss: 7.749399810563773e-05, Learning Rate: 0.001093\n",
      "Epoch 5038/40000, Loss: 0.00012924335896968842, Learning Rate: 0.001093\n",
      "Epoch 5039/40000, Loss: 0.00012641401553992182, Learning Rate: 0.001092\n",
      "Epoch 5040/40000, Loss: 7.597332296427339e-05, Learning Rate: 0.001092\n",
      "Epoch 5041/40000, Loss: 0.00013404146011453122, Learning Rate: 0.001092\n",
      "Epoch 5042/40000, Loss: 5.186330963624641e-05, Learning Rate: 0.001092\n",
      "Epoch 5043/40000, Loss: 4.952145536663011e-05, Learning Rate: 0.001092\n",
      "Epoch 5044/40000, Loss: 7.528820424340665e-05, Learning Rate: 0.001092\n",
      "Epoch 5045/40000, Loss: 6.103335181251168e-05, Learning Rate: 0.001092\n",
      "Epoch 5046/40000, Loss: 0.00013348196807783097, Learning Rate: 0.001092\n",
      "Epoch 5047/40000, Loss: 6.450767978094518e-05, Learning Rate: 0.001091\n",
      "Epoch 5048/40000, Loss: 9.920523734763265e-05, Learning Rate: 0.001091\n",
      "Epoch 5049/40000, Loss: 8.636360871605575e-05, Learning Rate: 0.001091\n",
      "Epoch 5050/40000, Loss: 0.00015682505909353495, Learning Rate: 0.001091\n",
      "Epoch 5051/40000, Loss: 9.287834109272808e-05, Learning Rate: 0.001091\n",
      "Epoch 5052/40000, Loss: 0.00014569086488336325, Learning Rate: 0.001091\n",
      "Epoch 5053/40000, Loss: 0.00013665520236827433, Learning Rate: 0.001091\n",
      "Epoch 5054/40000, Loss: 7.457144238287583e-05, Learning Rate: 0.001091\n",
      "Epoch 5055/40000, Loss: 9.345060971099883e-05, Learning Rate: 0.001090\n",
      "Epoch 5056/40000, Loss: 7.093338354025036e-05, Learning Rate: 0.001090\n",
      "Epoch 5057/40000, Loss: 0.0001558190124342218, Learning Rate: 0.001090\n",
      "Epoch 5058/40000, Loss: 0.00011778668704209849, Learning Rate: 0.001090\n",
      "Epoch 5059/40000, Loss: 0.00014543240831699222, Learning Rate: 0.001090\n",
      "Epoch 5060/40000, Loss: 0.00010233181819785386, Learning Rate: 0.001090\n",
      "Epoch 5061/40000, Loss: 0.00018369760073255748, Learning Rate: 0.001090\n",
      "Epoch 5062/40000, Loss: 8.897764200810343e-05, Learning Rate: 0.001089\n",
      "Epoch 5063/40000, Loss: 0.00011029950110241771, Learning Rate: 0.001089\n",
      "Epoch 5064/40000, Loss: 0.00013565769768320024, Learning Rate: 0.001089\n",
      "Epoch 5065/40000, Loss: 5.8167570387013257e-05, Learning Rate: 0.001089\n",
      "Epoch 5066/40000, Loss: 0.00015481995069421828, Learning Rate: 0.001089\n",
      "Epoch 5067/40000, Loss: 0.0001753865071805194, Learning Rate: 0.001089\n",
      "Epoch 5068/40000, Loss: 7.475067104678601e-05, Learning Rate: 0.001089\n",
      "Epoch 5069/40000, Loss: 0.0001623731222935021, Learning Rate: 0.001089\n",
      "Epoch 5070/40000, Loss: 8.608365897089243e-05, Learning Rate: 0.001088\n",
      "Epoch 5071/40000, Loss: 6.645938992733136e-05, Learning Rate: 0.001088\n",
      "Epoch 5072/40000, Loss: 5.675852662534453e-05, Learning Rate: 0.001088\n",
      "Epoch 5073/40000, Loss: 6.522701005451381e-05, Learning Rate: 0.001088\n",
      "Epoch 5074/40000, Loss: 5.9772039094241336e-05, Learning Rate: 0.001088\n",
      "Epoch 5075/40000, Loss: 5.323335062712431e-05, Learning Rate: 0.001088\n",
      "Epoch 5076/40000, Loss: 4.5450764446286485e-05, Learning Rate: 0.001088\n",
      "Epoch 5077/40000, Loss: 7.207172166090459e-05, Learning Rate: 0.001088\n",
      "Epoch 5078/40000, Loss: 0.0001279897551285103, Learning Rate: 0.001087\n",
      "Epoch 5079/40000, Loss: 0.000116949544462841, Learning Rate: 0.001087\n",
      "Epoch 5080/40000, Loss: 8.816026820568368e-05, Learning Rate: 0.001087\n",
      "Epoch 5081/40000, Loss: 8.91867748578079e-05, Learning Rate: 0.001087\n",
      "Epoch 5082/40000, Loss: 0.00012271059677004814, Learning Rate: 0.001087\n",
      "Epoch 5083/40000, Loss: 8.719205652596429e-05, Learning Rate: 0.001087\n",
      "Epoch 5084/40000, Loss: 8.594131213612854e-05, Learning Rate: 0.001087\n",
      "Epoch 5085/40000, Loss: 0.0001282662124140188, Learning Rate: 0.001086\n",
      "Epoch 5086/40000, Loss: 7.279888086486608e-05, Learning Rate: 0.001086\n",
      "Epoch 5087/40000, Loss: 6.988684617681429e-05, Learning Rate: 0.001086\n",
      "Epoch 5088/40000, Loss: 0.00013071687135379761, Learning Rate: 0.001086\n",
      "Epoch 5089/40000, Loss: 4.918882405036129e-05, Learning Rate: 0.001086\n",
      "Epoch 5090/40000, Loss: 0.0001239560078829527, Learning Rate: 0.001086\n",
      "Epoch 5091/40000, Loss: 0.0001364080817438662, Learning Rate: 0.001086\n",
      "Epoch 5092/40000, Loss: 0.0001416812010575086, Learning Rate: 0.001086\n",
      "Epoch 5093/40000, Loss: 5.782525840913877e-05, Learning Rate: 0.001085\n",
      "Epoch 5094/40000, Loss: 0.00011785288370447233, Learning Rate: 0.001085\n",
      "Epoch 5095/40000, Loss: 0.00014936199295334518, Learning Rate: 0.001085\n",
      "Epoch 5096/40000, Loss: 8.028576849028468e-05, Learning Rate: 0.001085\n",
      "Epoch 5097/40000, Loss: 0.00015880916907917708, Learning Rate: 0.001085\n",
      "Epoch 5098/40000, Loss: 9.883868915494531e-05, Learning Rate: 0.001085\n",
      "Epoch 5099/40000, Loss: 0.00016435365250799805, Learning Rate: 0.001085\n",
      "Epoch 5100/40000, Loss: 0.00015690790314693004, Learning Rate: 0.001085\n",
      "Epoch 5101/40000, Loss: 0.00010783781908685341, Learning Rate: 0.001084\n",
      "Epoch 5102/40000, Loss: 0.00011596209515118971, Learning Rate: 0.001084\n",
      "Epoch 5103/40000, Loss: 0.000460712646599859, Learning Rate: 0.001084\n",
      "Epoch 5104/40000, Loss: 0.00017178159032482654, Learning Rate: 0.001084\n",
      "Epoch 5105/40000, Loss: 0.0001819032768253237, Learning Rate: 0.001084\n",
      "Epoch 5106/40000, Loss: 8.820700168143958e-05, Learning Rate: 0.001084\n",
      "Epoch 5107/40000, Loss: 0.0001224593142978847, Learning Rate: 0.001084\n",
      "Epoch 5108/40000, Loss: 8.300143235828727e-05, Learning Rate: 0.001083\n",
      "Epoch 5109/40000, Loss: 0.0002455633075442165, Learning Rate: 0.001083\n",
      "Epoch 5110/40000, Loss: 5.7051907788263634e-05, Learning Rate: 0.001083\n",
      "Epoch 5111/40000, Loss: 0.00010696770186768845, Learning Rate: 0.001083\n",
      "Epoch 5112/40000, Loss: 0.00016885550576262176, Learning Rate: 0.001083\n",
      "Epoch 5113/40000, Loss: 0.00016451704141218215, Learning Rate: 0.001083\n",
      "Epoch 5114/40000, Loss: 9.105823846766725e-05, Learning Rate: 0.001083\n",
      "Epoch 5115/40000, Loss: 0.00015719301882199943, Learning Rate: 0.001083\n",
      "Epoch 5116/40000, Loss: 0.00010692016076063737, Learning Rate: 0.001082\n",
      "Epoch 5117/40000, Loss: 0.00010695336095523089, Learning Rate: 0.001082\n",
      "Epoch 5118/40000, Loss: 0.0001549270236864686, Learning Rate: 0.001082\n",
      "Epoch 5119/40000, Loss: 0.00013853060954716057, Learning Rate: 0.001082\n",
      "Epoch 5120/40000, Loss: 0.0001749993534758687, Learning Rate: 0.001082\n",
      "Epoch 5121/40000, Loss: 0.00013954335008747876, Learning Rate: 0.001082\n",
      "Epoch 5122/40000, Loss: 0.00010331338853575289, Learning Rate: 0.001082\n",
      "Epoch 5123/40000, Loss: 0.00014795838796999305, Learning Rate: 0.001082\n",
      "Epoch 5124/40000, Loss: 0.00014735848526470363, Learning Rate: 0.001081\n",
      "Epoch 5125/40000, Loss: 0.0001310526131419465, Learning Rate: 0.001081\n",
      "Epoch 5126/40000, Loss: 0.00014004585682414472, Learning Rate: 0.001081\n",
      "Epoch 5127/40000, Loss: 0.00012914587568957359, Learning Rate: 0.001081\n",
      "Epoch 5128/40000, Loss: 0.00010465627565281466, Learning Rate: 0.001081\n",
      "Epoch 5129/40000, Loss: 5.01738213642966e-05, Learning Rate: 0.001081\n",
      "Epoch 5130/40000, Loss: 0.00010927215771516785, Learning Rate: 0.001081\n",
      "Epoch 5131/40000, Loss: 0.00015987243386916816, Learning Rate: 0.001080\n",
      "Epoch 5132/40000, Loss: 0.00016569910803809762, Learning Rate: 0.001080\n",
      "Epoch 5133/40000, Loss: 9.206373943015933e-05, Learning Rate: 0.001080\n",
      "Epoch 5134/40000, Loss: 0.0002554316015448421, Learning Rate: 0.001080\n",
      "Epoch 5135/40000, Loss: 9.745441639097407e-05, Learning Rate: 0.001080\n",
      "Epoch 5136/40000, Loss: 0.00015581291518174112, Learning Rate: 0.001080\n",
      "Epoch 5137/40000, Loss: 0.00010914191807387397, Learning Rate: 0.001080\n",
      "Epoch 5138/40000, Loss: 0.00023763423087075353, Learning Rate: 0.001080\n",
      "Epoch 5139/40000, Loss: 0.00019621540559455752, Learning Rate: 0.001079\n",
      "Epoch 5140/40000, Loss: 0.00010265452146995813, Learning Rate: 0.001079\n",
      "Epoch 5141/40000, Loss: 0.00015929114306345582, Learning Rate: 0.001079\n",
      "Epoch 5142/40000, Loss: 0.0002840734086930752, Learning Rate: 0.001079\n",
      "Epoch 5143/40000, Loss: 0.00029399196500889957, Learning Rate: 0.001079\n",
      "Epoch 5144/40000, Loss: 0.00014026318967808038, Learning Rate: 0.001079\n",
      "Epoch 5145/40000, Loss: 0.00024895239039324224, Learning Rate: 0.001079\n",
      "Epoch 5146/40000, Loss: 0.00032554054632782936, Learning Rate: 0.001079\n",
      "Epoch 5147/40000, Loss: 0.00023595460515934974, Learning Rate: 0.001078\n",
      "Epoch 5148/40000, Loss: 0.00016277939721476287, Learning Rate: 0.001078\n",
      "Epoch 5149/40000, Loss: 0.00043555276351980865, Learning Rate: 0.001078\n",
      "Epoch 5150/40000, Loss: 0.00043069408275187016, Learning Rate: 0.001078\n",
      "Epoch 5151/40000, Loss: 0.00011452383478172123, Learning Rate: 0.001078\n",
      "Epoch 5152/40000, Loss: 0.00025315367383882403, Learning Rate: 0.001078\n",
      "Epoch 5153/40000, Loss: 0.0003317874507047236, Learning Rate: 0.001078\n",
      "Epoch 5154/40000, Loss: 0.0002261447225464508, Learning Rate: 0.001078\n",
      "Epoch 5155/40000, Loss: 0.00018149413517676294, Learning Rate: 0.001077\n",
      "Epoch 5156/40000, Loss: 0.00021223367366474122, Learning Rate: 0.001077\n",
      "Epoch 5157/40000, Loss: 0.00038560450775548816, Learning Rate: 0.001077\n",
      "Epoch 5158/40000, Loss: 9.169135591946542e-05, Learning Rate: 0.001077\n",
      "Epoch 5159/40000, Loss: 0.00014971394557505846, Learning Rate: 0.001077\n",
      "Epoch 5160/40000, Loss: 6.53252427582629e-05, Learning Rate: 0.001077\n",
      "Epoch 5161/40000, Loss: 9.001659782370552e-05, Learning Rate: 0.001077\n",
      "Epoch 5162/40000, Loss: 0.000151840562466532, Learning Rate: 0.001076\n",
      "Epoch 5163/40000, Loss: 0.00017460684466641396, Learning Rate: 0.001076\n",
      "Epoch 5164/40000, Loss: 0.00010078135528601706, Learning Rate: 0.001076\n",
      "Epoch 5165/40000, Loss: 0.0002130260254489258, Learning Rate: 0.001076\n",
      "Epoch 5166/40000, Loss: 0.00020209798822179437, Learning Rate: 0.001076\n",
      "Epoch 5167/40000, Loss: 5.7671579270390794e-05, Learning Rate: 0.001076\n",
      "Epoch 5168/40000, Loss: 0.00015426600293722004, Learning Rate: 0.001076\n",
      "Epoch 5169/40000, Loss: 9.59272583713755e-05, Learning Rate: 0.001076\n",
      "Epoch 5170/40000, Loss: 0.0002198969741584733, Learning Rate: 0.001075\n",
      "Epoch 5171/40000, Loss: 0.00011217415885766968, Learning Rate: 0.001075\n",
      "Epoch 5172/40000, Loss: 0.0001429314143024385, Learning Rate: 0.001075\n",
      "Epoch 5173/40000, Loss: 0.00018181954510509968, Learning Rate: 0.001075\n",
      "Epoch 5174/40000, Loss: 0.00011642611934803426, Learning Rate: 0.001075\n",
      "Epoch 5175/40000, Loss: 0.00011265455395914614, Learning Rate: 0.001075\n",
      "Epoch 5176/40000, Loss: 0.00015470637299586087, Learning Rate: 0.001075\n",
      "Epoch 5177/40000, Loss: 9.598591714166105e-05, Learning Rate: 0.001075\n",
      "Epoch 5178/40000, Loss: 0.00015936416457407176, Learning Rate: 0.001074\n",
      "Epoch 5179/40000, Loss: 0.00012217847688589245, Learning Rate: 0.001074\n",
      "Epoch 5180/40000, Loss: 9.038851567311212e-05, Learning Rate: 0.001074\n",
      "Epoch 5181/40000, Loss: 0.00010184215352637693, Learning Rate: 0.001074\n",
      "Epoch 5182/40000, Loss: 8.296964369947091e-05, Learning Rate: 0.001074\n",
      "Epoch 5183/40000, Loss: 9.011045040097088e-05, Learning Rate: 0.001074\n",
      "Epoch 5184/40000, Loss: 8.082295971689746e-05, Learning Rate: 0.001074\n",
      "Epoch 5185/40000, Loss: 0.00010136248602066189, Learning Rate: 0.001074\n",
      "Epoch 5186/40000, Loss: 0.00021078377903904766, Learning Rate: 0.001073\n",
      "Epoch 5187/40000, Loss: 7.507068949053064e-05, Learning Rate: 0.001073\n",
      "Epoch 5188/40000, Loss: 0.00010228851897409186, Learning Rate: 0.001073\n",
      "Epoch 5189/40000, Loss: 8.211607200792059e-05, Learning Rate: 0.001073\n",
      "Epoch 5190/40000, Loss: 8.803344098851085e-05, Learning Rate: 0.001073\n",
      "Epoch 5191/40000, Loss: 0.00017277541337534785, Learning Rate: 0.001073\n",
      "Epoch 5192/40000, Loss: 0.00017410065629519522, Learning Rate: 0.001073\n",
      "Epoch 5193/40000, Loss: 0.00010891666170209646, Learning Rate: 0.001072\n",
      "Epoch 5194/40000, Loss: 0.00017680396558716893, Learning Rate: 0.001072\n",
      "Epoch 5195/40000, Loss: 0.0001104633483919315, Learning Rate: 0.001072\n",
      "Epoch 5196/40000, Loss: 0.00015338812954723835, Learning Rate: 0.001072\n",
      "Epoch 5197/40000, Loss: 0.00015966758655849844, Learning Rate: 0.001072\n",
      "Epoch 5198/40000, Loss: 8.436110510956496e-05, Learning Rate: 0.001072\n",
      "Epoch 5199/40000, Loss: 0.00017489887250121683, Learning Rate: 0.001072\n",
      "Epoch 5200/40000, Loss: 0.00010463651415193453, Learning Rate: 0.001072\n",
      "Epoch 5201/40000, Loss: 0.00018635809828992933, Learning Rate: 0.001071\n",
      "Epoch 5202/40000, Loss: 0.00015709755825810134, Learning Rate: 0.001071\n",
      "Epoch 5203/40000, Loss: 0.00016191166650969535, Learning Rate: 0.001071\n",
      "Epoch 5204/40000, Loss: 0.0001296925765927881, Learning Rate: 0.001071\n",
      "Epoch 5205/40000, Loss: 5.2599338232539594e-05, Learning Rate: 0.001071\n",
      "Epoch 5206/40000, Loss: 9.060674346983433e-05, Learning Rate: 0.001071\n",
      "Epoch 5207/40000, Loss: 0.00012710278679151088, Learning Rate: 0.001071\n",
      "Epoch 5208/40000, Loss: 0.00012691270967479795, Learning Rate: 0.001071\n",
      "Epoch 5209/40000, Loss: 0.0001317737769568339, Learning Rate: 0.001070\n",
      "Epoch 5210/40000, Loss: 0.0001454718440072611, Learning Rate: 0.001070\n",
      "Epoch 5211/40000, Loss: 9.083858458325267e-05, Learning Rate: 0.001070\n",
      "Epoch 5212/40000, Loss: 0.00014741194900125265, Learning Rate: 0.001070\n",
      "Epoch 5213/40000, Loss: 7.731336518190801e-05, Learning Rate: 0.001070\n",
      "Epoch 5214/40000, Loss: 9.288183355238289e-05, Learning Rate: 0.001070\n",
      "Epoch 5215/40000, Loss: 5.699292887584306e-05, Learning Rate: 0.001070\n",
      "Epoch 5216/40000, Loss: 0.0001750270603224635, Learning Rate: 0.001070\n",
      "Epoch 5217/40000, Loss: 6.215544999577105e-05, Learning Rate: 0.001069\n",
      "Epoch 5218/40000, Loss: 0.00010258957627229393, Learning Rate: 0.001069\n",
      "Epoch 5219/40000, Loss: 0.00015101861208677292, Learning Rate: 0.001069\n",
      "Epoch 5220/40000, Loss: 0.00016730740026105195, Learning Rate: 0.001069\n",
      "Epoch 5221/40000, Loss: 0.00010996960918419063, Learning Rate: 0.001069\n",
      "Epoch 5222/40000, Loss: 7.842436025384814e-05, Learning Rate: 0.001069\n",
      "Epoch 5223/40000, Loss: 0.0001469651615479961, Learning Rate: 0.001069\n",
      "Epoch 5224/40000, Loss: 0.00010239634138997644, Learning Rate: 0.001069\n",
      "Epoch 5225/40000, Loss: 4.968102803104557e-05, Learning Rate: 0.001068\n",
      "Epoch 5226/40000, Loss: 4.6468045184155926e-05, Learning Rate: 0.001068\n",
      "Epoch 5227/40000, Loss: 0.00012775894720107317, Learning Rate: 0.001068\n",
      "Epoch 5228/40000, Loss: 8.367800910491496e-05, Learning Rate: 0.001068\n",
      "Epoch 5229/40000, Loss: 5.3605250286636874e-05, Learning Rate: 0.001068\n",
      "Epoch 5230/40000, Loss: 8.899613749235868e-05, Learning Rate: 0.001068\n",
      "Epoch 5231/40000, Loss: 0.00011591317888814956, Learning Rate: 0.001068\n",
      "Epoch 5232/40000, Loss: 7.196278602350503e-05, Learning Rate: 0.001067\n",
      "Epoch 5233/40000, Loss: 6.859168206574395e-05, Learning Rate: 0.001067\n",
      "Epoch 5234/40000, Loss: 7.6015800004825e-05, Learning Rate: 0.001067\n",
      "Epoch 5235/40000, Loss: 7.193307101260871e-05, Learning Rate: 0.001067\n",
      "Epoch 5236/40000, Loss: 9.386228339280933e-05, Learning Rate: 0.001067\n",
      "Epoch 5237/40000, Loss: 0.0001385729556204751, Learning Rate: 0.001067\n",
      "Epoch 5238/40000, Loss: 0.00012597686145454645, Learning Rate: 0.001067\n",
      "Epoch 5239/40000, Loss: 8.958314720075577e-05, Learning Rate: 0.001067\n",
      "Epoch 5240/40000, Loss: 9.717036300571635e-05, Learning Rate: 0.001066\n",
      "Epoch 5241/40000, Loss: 0.0001255624374607578, Learning Rate: 0.001066\n",
      "Epoch 5242/40000, Loss: 7.942770753288642e-05, Learning Rate: 0.001066\n",
      "Epoch 5243/40000, Loss: 0.00011743819050025195, Learning Rate: 0.001066\n",
      "Epoch 5244/40000, Loss: 0.00011391317821107805, Learning Rate: 0.001066\n",
      "Epoch 5245/40000, Loss: 8.733657887205482e-05, Learning Rate: 0.001066\n",
      "Epoch 5246/40000, Loss: 0.00011907402949873358, Learning Rate: 0.001066\n",
      "Epoch 5247/40000, Loss: 8.45245667733252e-05, Learning Rate: 0.001066\n",
      "Epoch 5248/40000, Loss: 7.158072548918426e-05, Learning Rate: 0.001065\n",
      "Epoch 5249/40000, Loss: 7.224184810183942e-05, Learning Rate: 0.001065\n",
      "Epoch 5250/40000, Loss: 5.408001015894115e-05, Learning Rate: 0.001065\n",
      "Epoch 5251/40000, Loss: 0.00014983865548856556, Learning Rate: 0.001065\n",
      "Epoch 5252/40000, Loss: 7.59588583605364e-05, Learning Rate: 0.001065\n",
      "Epoch 5253/40000, Loss: 0.00012533004337456077, Learning Rate: 0.001065\n",
      "Epoch 5254/40000, Loss: 9.367443999508396e-05, Learning Rate: 0.001065\n",
      "Epoch 5255/40000, Loss: 8.029479067772627e-05, Learning Rate: 0.001065\n",
      "Epoch 5256/40000, Loss: 7.352126704063267e-05, Learning Rate: 0.001064\n",
      "Epoch 5257/40000, Loss: 0.00011639652802841738, Learning Rate: 0.001064\n",
      "Epoch 5258/40000, Loss: 0.00011651074601104483, Learning Rate: 0.001064\n",
      "Epoch 5259/40000, Loss: 4.5129221689421684e-05, Learning Rate: 0.001064\n",
      "Epoch 5260/40000, Loss: 0.00011663192708510906, Learning Rate: 0.001064\n",
      "Epoch 5261/40000, Loss: 8.562780567444861e-05, Learning Rate: 0.001064\n",
      "Epoch 5262/40000, Loss: 0.00011959902622038499, Learning Rate: 0.001064\n",
      "Epoch 5263/40000, Loss: 8.533189975423738e-05, Learning Rate: 0.001064\n",
      "Epoch 5264/40000, Loss: 7.384162017842755e-05, Learning Rate: 0.001063\n",
      "Epoch 5265/40000, Loss: 7.143645780161023e-05, Learning Rate: 0.001063\n",
      "Epoch 5266/40000, Loss: 4.357076977612451e-05, Learning Rate: 0.001063\n",
      "Epoch 5267/40000, Loss: 0.00011977823305642232, Learning Rate: 0.001063\n",
      "Epoch 5268/40000, Loss: 0.00011704616917995736, Learning Rate: 0.001063\n",
      "Epoch 5269/40000, Loss: 0.00011685174831654876, Learning Rate: 0.001063\n",
      "Epoch 5270/40000, Loss: 6.766597653040662e-05, Learning Rate: 0.001063\n",
      "Epoch 5271/40000, Loss: 0.00011012848699465394, Learning Rate: 0.001062\n",
      "Epoch 5272/40000, Loss: 7.070662832120433e-05, Learning Rate: 0.001062\n",
      "Epoch 5273/40000, Loss: 0.00011092962813563645, Learning Rate: 0.001062\n",
      "Epoch 5274/40000, Loss: 8.335641905432567e-05, Learning Rate: 0.001062\n",
      "Epoch 5275/40000, Loss: 7.664075383218005e-05, Learning Rate: 0.001062\n",
      "Epoch 5276/40000, Loss: 8.461071411147714e-05, Learning Rate: 0.001062\n",
      "Epoch 5277/40000, Loss: 0.00011381178046576679, Learning Rate: 0.001062\n",
      "Epoch 5278/40000, Loss: 7.782379543641582e-05, Learning Rate: 0.001062\n",
      "Epoch 5279/40000, Loss: 6.467947969213128e-05, Learning Rate: 0.001061\n",
      "Epoch 5280/40000, Loss: 0.00012101681932108477, Learning Rate: 0.001061\n",
      "Epoch 5281/40000, Loss: 0.00012198409967822954, Learning Rate: 0.001061\n",
      "Epoch 5282/40000, Loss: 7.17564980732277e-05, Learning Rate: 0.001061\n",
      "Epoch 5283/40000, Loss: 8.737396274227649e-05, Learning Rate: 0.001061\n",
      "Epoch 5284/40000, Loss: 8.930956391850486e-05, Learning Rate: 0.001061\n",
      "Epoch 5285/40000, Loss: 0.00011781085777329281, Learning Rate: 0.001061\n",
      "Epoch 5286/40000, Loss: 0.0001180025574285537, Learning Rate: 0.001061\n",
      "Epoch 5287/40000, Loss: 8.077042730292305e-05, Learning Rate: 0.001060\n",
      "Epoch 5288/40000, Loss: 0.00012399887782521546, Learning Rate: 0.001060\n",
      "Epoch 5289/40000, Loss: 0.00011596375406952575, Learning Rate: 0.001060\n",
      "Epoch 5290/40000, Loss: 0.00011624438047874719, Learning Rate: 0.001060\n",
      "Epoch 5291/40000, Loss: 0.00012331086327321827, Learning Rate: 0.001060\n",
      "Epoch 5292/40000, Loss: 4.857387830270454e-05, Learning Rate: 0.001060\n",
      "Epoch 5293/40000, Loss: 0.000107139116153121, Learning Rate: 0.001060\n",
      "Epoch 5294/40000, Loss: 0.0001073403109330684, Learning Rate: 0.001060\n",
      "Epoch 5295/40000, Loss: 0.00010197560186497867, Learning Rate: 0.001059\n",
      "Epoch 5296/40000, Loss: 5.051628249930218e-05, Learning Rate: 0.001059\n",
      "Epoch 5297/40000, Loss: 0.00013578454672824591, Learning Rate: 0.001059\n",
      "Epoch 5298/40000, Loss: 0.00015016568067949265, Learning Rate: 0.001059\n",
      "Epoch 5299/40000, Loss: 0.00017876416677609086, Learning Rate: 0.001059\n",
      "Epoch 5300/40000, Loss: 0.00016520371718797833, Learning Rate: 0.001059\n",
      "Epoch 5301/40000, Loss: 0.0001581996475579217, Learning Rate: 0.001059\n",
      "Epoch 5302/40000, Loss: 0.00016313513333443552, Learning Rate: 0.001059\n",
      "Epoch 5303/40000, Loss: 0.00013804479385726154, Learning Rate: 0.001058\n",
      "Epoch 5304/40000, Loss: 9.886814223136753e-05, Learning Rate: 0.001058\n",
      "Epoch 5305/40000, Loss: 0.00013604603009298444, Learning Rate: 0.001058\n",
      "Epoch 5306/40000, Loss: 0.00010608224692987278, Learning Rate: 0.001058\n",
      "Epoch 5307/40000, Loss: 6.330566975520924e-05, Learning Rate: 0.001058\n",
      "Epoch 5308/40000, Loss: 0.00015793234342709184, Learning Rate: 0.001058\n",
      "Epoch 5309/40000, Loss: 0.00010126539564225823, Learning Rate: 0.001058\n",
      "Epoch 5310/40000, Loss: 5.629364386550151e-05, Learning Rate: 0.001058\n",
      "Epoch 5311/40000, Loss: 6.666610715910792e-05, Learning Rate: 0.001057\n",
      "Epoch 5312/40000, Loss: 0.00010077787010231987, Learning Rate: 0.001057\n",
      "Epoch 5313/40000, Loss: 0.00010029743134509772, Learning Rate: 0.001057\n",
      "Epoch 5314/40000, Loss: 9.800485713640228e-05, Learning Rate: 0.001057\n",
      "Epoch 5315/40000, Loss: 0.0001039087655954063, Learning Rate: 0.001057\n",
      "Epoch 5316/40000, Loss: 0.00013130791194271296, Learning Rate: 0.001057\n",
      "Epoch 5317/40000, Loss: 0.00015159959730226547, Learning Rate: 0.001057\n",
      "Epoch 5318/40000, Loss: 0.00014013680629432201, Learning Rate: 0.001057\n",
      "Epoch 5319/40000, Loss: 9.215240424964577e-05, Learning Rate: 0.001056\n",
      "Epoch 5320/40000, Loss: 0.00016412920376751572, Learning Rate: 0.001056\n",
      "Epoch 5321/40000, Loss: 0.0001919442438520491, Learning Rate: 0.001056\n",
      "Epoch 5322/40000, Loss: 0.00013066538667771965, Learning Rate: 0.001056\n",
      "Epoch 5323/40000, Loss: 0.00017730722902342677, Learning Rate: 0.001056\n",
      "Epoch 5324/40000, Loss: 8.626836643088609e-05, Learning Rate: 0.001056\n",
      "Epoch 5325/40000, Loss: 0.00016265636077150702, Learning Rate: 0.001056\n",
      "Epoch 5326/40000, Loss: 0.00010479153570486233, Learning Rate: 0.001056\n",
      "Epoch 5327/40000, Loss: 0.00017346211825497448, Learning Rate: 0.001055\n",
      "Epoch 5328/40000, Loss: 0.00016336484986823052, Learning Rate: 0.001055\n",
      "Epoch 5329/40000, Loss: 9.584115468896925e-05, Learning Rate: 0.001055\n",
      "Epoch 5330/40000, Loss: 0.000117400566523429, Learning Rate: 0.001055\n",
      "Epoch 5331/40000, Loss: 0.00021989426750224084, Learning Rate: 0.001055\n",
      "Epoch 5332/40000, Loss: 0.0001320823939749971, Learning Rate: 0.001055\n",
      "Epoch 5333/40000, Loss: 0.0001490968861617148, Learning Rate: 0.001055\n",
      "Epoch 5334/40000, Loss: 0.00011266671936027706, Learning Rate: 0.001054\n",
      "Epoch 5335/40000, Loss: 0.00010805186320794746, Learning Rate: 0.001054\n",
      "Epoch 5336/40000, Loss: 0.00012133794371038675, Learning Rate: 0.001054\n",
      "Epoch 5337/40000, Loss: 0.00010273232328472659, Learning Rate: 0.001054\n",
      "Epoch 5338/40000, Loss: 0.00012278770736884326, Learning Rate: 0.001054\n",
      "Epoch 5339/40000, Loss: 0.00014600116992369294, Learning Rate: 0.001054\n",
      "Epoch 5340/40000, Loss: 0.00013992845197208226, Learning Rate: 0.001054\n",
      "Epoch 5341/40000, Loss: 0.00011175376130267978, Learning Rate: 0.001054\n",
      "Epoch 5342/40000, Loss: 8.015230559976771e-05, Learning Rate: 0.001053\n",
      "Epoch 5343/40000, Loss: 9.801283886190504e-05, Learning Rate: 0.001053\n",
      "Epoch 5344/40000, Loss: 0.00010559062502579764, Learning Rate: 0.001053\n",
      "Epoch 5345/40000, Loss: 0.00010665528679965064, Learning Rate: 0.001053\n",
      "Epoch 5346/40000, Loss: 0.00017485446005593985, Learning Rate: 0.001053\n",
      "Epoch 5347/40000, Loss: 6.645664689131081e-05, Learning Rate: 0.001053\n",
      "Epoch 5348/40000, Loss: 7.637751696165651e-05, Learning Rate: 0.001053\n",
      "Epoch 5349/40000, Loss: 0.00013798032887279987, Learning Rate: 0.001053\n",
      "Epoch 5350/40000, Loss: 9.358239185530692e-05, Learning Rate: 0.001052\n",
      "Epoch 5351/40000, Loss: 5.738274921895936e-05, Learning Rate: 0.001052\n",
      "Epoch 5352/40000, Loss: 8.587948832428083e-05, Learning Rate: 0.001052\n",
      "Epoch 5353/40000, Loss: 0.00015323706611525267, Learning Rate: 0.001052\n",
      "Epoch 5354/40000, Loss: 9.107130608754233e-05, Learning Rate: 0.001052\n",
      "Epoch 5355/40000, Loss: 5.740666892961599e-05, Learning Rate: 0.001052\n",
      "Epoch 5356/40000, Loss: 0.00013888724788557738, Learning Rate: 0.001052\n",
      "Epoch 5357/40000, Loss: 7.32068219804205e-05, Learning Rate: 0.001052\n",
      "Epoch 5358/40000, Loss: 0.00015124266792554408, Learning Rate: 0.001051\n",
      "Epoch 5359/40000, Loss: 0.00010186902363784611, Learning Rate: 0.001051\n",
      "Epoch 5360/40000, Loss: 6.786861922591925e-05, Learning Rate: 0.001051\n",
      "Epoch 5361/40000, Loss: 0.00016106739349197596, Learning Rate: 0.001051\n",
      "Epoch 5362/40000, Loss: 0.00010306609328836203, Learning Rate: 0.001051\n",
      "Epoch 5363/40000, Loss: 0.0003769488539546728, Learning Rate: 0.001051\n",
      "Epoch 5364/40000, Loss: 0.00011342193465679884, Learning Rate: 0.001051\n",
      "Epoch 5365/40000, Loss: 0.00019481014169286937, Learning Rate: 0.001051\n",
      "Epoch 5366/40000, Loss: 0.00019463596981950104, Learning Rate: 0.001050\n",
      "Epoch 5367/40000, Loss: 0.00010701790597522631, Learning Rate: 0.001050\n",
      "Epoch 5368/40000, Loss: 9.779146785149351e-05, Learning Rate: 0.001050\n",
      "Epoch 5369/40000, Loss: 0.00012799672549590468, Learning Rate: 0.001050\n",
      "Epoch 5370/40000, Loss: 0.00020834938914049417, Learning Rate: 0.001050\n",
      "Epoch 5371/40000, Loss: 7.793804252287373e-05, Learning Rate: 0.001050\n",
      "Epoch 5372/40000, Loss: 0.00011130038183182478, Learning Rate: 0.001050\n",
      "Epoch 5373/40000, Loss: 0.000171575607964769, Learning Rate: 0.001050\n",
      "Epoch 5374/40000, Loss: 0.0001088380377041176, Learning Rate: 0.001049\n",
      "Epoch 5375/40000, Loss: 0.0002245440991828218, Learning Rate: 0.001049\n",
      "Epoch 5376/40000, Loss: 9.706572018330917e-05, Learning Rate: 0.001049\n",
      "Epoch 5377/40000, Loss: 0.00022337592963594943, Learning Rate: 0.001049\n",
      "Epoch 5378/40000, Loss: 0.00015784056449774653, Learning Rate: 0.001049\n",
      "Epoch 5379/40000, Loss: 8.76308768056333e-05, Learning Rate: 0.001049\n",
      "Epoch 5380/40000, Loss: 5.8425932365935296e-05, Learning Rate: 0.001049\n",
      "Epoch 5381/40000, Loss: 0.0002082151622744277, Learning Rate: 0.001049\n",
      "Epoch 5382/40000, Loss: 8.00635534687899e-05, Learning Rate: 0.001048\n",
      "Epoch 5383/40000, Loss: 0.00016980036161839962, Learning Rate: 0.001048\n",
      "Epoch 5384/40000, Loss: 8.87275455170311e-05, Learning Rate: 0.001048\n",
      "Epoch 5385/40000, Loss: 7.67378878663294e-05, Learning Rate: 0.001048\n",
      "Epoch 5386/40000, Loss: 0.00011968637409154326, Learning Rate: 0.001048\n",
      "Epoch 5387/40000, Loss: 0.000184270364115946, Learning Rate: 0.001048\n",
      "Epoch 5388/40000, Loss: 8.416022319579497e-05, Learning Rate: 0.001048\n",
      "Epoch 5389/40000, Loss: 8.857622742652893e-05, Learning Rate: 0.001048\n",
      "Epoch 5390/40000, Loss: 8.616934064775705e-05, Learning Rate: 0.001047\n",
      "Epoch 5391/40000, Loss: 4.636457742890343e-05, Learning Rate: 0.001047\n",
      "Epoch 5392/40000, Loss: 4.3230364099144936e-05, Learning Rate: 0.001047\n",
      "Epoch 5393/40000, Loss: 0.0001340244634775445, Learning Rate: 0.001047\n",
      "Epoch 5394/40000, Loss: 6.864589522592723e-05, Learning Rate: 0.001047\n",
      "Epoch 5395/40000, Loss: 0.0001393911225022748, Learning Rate: 0.001047\n",
      "Epoch 5396/40000, Loss: 0.00013023406791035086, Learning Rate: 0.001047\n",
      "Epoch 5397/40000, Loss: 0.00011935296060983092, Learning Rate: 0.001047\n",
      "Epoch 5398/40000, Loss: 6.660498183919117e-05, Learning Rate: 0.001046\n",
      "Epoch 5399/40000, Loss: 7.525901310145855e-05, Learning Rate: 0.001046\n",
      "Epoch 5400/40000, Loss: 4.098465797142126e-05, Learning Rate: 0.001046\n",
      "Epoch 5401/40000, Loss: 7.026324601611122e-05, Learning Rate: 0.001046\n",
      "Epoch 5402/40000, Loss: 8.125478052534163e-05, Learning Rate: 0.001046\n",
      "Epoch 5403/40000, Loss: 8.654522389406338e-05, Learning Rate: 0.001046\n",
      "Epoch 5404/40000, Loss: 7.192759221652523e-05, Learning Rate: 0.001046\n",
      "Epoch 5405/40000, Loss: 6.907845090609044e-05, Learning Rate: 0.001046\n",
      "Epoch 5406/40000, Loss: 0.00011111496132798493, Learning Rate: 0.001045\n",
      "Epoch 5407/40000, Loss: 7.432189158862457e-05, Learning Rate: 0.001045\n",
      "Epoch 5408/40000, Loss: 7.056073809508234e-05, Learning Rate: 0.001045\n",
      "Epoch 5409/40000, Loss: 0.00012655525642912835, Learning Rate: 0.001045\n",
      "Epoch 5410/40000, Loss: 7.538079808000475e-05, Learning Rate: 0.001045\n",
      "Epoch 5411/40000, Loss: 6.958196172490716e-05, Learning Rate: 0.001045\n",
      "Epoch 5412/40000, Loss: 4.303999958210625e-05, Learning Rate: 0.001045\n",
      "Epoch 5413/40000, Loss: 9.083905752049759e-05, Learning Rate: 0.001045\n",
      "Epoch 5414/40000, Loss: 0.00012631998106371611, Learning Rate: 0.001044\n",
      "Epoch 5415/40000, Loss: 6.736772775184363e-05, Learning Rate: 0.001044\n",
      "Epoch 5416/40000, Loss: 4.051463838550262e-05, Learning Rate: 0.001044\n",
      "Epoch 5417/40000, Loss: 3.996299346908927e-05, Learning Rate: 0.001044\n",
      "Epoch 5418/40000, Loss: 0.0001110757002606988, Learning Rate: 0.001044\n",
      "Epoch 5419/40000, Loss: 6.929579103598371e-05, Learning Rate: 0.001044\n",
      "Epoch 5420/40000, Loss: 6.772206688765436e-05, Learning Rate: 0.001044\n",
      "Epoch 5421/40000, Loss: 0.00011826292757177725, Learning Rate: 0.001044\n",
      "Epoch 5422/40000, Loss: 0.0001175481520476751, Learning Rate: 0.001043\n",
      "Epoch 5423/40000, Loss: 7.237228419398889e-05, Learning Rate: 0.001043\n",
      "Epoch 5424/40000, Loss: 7.00030432199128e-05, Learning Rate: 0.001043\n",
      "Epoch 5425/40000, Loss: 8.481369877699763e-05, Learning Rate: 0.001043\n",
      "Epoch 5426/40000, Loss: 7.879285840317607e-05, Learning Rate: 0.001043\n",
      "Epoch 5427/40000, Loss: 7.824198110029101e-05, Learning Rate: 0.001043\n",
      "Epoch 5428/40000, Loss: 4.32750275649596e-05, Learning Rate: 0.001043\n",
      "Epoch 5429/40000, Loss: 0.00012477985001169145, Learning Rate: 0.001043\n",
      "Epoch 5430/40000, Loss: 4.327112401369959e-05, Learning Rate: 0.001042\n",
      "Epoch 5431/40000, Loss: 4.235271262587048e-05, Learning Rate: 0.001042\n",
      "Epoch 5432/40000, Loss: 8.502178388880566e-05, Learning Rate: 0.001042\n",
      "Epoch 5433/40000, Loss: 8.298469037981704e-05, Learning Rate: 0.001042\n",
      "Epoch 5434/40000, Loss: 7.636377267772332e-05, Learning Rate: 0.001042\n",
      "Epoch 5435/40000, Loss: 4.059051934746094e-05, Learning Rate: 0.001042\n",
      "Epoch 5436/40000, Loss: 0.00011034128692699596, Learning Rate: 0.001042\n",
      "Epoch 5437/40000, Loss: 6.966116052353755e-05, Learning Rate: 0.001042\n",
      "Epoch 5438/40000, Loss: 0.00011443968105595559, Learning Rate: 0.001041\n",
      "Epoch 5439/40000, Loss: 0.00012566513032652438, Learning Rate: 0.001041\n",
      "Epoch 5440/40000, Loss: 4.354181146482006e-05, Learning Rate: 0.001041\n",
      "Epoch 5441/40000, Loss: 0.00014747932436876, Learning Rate: 0.001041\n",
      "Epoch 5442/40000, Loss: 0.00013530456635635346, Learning Rate: 0.001041\n",
      "Epoch 5443/40000, Loss: 7.646536687389016e-05, Learning Rate: 0.001041\n",
      "Epoch 5444/40000, Loss: 0.0001276208204217255, Learning Rate: 0.001041\n",
      "Epoch 5445/40000, Loss: 8.671254181535915e-05, Learning Rate: 0.001041\n",
      "Epoch 5446/40000, Loss: 4.770680607180111e-05, Learning Rate: 0.001040\n",
      "Epoch 5447/40000, Loss: 0.0001280480355489999, Learning Rate: 0.001040\n",
      "Epoch 5448/40000, Loss: 0.00010488004772923887, Learning Rate: 0.001040\n",
      "Epoch 5449/40000, Loss: 0.00011827606795122847, Learning Rate: 0.001040\n",
      "Epoch 5450/40000, Loss: 0.00016834035341162235, Learning Rate: 0.001040\n",
      "Epoch 5451/40000, Loss: 0.00011106098827440292, Learning Rate: 0.001040\n",
      "Epoch 5452/40000, Loss: 0.0001743212778819725, Learning Rate: 0.001040\n",
      "Epoch 5453/40000, Loss: 9.366717131342739e-05, Learning Rate: 0.001040\n",
      "Epoch 5454/40000, Loss: 0.00014923051639925689, Learning Rate: 0.001039\n",
      "Epoch 5455/40000, Loss: 0.00010856465087272227, Learning Rate: 0.001039\n",
      "Epoch 5456/40000, Loss: 0.00011614829418249428, Learning Rate: 0.001039\n",
      "Epoch 5457/40000, Loss: 0.0001796487340470776, Learning Rate: 0.001039\n",
      "Epoch 5458/40000, Loss: 0.00011784320668084547, Learning Rate: 0.001039\n",
      "Epoch 5459/40000, Loss: 0.00010391476098448038, Learning Rate: 0.001039\n",
      "Epoch 5460/40000, Loss: 6.702096288790926e-05, Learning Rate: 0.001039\n",
      "Epoch 5461/40000, Loss: 0.00012675317702814937, Learning Rate: 0.001039\n",
      "Epoch 5462/40000, Loss: 0.0001328337239101529, Learning Rate: 0.001038\n",
      "Epoch 5463/40000, Loss: 0.00012206920655444264, Learning Rate: 0.001038\n",
      "Epoch 5464/40000, Loss: 0.00016211910406127572, Learning Rate: 0.001038\n",
      "Epoch 5465/40000, Loss: 0.00013424502685666084, Learning Rate: 0.001038\n",
      "Epoch 5466/40000, Loss: 7.923908560769632e-05, Learning Rate: 0.001038\n",
      "Epoch 5467/40000, Loss: 5.466919901664369e-05, Learning Rate: 0.001038\n",
      "Epoch 5468/40000, Loss: 5.448105002869852e-05, Learning Rate: 0.001038\n",
      "Epoch 5469/40000, Loss: 0.00013278106052894145, Learning Rate: 0.001038\n",
      "Epoch 5470/40000, Loss: 0.00014812775771133602, Learning Rate: 0.001037\n",
      "Epoch 5471/40000, Loss: 9.733143815537915e-05, Learning Rate: 0.001037\n",
      "Epoch 5472/40000, Loss: 0.0001219921832671389, Learning Rate: 0.001037\n",
      "Epoch 5473/40000, Loss: 9.288796718465164e-05, Learning Rate: 0.001037\n",
      "Epoch 5474/40000, Loss: 0.00018600831390358508, Learning Rate: 0.001037\n",
      "Epoch 5475/40000, Loss: 0.00016844853234943002, Learning Rate: 0.001037\n",
      "Epoch 5476/40000, Loss: 0.00013579214282799512, Learning Rate: 0.001037\n",
      "Epoch 5477/40000, Loss: 0.00013779687287751585, Learning Rate: 0.001037\n",
      "Epoch 5478/40000, Loss: 9.447624324820936e-05, Learning Rate: 0.001036\n",
      "Epoch 5479/40000, Loss: 0.00013057483010925353, Learning Rate: 0.001036\n",
      "Epoch 5480/40000, Loss: 8.571758371545002e-05, Learning Rate: 0.001036\n",
      "Epoch 5481/40000, Loss: 5.466943548526615e-05, Learning Rate: 0.001036\n",
      "Epoch 5482/40000, Loss: 0.00013739318819716573, Learning Rate: 0.001036\n",
      "Epoch 5483/40000, Loss: 9.981837501982227e-05, Learning Rate: 0.001036\n",
      "Epoch 5484/40000, Loss: 0.00011727706441888586, Learning Rate: 0.001036\n",
      "Epoch 5485/40000, Loss: 0.00021809519967064261, Learning Rate: 0.001036\n",
      "Epoch 5486/40000, Loss: 0.00014753466530237347, Learning Rate: 0.001035\n",
      "Epoch 5487/40000, Loss: 0.00014559607370756567, Learning Rate: 0.001035\n",
      "Epoch 5488/40000, Loss: 0.0001629793259780854, Learning Rate: 0.001035\n",
      "Epoch 5489/40000, Loss: 0.0001237040851265192, Learning Rate: 0.001035\n",
      "Epoch 5490/40000, Loss: 0.00019373941177036613, Learning Rate: 0.001035\n",
      "Epoch 5491/40000, Loss: 0.00015922840975690633, Learning Rate: 0.001035\n",
      "Epoch 5492/40000, Loss: 0.0001160579122370109, Learning Rate: 0.001035\n",
      "Epoch 5493/40000, Loss: 8.4442894149106e-05, Learning Rate: 0.001035\n",
      "Epoch 5494/40000, Loss: 8.67341150296852e-05, Learning Rate: 0.001034\n",
      "Epoch 5495/40000, Loss: 0.00015486124902963638, Learning Rate: 0.001034\n",
      "Epoch 5496/40000, Loss: 8.357313345186412e-05, Learning Rate: 0.001034\n",
      "Epoch 5497/40000, Loss: 7.728966738795862e-05, Learning Rate: 0.001034\n",
      "Epoch 5498/40000, Loss: 7.740120781818405e-05, Learning Rate: 0.001034\n",
      "Epoch 5499/40000, Loss: 0.00014132054639048874, Learning Rate: 0.001034\n",
      "Epoch 5500/40000, Loss: 9.967960068024695e-05, Learning Rate: 0.001034\n",
      "Epoch 5501/40000, Loss: 0.0001406385563313961, Learning Rate: 0.001034\n",
      "Epoch 5502/40000, Loss: 0.0001401510671712458, Learning Rate: 0.001033\n",
      "Epoch 5503/40000, Loss: 0.00012318942754063755, Learning Rate: 0.001033\n",
      "Epoch 5504/40000, Loss: 9.067505015991628e-05, Learning Rate: 0.001033\n",
      "Epoch 5505/40000, Loss: 7.476963946828619e-05, Learning Rate: 0.001033\n",
      "Epoch 5506/40000, Loss: 0.00011373622692190111, Learning Rate: 0.001033\n",
      "Epoch 5507/40000, Loss: 9.396680252393708e-05, Learning Rate: 0.001033\n",
      "Epoch 5508/40000, Loss: 0.00010774096881505102, Learning Rate: 0.001033\n",
      "Epoch 5509/40000, Loss: 9.44640560192056e-05, Learning Rate: 0.001033\n",
      "Epoch 5510/40000, Loss: 0.0001720599248073995, Learning Rate: 0.001032\n",
      "Epoch 5511/40000, Loss: 0.00015816533414181322, Learning Rate: 0.001032\n",
      "Epoch 5512/40000, Loss: 0.00014466010907199234, Learning Rate: 0.001032\n",
      "Epoch 5513/40000, Loss: 7.694326632190496e-05, Learning Rate: 0.001032\n",
      "Epoch 5514/40000, Loss: 0.0001754433906171471, Learning Rate: 0.001032\n",
      "Epoch 5515/40000, Loss: 0.0003977796295657754, Learning Rate: 0.001032\n",
      "Epoch 5516/40000, Loss: 0.00023728566884528846, Learning Rate: 0.001032\n",
      "Epoch 5517/40000, Loss: 0.00022660981630906463, Learning Rate: 0.001032\n",
      "Epoch 5518/40000, Loss: 0.0001096726773539558, Learning Rate: 0.001031\n",
      "Epoch 5519/40000, Loss: 0.0001442056382074952, Learning Rate: 0.001031\n",
      "Epoch 5520/40000, Loss: 0.00010726958862505853, Learning Rate: 0.001031\n",
      "Epoch 5521/40000, Loss: 0.00030858805985189974, Learning Rate: 0.001031\n",
      "Epoch 5522/40000, Loss: 0.0001547779538668692, Learning Rate: 0.001031\n",
      "Epoch 5523/40000, Loss: 0.00016823476471472532, Learning Rate: 0.001031\n",
      "Epoch 5524/40000, Loss: 0.0001426645612809807, Learning Rate: 0.001031\n",
      "Epoch 5525/40000, Loss: 5.906164733460173e-05, Learning Rate: 0.001031\n",
      "Epoch 5526/40000, Loss: 9.573090210324153e-05, Learning Rate: 0.001030\n",
      "Epoch 5527/40000, Loss: 8.990355127025396e-05, Learning Rate: 0.001030\n",
      "Epoch 5528/40000, Loss: 9.033784590428695e-05, Learning Rate: 0.001030\n",
      "Epoch 5529/40000, Loss: 7.816011202521622e-05, Learning Rate: 0.001030\n",
      "Epoch 5530/40000, Loss: 0.00010184351413045079, Learning Rate: 0.001030\n",
      "Epoch 5531/40000, Loss: 0.00011078410170739517, Learning Rate: 0.001030\n",
      "Epoch 5532/40000, Loss: 0.00013021609629504383, Learning Rate: 0.001030\n",
      "Epoch 5533/40000, Loss: 0.00015721394447609782, Learning Rate: 0.001030\n",
      "Epoch 5534/40000, Loss: 7.356028072535992e-05, Learning Rate: 0.001029\n",
      "Epoch 5535/40000, Loss: 0.00012594902364071459, Learning Rate: 0.001029\n",
      "Epoch 5536/40000, Loss: 0.00012108110968256369, Learning Rate: 0.001029\n",
      "Epoch 5537/40000, Loss: 0.00010527777340030298, Learning Rate: 0.001029\n",
      "Epoch 5538/40000, Loss: 5.881984907318838e-05, Learning Rate: 0.001029\n",
      "Epoch 5539/40000, Loss: 9.436052641831338e-05, Learning Rate: 0.001029\n",
      "Epoch 5540/40000, Loss: 0.0001133566111093387, Learning Rate: 0.001029\n",
      "Epoch 5541/40000, Loss: 5.8038454881170765e-05, Learning Rate: 0.001029\n",
      "Epoch 5542/40000, Loss: 0.00013096501061227173, Learning Rate: 0.001028\n",
      "Epoch 5543/40000, Loss: 9.399602276971564e-05, Learning Rate: 0.001028\n",
      "Epoch 5544/40000, Loss: 0.00011215671838726848, Learning Rate: 0.001028\n",
      "Epoch 5545/40000, Loss: 4.327160058892332e-05, Learning Rate: 0.001028\n",
      "Epoch 5546/40000, Loss: 0.00011803470988525078, Learning Rate: 0.001028\n",
      "Epoch 5547/40000, Loss: 6.783820572309196e-05, Learning Rate: 0.001028\n",
      "Epoch 5548/40000, Loss: 8.621766028227285e-05, Learning Rate: 0.001028\n",
      "Epoch 5549/40000, Loss: 8.427951252087951e-05, Learning Rate: 0.001028\n",
      "Epoch 5550/40000, Loss: 4.1592997149564326e-05, Learning Rate: 0.001028\n",
      "Epoch 5551/40000, Loss: 8.938655082602054e-05, Learning Rate: 0.001027\n",
      "Epoch 5552/40000, Loss: 7.63353455113247e-05, Learning Rate: 0.001027\n",
      "Epoch 5553/40000, Loss: 6.776319787604734e-05, Learning Rate: 0.001027\n",
      "Epoch 5554/40000, Loss: 6.574990402441472e-05, Learning Rate: 0.001027\n",
      "Epoch 5555/40000, Loss: 4.085668479092419e-05, Learning Rate: 0.001027\n",
      "Epoch 5556/40000, Loss: 8.789455750957131e-05, Learning Rate: 0.001027\n",
      "Epoch 5557/40000, Loss: 9.01559615158476e-05, Learning Rate: 0.001027\n",
      "Epoch 5558/40000, Loss: 9.151819540420547e-05, Learning Rate: 0.001027\n",
      "Epoch 5559/40000, Loss: 9.137424785876647e-05, Learning Rate: 0.001026\n",
      "Epoch 5560/40000, Loss: 4.79987938888371e-05, Learning Rate: 0.001026\n",
      "Epoch 5561/40000, Loss: 6.789086910430342e-05, Learning Rate: 0.001026\n",
      "Epoch 5562/40000, Loss: 0.00013349195069167763, Learning Rate: 0.001026\n",
      "Epoch 5563/40000, Loss: 4.670762427849695e-05, Learning Rate: 0.001026\n",
      "Epoch 5564/40000, Loss: 0.00012747828441206366, Learning Rate: 0.001026\n",
      "Epoch 5565/40000, Loss: 5.094852895126678e-05, Learning Rate: 0.001026\n",
      "Epoch 5566/40000, Loss: 0.00011623238970059901, Learning Rate: 0.001026\n",
      "Epoch 5567/40000, Loss: 7.53317290218547e-05, Learning Rate: 0.001025\n",
      "Epoch 5568/40000, Loss: 0.00013839446182828397, Learning Rate: 0.001025\n",
      "Epoch 5569/40000, Loss: 0.0001033665394061245, Learning Rate: 0.001025\n",
      "Epoch 5570/40000, Loss: 9.680438233772293e-05, Learning Rate: 0.001025\n",
      "Epoch 5571/40000, Loss: 0.00010495137394173071, Learning Rate: 0.001025\n",
      "Epoch 5572/40000, Loss: 9.459228022024035e-05, Learning Rate: 0.001025\n",
      "Epoch 5573/40000, Loss: 0.00022446051298175007, Learning Rate: 0.001025\n",
      "Epoch 5574/40000, Loss: 0.00010056262544821948, Learning Rate: 0.001025\n",
      "Epoch 5575/40000, Loss: 9.355531074106693e-05, Learning Rate: 0.001024\n",
      "Epoch 5576/40000, Loss: 6.607986870221794e-05, Learning Rate: 0.001024\n",
      "Epoch 5577/40000, Loss: 0.00017737153393682092, Learning Rate: 0.001024\n",
      "Epoch 5578/40000, Loss: 7.329995423788205e-05, Learning Rate: 0.001024\n",
      "Epoch 5579/40000, Loss: 0.00010960608051391318, Learning Rate: 0.001024\n",
      "Epoch 5580/40000, Loss: 0.0001193969656014815, Learning Rate: 0.001024\n",
      "Epoch 5581/40000, Loss: 8.413138129981235e-05, Learning Rate: 0.001024\n",
      "Epoch 5582/40000, Loss: 0.00018281913071405143, Learning Rate: 0.001024\n",
      "Epoch 5583/40000, Loss: 0.00012276993948034942, Learning Rate: 0.001023\n",
      "Epoch 5584/40000, Loss: 0.00011150776117574424, Learning Rate: 0.001023\n",
      "Epoch 5585/40000, Loss: 0.00011731348058674484, Learning Rate: 0.001023\n",
      "Epoch 5586/40000, Loss: 5.836690615979023e-05, Learning Rate: 0.001023\n",
      "Epoch 5587/40000, Loss: 0.0001667123578954488, Learning Rate: 0.001023\n",
      "Epoch 5588/40000, Loss: 0.0001728183269733563, Learning Rate: 0.001023\n",
      "Epoch 5589/40000, Loss: 0.00016059615882113576, Learning Rate: 0.001023\n",
      "Epoch 5590/40000, Loss: 0.00018131575779989362, Learning Rate: 0.001023\n",
      "Epoch 5591/40000, Loss: 0.00015065296611282974, Learning Rate: 0.001022\n",
      "Epoch 5592/40000, Loss: 9.295897325500846e-05, Learning Rate: 0.001022\n",
      "Epoch 5593/40000, Loss: 5.129673445480876e-05, Learning Rate: 0.001022\n",
      "Epoch 5594/40000, Loss: 7.110044680302963e-05, Learning Rate: 0.001022\n",
      "Epoch 5595/40000, Loss: 0.0001229086919920519, Learning Rate: 0.001022\n",
      "Epoch 5596/40000, Loss: 0.00011271902621956542, Learning Rate: 0.001022\n",
      "Epoch 5597/40000, Loss: 7.451177953043953e-05, Learning Rate: 0.001022\n",
      "Epoch 5598/40000, Loss: 0.00014214821567293257, Learning Rate: 0.001022\n",
      "Epoch 5599/40000, Loss: 0.00013237730308901519, Learning Rate: 0.001021\n",
      "Epoch 5600/40000, Loss: 8.084876753855497e-05, Learning Rate: 0.001021\n",
      "Epoch 5601/40000, Loss: 5.3996707720216364e-05, Learning Rate: 0.001021\n",
      "Epoch 5602/40000, Loss: 9.315676288679242e-05, Learning Rate: 0.001021\n",
      "Epoch 5603/40000, Loss: 0.00011074712529079989, Learning Rate: 0.001021\n",
      "Epoch 5604/40000, Loss: 7.269940397236496e-05, Learning Rate: 0.001021\n",
      "Epoch 5605/40000, Loss: 9.336043876828626e-05, Learning Rate: 0.001021\n",
      "Epoch 5606/40000, Loss: 0.00014212664973456413, Learning Rate: 0.001021\n",
      "Epoch 5607/40000, Loss: 0.00013108152779750526, Learning Rate: 0.001021\n",
      "Epoch 5608/40000, Loss: 0.00012056999548804015, Learning Rate: 0.001020\n",
      "Epoch 5609/40000, Loss: 8.566901669837534e-05, Learning Rate: 0.001020\n",
      "Epoch 5610/40000, Loss: 0.00013219367247074842, Learning Rate: 0.001020\n",
      "Epoch 5611/40000, Loss: 0.00010667159222066402, Learning Rate: 0.001020\n",
      "Epoch 5612/40000, Loss: 9.774116915650666e-05, Learning Rate: 0.001020\n",
      "Epoch 5613/40000, Loss: 0.00012811360647901893, Learning Rate: 0.001020\n",
      "Epoch 5614/40000, Loss: 4.7550463932566345e-05, Learning Rate: 0.001020\n",
      "Epoch 5615/40000, Loss: 0.00018897835980169475, Learning Rate: 0.001020\n",
      "Epoch 5616/40000, Loss: 5.3339208534453064e-05, Learning Rate: 0.001019\n",
      "Epoch 5617/40000, Loss: 5.0477097829570994e-05, Learning Rate: 0.001019\n",
      "Epoch 5618/40000, Loss: 0.0001227908651344478, Learning Rate: 0.001019\n",
      "Epoch 5619/40000, Loss: 4.9893838877324015e-05, Learning Rate: 0.001019\n",
      "Epoch 5620/40000, Loss: 4.574056947603822e-05, Learning Rate: 0.001019\n",
      "Epoch 5621/40000, Loss: 0.00012770778266713023, Learning Rate: 0.001019\n",
      "Epoch 5622/40000, Loss: 0.0001189365066238679, Learning Rate: 0.001019\n",
      "Epoch 5623/40000, Loss: 0.00026956305373460054, Learning Rate: 0.001019\n",
      "Epoch 5624/40000, Loss: 9.414236410520971e-05, Learning Rate: 0.001018\n",
      "Epoch 5625/40000, Loss: 0.00017297040903940797, Learning Rate: 0.001018\n",
      "Epoch 5626/40000, Loss: 4.8428231821162626e-05, Learning Rate: 0.001018\n",
      "Epoch 5627/40000, Loss: 4.631630508811213e-05, Learning Rate: 0.001018\n",
      "Epoch 5628/40000, Loss: 9.771952318260446e-05, Learning Rate: 0.001018\n",
      "Epoch 5629/40000, Loss: 0.00012860773131251335, Learning Rate: 0.001018\n",
      "Epoch 5630/40000, Loss: 7.227217429317534e-05, Learning Rate: 0.001018\n",
      "Epoch 5631/40000, Loss: 7.75119406171143e-05, Learning Rate: 0.001018\n",
      "Epoch 5632/40000, Loss: 8.90038427314721e-05, Learning Rate: 0.001017\n",
      "Epoch 5633/40000, Loss: 0.00013914048031438142, Learning Rate: 0.001017\n",
      "Epoch 5634/40000, Loss: 0.0001554583723191172, Learning Rate: 0.001017\n",
      "Epoch 5635/40000, Loss: 5.1814560720231384e-05, Learning Rate: 0.001017\n",
      "Epoch 5636/40000, Loss: 9.355909423902631e-05, Learning Rate: 0.001017\n",
      "Epoch 5637/40000, Loss: 0.00013013635179959238, Learning Rate: 0.001017\n",
      "Epoch 5638/40000, Loss: 0.00010902596841333434, Learning Rate: 0.001017\n",
      "Epoch 5639/40000, Loss: 0.00016514175513293594, Learning Rate: 0.001017\n",
      "Epoch 5640/40000, Loss: 0.0001502915984019637, Learning Rate: 0.001016\n",
      "Epoch 5641/40000, Loss: 8.252172847278416e-05, Learning Rate: 0.001016\n",
      "Epoch 5642/40000, Loss: 7.047719554975629e-05, Learning Rate: 0.001016\n",
      "Epoch 5643/40000, Loss: 6.505394412670285e-05, Learning Rate: 0.001016\n",
      "Epoch 5644/40000, Loss: 0.00011182902380824089, Learning Rate: 0.001016\n",
      "Epoch 5645/40000, Loss: 7.624030695296824e-05, Learning Rate: 0.001016\n",
      "Epoch 5646/40000, Loss: 7.301602454390377e-05, Learning Rate: 0.001016\n",
      "Epoch 5647/40000, Loss: 6.934090924914926e-05, Learning Rate: 0.001016\n",
      "Epoch 5648/40000, Loss: 7.2458729846403e-05, Learning Rate: 0.001015\n",
      "Epoch 5649/40000, Loss: 0.00012636961764656007, Learning Rate: 0.001015\n",
      "Epoch 5650/40000, Loss: 0.00015515201084781438, Learning Rate: 0.001015\n",
      "Epoch 5651/40000, Loss: 8.511375926900655e-05, Learning Rate: 0.001015\n",
      "Epoch 5652/40000, Loss: 0.00014306126104202121, Learning Rate: 0.001015\n",
      "Epoch 5653/40000, Loss: 0.00012104662164347246, Learning Rate: 0.001015\n",
      "Epoch 5654/40000, Loss: 8.287766831927001e-05, Learning Rate: 0.001015\n",
      "Epoch 5655/40000, Loss: 0.00011439119407441467, Learning Rate: 0.001015\n",
      "Epoch 5656/40000, Loss: 6.747814040863886e-05, Learning Rate: 0.001015\n",
      "Epoch 5657/40000, Loss: 5.1861992687918246e-05, Learning Rate: 0.001014\n",
      "Epoch 5658/40000, Loss: 9.228505950886756e-05, Learning Rate: 0.001014\n",
      "Epoch 5659/40000, Loss: 0.00012544346100185066, Learning Rate: 0.001014\n",
      "Epoch 5660/40000, Loss: 8.918740786612034e-05, Learning Rate: 0.001014\n",
      "Epoch 5661/40000, Loss: 0.0001071278311428614, Learning Rate: 0.001014\n",
      "Epoch 5662/40000, Loss: 8.796725887805223e-05, Learning Rate: 0.001014\n",
      "Epoch 5663/40000, Loss: 8.537482790416107e-05, Learning Rate: 0.001014\n",
      "Epoch 5664/40000, Loss: 0.00011235105193918571, Learning Rate: 0.001014\n",
      "Epoch 5665/40000, Loss: 0.00011124648881377652, Learning Rate: 0.001013\n",
      "Epoch 5666/40000, Loss: 0.00011015161726390943, Learning Rate: 0.001013\n",
      "Epoch 5667/40000, Loss: 6.537323497468606e-05, Learning Rate: 0.001013\n",
      "Epoch 5668/40000, Loss: 0.00011602604354266077, Learning Rate: 0.001013\n",
      "Epoch 5669/40000, Loss: 8.22440633783117e-05, Learning Rate: 0.001013\n",
      "Epoch 5670/40000, Loss: 0.0001224497245857492, Learning Rate: 0.001013\n",
      "Epoch 5671/40000, Loss: 6.998598109930754e-05, Learning Rate: 0.001013\n",
      "Epoch 5672/40000, Loss: 4.1780946048675105e-05, Learning Rate: 0.001013\n",
      "Epoch 5673/40000, Loss: 0.00011953374632867053, Learning Rate: 0.001012\n",
      "Epoch 5674/40000, Loss: 0.00010798004950629547, Learning Rate: 0.001012\n",
      "Epoch 5675/40000, Loss: 4.089490175829269e-05, Learning Rate: 0.001012\n",
      "Epoch 5676/40000, Loss: 3.9046390156727284e-05, Learning Rate: 0.001012\n",
      "Epoch 5677/40000, Loss: 0.00012007527402602136, Learning Rate: 0.001012\n",
      "Epoch 5678/40000, Loss: 9.077064169105142e-05, Learning Rate: 0.001012\n",
      "Epoch 5679/40000, Loss: 7.516352343373e-05, Learning Rate: 0.001012\n",
      "Epoch 5680/40000, Loss: 7.466851093340665e-05, Learning Rate: 0.001012\n",
      "Epoch 5681/40000, Loss: 8.280356269096956e-05, Learning Rate: 0.001011\n",
      "Epoch 5682/40000, Loss: 8.30764474812895e-05, Learning Rate: 0.001011\n",
      "Epoch 5683/40000, Loss: 7.578405347885564e-05, Learning Rate: 0.001011\n",
      "Epoch 5684/40000, Loss: 0.00011406927660573274, Learning Rate: 0.001011\n",
      "Epoch 5685/40000, Loss: 3.884900434059091e-05, Learning Rate: 0.001011\n",
      "Epoch 5686/40000, Loss: 0.00010597178334137425, Learning Rate: 0.001011\n",
      "Epoch 5687/40000, Loss: 6.430846406146884e-05, Learning Rate: 0.001011\n",
      "Epoch 5688/40000, Loss: 6.306561408564448e-05, Learning Rate: 0.001011\n",
      "Epoch 5689/40000, Loss: 7.659474795218557e-05, Learning Rate: 0.001011\n",
      "Epoch 5690/40000, Loss: 6.543151539517567e-05, Learning Rate: 0.001010\n",
      "Epoch 5691/40000, Loss: 0.00011491372424643487, Learning Rate: 0.001010\n",
      "Epoch 5692/40000, Loss: 8.050761243794113e-05, Learning Rate: 0.001010\n",
      "Epoch 5693/40000, Loss: 0.00011412817548261955, Learning Rate: 0.001010\n",
      "Epoch 5694/40000, Loss: 8.303621871164069e-05, Learning Rate: 0.001010\n",
      "Epoch 5695/40000, Loss: 3.887084312736988e-05, Learning Rate: 0.001010\n",
      "Epoch 5696/40000, Loss: 8.501075353706256e-05, Learning Rate: 0.001010\n",
      "Epoch 5697/40000, Loss: 9.524667984806001e-05, Learning Rate: 0.001010\n",
      "Epoch 5698/40000, Loss: 0.00012526065984275192, Learning Rate: 0.001009\n",
      "Epoch 5699/40000, Loss: 8.745424565859139e-05, Learning Rate: 0.001009\n",
      "Epoch 5700/40000, Loss: 0.0001214421063195914, Learning Rate: 0.001009\n",
      "Epoch 5701/40000, Loss: 0.00012687983689829707, Learning Rate: 0.001009\n",
      "Epoch 5702/40000, Loss: 9.719168883748353e-05, Learning Rate: 0.001009\n",
      "Epoch 5703/40000, Loss: 0.00014605394972022623, Learning Rate: 0.001009\n",
      "Epoch 5704/40000, Loss: 0.00014059974637348205, Learning Rate: 0.001009\n",
      "Epoch 5705/40000, Loss: 0.0001303438621107489, Learning Rate: 0.001009\n",
      "Epoch 5706/40000, Loss: 0.00013641359691973776, Learning Rate: 0.001008\n",
      "Epoch 5707/40000, Loss: 0.00021141562319826335, Learning Rate: 0.001008\n",
      "Epoch 5708/40000, Loss: 0.00011777511099353433, Learning Rate: 0.001008\n",
      "Epoch 5709/40000, Loss: 0.0001324526674579829, Learning Rate: 0.001008\n",
      "Epoch 5710/40000, Loss: 0.0001269950153073296, Learning Rate: 0.001008\n",
      "Epoch 5711/40000, Loss: 0.00019210917525924742, Learning Rate: 0.001008\n",
      "Epoch 5712/40000, Loss: 9.393296204507351e-05, Learning Rate: 0.001008\n",
      "Epoch 5713/40000, Loss: 0.00042732388828881085, Learning Rate: 0.001008\n",
      "Epoch 5714/40000, Loss: 0.0001493884192313999, Learning Rate: 0.001007\n",
      "Epoch 5715/40000, Loss: 0.00024889170890673995, Learning Rate: 0.001007\n",
      "Epoch 5716/40000, Loss: 0.00028127862606197596, Learning Rate: 0.001007\n",
      "Epoch 5717/40000, Loss: 0.00015938909200485796, Learning Rate: 0.001007\n",
      "Epoch 5718/40000, Loss: 0.0005267239757813513, Learning Rate: 0.001007\n",
      "Epoch 5719/40000, Loss: 0.0002158220304409042, Learning Rate: 0.001007\n",
      "Epoch 5720/40000, Loss: 0.0003394911764189601, Learning Rate: 0.001007\n",
      "Epoch 5721/40000, Loss: 0.0002821357629727572, Learning Rate: 0.001007\n",
      "Epoch 5722/40000, Loss: 0.000238216103753075, Learning Rate: 0.001007\n",
      "Epoch 5723/40000, Loss: 0.00011021799582522362, Learning Rate: 0.001006\n",
      "Epoch 5724/40000, Loss: 0.0004382002225611359, Learning Rate: 0.001006\n",
      "Epoch 5725/40000, Loss: 0.0002921238192357123, Learning Rate: 0.001006\n",
      "Epoch 5726/40000, Loss: 0.0006940098246559501, Learning Rate: 0.001006\n",
      "Epoch 5727/40000, Loss: 0.000307668000459671, Learning Rate: 0.001006\n",
      "Epoch 5728/40000, Loss: 0.0002370005677221343, Learning Rate: 0.001006\n",
      "Epoch 5729/40000, Loss: 9.778443200048059e-05, Learning Rate: 0.001006\n",
      "Epoch 5730/40000, Loss: 8.731260459171608e-05, Learning Rate: 0.001006\n",
      "Epoch 5731/40000, Loss: 0.00010860742622753605, Learning Rate: 0.001005\n",
      "Epoch 5732/40000, Loss: 5.845093255629763e-05, Learning Rate: 0.001005\n",
      "Epoch 5733/40000, Loss: 0.00010810228559421375, Learning Rate: 0.001005\n",
      "Epoch 5734/40000, Loss: 7.157189975259826e-05, Learning Rate: 0.001005\n",
      "Epoch 5735/40000, Loss: 0.0001150922616943717, Learning Rate: 0.001005\n",
      "Epoch 5736/40000, Loss: 0.0001510869333287701, Learning Rate: 0.001005\n",
      "Epoch 5737/40000, Loss: 0.00012483559839893132, Learning Rate: 0.001005\n",
      "Epoch 5738/40000, Loss: 0.00012044821050949395, Learning Rate: 0.001005\n",
      "Epoch 5739/40000, Loss: 0.0001494887692388147, Learning Rate: 0.001004\n",
      "Epoch 5740/40000, Loss: 0.00011532995995366946, Learning Rate: 0.001004\n",
      "Epoch 5741/40000, Loss: 0.0001720003056107089, Learning Rate: 0.001004\n",
      "Epoch 5742/40000, Loss: 0.00013336037227418274, Learning Rate: 0.001004\n",
      "Epoch 5743/40000, Loss: 0.00011075402289861813, Learning Rate: 0.001004\n",
      "Epoch 5744/40000, Loss: 7.765452028252184e-05, Learning Rate: 0.001004\n",
      "Epoch 5745/40000, Loss: 5.163783498574048e-05, Learning Rate: 0.001004\n",
      "Epoch 5746/40000, Loss: 8.677727601025254e-05, Learning Rate: 0.001004\n",
      "Epoch 5747/40000, Loss: 8.651678217574954e-05, Learning Rate: 0.001004\n",
      "Epoch 5748/40000, Loss: 7.041308708721772e-05, Learning Rate: 0.001003\n",
      "Epoch 5749/40000, Loss: 4.2303297959733754e-05, Learning Rate: 0.001003\n",
      "Epoch 5750/40000, Loss: 0.00011846011329907924, Learning Rate: 0.001003\n",
      "Epoch 5751/40000, Loss: 4.92231993121095e-05, Learning Rate: 0.001003\n",
      "Epoch 5752/40000, Loss: 0.00011672294931486249, Learning Rate: 0.001003\n",
      "Epoch 5753/40000, Loss: 0.00011004426778526977, Learning Rate: 0.001003\n",
      "Epoch 5754/40000, Loss: 6.50524307275191e-05, Learning Rate: 0.001003\n",
      "Epoch 5755/40000, Loss: 8.698961755726486e-05, Learning Rate: 0.001003\n",
      "Epoch 5756/40000, Loss: 0.00011275618453510106, Learning Rate: 0.001002\n",
      "Epoch 5757/40000, Loss: 8.438753138761967e-05, Learning Rate: 0.001002\n",
      "Epoch 5758/40000, Loss: 0.0001307963248109445, Learning Rate: 0.001002\n",
      "Epoch 5759/40000, Loss: 6.661967927357182e-05, Learning Rate: 0.001002\n",
      "Epoch 5760/40000, Loss: 7.679539703531191e-05, Learning Rate: 0.001002\n",
      "Epoch 5761/40000, Loss: 9.794285142561421e-05, Learning Rate: 0.001002\n",
      "Epoch 5762/40000, Loss: 0.00011496966908453032, Learning Rate: 0.001002\n",
      "Epoch 5763/40000, Loss: 5.451146716950461e-05, Learning Rate: 0.001002\n",
      "Epoch 5764/40000, Loss: 6.753052730346099e-05, Learning Rate: 0.001001\n",
      "Epoch 5765/40000, Loss: 9.921091987052932e-05, Learning Rate: 0.001001\n",
      "Epoch 5766/40000, Loss: 9.824522567214444e-05, Learning Rate: 0.001001\n",
      "Epoch 5767/40000, Loss: 9.679692448116839e-05, Learning Rate: 0.001001\n",
      "Epoch 5768/40000, Loss: 0.00010253388609271497, Learning Rate: 0.001001\n",
      "Epoch 5769/40000, Loss: 0.00016031062114052474, Learning Rate: 0.001001\n",
      "Epoch 5770/40000, Loss: 4.7737699787830934e-05, Learning Rate: 0.001001\n",
      "Epoch 5771/40000, Loss: 7.566280692117289e-05, Learning Rate: 0.001001\n",
      "Epoch 5772/40000, Loss: 0.0001198617901536636, Learning Rate: 0.001001\n",
      "Epoch 5773/40000, Loss: 5.374190368456766e-05, Learning Rate: 0.001000\n",
      "Epoch 5774/40000, Loss: 0.00013016069715376943, Learning Rate: 0.001000\n",
      "Epoch 5775/40000, Loss: 0.00013819434389006346, Learning Rate: 0.001000\n",
      "Epoch 5776/40000, Loss: 9.351018525194377e-05, Learning Rate: 0.001000\n",
      "Epoch 5777/40000, Loss: 8.659520244691521e-05, Learning Rate: 0.001000\n",
      "Epoch 5778/40000, Loss: 0.00013355455303099006, Learning Rate: 0.001000\n",
      "Epoch 5779/40000, Loss: 0.00011941895354539156, Learning Rate: 0.001000\n",
      "Epoch 5780/40000, Loss: 7.005641236901283e-05, Learning Rate: 0.001000\n",
      "Epoch 5781/40000, Loss: 0.0001425080990884453, Learning Rate: 0.000999\n",
      "Epoch 5782/40000, Loss: 8.229369996115565e-05, Learning Rate: 0.000999\n",
      "Epoch 5783/40000, Loss: 3.969920726376586e-05, Learning Rate: 0.000999\n",
      "Epoch 5784/40000, Loss: 3.9949140045791864e-05, Learning Rate: 0.000999\n",
      "Epoch 5785/40000, Loss: 6.51286100037396e-05, Learning Rate: 0.000999\n",
      "Epoch 5786/40000, Loss: 7.706313772359863e-05, Learning Rate: 0.000999\n",
      "Epoch 5787/40000, Loss: 4.019029802293517e-05, Learning Rate: 0.000999\n",
      "Epoch 5788/40000, Loss: 3.808084147749469e-05, Learning Rate: 0.000999\n",
      "Epoch 5789/40000, Loss: 7.634736539330333e-05, Learning Rate: 0.000998\n",
      "Epoch 5790/40000, Loss: 3.8627036701655015e-05, Learning Rate: 0.000998\n",
      "Epoch 5791/40000, Loss: 3.792675488512032e-05, Learning Rate: 0.000998\n",
      "Epoch 5792/40000, Loss: 0.00010478735202923417, Learning Rate: 0.000998\n",
      "Epoch 5793/40000, Loss: 6.45798645564355e-05, Learning Rate: 0.000998\n",
      "Epoch 5794/40000, Loss: 8.15437306300737e-05, Learning Rate: 0.000998\n",
      "Epoch 5795/40000, Loss: 0.00010519067291170359, Learning Rate: 0.000998\n",
      "Epoch 5796/40000, Loss: 6.68876527925022e-05, Learning Rate: 0.000998\n",
      "Epoch 5797/40000, Loss: 8.432366303168237e-05, Learning Rate: 0.000998\n",
      "Epoch 5798/40000, Loss: 7.400301547022536e-05, Learning Rate: 0.000997\n",
      "Epoch 5799/40000, Loss: 0.00011179078865097836, Learning Rate: 0.000997\n",
      "Epoch 5800/40000, Loss: 6.377245153998956e-05, Learning Rate: 0.000997\n",
      "Epoch 5801/40000, Loss: 8.142647857312113e-05, Learning Rate: 0.000997\n",
      "Epoch 5802/40000, Loss: 0.0001068318888428621, Learning Rate: 0.000997\n",
      "Epoch 5803/40000, Loss: 0.00010479773482074961, Learning Rate: 0.000997\n",
      "Epoch 5804/40000, Loss: 8.428141882177442e-05, Learning Rate: 0.000997\n",
      "Epoch 5805/40000, Loss: 3.758054299396463e-05, Learning Rate: 0.000997\n",
      "Epoch 5806/40000, Loss: 6.429185305023566e-05, Learning Rate: 0.000996\n",
      "Epoch 5807/40000, Loss: 6.454016693169251e-05, Learning Rate: 0.000996\n",
      "Epoch 5808/40000, Loss: 0.00011773563164751977, Learning Rate: 0.000996\n",
      "Epoch 5809/40000, Loss: 8.560787682654336e-05, Learning Rate: 0.000996\n",
      "Epoch 5810/40000, Loss: 9.290293382946402e-05, Learning Rate: 0.000996\n",
      "Epoch 5811/40000, Loss: 6.824652518844232e-05, Learning Rate: 0.000996\n",
      "Epoch 5812/40000, Loss: 8.732479909667745e-05, Learning Rate: 0.000996\n",
      "Epoch 5813/40000, Loss: 5.5553126003360376e-05, Learning Rate: 0.000996\n",
      "Epoch 5814/40000, Loss: 0.00012311131285969168, Learning Rate: 0.000995\n",
      "Epoch 5815/40000, Loss: 7.023698708508164e-05, Learning Rate: 0.000995\n",
      "Epoch 5816/40000, Loss: 0.000166268102475442, Learning Rate: 0.000995\n",
      "Epoch 5817/40000, Loss: 0.00010776023555081338, Learning Rate: 0.000995\n",
      "Epoch 5818/40000, Loss: 0.0001617589296074584, Learning Rate: 0.000995\n",
      "Epoch 5819/40000, Loss: 0.00016994161705952138, Learning Rate: 0.000995\n",
      "Epoch 5820/40000, Loss: 0.00019065308151766658, Learning Rate: 0.000995\n",
      "Epoch 5821/40000, Loss: 0.00019911288109142333, Learning Rate: 0.000995\n",
      "Epoch 5822/40000, Loss: 0.0001716284896247089, Learning Rate: 0.000995\n",
      "Epoch 5823/40000, Loss: 0.00014933708007447422, Learning Rate: 0.000994\n",
      "Epoch 5824/40000, Loss: 0.000153298897203058, Learning Rate: 0.000994\n",
      "Epoch 5825/40000, Loss: 0.00010703914449550211, Learning Rate: 0.000994\n",
      "Epoch 5826/40000, Loss: 0.0001434117730241269, Learning Rate: 0.000994\n",
      "Epoch 5827/40000, Loss: 9.118238813243806e-05, Learning Rate: 0.000994\n",
      "Epoch 5828/40000, Loss: 0.00011070494656451046, Learning Rate: 0.000994\n",
      "Epoch 5829/40000, Loss: 0.00010242329881293699, Learning Rate: 0.000994\n",
      "Epoch 5830/40000, Loss: 0.0001373816339764744, Learning Rate: 0.000994\n",
      "Epoch 5831/40000, Loss: 8.585922478232533e-05, Learning Rate: 0.000993\n",
      "Epoch 5832/40000, Loss: 6.638812192250043e-05, Learning Rate: 0.000993\n",
      "Epoch 5833/40000, Loss: 9.906594641506672e-05, Learning Rate: 0.000993\n",
      "Epoch 5834/40000, Loss: 0.00010346424824092537, Learning Rate: 0.000993\n",
      "Epoch 5835/40000, Loss: 0.00013655226211994886, Learning Rate: 0.000993\n",
      "Epoch 5836/40000, Loss: 7.171870674937963e-05, Learning Rate: 0.000993\n",
      "Epoch 5837/40000, Loss: 5.761123975389637e-05, Learning Rate: 0.000993\n",
      "Epoch 5838/40000, Loss: 8.760996570345014e-05, Learning Rate: 0.000993\n",
      "Epoch 5839/40000, Loss: 8.312838326673955e-05, Learning Rate: 0.000992\n",
      "Epoch 5840/40000, Loss: 6.905994814587757e-05, Learning Rate: 0.000992\n",
      "Epoch 5841/40000, Loss: 8.706002699909732e-05, Learning Rate: 0.000992\n",
      "Epoch 5842/40000, Loss: 0.0001228572946274653, Learning Rate: 0.000992\n",
      "Epoch 5843/40000, Loss: 0.0001336638379143551, Learning Rate: 0.000992\n",
      "Epoch 5844/40000, Loss: 0.0001294988178415224, Learning Rate: 0.000992\n",
      "Epoch 5845/40000, Loss: 7.83633440732956e-05, Learning Rate: 0.000992\n",
      "Epoch 5846/40000, Loss: 4.2567473428789526e-05, Learning Rate: 0.000992\n",
      "Epoch 5847/40000, Loss: 0.0001391070254612714, Learning Rate: 0.000992\n",
      "Epoch 5848/40000, Loss: 0.0001098628417821601, Learning Rate: 0.000991\n",
      "Epoch 5849/40000, Loss: 8.292025449918583e-05, Learning Rate: 0.000991\n",
      "Epoch 5850/40000, Loss: 7.566150452475995e-05, Learning Rate: 0.000991\n",
      "Epoch 5851/40000, Loss: 0.00011085050937253982, Learning Rate: 0.000991\n",
      "Epoch 5852/40000, Loss: 8.178420102922246e-05, Learning Rate: 0.000991\n",
      "Epoch 5853/40000, Loss: 6.929186929482967e-05, Learning Rate: 0.000991\n",
      "Epoch 5854/40000, Loss: 0.00011873025505337864, Learning Rate: 0.000991\n",
      "Epoch 5855/40000, Loss: 0.00011959820403717458, Learning Rate: 0.000991\n",
      "Epoch 5856/40000, Loss: 0.00012168566172476858, Learning Rate: 0.000990\n",
      "Epoch 5857/40000, Loss: 7.718135020695627e-05, Learning Rate: 0.000990\n",
      "Epoch 5858/40000, Loss: 0.0001337335561402142, Learning Rate: 0.000990\n",
      "Epoch 5859/40000, Loss: 0.00011639388685580343, Learning Rate: 0.000990\n",
      "Epoch 5860/40000, Loss: 8.393055031774566e-05, Learning Rate: 0.000990\n",
      "Epoch 5861/40000, Loss: 6.48696004645899e-05, Learning Rate: 0.000990\n",
      "Epoch 5862/40000, Loss: 0.00011991456995019689, Learning Rate: 0.000990\n",
      "Epoch 5863/40000, Loss: 8.855392661644146e-05, Learning Rate: 0.000990\n",
      "Epoch 5864/40000, Loss: 8.309512486448511e-05, Learning Rate: 0.000990\n",
      "Epoch 5865/40000, Loss: 0.00014086153532844037, Learning Rate: 0.000989\n",
      "Epoch 5866/40000, Loss: 8.43611269374378e-05, Learning Rate: 0.000989\n",
      "Epoch 5867/40000, Loss: 9.480487642576918e-05, Learning Rate: 0.000989\n",
      "Epoch 5868/40000, Loss: 0.00016536208568140864, Learning Rate: 0.000989\n",
      "Epoch 5869/40000, Loss: 8.132113725878298e-05, Learning Rate: 0.000989\n",
      "Epoch 5870/40000, Loss: 0.0002697439631447196, Learning Rate: 0.000989\n",
      "Epoch 5871/40000, Loss: 0.00019781786249950528, Learning Rate: 0.000989\n",
      "Epoch 5872/40000, Loss: 0.00047408955288119614, Learning Rate: 0.000989\n",
      "Epoch 5873/40000, Loss: 0.00010804578778333962, Learning Rate: 0.000988\n",
      "Epoch 5874/40000, Loss: 0.00010236389061901718, Learning Rate: 0.000988\n",
      "Epoch 5875/40000, Loss: 0.0006457279669120908, Learning Rate: 0.000988\n",
      "Epoch 5876/40000, Loss: 7.836177974240854e-05, Learning Rate: 0.000988\n",
      "Epoch 5877/40000, Loss: 0.00013393703557085246, Learning Rate: 0.000988\n",
      "Epoch 5878/40000, Loss: 0.00016603298718109727, Learning Rate: 0.000988\n",
      "Epoch 5879/40000, Loss: 0.00010709180060075596, Learning Rate: 0.000988\n",
      "Epoch 5880/40000, Loss: 0.00018507553613744676, Learning Rate: 0.000988\n",
      "Epoch 5881/40000, Loss: 0.0001041082214214839, Learning Rate: 0.000987\n",
      "Epoch 5882/40000, Loss: 9.84796424745582e-05, Learning Rate: 0.000987\n",
      "Epoch 5883/40000, Loss: 7.746076880721375e-05, Learning Rate: 0.000987\n",
      "Epoch 5884/40000, Loss: 0.00010524719982640818, Learning Rate: 0.000987\n",
      "Epoch 5885/40000, Loss: 9.516820864519104e-05, Learning Rate: 0.000987\n",
      "Epoch 5886/40000, Loss: 0.00010555433254921809, Learning Rate: 0.000987\n",
      "Epoch 5887/40000, Loss: 0.0001449671108275652, Learning Rate: 0.000987\n",
      "Epoch 5888/40000, Loss: 0.00012474368850234896, Learning Rate: 0.000987\n",
      "Epoch 5889/40000, Loss: 9.332646732218564e-05, Learning Rate: 0.000987\n",
      "Epoch 5890/40000, Loss: 6.313443736871704e-05, Learning Rate: 0.000986\n",
      "Epoch 5891/40000, Loss: 0.00010004776413552463, Learning Rate: 0.000986\n",
      "Epoch 5892/40000, Loss: 0.00015499441360589117, Learning Rate: 0.000986\n",
      "Epoch 5893/40000, Loss: 0.00011187193013029173, Learning Rate: 0.000986\n",
      "Epoch 5894/40000, Loss: 0.00011802332301158458, Learning Rate: 0.000986\n",
      "Epoch 5895/40000, Loss: 0.00013552072050515562, Learning Rate: 0.000986\n",
      "Epoch 5896/40000, Loss: 0.00012934849655721337, Learning Rate: 0.000986\n",
      "Epoch 5897/40000, Loss: 5.875058923265897e-05, Learning Rate: 0.000986\n",
      "Epoch 5898/40000, Loss: 7.321505108848214e-05, Learning Rate: 0.000985\n",
      "Epoch 5899/40000, Loss: 8.397195779252797e-05, Learning Rate: 0.000985\n",
      "Epoch 5900/40000, Loss: 8.108212932711467e-05, Learning Rate: 0.000985\n",
      "Epoch 5901/40000, Loss: 8.041641558520496e-05, Learning Rate: 0.000985\n",
      "Epoch 5902/40000, Loss: 0.00012653785233851522, Learning Rate: 0.000985\n",
      "Epoch 5903/40000, Loss: 0.00011930463369935751, Learning Rate: 0.000985\n",
      "Epoch 5904/40000, Loss: 8.253315900219604e-05, Learning Rate: 0.000985\n",
      "Epoch 5905/40000, Loss: 6.511483661597595e-05, Learning Rate: 0.000985\n",
      "Epoch 5906/40000, Loss: 9.575851436238736e-05, Learning Rate: 0.000985\n",
      "Epoch 5907/40000, Loss: 8.51136283017695e-05, Learning Rate: 0.000984\n",
      "Epoch 5908/40000, Loss: 6.536415457958356e-05, Learning Rate: 0.000984\n",
      "Epoch 5909/40000, Loss: 4.6641860535601154e-05, Learning Rate: 0.000984\n",
      "Epoch 5910/40000, Loss: 7.006149826338515e-05, Learning Rate: 0.000984\n",
      "Epoch 5911/40000, Loss: 5.547590262722224e-05, Learning Rate: 0.000984\n",
      "Epoch 5912/40000, Loss: 0.0001320654700975865, Learning Rate: 0.000984\n",
      "Epoch 5913/40000, Loss: 9.997738379752263e-05, Learning Rate: 0.000984\n",
      "Epoch 5914/40000, Loss: 0.00015320072998292744, Learning Rate: 0.000984\n",
      "Epoch 5915/40000, Loss: 0.00014891546743456274, Learning Rate: 0.000983\n",
      "Epoch 5916/40000, Loss: 0.00017271153046749532, Learning Rate: 0.000983\n",
      "Epoch 5917/40000, Loss: 0.00010783542529679835, Learning Rate: 0.000983\n",
      "Epoch 5918/40000, Loss: 9.459300781600177e-05, Learning Rate: 0.000983\n",
      "Epoch 5919/40000, Loss: 0.00010265589662594721, Learning Rate: 0.000983\n",
      "Epoch 5920/40000, Loss: 5.9547939599724486e-05, Learning Rate: 0.000983\n",
      "Epoch 5921/40000, Loss: 0.00013946014223620296, Learning Rate: 0.000983\n",
      "Epoch 5922/40000, Loss: 7.924644887680188e-05, Learning Rate: 0.000983\n",
      "Epoch 5923/40000, Loss: 0.0001320359733654186, Learning Rate: 0.000983\n",
      "Epoch 5924/40000, Loss: 0.00013535530888475478, Learning Rate: 0.000982\n",
      "Epoch 5925/40000, Loss: 8.032269397517666e-05, Learning Rate: 0.000982\n",
      "Epoch 5926/40000, Loss: 5.053266431787051e-05, Learning Rate: 0.000982\n",
      "Epoch 5927/40000, Loss: 0.00012793036876246333, Learning Rate: 0.000982\n",
      "Epoch 5928/40000, Loss: 0.0001426282397005707, Learning Rate: 0.000982\n",
      "Epoch 5929/40000, Loss: 0.00010168089647777379, Learning Rate: 0.000982\n",
      "Epoch 5930/40000, Loss: 5.2613420848501846e-05, Learning Rate: 0.000982\n",
      "Epoch 5931/40000, Loss: 0.00012566219083964825, Learning Rate: 0.000982\n",
      "Epoch 5932/40000, Loss: 7.36262954887934e-05, Learning Rate: 0.000981\n",
      "Epoch 5933/40000, Loss: 4.9056146963266656e-05, Learning Rate: 0.000981\n",
      "Epoch 5934/40000, Loss: 9.137342567555606e-05, Learning Rate: 0.000981\n",
      "Epoch 5935/40000, Loss: 7.000036566751078e-05, Learning Rate: 0.000981\n",
      "Epoch 5936/40000, Loss: 9.345109720015898e-05, Learning Rate: 0.000981\n",
      "Epoch 5937/40000, Loss: 0.00011131087376270443, Learning Rate: 0.000981\n",
      "Epoch 5938/40000, Loss: 7.26221696822904e-05, Learning Rate: 0.000981\n",
      "Epoch 5939/40000, Loss: 9.366116137243807e-05, Learning Rate: 0.000981\n",
      "Epoch 5940/40000, Loss: 5.546252941712737e-05, Learning Rate: 0.000981\n",
      "Epoch 5941/40000, Loss: 0.0001252357178600505, Learning Rate: 0.000980\n",
      "Epoch 5942/40000, Loss: 5.52840247109998e-05, Learning Rate: 0.000980\n",
      "Epoch 5943/40000, Loss: 4.1455372411292046e-05, Learning Rate: 0.000980\n",
      "Epoch 5944/40000, Loss: 0.0001076969929272309, Learning Rate: 0.000980\n",
      "Epoch 5945/40000, Loss: 0.00010298527195118368, Learning Rate: 0.000980\n",
      "Epoch 5946/40000, Loss: 8.652023097965866e-05, Learning Rate: 0.000980\n",
      "Epoch 5947/40000, Loss: 0.00014018795627634972, Learning Rate: 0.000980\n",
      "Epoch 5948/40000, Loss: 9.183384827338159e-05, Learning Rate: 0.000980\n",
      "Epoch 5949/40000, Loss: 7.368536171270534e-05, Learning Rate: 0.000979\n",
      "Epoch 5950/40000, Loss: 0.00011291700502624735, Learning Rate: 0.000979\n",
      "Epoch 5951/40000, Loss: 8.325866656377912e-05, Learning Rate: 0.000979\n",
      "Epoch 5952/40000, Loss: 0.00014104580623097718, Learning Rate: 0.000979\n",
      "Epoch 5953/40000, Loss: 6.660257349722087e-05, Learning Rate: 0.000979\n",
      "Epoch 5954/40000, Loss: 8.81088781170547e-05, Learning Rate: 0.000979\n",
      "Epoch 5955/40000, Loss: 9.16609205887653e-05, Learning Rate: 0.000979\n",
      "Epoch 5956/40000, Loss: 0.00011086377344327047, Learning Rate: 0.000979\n",
      "Epoch 5957/40000, Loss: 0.00012496467388700694, Learning Rate: 0.000979\n",
      "Epoch 5958/40000, Loss: 0.00012224441161379218, Learning Rate: 0.000978\n",
      "Epoch 5959/40000, Loss: 0.00012478667485993356, Learning Rate: 0.000978\n",
      "Epoch 5960/40000, Loss: 0.00011805483518401161, Learning Rate: 0.000978\n",
      "Epoch 5961/40000, Loss: 0.00012489013897720724, Learning Rate: 0.000978\n",
      "Epoch 5962/40000, Loss: 9.056609269464388e-05, Learning Rate: 0.000978\n",
      "Epoch 5963/40000, Loss: 9.757171210367233e-05, Learning Rate: 0.000978\n",
      "Epoch 5964/40000, Loss: 0.00013705191668123007, Learning Rate: 0.000978\n",
      "Epoch 5965/40000, Loss: 0.00013454954023472965, Learning Rate: 0.000978\n",
      "Epoch 5966/40000, Loss: 0.00011942065611947328, Learning Rate: 0.000977\n",
      "Epoch 5967/40000, Loss: 7.096580520737916e-05, Learning Rate: 0.000977\n",
      "Epoch 5968/40000, Loss: 4.713147063739598e-05, Learning Rate: 0.000977\n",
      "Epoch 5969/40000, Loss: 5.5339947721222416e-05, Learning Rate: 0.000977\n",
      "Epoch 5970/40000, Loss: 0.00012879482528660446, Learning Rate: 0.000977\n",
      "Epoch 5971/40000, Loss: 7.503858068957925e-05, Learning Rate: 0.000977\n",
      "Epoch 5972/40000, Loss: 0.00013393656990956515, Learning Rate: 0.000977\n",
      "Epoch 5973/40000, Loss: 0.00010451334674144164, Learning Rate: 0.000977\n",
      "Epoch 5974/40000, Loss: 0.0001382833142997697, Learning Rate: 0.000977\n",
      "Epoch 5975/40000, Loss: 9.035482071340084e-05, Learning Rate: 0.000976\n",
      "Epoch 5976/40000, Loss: 0.00010964422835968435, Learning Rate: 0.000976\n",
      "Epoch 5977/40000, Loss: 6.44401807221584e-05, Learning Rate: 0.000976\n",
      "Epoch 5978/40000, Loss: 8.810366853140295e-05, Learning Rate: 0.000976\n",
      "Epoch 5979/40000, Loss: 6.78869109833613e-05, Learning Rate: 0.000976\n",
      "Epoch 5980/40000, Loss: 0.00015127210645005107, Learning Rate: 0.000976\n",
      "Epoch 5981/40000, Loss: 6.028474308550358e-05, Learning Rate: 0.000976\n",
      "Epoch 5982/40000, Loss: 0.00018287167767994106, Learning Rate: 0.000976\n",
      "Epoch 5983/40000, Loss: 0.00013497218606062233, Learning Rate: 0.000975\n",
      "Epoch 5984/40000, Loss: 0.00016327339108102024, Learning Rate: 0.000975\n",
      "Epoch 5985/40000, Loss: 9.470977238379419e-05, Learning Rate: 0.000975\n",
      "Epoch 5986/40000, Loss: 5.253043127595447e-05, Learning Rate: 0.000975\n",
      "Epoch 5987/40000, Loss: 0.0001499295758549124, Learning Rate: 0.000975\n",
      "Epoch 5988/40000, Loss: 7.977314089657739e-05, Learning Rate: 0.000975\n",
      "Epoch 5989/40000, Loss: 9.138966561295092e-05, Learning Rate: 0.000975\n",
      "Epoch 5990/40000, Loss: 6.731974281137809e-05, Learning Rate: 0.000975\n",
      "Epoch 5991/40000, Loss: 9.414878877578303e-05, Learning Rate: 0.000975\n",
      "Epoch 5992/40000, Loss: 0.00015121819160412997, Learning Rate: 0.000974\n",
      "Epoch 5993/40000, Loss: 0.00012966465146746486, Learning Rate: 0.000974\n",
      "Epoch 5994/40000, Loss: 8.811782754492015e-05, Learning Rate: 0.000974\n",
      "Epoch 5995/40000, Loss: 0.00012370699550956488, Learning Rate: 0.000974\n",
      "Epoch 5996/40000, Loss: 8.522855932824314e-05, Learning Rate: 0.000974\n",
      "Epoch 5997/40000, Loss: 8.324825466843322e-05, Learning Rate: 0.000974\n",
      "Epoch 5998/40000, Loss: 8.505479490850121e-05, Learning Rate: 0.000974\n",
      "Epoch 5999/40000, Loss: 7.54327920731157e-05, Learning Rate: 0.000974\n",
      "Epoch 6000/40000, Loss: 0.0001286127808270976, Learning Rate: 0.000973\n",
      "Epoch 6001/40000, Loss: 6.387018220266327e-05, Learning Rate: 0.000973\n",
      "Epoch 6002/40000, Loss: 3.864835889544338e-05, Learning Rate: 0.000973\n",
      "Epoch 6003/40000, Loss: 6.568292155861855e-05, Learning Rate: 0.000973\n",
      "Epoch 6004/40000, Loss: 3.941417526220903e-05, Learning Rate: 0.000973\n",
      "Epoch 6005/40000, Loss: 0.00010469066182849929, Learning Rate: 0.000973\n",
      "Epoch 6006/40000, Loss: 6.23968371655792e-05, Learning Rate: 0.000973\n",
      "Epoch 6007/40000, Loss: 0.00010411691619083285, Learning Rate: 0.000973\n",
      "Epoch 6008/40000, Loss: 3.814182855421677e-05, Learning Rate: 0.000973\n",
      "Epoch 6009/40000, Loss: 7.955006731208414e-05, Learning Rate: 0.000972\n",
      "Epoch 6010/40000, Loss: 6.401947030099109e-05, Learning Rate: 0.000972\n",
      "Epoch 6011/40000, Loss: 0.00012788933236151934, Learning Rate: 0.000972\n",
      "Epoch 6012/40000, Loss: 0.00011709742830134928, Learning Rate: 0.000972\n",
      "Epoch 6013/40000, Loss: 8.631152013549581e-05, Learning Rate: 0.000972\n",
      "Epoch 6014/40000, Loss: 5.50292716070544e-05, Learning Rate: 0.000972\n",
      "Epoch 6015/40000, Loss: 0.00015239312779158354, Learning Rate: 0.000972\n",
      "Epoch 6016/40000, Loss: 0.00012614209845196456, Learning Rate: 0.000972\n",
      "Epoch 6017/40000, Loss: 9.467101335758343e-05, Learning Rate: 0.000972\n",
      "Epoch 6018/40000, Loss: 0.00010673684300854802, Learning Rate: 0.000971\n",
      "Epoch 6019/40000, Loss: 9.308767766924575e-05, Learning Rate: 0.000971\n",
      "Epoch 6020/40000, Loss: 7.344994810409844e-05, Learning Rate: 0.000971\n",
      "Epoch 6021/40000, Loss: 5.2702198445331305e-05, Learning Rate: 0.000971\n",
      "Epoch 6022/40000, Loss: 0.00011276199802523479, Learning Rate: 0.000971\n",
      "Epoch 6023/40000, Loss: 8.840576629154384e-05, Learning Rate: 0.000971\n",
      "Epoch 6024/40000, Loss: 9.297948417952284e-05, Learning Rate: 0.000971\n",
      "Epoch 6025/40000, Loss: 0.00011082124547101557, Learning Rate: 0.000971\n",
      "Epoch 6026/40000, Loss: 0.00011179675493622199, Learning Rate: 0.000970\n",
      "Epoch 6027/40000, Loss: 7.730393554084003e-05, Learning Rate: 0.000970\n",
      "Epoch 6028/40000, Loss: 0.00015994261775631458, Learning Rate: 0.000970\n",
      "Epoch 6029/40000, Loss: 0.00011582396109588444, Learning Rate: 0.000970\n",
      "Epoch 6030/40000, Loss: 0.00011725311196641997, Learning Rate: 0.000970\n",
      "Epoch 6031/40000, Loss: 9.213860175805166e-05, Learning Rate: 0.000970\n",
      "Epoch 6032/40000, Loss: 0.00017433868197258562, Learning Rate: 0.000970\n",
      "Epoch 6033/40000, Loss: 0.00012336975487414747, Learning Rate: 0.000970\n",
      "Epoch 6034/40000, Loss: 0.00010391937394160777, Learning Rate: 0.000970\n",
      "Epoch 6035/40000, Loss: 7.802377513144165e-05, Learning Rate: 0.000969\n",
      "Epoch 6036/40000, Loss: 6.231633597053587e-05, Learning Rate: 0.000969\n",
      "Epoch 6037/40000, Loss: 8.537289977539331e-05, Learning Rate: 0.000969\n",
      "Epoch 6038/40000, Loss: 0.00013694378139916807, Learning Rate: 0.000969\n",
      "Epoch 6039/40000, Loss: 9.723941184347495e-05, Learning Rate: 0.000969\n",
      "Epoch 6040/40000, Loss: 0.00012075340055162087, Learning Rate: 0.000969\n",
      "Epoch 6041/40000, Loss: 0.0001340293965768069, Learning Rate: 0.000969\n",
      "Epoch 6042/40000, Loss: 0.00012446989421732724, Learning Rate: 0.000969\n",
      "Epoch 6043/40000, Loss: 8.29531709314324e-05, Learning Rate: 0.000968\n",
      "Epoch 6044/40000, Loss: 9.97471870505251e-05, Learning Rate: 0.000968\n",
      "Epoch 6045/40000, Loss: 6.904028123244643e-05, Learning Rate: 0.000968\n",
      "Epoch 6046/40000, Loss: 0.000131293767481111, Learning Rate: 0.000968\n",
      "Epoch 6047/40000, Loss: 9.990806574933231e-05, Learning Rate: 0.000968\n",
      "Epoch 6048/40000, Loss: 9.464874892728403e-05, Learning Rate: 0.000968\n",
      "Epoch 6049/40000, Loss: 0.00012293626787140965, Learning Rate: 0.000968\n",
      "Epoch 6050/40000, Loss: 0.00012329869787208736, Learning Rate: 0.000968\n",
      "Epoch 6051/40000, Loss: 0.00012427130423020571, Learning Rate: 0.000968\n",
      "Epoch 6052/40000, Loss: 7.127883145585656e-05, Learning Rate: 0.000967\n",
      "Epoch 6053/40000, Loss: 7.598725642310455e-05, Learning Rate: 0.000967\n",
      "Epoch 6054/40000, Loss: 8.998668636195362e-05, Learning Rate: 0.000967\n",
      "Epoch 6055/40000, Loss: 0.0001277195115108043, Learning Rate: 0.000967\n",
      "Epoch 6056/40000, Loss: 5.372739542508498e-05, Learning Rate: 0.000967\n",
      "Epoch 6057/40000, Loss: 0.00010321622539777309, Learning Rate: 0.000967\n",
      "Epoch 6058/40000, Loss: 0.00016473517462145537, Learning Rate: 0.000967\n",
      "Epoch 6059/40000, Loss: 0.00010304486932000145, Learning Rate: 0.000967\n",
      "Epoch 6060/40000, Loss: 0.00014102764544077218, Learning Rate: 0.000967\n",
      "Epoch 6061/40000, Loss: 0.00012163618521299213, Learning Rate: 0.000966\n",
      "Epoch 6062/40000, Loss: 0.00016461123595945537, Learning Rate: 0.000966\n",
      "Epoch 6063/40000, Loss: 0.00013474487059284002, Learning Rate: 0.000966\n",
      "Epoch 6064/40000, Loss: 9.170621342491359e-05, Learning Rate: 0.000966\n",
      "Epoch 6065/40000, Loss: 8.542709110770375e-05, Learning Rate: 0.000966\n",
      "Epoch 6066/40000, Loss: 7.794909470248967e-05, Learning Rate: 0.000966\n",
      "Epoch 6067/40000, Loss: 0.00013771728845313191, Learning Rate: 0.000966\n",
      "Epoch 6068/40000, Loss: 8.953754877438769e-05, Learning Rate: 0.000966\n",
      "Epoch 6069/40000, Loss: 0.0001352790422970429, Learning Rate: 0.000965\n",
      "Epoch 6070/40000, Loss: 5.600557415164076e-05, Learning Rate: 0.000965\n",
      "Epoch 6071/40000, Loss: 8.235905988840386e-05, Learning Rate: 0.000965\n",
      "Epoch 6072/40000, Loss: 0.00013348831271287054, Learning Rate: 0.000965\n",
      "Epoch 6073/40000, Loss: 8.772381988819689e-05, Learning Rate: 0.000965\n",
      "Epoch 6074/40000, Loss: 4.732986053568311e-05, Learning Rate: 0.000965\n",
      "Epoch 6075/40000, Loss: 8.544816955691203e-05, Learning Rate: 0.000965\n",
      "Epoch 6076/40000, Loss: 6.95645430823788e-05, Learning Rate: 0.000965\n",
      "Epoch 6077/40000, Loss: 8.454789349343628e-05, Learning Rate: 0.000965\n",
      "Epoch 6078/40000, Loss: 3.822711732937023e-05, Learning Rate: 0.000964\n",
      "Epoch 6079/40000, Loss: 7.928360719233751e-05, Learning Rate: 0.000964\n",
      "Epoch 6080/40000, Loss: 7.625191938132048e-05, Learning Rate: 0.000964\n",
      "Epoch 6081/40000, Loss: 0.0001168289891211316, Learning Rate: 0.000964\n",
      "Epoch 6082/40000, Loss: 0.00011576242832234129, Learning Rate: 0.000964\n",
      "Epoch 6083/40000, Loss: 0.00011060419637942687, Learning Rate: 0.000964\n",
      "Epoch 6084/40000, Loss: 0.00011151599028380588, Learning Rate: 0.000964\n",
      "Epoch 6085/40000, Loss: 8.083976717898622e-05, Learning Rate: 0.000964\n",
      "Epoch 6086/40000, Loss: 8.349093695869669e-05, Learning Rate: 0.000964\n",
      "Epoch 6087/40000, Loss: 0.00011056681250920519, Learning Rate: 0.000963\n",
      "Epoch 6088/40000, Loss: 0.00010489962005522102, Learning Rate: 0.000963\n",
      "Epoch 6089/40000, Loss: 0.00012725732813123614, Learning Rate: 0.000963\n",
      "Epoch 6090/40000, Loss: 4.568394797388464e-05, Learning Rate: 0.000963\n",
      "Epoch 6091/40000, Loss: 6.747420411556959e-05, Learning Rate: 0.000963\n",
      "Epoch 6092/40000, Loss: 6.661970837740228e-05, Learning Rate: 0.000963\n",
      "Epoch 6093/40000, Loss: 8.76170743140392e-05, Learning Rate: 0.000963\n",
      "Epoch 6094/40000, Loss: 0.0001135224083554931, Learning Rate: 0.000963\n",
      "Epoch 6095/40000, Loss: 7.525301043642685e-05, Learning Rate: 0.000962\n",
      "Epoch 6096/40000, Loss: 0.00013481594214681536, Learning Rate: 0.000962\n",
      "Epoch 6097/40000, Loss: 9.122717892751098e-05, Learning Rate: 0.000962\n",
      "Epoch 6098/40000, Loss: 7.59234608267434e-05, Learning Rate: 0.000962\n",
      "Epoch 6099/40000, Loss: 0.0001377659064019099, Learning Rate: 0.000962\n",
      "Epoch 6100/40000, Loss: 0.0001237508113263175, Learning Rate: 0.000962\n",
      "Epoch 6101/40000, Loss: 0.00010286840551998466, Learning Rate: 0.000962\n",
      "Epoch 6102/40000, Loss: 6.561187183251604e-05, Learning Rate: 0.000962\n",
      "Epoch 6103/40000, Loss: 0.00010395102435722947, Learning Rate: 0.000962\n",
      "Epoch 6104/40000, Loss: 0.00013170905003789812, Learning Rate: 0.000961\n",
      "Epoch 6105/40000, Loss: 0.00011818924394901842, Learning Rate: 0.000961\n",
      "Epoch 6106/40000, Loss: 8.509797771694139e-05, Learning Rate: 0.000961\n",
      "Epoch 6107/40000, Loss: 6.838590343249962e-05, Learning Rate: 0.000961\n",
      "Epoch 6108/40000, Loss: 0.00012439329293556511, Learning Rate: 0.000961\n",
      "Epoch 6109/40000, Loss: 0.00011153430386912078, Learning Rate: 0.000961\n",
      "Epoch 6110/40000, Loss: 4.732948582386598e-05, Learning Rate: 0.000961\n",
      "Epoch 6111/40000, Loss: 8.326072565978393e-05, Learning Rate: 0.000961\n",
      "Epoch 6112/40000, Loss: 7.552893657702953e-05, Learning Rate: 0.000961\n",
      "Epoch 6113/40000, Loss: 0.00014661398017778993, Learning Rate: 0.000960\n",
      "Epoch 6114/40000, Loss: 0.00012995155702810735, Learning Rate: 0.000960\n",
      "Epoch 6115/40000, Loss: 0.00017697489238344133, Learning Rate: 0.000960\n",
      "Epoch 6116/40000, Loss: 0.00014800216013099998, Learning Rate: 0.000960\n",
      "Epoch 6117/40000, Loss: 0.00013052976282779127, Learning Rate: 0.000960\n",
      "Epoch 6118/40000, Loss: 5.611521191895008e-05, Learning Rate: 0.000960\n",
      "Epoch 6119/40000, Loss: 8.524908480467275e-05, Learning Rate: 0.000960\n",
      "Epoch 6120/40000, Loss: 7.861401536501944e-05, Learning Rate: 0.000960\n",
      "Epoch 6121/40000, Loss: 8.189432264771312e-05, Learning Rate: 0.000959\n",
      "Epoch 6122/40000, Loss: 6.403304723789915e-05, Learning Rate: 0.000959\n",
      "Epoch 6123/40000, Loss: 0.00010629773896653205, Learning Rate: 0.000959\n",
      "Epoch 6124/40000, Loss: 0.00010420092439744622, Learning Rate: 0.000959\n",
      "Epoch 6125/40000, Loss: 3.952936822315678e-05, Learning Rate: 0.000959\n",
      "Epoch 6126/40000, Loss: 3.703389302245341e-05, Learning Rate: 0.000959\n",
      "Epoch 6127/40000, Loss: 8.033563062781468e-05, Learning Rate: 0.000959\n",
      "Epoch 6128/40000, Loss: 0.0001135580605478026, Learning Rate: 0.000959\n",
      "Epoch 6129/40000, Loss: 7.291614747373387e-05, Learning Rate: 0.000959\n",
      "Epoch 6130/40000, Loss: 0.00010349243530072272, Learning Rate: 0.000958\n",
      "Epoch 6131/40000, Loss: 0.00010627507435856387, Learning Rate: 0.000958\n",
      "Epoch 6132/40000, Loss: 0.00011467172589618713, Learning Rate: 0.000958\n",
      "Epoch 6133/40000, Loss: 6.183300138218328e-05, Learning Rate: 0.000958\n",
      "Epoch 6134/40000, Loss: 6.164328078739345e-05, Learning Rate: 0.000958\n",
      "Epoch 6135/40000, Loss: 0.00011170629295520484, Learning Rate: 0.000958\n",
      "Epoch 6136/40000, Loss: 7.9550409282092e-05, Learning Rate: 0.000958\n",
      "Epoch 6137/40000, Loss: 3.916257992386818e-05, Learning Rate: 0.000958\n",
      "Epoch 6138/40000, Loss: 6.248873978620395e-05, Learning Rate: 0.000958\n",
      "Epoch 6139/40000, Loss: 7.313671085285023e-05, Learning Rate: 0.000957\n",
      "Epoch 6140/40000, Loss: 6.212221342138946e-05, Learning Rate: 0.000957\n",
      "Epoch 6141/40000, Loss: 6.927980575710535e-05, Learning Rate: 0.000957\n",
      "Epoch 6142/40000, Loss: 6.856552499812096e-05, Learning Rate: 0.000957\n",
      "Epoch 6143/40000, Loss: 0.00011111744970548898, Learning Rate: 0.000957\n",
      "Epoch 6144/40000, Loss: 7.083870150381699e-05, Learning Rate: 0.000957\n",
      "Epoch 6145/40000, Loss: 7.961178198456764e-05, Learning Rate: 0.000957\n",
      "Epoch 6146/40000, Loss: 6.265094270929694e-05, Learning Rate: 0.000957\n",
      "Epoch 6147/40000, Loss: 6.0966402088524774e-05, Learning Rate: 0.000956\n",
      "Epoch 6148/40000, Loss: 0.00011919542157556862, Learning Rate: 0.000956\n",
      "Epoch 6149/40000, Loss: 7.909550913609564e-05, Learning Rate: 0.000956\n",
      "Epoch 6150/40000, Loss: 3.852893132716417e-05, Learning Rate: 0.000956\n",
      "Epoch 6151/40000, Loss: 6.679735815851018e-05, Learning Rate: 0.000956\n",
      "Epoch 6152/40000, Loss: 8.752192661631852e-05, Learning Rate: 0.000956\n",
      "Epoch 6153/40000, Loss: 8.513704233337194e-05, Learning Rate: 0.000956\n",
      "Epoch 6154/40000, Loss: 0.00010380207095295191, Learning Rate: 0.000956\n",
      "Epoch 6155/40000, Loss: 0.00012598936154972762, Learning Rate: 0.000956\n",
      "Epoch 6156/40000, Loss: 0.00015270673611667007, Learning Rate: 0.000955\n",
      "Epoch 6157/40000, Loss: 0.00016120834334287792, Learning Rate: 0.000955\n",
      "Epoch 6158/40000, Loss: 7.74698200984858e-05, Learning Rate: 0.000955\n",
      "Epoch 6159/40000, Loss: 7.986697892192751e-05, Learning Rate: 0.000955\n",
      "Epoch 6160/40000, Loss: 0.0001924135140143335, Learning Rate: 0.000955\n",
      "Epoch 6161/40000, Loss: 0.0001679996494203806, Learning Rate: 0.000955\n",
      "Epoch 6162/40000, Loss: 0.00014541427663061768, Learning Rate: 0.000955\n",
      "Epoch 6163/40000, Loss: 0.00017520965775474906, Learning Rate: 0.000955\n",
      "Epoch 6164/40000, Loss: 0.00018846573948394507, Learning Rate: 0.000955\n",
      "Epoch 6165/40000, Loss: 7.69714533817023e-05, Learning Rate: 0.000954\n",
      "Epoch 6166/40000, Loss: 0.00012701135710813105, Learning Rate: 0.000954\n",
      "Epoch 6167/40000, Loss: 5.675637657986954e-05, Learning Rate: 0.000954\n",
      "Epoch 6168/40000, Loss: 9.41329199122265e-05, Learning Rate: 0.000954\n",
      "Epoch 6169/40000, Loss: 8.07271571829915e-05, Learning Rate: 0.000954\n",
      "Epoch 6170/40000, Loss: 8.426648855675012e-05, Learning Rate: 0.000954\n",
      "Epoch 6171/40000, Loss: 9.645104728406295e-05, Learning Rate: 0.000954\n",
      "Epoch 6172/40000, Loss: 4.165585050941445e-05, Learning Rate: 0.000954\n",
      "Epoch 6173/40000, Loss: 8.558333502151072e-05, Learning Rate: 0.000953\n",
      "Epoch 6174/40000, Loss: 4.792306208400987e-05, Learning Rate: 0.000953\n",
      "Epoch 6175/40000, Loss: 0.00010878444300033152, Learning Rate: 0.000953\n",
      "Epoch 6176/40000, Loss: 0.00011042754340451211, Learning Rate: 0.000953\n",
      "Epoch 6177/40000, Loss: 0.00011630910739768296, Learning Rate: 0.000953\n",
      "Epoch 6178/40000, Loss: 7.448461110470816e-05, Learning Rate: 0.000953\n",
      "Epoch 6179/40000, Loss: 4.212312342133373e-05, Learning Rate: 0.000953\n",
      "Epoch 6180/40000, Loss: 8.663242624606937e-05, Learning Rate: 0.000953\n",
      "Epoch 6181/40000, Loss: 8.524262375431135e-05, Learning Rate: 0.000953\n",
      "Epoch 6182/40000, Loss: 3.783716238103807e-05, Learning Rate: 0.000952\n",
      "Epoch 6183/40000, Loss: 8.285942021757364e-05, Learning Rate: 0.000952\n",
      "Epoch 6184/40000, Loss: 0.00012792021152563393, Learning Rate: 0.000952\n",
      "Epoch 6185/40000, Loss: 0.00010856195876840502, Learning Rate: 0.000952\n",
      "Epoch 6186/40000, Loss: 8.17560066934675e-05, Learning Rate: 0.000952\n",
      "Epoch 6187/40000, Loss: 0.00011831153824459761, Learning Rate: 0.000952\n",
      "Epoch 6188/40000, Loss: 0.0001411959674442187, Learning Rate: 0.000952\n",
      "Epoch 6189/40000, Loss: 9.216438775183633e-05, Learning Rate: 0.000952\n",
      "Epoch 6190/40000, Loss: 7.120113878045231e-05, Learning Rate: 0.000952\n",
      "Epoch 6191/40000, Loss: 0.00017830818251240999, Learning Rate: 0.000951\n",
      "Epoch 6192/40000, Loss: 0.00016354204853996634, Learning Rate: 0.000951\n",
      "Epoch 6193/40000, Loss: 8.723954670131207e-05, Learning Rate: 0.000951\n",
      "Epoch 6194/40000, Loss: 5.494150900631212e-05, Learning Rate: 0.000951\n",
      "Epoch 6195/40000, Loss: 8.427697321167216e-05, Learning Rate: 0.000951\n",
      "Epoch 6196/40000, Loss: 6.157617463031784e-05, Learning Rate: 0.000951\n",
      "Epoch 6197/40000, Loss: 0.0001432816352462396, Learning Rate: 0.000951\n",
      "Epoch 6198/40000, Loss: 8.798770431894809e-05, Learning Rate: 0.000951\n",
      "Epoch 6199/40000, Loss: 0.00017389054119121283, Learning Rate: 0.000951\n",
      "Epoch 6200/40000, Loss: 0.00012972383410669863, Learning Rate: 0.000950\n",
      "Epoch 6201/40000, Loss: 0.00013004991342313588, Learning Rate: 0.000950\n",
      "Epoch 6202/40000, Loss: 7.53109052311629e-05, Learning Rate: 0.000950\n",
      "Epoch 6203/40000, Loss: 0.00012460246216505766, Learning Rate: 0.000950\n",
      "Epoch 6204/40000, Loss: 9.299894009018317e-05, Learning Rate: 0.000950\n",
      "Epoch 6205/40000, Loss: 0.00011036853538826108, Learning Rate: 0.000950\n",
      "Epoch 6206/40000, Loss: 0.00012663585948757827, Learning Rate: 0.000950\n",
      "Epoch 6207/40000, Loss: 4.80129529023543e-05, Learning Rate: 0.000950\n",
      "Epoch 6208/40000, Loss: 7.22562544979155e-05, Learning Rate: 0.000949\n",
      "Epoch 6209/40000, Loss: 9.348522871732712e-05, Learning Rate: 0.000949\n",
      "Epoch 6210/40000, Loss: 0.0001365739735774696, Learning Rate: 0.000949\n",
      "Epoch 6211/40000, Loss: 8.404492837144062e-05, Learning Rate: 0.000949\n",
      "Epoch 6212/40000, Loss: 0.00013684631267096847, Learning Rate: 0.000949\n",
      "Epoch 6213/40000, Loss: 0.000107545405626297, Learning Rate: 0.000949\n",
      "Epoch 6214/40000, Loss: 8.884919225238264e-05, Learning Rate: 0.000949\n",
      "Epoch 6215/40000, Loss: 7.848186214687303e-05, Learning Rate: 0.000949\n",
      "Epoch 6216/40000, Loss: 7.033902511466295e-05, Learning Rate: 0.000949\n",
      "Epoch 6217/40000, Loss: 0.0001360906462650746, Learning Rate: 0.000948\n",
      "Epoch 6218/40000, Loss: 4.606247603078373e-05, Learning Rate: 0.000948\n",
      "Epoch 6219/40000, Loss: 0.00013605039566755295, Learning Rate: 0.000948\n",
      "Epoch 6220/40000, Loss: 8.395707118324935e-05, Learning Rate: 0.000948\n",
      "Epoch 6221/40000, Loss: 0.00011716255539795384, Learning Rate: 0.000948\n",
      "Epoch 6222/40000, Loss: 5.225415588938631e-05, Learning Rate: 0.000948\n",
      "Epoch 6223/40000, Loss: 0.0001358508743578568, Learning Rate: 0.000948\n",
      "Epoch 6224/40000, Loss: 6.399406993295997e-05, Learning Rate: 0.000948\n",
      "Epoch 6225/40000, Loss: 4.9180263886228204e-05, Learning Rate: 0.000948\n",
      "Epoch 6226/40000, Loss: 9.181519999401644e-05, Learning Rate: 0.000947\n",
      "Epoch 6227/40000, Loss: 8.600644650869071e-05, Learning Rate: 0.000947\n",
      "Epoch 6228/40000, Loss: 0.00012348582095000893, Learning Rate: 0.000947\n",
      "Epoch 6229/40000, Loss: 0.00010830868995981291, Learning Rate: 0.000947\n",
      "Epoch 6230/40000, Loss: 4.6593122533522546e-05, Learning Rate: 0.000947\n",
      "Epoch 6231/40000, Loss: 0.00012564464122988284, Learning Rate: 0.000947\n",
      "Epoch 6232/40000, Loss: 9.255229088012129e-05, Learning Rate: 0.000947\n",
      "Epoch 6233/40000, Loss: 0.00012235449685249478, Learning Rate: 0.000947\n",
      "Epoch 6234/40000, Loss: 6.009899152559228e-05, Learning Rate: 0.000947\n",
      "Epoch 6235/40000, Loss: 0.00010916712926700711, Learning Rate: 0.000946\n",
      "Epoch 6236/40000, Loss: 8.396789780817926e-05, Learning Rate: 0.000946\n",
      "Epoch 6237/40000, Loss: 0.00012116272409912199, Learning Rate: 0.000946\n",
      "Epoch 6238/40000, Loss: 7.395636930596083e-05, Learning Rate: 0.000946\n",
      "Epoch 6239/40000, Loss: 0.00011398472997825593, Learning Rate: 0.000946\n",
      "Epoch 6240/40000, Loss: 8.214300032705069e-05, Learning Rate: 0.000946\n",
      "Epoch 6241/40000, Loss: 0.00011678899318212643, Learning Rate: 0.000946\n",
      "Epoch 6242/40000, Loss: 5.229243106441572e-05, Learning Rate: 0.000946\n",
      "Epoch 6243/40000, Loss: 0.00010169587039854378, Learning Rate: 0.000946\n",
      "Epoch 6244/40000, Loss: 7.571202149847522e-05, Learning Rate: 0.000945\n",
      "Epoch 6245/40000, Loss: 0.00011842891399282962, Learning Rate: 0.000945\n",
      "Epoch 6246/40000, Loss: 0.00012950721429660916, Learning Rate: 0.000945\n",
      "Epoch 6247/40000, Loss: 8.435818745056167e-05, Learning Rate: 0.000945\n",
      "Epoch 6248/40000, Loss: 0.00011914258357137442, Learning Rate: 0.000945\n",
      "Epoch 6249/40000, Loss: 4.289013668312691e-05, Learning Rate: 0.000945\n",
      "Epoch 6250/40000, Loss: 9.203107765642926e-05, Learning Rate: 0.000945\n",
      "Epoch 6251/40000, Loss: 5.486293957801536e-05, Learning Rate: 0.000945\n",
      "Epoch 6252/40000, Loss: 0.00011627707135630772, Learning Rate: 0.000944\n",
      "Epoch 6253/40000, Loss: 0.00012060451263096184, Learning Rate: 0.000944\n",
      "Epoch 6254/40000, Loss: 0.00011389506835257635, Learning Rate: 0.000944\n",
      "Epoch 6255/40000, Loss: 8.505619189236313e-05, Learning Rate: 0.000944\n",
      "Epoch 6256/40000, Loss: 0.0001170308023574762, Learning Rate: 0.000944\n",
      "Epoch 6257/40000, Loss: 9.441158908884972e-05, Learning Rate: 0.000944\n",
      "Epoch 6258/40000, Loss: 8.861821697792038e-05, Learning Rate: 0.000944\n",
      "Epoch 6259/40000, Loss: 6.917979044374079e-05, Learning Rate: 0.000944\n",
      "Epoch 6260/40000, Loss: 6.54556424706243e-05, Learning Rate: 0.000944\n",
      "Epoch 6261/40000, Loss: 9.041372686624527e-05, Learning Rate: 0.000943\n",
      "Epoch 6262/40000, Loss: 7.453571015503258e-05, Learning Rate: 0.000943\n",
      "Epoch 6263/40000, Loss: 8.906769653549418e-05, Learning Rate: 0.000943\n",
      "Epoch 6264/40000, Loss: 0.00012851589417550713, Learning Rate: 0.000943\n",
      "Epoch 6265/40000, Loss: 0.00011348230327712372, Learning Rate: 0.000943\n",
      "Epoch 6266/40000, Loss: 0.00023550164769403636, Learning Rate: 0.000943\n",
      "Epoch 6267/40000, Loss: 6.628674600506201e-05, Learning Rate: 0.000943\n",
      "Epoch 6268/40000, Loss: 0.00011240845924476162, Learning Rate: 0.000943\n",
      "Epoch 6269/40000, Loss: 0.00010337121784687042, Learning Rate: 0.000943\n",
      "Epoch 6270/40000, Loss: 9.009364293888211e-05, Learning Rate: 0.000942\n",
      "Epoch 6271/40000, Loss: 0.00034862817847169936, Learning Rate: 0.000942\n",
      "Epoch 6272/40000, Loss: 0.0001407020608894527, Learning Rate: 0.000942\n",
      "Epoch 6273/40000, Loss: 0.0001169515453511849, Learning Rate: 0.000942\n",
      "Epoch 6274/40000, Loss: 0.00014652317622676492, Learning Rate: 0.000942\n",
      "Epoch 6275/40000, Loss: 0.0001513957977294922, Learning Rate: 0.000942\n",
      "Epoch 6276/40000, Loss: 9.580138430465013e-05, Learning Rate: 0.000942\n",
      "Epoch 6277/40000, Loss: 0.000234943741816096, Learning Rate: 0.000942\n",
      "Epoch 6278/40000, Loss: 6.915395351825282e-05, Learning Rate: 0.000942\n",
      "Epoch 6279/40000, Loss: 5.599779979093e-05, Learning Rate: 0.000941\n",
      "Epoch 6280/40000, Loss: 5.5743563279975206e-05, Learning Rate: 0.000941\n",
      "Epoch 6281/40000, Loss: 9.498673171037808e-05, Learning Rate: 0.000941\n",
      "Epoch 6282/40000, Loss: 9.003558079712093e-05, Learning Rate: 0.000941\n",
      "Epoch 6283/40000, Loss: 0.00010962116357404739, Learning Rate: 0.000941\n",
      "Epoch 6284/40000, Loss: 8.680238533997908e-05, Learning Rate: 0.000941\n",
      "Epoch 6285/40000, Loss: 0.00012807222083210945, Learning Rate: 0.000941\n",
      "Epoch 6286/40000, Loss: 0.00011739699402824044, Learning Rate: 0.000941\n",
      "Epoch 6287/40000, Loss: 8.270308171631768e-05, Learning Rate: 0.000941\n",
      "Epoch 6288/40000, Loss: 8.00212801550515e-05, Learning Rate: 0.000940\n",
      "Epoch 6289/40000, Loss: 6.37482589809224e-05, Learning Rate: 0.000940\n",
      "Epoch 6290/40000, Loss: 3.895973350154236e-05, Learning Rate: 0.000940\n",
      "Epoch 6291/40000, Loss: 0.00011514045763760805, Learning Rate: 0.000940\n",
      "Epoch 6292/40000, Loss: 3.667428973130882e-05, Learning Rate: 0.000940\n",
      "Epoch 6293/40000, Loss: 0.00011215147969778627, Learning Rate: 0.000940\n",
      "Epoch 6294/40000, Loss: 6.234853208297864e-05, Learning Rate: 0.000940\n",
      "Epoch 6295/40000, Loss: 8.549202175345272e-05, Learning Rate: 0.000940\n",
      "Epoch 6296/40000, Loss: 7.867371459724382e-05, Learning Rate: 0.000940\n",
      "Epoch 6297/40000, Loss: 7.322068995563313e-05, Learning Rate: 0.000939\n",
      "Epoch 6298/40000, Loss: 6.037120328983292e-05, Learning Rate: 0.000939\n",
      "Epoch 6299/40000, Loss: 0.00010446205851621926, Learning Rate: 0.000939\n",
      "Epoch 6300/40000, Loss: 0.00011083723802585155, Learning Rate: 0.000939\n",
      "Epoch 6301/40000, Loss: 3.519970050547272e-05, Learning Rate: 0.000939\n",
      "Epoch 6302/40000, Loss: 0.00011804124369518831, Learning Rate: 0.000939\n",
      "Epoch 6303/40000, Loss: 6.198857590788975e-05, Learning Rate: 0.000939\n",
      "Epoch 6304/40000, Loss: 0.00013990873412694782, Learning Rate: 0.000939\n",
      "Epoch 6305/40000, Loss: 6.995386502239853e-05, Learning Rate: 0.000939\n",
      "Epoch 6306/40000, Loss: 6.251050217542797e-05, Learning Rate: 0.000938\n",
      "Epoch 6307/40000, Loss: 0.00010303756425855681, Learning Rate: 0.000938\n",
      "Epoch 6308/40000, Loss: 0.00011061707482440397, Learning Rate: 0.000938\n",
      "Epoch 6309/40000, Loss: 0.00011015700874850154, Learning Rate: 0.000938\n",
      "Epoch 6310/40000, Loss: 0.00010061985813081264, Learning Rate: 0.000938\n",
      "Epoch 6311/40000, Loss: 0.00010312191443517804, Learning Rate: 0.000938\n",
      "Epoch 6312/40000, Loss: 0.0001010358682833612, Learning Rate: 0.000938\n",
      "Epoch 6313/40000, Loss: 7.874936272855848e-05, Learning Rate: 0.000938\n",
      "Epoch 6314/40000, Loss: 6.386589666362852e-05, Learning Rate: 0.000937\n",
      "Epoch 6315/40000, Loss: 6.360132829286158e-05, Learning Rate: 0.000937\n",
      "Epoch 6316/40000, Loss: 6.206322723301128e-05, Learning Rate: 0.000937\n",
      "Epoch 6317/40000, Loss: 7.738908607279882e-05, Learning Rate: 0.000937\n",
      "Epoch 6318/40000, Loss: 6.350962212309241e-05, Learning Rate: 0.000937\n",
      "Epoch 6319/40000, Loss: 0.0001133330661104992, Learning Rate: 0.000937\n",
      "Epoch 6320/40000, Loss: 3.792019924730994e-05, Learning Rate: 0.000937\n",
      "Epoch 6321/40000, Loss: 6.603918154723942e-05, Learning Rate: 0.000937\n",
      "Epoch 6322/40000, Loss: 4.1819072066573426e-05, Learning Rate: 0.000937\n",
      "Epoch 6323/40000, Loss: 0.00011027151049347594, Learning Rate: 0.000936\n",
      "Epoch 6324/40000, Loss: 5.542490544030443e-05, Learning Rate: 0.000936\n",
      "Epoch 6325/40000, Loss: 3.963540439144708e-05, Learning Rate: 0.000936\n",
      "Epoch 6326/40000, Loss: 0.00011903949780389667, Learning Rate: 0.000936\n",
      "Epoch 6327/40000, Loss: 0.0001109420700231567, Learning Rate: 0.000936\n",
      "Epoch 6328/40000, Loss: 0.00012272712774574757, Learning Rate: 0.000936\n",
      "Epoch 6329/40000, Loss: 4.625212022801861e-05, Learning Rate: 0.000936\n",
      "Epoch 6330/40000, Loss: 5.972777580609545e-05, Learning Rate: 0.000936\n",
      "Epoch 6331/40000, Loss: 0.0001230622874572873, Learning Rate: 0.000936\n",
      "Epoch 6332/40000, Loss: 0.00010600070527289063, Learning Rate: 0.000935\n",
      "Epoch 6333/40000, Loss: 7.925022509880364e-05, Learning Rate: 0.000935\n",
      "Epoch 6334/40000, Loss: 9.717375360196456e-05, Learning Rate: 0.000935\n",
      "Epoch 6335/40000, Loss: 6.748758460162207e-05, Learning Rate: 0.000935\n",
      "Epoch 6336/40000, Loss: 5.138823325978592e-05, Learning Rate: 0.000935\n",
      "Epoch 6337/40000, Loss: 8.21117137093097e-05, Learning Rate: 0.000935\n",
      "Epoch 6338/40000, Loss: 3.704231494339183e-05, Learning Rate: 0.000935\n",
      "Epoch 6339/40000, Loss: 8.721354242879897e-05, Learning Rate: 0.000935\n",
      "Epoch 6340/40000, Loss: 7.8605989983771e-05, Learning Rate: 0.000935\n",
      "Epoch 6341/40000, Loss: 4.0490704122930765e-05, Learning Rate: 0.000934\n",
      "Epoch 6342/40000, Loss: 8.514688670402393e-05, Learning Rate: 0.000934\n",
      "Epoch 6343/40000, Loss: 8.357387559954077e-05, Learning Rate: 0.000934\n",
      "Epoch 6344/40000, Loss: 6.637601472903043e-05, Learning Rate: 0.000934\n",
      "Epoch 6345/40000, Loss: 8.476385846734047e-05, Learning Rate: 0.000934\n",
      "Epoch 6346/40000, Loss: 8.366845577256754e-05, Learning Rate: 0.000934\n",
      "Epoch 6347/40000, Loss: 0.00014130253111943603, Learning Rate: 0.000934\n",
      "Epoch 6348/40000, Loss: 0.00012360088294371963, Learning Rate: 0.000934\n",
      "Epoch 6349/40000, Loss: 9.2649097496178e-05, Learning Rate: 0.000934\n",
      "Epoch 6350/40000, Loss: 0.00016428178059868515, Learning Rate: 0.000933\n",
      "Epoch 6351/40000, Loss: 0.0001585103600518778, Learning Rate: 0.000933\n",
      "Epoch 6352/40000, Loss: 7.71085760788992e-05, Learning Rate: 0.000933\n",
      "Epoch 6353/40000, Loss: 4.644456203095615e-05, Learning Rate: 0.000933\n",
      "Epoch 6354/40000, Loss: 0.00012358710227999836, Learning Rate: 0.000933\n",
      "Epoch 6355/40000, Loss: 6.822255090810359e-05, Learning Rate: 0.000933\n",
      "Epoch 6356/40000, Loss: 5.457361476146616e-05, Learning Rate: 0.000933\n",
      "Epoch 6357/40000, Loss: 4.603784327628091e-05, Learning Rate: 0.000933\n",
      "Epoch 6358/40000, Loss: 4.6744658902753145e-05, Learning Rate: 0.000933\n",
      "Epoch 6359/40000, Loss: 0.0001347107463516295, Learning Rate: 0.000932\n",
      "Epoch 6360/40000, Loss: 9.056506678462029e-05, Learning Rate: 0.000932\n",
      "Epoch 6361/40000, Loss: 8.068012539297342e-05, Learning Rate: 0.000932\n",
      "Epoch 6362/40000, Loss: 4.98703375342302e-05, Learning Rate: 0.000932\n",
      "Epoch 6363/40000, Loss: 0.00010258758993586525, Learning Rate: 0.000932\n",
      "Epoch 6364/40000, Loss: 9.083202894544229e-05, Learning Rate: 0.000932\n",
      "Epoch 6365/40000, Loss: 8.947782771429047e-05, Learning Rate: 0.000932\n",
      "Epoch 6366/40000, Loss: 0.00013979700452182442, Learning Rate: 0.000932\n",
      "Epoch 6367/40000, Loss: 8.221662574214861e-05, Learning Rate: 0.000932\n",
      "Epoch 6368/40000, Loss: 9.706086711958051e-05, Learning Rate: 0.000931\n",
      "Epoch 6369/40000, Loss: 8.768615225562826e-05, Learning Rate: 0.000931\n",
      "Epoch 6370/40000, Loss: 4.5589840738102794e-05, Learning Rate: 0.000931\n",
      "Epoch 6371/40000, Loss: 6.422619480872527e-05, Learning Rate: 0.000931\n",
      "Epoch 6372/40000, Loss: 0.00013778485299553722, Learning Rate: 0.000931\n",
      "Epoch 6373/40000, Loss: 7.816977449692786e-05, Learning Rate: 0.000931\n",
      "Epoch 6374/40000, Loss: 0.00013931814464740455, Learning Rate: 0.000931\n",
      "Epoch 6375/40000, Loss: 0.0001236561656696722, Learning Rate: 0.000931\n",
      "Epoch 6376/40000, Loss: 7.796512363711372e-05, Learning Rate: 0.000931\n",
      "Epoch 6377/40000, Loss: 4.2925581510644406e-05, Learning Rate: 0.000930\n",
      "Epoch 6378/40000, Loss: 7.922995428089052e-05, Learning Rate: 0.000930\n",
      "Epoch 6379/40000, Loss: 6.41923033981584e-05, Learning Rate: 0.000930\n",
      "Epoch 6380/40000, Loss: 6.450855289585888e-05, Learning Rate: 0.000930\n",
      "Epoch 6381/40000, Loss: 7.197993545560166e-05, Learning Rate: 0.000930\n",
      "Epoch 6382/40000, Loss: 7.545299013145268e-05, Learning Rate: 0.000930\n",
      "Epoch 6383/40000, Loss: 4.424626240506768e-05, Learning Rate: 0.000930\n",
      "Epoch 6384/40000, Loss: 0.00010663143621059135, Learning Rate: 0.000930\n",
      "Epoch 6385/40000, Loss: 8.430743764620274e-05, Learning Rate: 0.000930\n",
      "Epoch 6386/40000, Loss: 5.879321906832047e-05, Learning Rate: 0.000929\n",
      "Epoch 6387/40000, Loss: 0.00012288587458897382, Learning Rate: 0.000929\n",
      "Epoch 6388/40000, Loss: 7.907155668362975e-05, Learning Rate: 0.000929\n",
      "Epoch 6389/40000, Loss: 0.00011492040357552469, Learning Rate: 0.000929\n",
      "Epoch 6390/40000, Loss: 0.0001089919896912761, Learning Rate: 0.000929\n",
      "Epoch 6391/40000, Loss: 4.052445729030296e-05, Learning Rate: 0.000929\n",
      "Epoch 6392/40000, Loss: 3.606085374485701e-05, Learning Rate: 0.000929\n",
      "Epoch 6393/40000, Loss: 6.236509943846613e-05, Learning Rate: 0.000929\n",
      "Epoch 6394/40000, Loss: 7.250344060594216e-05, Learning Rate: 0.000929\n",
      "Epoch 6395/40000, Loss: 7.024714432191104e-05, Learning Rate: 0.000928\n",
      "Epoch 6396/40000, Loss: 0.00011071348853874952, Learning Rate: 0.000928\n",
      "Epoch 6397/40000, Loss: 5.94125158386305e-05, Learning Rate: 0.000928\n",
      "Epoch 6398/40000, Loss: 0.00011305458610877395, Learning Rate: 0.000928\n",
      "Epoch 6399/40000, Loss: 7.56615336285904e-05, Learning Rate: 0.000928\n",
      "Epoch 6400/40000, Loss: 0.00011376786278560758, Learning Rate: 0.000928\n",
      "Epoch 6401/40000, Loss: 0.00011140085553051904, Learning Rate: 0.000928\n",
      "Epoch 6402/40000, Loss: 8.037956285988912e-05, Learning Rate: 0.000928\n",
      "Epoch 6403/40000, Loss: 6.951008981559426e-05, Learning Rate: 0.000928\n",
      "Epoch 6404/40000, Loss: 0.00010781952005345374, Learning Rate: 0.000927\n",
      "Epoch 6405/40000, Loss: 8.022653491934761e-05, Learning Rate: 0.000927\n",
      "Epoch 6406/40000, Loss: 4.6638782805530354e-05, Learning Rate: 0.000927\n",
      "Epoch 6407/40000, Loss: 8.776928734732792e-05, Learning Rate: 0.000927\n",
      "Epoch 6408/40000, Loss: 0.0001340878225164488, Learning Rate: 0.000927\n",
      "Epoch 6409/40000, Loss: 4.831394107895903e-05, Learning Rate: 0.000927\n",
      "Epoch 6410/40000, Loss: 0.00014584703603759408, Learning Rate: 0.000927\n",
      "Epoch 6411/40000, Loss: 6.554112042067572e-05, Learning Rate: 0.000927\n",
      "Epoch 6412/40000, Loss: 0.00015798395907040685, Learning Rate: 0.000927\n",
      "Epoch 6413/40000, Loss: 7.750427903374657e-05, Learning Rate: 0.000926\n",
      "Epoch 6414/40000, Loss: 0.0001170051473309286, Learning Rate: 0.000926\n",
      "Epoch 6415/40000, Loss: 7.356746937148273e-05, Learning Rate: 0.000926\n",
      "Epoch 6416/40000, Loss: 8.196318958653137e-05, Learning Rate: 0.000926\n",
      "Epoch 6417/40000, Loss: 5.003490878152661e-05, Learning Rate: 0.000926\n",
      "Epoch 6418/40000, Loss: 8.766947576077655e-05, Learning Rate: 0.000926\n",
      "Epoch 6419/40000, Loss: 0.00012902911112178117, Learning Rate: 0.000926\n",
      "Epoch 6420/40000, Loss: 9.555414726492018e-05, Learning Rate: 0.000926\n",
      "Epoch 6421/40000, Loss: 0.0001432377175660804, Learning Rate: 0.000926\n",
      "Epoch 6422/40000, Loss: 0.00016470818081870675, Learning Rate: 0.000925\n",
      "Epoch 6423/40000, Loss: 9.927567589329556e-05, Learning Rate: 0.000925\n",
      "Epoch 6424/40000, Loss: 0.00013095430040266365, Learning Rate: 0.000925\n",
      "Epoch 6425/40000, Loss: 9.506369679002091e-05, Learning Rate: 0.000925\n",
      "Epoch 6426/40000, Loss: 0.0001679890410741791, Learning Rate: 0.000925\n",
      "Epoch 6427/40000, Loss: 5.07973600178957e-05, Learning Rate: 0.000925\n",
      "Epoch 6428/40000, Loss: 9.098768350668252e-05, Learning Rate: 0.000925\n",
      "Epoch 6429/40000, Loss: 8.411395538132638e-05, Learning Rate: 0.000925\n",
      "Epoch 6430/40000, Loss: 8.160352444974706e-05, Learning Rate: 0.000925\n",
      "Epoch 6431/40000, Loss: 7.99241170170717e-05, Learning Rate: 0.000924\n",
      "Epoch 6432/40000, Loss: 6.73835602356121e-05, Learning Rate: 0.000924\n",
      "Epoch 6433/40000, Loss: 4.870412521995604e-05, Learning Rate: 0.000924\n",
      "Epoch 6434/40000, Loss: 8.378720667678863e-05, Learning Rate: 0.000924\n",
      "Epoch 6435/40000, Loss: 0.00011894044291693717, Learning Rate: 0.000924\n",
      "Epoch 6436/40000, Loss: 4.5388744183583185e-05, Learning Rate: 0.000924\n",
      "Epoch 6437/40000, Loss: 9.22688195714727e-05, Learning Rate: 0.000924\n",
      "Epoch 6438/40000, Loss: 0.0001202067214762792, Learning Rate: 0.000924\n",
      "Epoch 6439/40000, Loss: 8.68271235958673e-05, Learning Rate: 0.000924\n",
      "Epoch 6440/40000, Loss: 0.00016936320753302425, Learning Rate: 0.000923\n",
      "Epoch 6441/40000, Loss: 5.262646300252527e-05, Learning Rate: 0.000923\n",
      "Epoch 6442/40000, Loss: 7.136040949262679e-05, Learning Rate: 0.000923\n",
      "Epoch 6443/40000, Loss: 7.429601100739092e-05, Learning Rate: 0.000923\n",
      "Epoch 6444/40000, Loss: 9.183774091070518e-05, Learning Rate: 0.000923\n",
      "Epoch 6445/40000, Loss: 0.00013443443458527327, Learning Rate: 0.000923\n",
      "Epoch 6446/40000, Loss: 0.0001615287474123761, Learning Rate: 0.000923\n",
      "Epoch 6447/40000, Loss: 0.00016253413923550397, Learning Rate: 0.000923\n",
      "Epoch 6448/40000, Loss: 0.00015884949243627489, Learning Rate: 0.000923\n",
      "Epoch 6449/40000, Loss: 0.00015643666847608984, Learning Rate: 0.000922\n",
      "Epoch 6450/40000, Loss: 0.00018225274106953293, Learning Rate: 0.000922\n",
      "Epoch 6451/40000, Loss: 0.0001780011662049219, Learning Rate: 0.000922\n",
      "Epoch 6452/40000, Loss: 0.0001956036576302722, Learning Rate: 0.000922\n",
      "Epoch 6453/40000, Loss: 0.0001039973649312742, Learning Rate: 0.000922\n",
      "Epoch 6454/40000, Loss: 0.00010401831968920305, Learning Rate: 0.000922\n",
      "Epoch 6455/40000, Loss: 8.612010424258187e-05, Learning Rate: 0.000922\n",
      "Epoch 6456/40000, Loss: 8.187998901121318e-05, Learning Rate: 0.000922\n",
      "Epoch 6457/40000, Loss: 0.00014375093451235443, Learning Rate: 0.000922\n",
      "Epoch 6458/40000, Loss: 0.0001492120500188321, Learning Rate: 0.000921\n",
      "Epoch 6459/40000, Loss: 0.00014505644503515214, Learning Rate: 0.000921\n",
      "Epoch 6460/40000, Loss: 9.382926509715617e-05, Learning Rate: 0.000921\n",
      "Epoch 6461/40000, Loss: 0.00012700608931481838, Learning Rate: 0.000921\n",
      "Epoch 6462/40000, Loss: 9.929703082889318e-05, Learning Rate: 0.000921\n",
      "Epoch 6463/40000, Loss: 0.00011912905756616965, Learning Rate: 0.000921\n",
      "Epoch 6464/40000, Loss: 0.0001293954555876553, Learning Rate: 0.000921\n",
      "Epoch 6465/40000, Loss: 0.00018784608982969075, Learning Rate: 0.000921\n",
      "Epoch 6466/40000, Loss: 0.00016685959417372942, Learning Rate: 0.000921\n",
      "Epoch 6467/40000, Loss: 0.00013251902419142425, Learning Rate: 0.000920\n",
      "Epoch 6468/40000, Loss: 8.704375068191439e-05, Learning Rate: 0.000920\n",
      "Epoch 6469/40000, Loss: 7.518989150412381e-05, Learning Rate: 0.000920\n",
      "Epoch 6470/40000, Loss: 0.0001407473610015586, Learning Rate: 0.000920\n",
      "Epoch 6471/40000, Loss: 0.00011714336869772524, Learning Rate: 0.000920\n",
      "Epoch 6472/40000, Loss: 0.00013388070510700345, Learning Rate: 0.000920\n",
      "Epoch 6473/40000, Loss: 9.086298814509064e-05, Learning Rate: 0.000920\n",
      "Epoch 6474/40000, Loss: 0.0001288061757804826, Learning Rate: 0.000920\n",
      "Epoch 6475/40000, Loss: 0.00015685441030655056, Learning Rate: 0.000920\n",
      "Epoch 6476/40000, Loss: 0.00013541316729970276, Learning Rate: 0.000919\n",
      "Epoch 6477/40000, Loss: 0.0001281956647289917, Learning Rate: 0.000919\n",
      "Epoch 6478/40000, Loss: 4.968484427081421e-05, Learning Rate: 0.000919\n",
      "Epoch 6479/40000, Loss: 0.00014028798614162952, Learning Rate: 0.000919\n",
      "Epoch 6480/40000, Loss: 0.00011855670163640752, Learning Rate: 0.000919\n",
      "Epoch 6481/40000, Loss: 0.0001136178761953488, Learning Rate: 0.000919\n",
      "Epoch 6482/40000, Loss: 8.350142161361873e-05, Learning Rate: 0.000919\n",
      "Epoch 6483/40000, Loss: 7.946387631818652e-05, Learning Rate: 0.000919\n",
      "Epoch 6484/40000, Loss: 0.00010704374290071428, Learning Rate: 0.000919\n",
      "Epoch 6485/40000, Loss: 7.437732710968703e-05, Learning Rate: 0.000918\n",
      "Epoch 6486/40000, Loss: 4.8827681894181296e-05, Learning Rate: 0.000918\n",
      "Epoch 6487/40000, Loss: 8.832203457131982e-05, Learning Rate: 0.000918\n",
      "Epoch 6488/40000, Loss: 7.923461816972122e-05, Learning Rate: 0.000918\n",
      "Epoch 6489/40000, Loss: 7.308164640562609e-05, Learning Rate: 0.000918\n",
      "Epoch 6490/40000, Loss: 0.00010532413580222055, Learning Rate: 0.000918\n",
      "Epoch 6491/40000, Loss: 0.00010912971629295498, Learning Rate: 0.000918\n",
      "Epoch 6492/40000, Loss: 8.270423131762072e-05, Learning Rate: 0.000918\n",
      "Epoch 6493/40000, Loss: 3.528735396685079e-05, Learning Rate: 0.000918\n",
      "Epoch 6494/40000, Loss: 6.070452218409628e-05, Learning Rate: 0.000917\n",
      "Epoch 6495/40000, Loss: 7.798610022291541e-05, Learning Rate: 0.000917\n",
      "Epoch 6496/40000, Loss: 3.258898505009711e-05, Learning Rate: 0.000917\n",
      "Epoch 6497/40000, Loss: 9.922533354256302e-05, Learning Rate: 0.000917\n",
      "Epoch 6498/40000, Loss: 7.238634134409949e-05, Learning Rate: 0.000917\n",
      "Epoch 6499/40000, Loss: 9.969630627892911e-05, Learning Rate: 0.000917\n",
      "Epoch 6500/40000, Loss: 6.62871971144341e-05, Learning Rate: 0.000917\n",
      "Epoch 6501/40000, Loss: 6.720168312313035e-05, Learning Rate: 0.000917\n",
      "Epoch 6502/40000, Loss: 0.00010767795902211219, Learning Rate: 0.000917\n",
      "Epoch 6503/40000, Loss: 3.2882737286854535e-05, Learning Rate: 0.000916\n",
      "Epoch 6504/40000, Loss: 7.637344242539257e-05, Learning Rate: 0.000916\n",
      "Epoch 6505/40000, Loss: 7.491463475162163e-05, Learning Rate: 0.000916\n",
      "Epoch 6506/40000, Loss: 7.593112241011113e-05, Learning Rate: 0.000916\n",
      "Epoch 6507/40000, Loss: 7.726090552750975e-05, Learning Rate: 0.000916\n",
      "Epoch 6508/40000, Loss: 7.119857764337212e-05, Learning Rate: 0.000916\n",
      "Epoch 6509/40000, Loss: 9.684493124950677e-05, Learning Rate: 0.000916\n",
      "Epoch 6510/40000, Loss: 7.466660463251173e-05, Learning Rate: 0.000916\n",
      "Epoch 6511/40000, Loss: 7.694086525589228e-05, Learning Rate: 0.000916\n",
      "Epoch 6512/40000, Loss: 0.00010543112875893712, Learning Rate: 0.000915\n",
      "Epoch 6513/40000, Loss: 7.498028571717441e-05, Learning Rate: 0.000915\n",
      "Epoch 6514/40000, Loss: 0.00011161513248225674, Learning Rate: 0.000915\n",
      "Epoch 6515/40000, Loss: 7.58645182941109e-05, Learning Rate: 0.000915\n",
      "Epoch 6516/40000, Loss: 7.610571628902107e-05, Learning Rate: 0.000915\n",
      "Epoch 6517/40000, Loss: 0.00012543702905531973, Learning Rate: 0.000915\n",
      "Epoch 6518/40000, Loss: 6.137731543276459e-05, Learning Rate: 0.000915\n",
      "Epoch 6519/40000, Loss: 0.00016667461022734642, Learning Rate: 0.000915\n",
      "Epoch 6520/40000, Loss: 3.690562516567297e-05, Learning Rate: 0.000915\n",
      "Epoch 6521/40000, Loss: 6.150468834675848e-05, Learning Rate: 0.000914\n",
      "Epoch 6522/40000, Loss: 6.176281749503687e-05, Learning Rate: 0.000914\n",
      "Epoch 6523/40000, Loss: 3.438635030761361e-05, Learning Rate: 0.000914\n",
      "Epoch 6524/40000, Loss: 9.535888239042833e-05, Learning Rate: 0.000914\n",
      "Epoch 6525/40000, Loss: 3.539588215062395e-05, Learning Rate: 0.000914\n",
      "Epoch 6526/40000, Loss: 3.768325041164644e-05, Learning Rate: 0.000914\n",
      "Epoch 6527/40000, Loss: 6.972913251956925e-05, Learning Rate: 0.000914\n",
      "Epoch 6528/40000, Loss: 0.00012346731091383845, Learning Rate: 0.000914\n",
      "Epoch 6529/40000, Loss: 8.01804126240313e-05, Learning Rate: 0.000914\n",
      "Epoch 6530/40000, Loss: 7.817520963726565e-05, Learning Rate: 0.000914\n",
      "Epoch 6531/40000, Loss: 6.316910730674863e-05, Learning Rate: 0.000913\n",
      "Epoch 6532/40000, Loss: 7.515361357945949e-05, Learning Rate: 0.000913\n",
      "Epoch 6533/40000, Loss: 6.471145752584562e-05, Learning Rate: 0.000913\n",
      "Epoch 6534/40000, Loss: 8.344991510966793e-05, Learning Rate: 0.000913\n",
      "Epoch 6535/40000, Loss: 0.0001193618300021626, Learning Rate: 0.000913\n",
      "Epoch 6536/40000, Loss: 7.906876999186352e-05, Learning Rate: 0.000913\n",
      "Epoch 6537/40000, Loss: 0.00013584129919763654, Learning Rate: 0.000913\n",
      "Epoch 6538/40000, Loss: 7.102270319592208e-05, Learning Rate: 0.000913\n",
      "Epoch 6539/40000, Loss: 7.785723573761061e-05, Learning Rate: 0.000913\n",
      "Epoch 6540/40000, Loss: 7.117399945855141e-05, Learning Rate: 0.000912\n",
      "Epoch 6541/40000, Loss: 0.00015470897778868675, Learning Rate: 0.000912\n",
      "Epoch 6542/40000, Loss: 0.0001892044529085979, Learning Rate: 0.000912\n",
      "Epoch 6543/40000, Loss: 0.00017509577446617186, Learning Rate: 0.000912\n",
      "Epoch 6544/40000, Loss: 0.00014913837367203087, Learning Rate: 0.000912\n",
      "Epoch 6545/40000, Loss: 9.786104783415794e-05, Learning Rate: 0.000912\n",
      "Epoch 6546/40000, Loss: 0.00012885339674539864, Learning Rate: 0.000912\n",
      "Epoch 6547/40000, Loss: 0.00012422713916748762, Learning Rate: 0.000912\n",
      "Epoch 6548/40000, Loss: 8.585458272136748e-05, Learning Rate: 0.000912\n",
      "Epoch 6549/40000, Loss: 0.00023589191550854594, Learning Rate: 0.000911\n",
      "Epoch 6550/40000, Loss: 0.00022378431458491832, Learning Rate: 0.000911\n",
      "Epoch 6551/40000, Loss: 0.0001064854150172323, Learning Rate: 0.000911\n",
      "Epoch 6552/40000, Loss: 0.00018174920114688575, Learning Rate: 0.000911\n",
      "Epoch 6553/40000, Loss: 6.394313095370308e-05, Learning Rate: 0.000911\n",
      "Epoch 6554/40000, Loss: 5.162478191778064e-05, Learning Rate: 0.000911\n",
      "Epoch 6555/40000, Loss: 7.431347330566496e-05, Learning Rate: 0.000911\n",
      "Epoch 6556/40000, Loss: 8.973030344350263e-05, Learning Rate: 0.000911\n",
      "Epoch 6557/40000, Loss: 0.00011844397522509098, Learning Rate: 0.000911\n",
      "Epoch 6558/40000, Loss: 0.00012155246804468334, Learning Rate: 0.000910\n",
      "Epoch 6559/40000, Loss: 0.00015042284212540835, Learning Rate: 0.000910\n",
      "Epoch 6560/40000, Loss: 0.00010822511103469878, Learning Rate: 0.000910\n",
      "Epoch 6561/40000, Loss: 0.00038714054971933365, Learning Rate: 0.000910\n",
      "Epoch 6562/40000, Loss: 0.00010881658818107098, Learning Rate: 0.000910\n",
      "Epoch 6563/40000, Loss: 0.00010555267363088205, Learning Rate: 0.000910\n",
      "Epoch 6564/40000, Loss: 9.065541962627321e-05, Learning Rate: 0.000910\n",
      "Epoch 6565/40000, Loss: 0.00013559349463321269, Learning Rate: 0.000910\n",
      "Epoch 6566/40000, Loss: 0.00011484452261356637, Learning Rate: 0.000910\n",
      "Epoch 6567/40000, Loss: 7.253897638292983e-05, Learning Rate: 0.000909\n",
      "Epoch 6568/40000, Loss: 6.571522681042552e-05, Learning Rate: 0.000909\n",
      "Epoch 6569/40000, Loss: 6.370442133629695e-05, Learning Rate: 0.000909\n",
      "Epoch 6570/40000, Loss: 0.0001151013420894742, Learning Rate: 0.000909\n",
      "Epoch 6571/40000, Loss: 0.0001239559060195461, Learning Rate: 0.000909\n",
      "Epoch 6572/40000, Loss: 0.00012227584375068545, Learning Rate: 0.000909\n",
      "Epoch 6573/40000, Loss: 0.00013000957551412284, Learning Rate: 0.000909\n",
      "Epoch 6574/40000, Loss: 0.0001131615717895329, Learning Rate: 0.000909\n",
      "Epoch 6575/40000, Loss: 0.00011725166405085474, Learning Rate: 0.000909\n",
      "Epoch 6576/40000, Loss: 0.00011409402213757858, Learning Rate: 0.000908\n",
      "Epoch 6577/40000, Loss: 9.372171189170331e-05, Learning Rate: 0.000908\n",
      "Epoch 6578/40000, Loss: 0.00013888694229535758, Learning Rate: 0.000908\n",
      "Epoch 6579/40000, Loss: 0.00011514015204738826, Learning Rate: 0.000908\n",
      "Epoch 6580/40000, Loss: 0.00011427538265706971, Learning Rate: 0.000908\n",
      "Epoch 6581/40000, Loss: 0.00011645356426015496, Learning Rate: 0.000908\n",
      "Epoch 6582/40000, Loss: 0.0001341112656518817, Learning Rate: 0.000908\n",
      "Epoch 6583/40000, Loss: 7.121401722542942e-05, Learning Rate: 0.000908\n",
      "Epoch 6584/40000, Loss: 0.00010082514199893922, Learning Rate: 0.000908\n",
      "Epoch 6585/40000, Loss: 0.00010036304593086243, Learning Rate: 0.000908\n",
      "Epoch 6586/40000, Loss: 9.921266610035673e-05, Learning Rate: 0.000907\n",
      "Epoch 6587/40000, Loss: 0.00010900201596086845, Learning Rate: 0.000907\n",
      "Epoch 6588/40000, Loss: 9.38285156735219e-05, Learning Rate: 0.000907\n",
      "Epoch 6589/40000, Loss: 6.688140274491161e-05, Learning Rate: 0.000907\n",
      "Epoch 6590/40000, Loss: 8.381698717130348e-05, Learning Rate: 0.000907\n",
      "Epoch 6591/40000, Loss: 0.00013210051110945642, Learning Rate: 0.000907\n",
      "Epoch 6592/40000, Loss: 8.226786303566769e-05, Learning Rate: 0.000907\n",
      "Epoch 6593/40000, Loss: 6.728409789502621e-05, Learning Rate: 0.000907\n",
      "Epoch 6594/40000, Loss: 0.00010888458200497553, Learning Rate: 0.000907\n",
      "Epoch 6595/40000, Loss: 4.039502528030425e-05, Learning Rate: 0.000906\n",
      "Epoch 6596/40000, Loss: 0.00010589336307020858, Learning Rate: 0.000906\n",
      "Epoch 6597/40000, Loss: 0.00011766905663534999, Learning Rate: 0.000906\n",
      "Epoch 6598/40000, Loss: 6.247420969884843e-05, Learning Rate: 0.000906\n",
      "Epoch 6599/40000, Loss: 8.002589311217889e-05, Learning Rate: 0.000906\n",
      "Epoch 6600/40000, Loss: 7.788272341713309e-05, Learning Rate: 0.000906\n",
      "Epoch 6601/40000, Loss: 0.00011137867841171101, Learning Rate: 0.000906\n",
      "Epoch 6602/40000, Loss: 0.00010808870138134807, Learning Rate: 0.000906\n",
      "Epoch 6603/40000, Loss: 3.963614653912373e-05, Learning Rate: 0.000906\n",
      "Epoch 6604/40000, Loss: 0.0001042825824697502, Learning Rate: 0.000905\n",
      "Epoch 6605/40000, Loss: 7.763477333355695e-05, Learning Rate: 0.000905\n",
      "Epoch 6606/40000, Loss: 4.3221982195973396e-05, Learning Rate: 0.000905\n",
      "Epoch 6607/40000, Loss: 0.00012170882109785452, Learning Rate: 0.000905\n",
      "Epoch 6608/40000, Loss: 0.00011862730025313795, Learning Rate: 0.000905\n",
      "Epoch 6609/40000, Loss: 8.379874634556472e-05, Learning Rate: 0.000905\n",
      "Epoch 6610/40000, Loss: 7.918852497823536e-05, Learning Rate: 0.000905\n",
      "Epoch 6611/40000, Loss: 8.18843545857817e-05, Learning Rate: 0.000905\n",
      "Epoch 6612/40000, Loss: 0.00021760496019851416, Learning Rate: 0.000905\n",
      "Epoch 6613/40000, Loss: 7.466878741979599e-05, Learning Rate: 0.000904\n",
      "Epoch 6614/40000, Loss: 0.00015631965652573854, Learning Rate: 0.000904\n",
      "Epoch 6615/40000, Loss: 5.215483906795271e-05, Learning Rate: 0.000904\n",
      "Epoch 6616/40000, Loss: 0.00017636532720644027, Learning Rate: 0.000904\n",
      "Epoch 6617/40000, Loss: 0.00011714660649886355, Learning Rate: 0.000904\n",
      "Epoch 6618/40000, Loss: 0.00016504315135534853, Learning Rate: 0.000904\n",
      "Epoch 6619/40000, Loss: 0.00013044160732533783, Learning Rate: 0.000904\n",
      "Epoch 6620/40000, Loss: 0.00012092636461602524, Learning Rate: 0.000904\n",
      "Epoch 6621/40000, Loss: 8.546988101443276e-05, Learning Rate: 0.000904\n",
      "Epoch 6622/40000, Loss: 6.397463585017249e-05, Learning Rate: 0.000903\n",
      "Epoch 6623/40000, Loss: 5.939851689618081e-05, Learning Rate: 0.000903\n",
      "Epoch 6624/40000, Loss: 0.00010284627205692232, Learning Rate: 0.000903\n",
      "Epoch 6625/40000, Loss: 8.63668174133636e-05, Learning Rate: 0.000903\n",
      "Epoch 6626/40000, Loss: 3.515242497087456e-05, Learning Rate: 0.000903\n",
      "Epoch 6627/40000, Loss: 0.0001104977709474042, Learning Rate: 0.000903\n",
      "Epoch 6628/40000, Loss: 4.525042459135875e-05, Learning Rate: 0.000903\n",
      "Epoch 6629/40000, Loss: 0.00012524303747341037, Learning Rate: 0.000903\n",
      "Epoch 6630/40000, Loss: 0.00010847913654288277, Learning Rate: 0.000903\n",
      "Epoch 6631/40000, Loss: 3.3806791179813445e-05, Learning Rate: 0.000903\n",
      "Epoch 6632/40000, Loss: 3.267469583079219e-05, Learning Rate: 0.000902\n",
      "Epoch 6633/40000, Loss: 0.00010554215259617195, Learning Rate: 0.000902\n",
      "Epoch 6634/40000, Loss: 6.779839895898476e-05, Learning Rate: 0.000902\n",
      "Epoch 6635/40000, Loss: 0.00010712595394579694, Learning Rate: 0.000902\n",
      "Epoch 6636/40000, Loss: 7.963092502905056e-05, Learning Rate: 0.000902\n",
      "Epoch 6637/40000, Loss: 8.23779555503279e-05, Learning Rate: 0.000902\n",
      "Epoch 6638/40000, Loss: 6.19044149061665e-05, Learning Rate: 0.000902\n",
      "Epoch 6639/40000, Loss: 9.789779142010957e-05, Learning Rate: 0.000902\n",
      "Epoch 6640/40000, Loss: 8.338078623637557e-05, Learning Rate: 0.000902\n",
      "Epoch 6641/40000, Loss: 7.890669803600758e-05, Learning Rate: 0.000901\n",
      "Epoch 6642/40000, Loss: 0.00012676697224378586, Learning Rate: 0.000901\n",
      "Epoch 6643/40000, Loss: 8.691187395015731e-05, Learning Rate: 0.000901\n",
      "Epoch 6644/40000, Loss: 0.000110490174847655, Learning Rate: 0.000901\n",
      "Epoch 6645/40000, Loss: 7.692718645557761e-05, Learning Rate: 0.000901\n",
      "Epoch 6646/40000, Loss: 7.366909994743764e-05, Learning Rate: 0.000901\n",
      "Epoch 6647/40000, Loss: 0.00011254067067056894, Learning Rate: 0.000901\n",
      "Epoch 6648/40000, Loss: 0.00013704631419386715, Learning Rate: 0.000901\n",
      "Epoch 6649/40000, Loss: 0.00014241372991818935, Learning Rate: 0.000901\n",
      "Epoch 6650/40000, Loss: 7.790677773300558e-05, Learning Rate: 0.000900\n",
      "Epoch 6651/40000, Loss: 4.702690421254374e-05, Learning Rate: 0.000900\n",
      "Epoch 6652/40000, Loss: 0.00011998876288998872, Learning Rate: 0.000900\n",
      "Epoch 6653/40000, Loss: 6.990975816734135e-05, Learning Rate: 0.000900\n",
      "Epoch 6654/40000, Loss: 8.276948938146234e-05, Learning Rate: 0.000900\n",
      "Epoch 6655/40000, Loss: 0.0001187689631478861, Learning Rate: 0.000900\n",
      "Epoch 6656/40000, Loss: 6.458994175773114e-05, Learning Rate: 0.000900\n",
      "Epoch 6657/40000, Loss: 6.526083598146215e-05, Learning Rate: 0.000900\n",
      "Epoch 6658/40000, Loss: 6.437748379539698e-05, Learning Rate: 0.000900\n",
      "Epoch 6659/40000, Loss: 0.00011554468801477924, Learning Rate: 0.000899\n",
      "Epoch 6660/40000, Loss: 0.00010891046986216679, Learning Rate: 0.000899\n",
      "Epoch 6661/40000, Loss: 9.677818161435425e-05, Learning Rate: 0.000899\n",
      "Epoch 6662/40000, Loss: 4.262605216354132e-05, Learning Rate: 0.000899\n",
      "Epoch 6663/40000, Loss: 9.54963979893364e-05, Learning Rate: 0.000899\n",
      "Epoch 6664/40000, Loss: 0.00012996609439142048, Learning Rate: 0.000899\n",
      "Epoch 6665/40000, Loss: 0.00010662249405868351, Learning Rate: 0.000899\n",
      "Epoch 6666/40000, Loss: 7.668297621421516e-05, Learning Rate: 0.000899\n",
      "Epoch 6667/40000, Loss: 0.00010287189797963947, Learning Rate: 0.000899\n",
      "Epoch 6668/40000, Loss: 7.31339750927873e-05, Learning Rate: 0.000899\n",
      "Epoch 6669/40000, Loss: 6.73521135468036e-05, Learning Rate: 0.000898\n",
      "Epoch 6670/40000, Loss: 3.38724312314298e-05, Learning Rate: 0.000898\n",
      "Epoch 6671/40000, Loss: 8.098357648123056e-05, Learning Rate: 0.000898\n",
      "Epoch 6672/40000, Loss: 0.00010095589095726609, Learning Rate: 0.000898\n",
      "Epoch 6673/40000, Loss: 3.5107113944832236e-05, Learning Rate: 0.000898\n",
      "Epoch 6674/40000, Loss: 7.75752414483577e-05, Learning Rate: 0.000898\n",
      "Epoch 6675/40000, Loss: 6.014391692588106e-05, Learning Rate: 0.000898\n",
      "Epoch 6676/40000, Loss: 6.775077781639993e-05, Learning Rate: 0.000898\n",
      "Epoch 6677/40000, Loss: 0.00010757005657069385, Learning Rate: 0.000898\n",
      "Epoch 6678/40000, Loss: 3.265026680310257e-05, Learning Rate: 0.000897\n",
      "Epoch 6679/40000, Loss: 7.667301542824134e-05, Learning Rate: 0.000897\n",
      "Epoch 6680/40000, Loss: 0.00010877438762690872, Learning Rate: 0.000897\n",
      "Epoch 6681/40000, Loss: 8.119343692669645e-05, Learning Rate: 0.000897\n",
      "Epoch 6682/40000, Loss: 0.00011574817472137511, Learning Rate: 0.000897\n",
      "Epoch 6683/40000, Loss: 0.00011089057079516351, Learning Rate: 0.000897\n",
      "Epoch 6684/40000, Loss: 0.00011652240937110037, Learning Rate: 0.000897\n",
      "Epoch 6685/40000, Loss: 0.0001202278072014451, Learning Rate: 0.000897\n",
      "Epoch 6686/40000, Loss: 8.18239786894992e-05, Learning Rate: 0.000897\n",
      "Epoch 6687/40000, Loss: 7.331666711252183e-05, Learning Rate: 0.000896\n",
      "Epoch 6688/40000, Loss: 7.563835970358923e-05, Learning Rate: 0.000896\n",
      "Epoch 6689/40000, Loss: 6.765981379430741e-05, Learning Rate: 0.000896\n",
      "Epoch 6690/40000, Loss: 8.599611464887857e-05, Learning Rate: 0.000896\n",
      "Epoch 6691/40000, Loss: 0.0001167903101304546, Learning Rate: 0.000896\n",
      "Epoch 6692/40000, Loss: 0.00011220006126677617, Learning Rate: 0.000896\n",
      "Epoch 6693/40000, Loss: 0.00011283937783446163, Learning Rate: 0.000896\n",
      "Epoch 6694/40000, Loss: 7.007504609646276e-05, Learning Rate: 0.000896\n",
      "Epoch 6695/40000, Loss: 4.0856666601030156e-05, Learning Rate: 0.000896\n",
      "Epoch 6696/40000, Loss: 6.952600961085409e-05, Learning Rate: 0.000895\n",
      "Epoch 6697/40000, Loss: 3.978341555921361e-05, Learning Rate: 0.000895\n",
      "Epoch 6698/40000, Loss: 0.00011815415200544521, Learning Rate: 0.000895\n",
      "Epoch 6699/40000, Loss: 8.702173363417387e-05, Learning Rate: 0.000895\n",
      "Epoch 6700/40000, Loss: 0.00011724200157914311, Learning Rate: 0.000895\n",
      "Epoch 6701/40000, Loss: 0.0001257622061530128, Learning Rate: 0.000895\n",
      "Epoch 6702/40000, Loss: 0.00014683551853522658, Learning Rate: 0.000895\n",
      "Epoch 6703/40000, Loss: 0.00012899214925710112, Learning Rate: 0.000895\n",
      "Epoch 6704/40000, Loss: 6.533333362312987e-05, Learning Rate: 0.000895\n",
      "Epoch 6705/40000, Loss: 0.00015865197929088026, Learning Rate: 0.000895\n",
      "Epoch 6706/40000, Loss: 0.00012603410868905485, Learning Rate: 0.000894\n",
      "Epoch 6707/40000, Loss: 0.00018854816153179854, Learning Rate: 0.000894\n",
      "Epoch 6708/40000, Loss: 0.0001071932929335162, Learning Rate: 0.000894\n",
      "Epoch 6709/40000, Loss: 0.0001293528766836971, Learning Rate: 0.000894\n",
      "Epoch 6710/40000, Loss: 8.08330369181931e-05, Learning Rate: 0.000894\n",
      "Epoch 6711/40000, Loss: 0.00017617811681702733, Learning Rate: 0.000894\n",
      "Epoch 6712/40000, Loss: 0.00019075073942076415, Learning Rate: 0.000894\n",
      "Epoch 6713/40000, Loss: 0.00011319659824948758, Learning Rate: 0.000894\n",
      "Epoch 6714/40000, Loss: 0.0001592170592630282, Learning Rate: 0.000894\n",
      "Epoch 6715/40000, Loss: 0.00015744275879114866, Learning Rate: 0.000893\n",
      "Epoch 6716/40000, Loss: 0.00011695198918459937, Learning Rate: 0.000893\n",
      "Epoch 6717/40000, Loss: 0.00012873268860857934, Learning Rate: 0.000893\n",
      "Epoch 6718/40000, Loss: 0.00011199260188732296, Learning Rate: 0.000893\n",
      "Epoch 6719/40000, Loss: 0.00014673282566946, Learning Rate: 0.000893\n",
      "Epoch 6720/40000, Loss: 0.00014608212222810835, Learning Rate: 0.000893\n",
      "Epoch 6721/40000, Loss: 9.032254456542432e-05, Learning Rate: 0.000893\n",
      "Epoch 6722/40000, Loss: 0.00016768736531957984, Learning Rate: 0.000893\n",
      "Epoch 6723/40000, Loss: 0.0001628215832170099, Learning Rate: 0.000893\n",
      "Epoch 6724/40000, Loss: 8.704854553798214e-05, Learning Rate: 0.000892\n",
      "Epoch 6725/40000, Loss: 6.62686288706027e-05, Learning Rate: 0.000892\n",
      "Epoch 6726/40000, Loss: 4.923892265651375e-05, Learning Rate: 0.000892\n",
      "Epoch 6727/40000, Loss: 0.00010710321657825261, Learning Rate: 0.000892\n",
      "Epoch 6728/40000, Loss: 7.436379382852465e-05, Learning Rate: 0.000892\n",
      "Epoch 6729/40000, Loss: 0.0001431030104868114, Learning Rate: 0.000892\n",
      "Epoch 6730/40000, Loss: 8.226597856264561e-05, Learning Rate: 0.000892\n",
      "Epoch 6731/40000, Loss: 0.0001241805439349264, Learning Rate: 0.000892\n",
      "Epoch 6732/40000, Loss: 0.00015803633141331375, Learning Rate: 0.000892\n",
      "Epoch 6733/40000, Loss: 0.00012110061652492732, Learning Rate: 0.000892\n",
      "Epoch 6734/40000, Loss: 8.777896437095478e-05, Learning Rate: 0.000891\n",
      "Epoch 6735/40000, Loss: 6.738654337823391e-05, Learning Rate: 0.000891\n",
      "Epoch 6736/40000, Loss: 0.00016287359176203609, Learning Rate: 0.000891\n",
      "Epoch 6737/40000, Loss: 0.00012476203846745193, Learning Rate: 0.000891\n",
      "Epoch 6738/40000, Loss: 5.285963925416581e-05, Learning Rate: 0.000891\n",
      "Epoch 6739/40000, Loss: 0.00016209311434067786, Learning Rate: 0.000891\n",
      "Epoch 6740/40000, Loss: 8.119730046018958e-05, Learning Rate: 0.000891\n",
      "Epoch 6741/40000, Loss: 8.066291047725827e-05, Learning Rate: 0.000891\n",
      "Epoch 6742/40000, Loss: 8.53348319651559e-05, Learning Rate: 0.000891\n",
      "Epoch 6743/40000, Loss: 7.930367428343743e-05, Learning Rate: 0.000890\n",
      "Epoch 6744/40000, Loss: 7.784707850078121e-05, Learning Rate: 0.000890\n",
      "Epoch 6745/40000, Loss: 6.073262920835987e-05, Learning Rate: 0.000890\n",
      "Epoch 6746/40000, Loss: 9.025145845953375e-05, Learning Rate: 0.000890\n",
      "Epoch 6747/40000, Loss: 0.00010273936641169712, Learning Rate: 0.000890\n",
      "Epoch 6748/40000, Loss: 7.503812958020717e-05, Learning Rate: 0.000890\n",
      "Epoch 6749/40000, Loss: 0.00011893755436176434, Learning Rate: 0.000890\n",
      "Epoch 6750/40000, Loss: 3.727041985257529e-05, Learning Rate: 0.000890\n",
      "Epoch 6751/40000, Loss: 8.719055040273815e-05, Learning Rate: 0.000890\n",
      "Epoch 6752/40000, Loss: 0.00015291478484869003, Learning Rate: 0.000889\n",
      "Epoch 6753/40000, Loss: 7.705160533078015e-05, Learning Rate: 0.000889\n",
      "Epoch 6754/40000, Loss: 0.00011157916014781222, Learning Rate: 0.000889\n",
      "Epoch 6755/40000, Loss: 8.532211359124631e-05, Learning Rate: 0.000889\n",
      "Epoch 6756/40000, Loss: 0.00012617072206921875, Learning Rate: 0.000889\n",
      "Epoch 6757/40000, Loss: 0.00010332114470656961, Learning Rate: 0.000889\n",
      "Epoch 6758/40000, Loss: 3.633366941357963e-05, Learning Rate: 0.000889\n",
      "Epoch 6759/40000, Loss: 6.930744712008163e-05, Learning Rate: 0.000889\n",
      "Epoch 6760/40000, Loss: 0.000104566162917763, Learning Rate: 0.000889\n",
      "Epoch 6761/40000, Loss: 6.0888367443112656e-05, Learning Rate: 0.000889\n",
      "Epoch 6762/40000, Loss: 0.00010604319686535746, Learning Rate: 0.000888\n",
      "Epoch 6763/40000, Loss: 6.95742855896242e-05, Learning Rate: 0.000888\n",
      "Epoch 6764/40000, Loss: 0.0001094746621674858, Learning Rate: 0.000888\n",
      "Epoch 6765/40000, Loss: 6.172001303639263e-05, Learning Rate: 0.000888\n",
      "Epoch 6766/40000, Loss: 6.585271330550313e-05, Learning Rate: 0.000888\n",
      "Epoch 6767/40000, Loss: 7.894964801380411e-05, Learning Rate: 0.000888\n",
      "Epoch 6768/40000, Loss: 0.00010798453877214342, Learning Rate: 0.000888\n",
      "Epoch 6769/40000, Loss: 7.606144208693877e-05, Learning Rate: 0.000888\n",
      "Epoch 6770/40000, Loss: 3.121676854789257e-05, Learning Rate: 0.000888\n",
      "Epoch 6771/40000, Loss: 5.821904051117599e-05, Learning Rate: 0.000887\n",
      "Epoch 6772/40000, Loss: 9.48209926718846e-05, Learning Rate: 0.000887\n",
      "Epoch 6773/40000, Loss: 9.424158633919433e-05, Learning Rate: 0.000887\n",
      "Epoch 6774/40000, Loss: 9.355741349281743e-05, Learning Rate: 0.000887\n",
      "Epoch 6775/40000, Loss: 5.728290125261992e-05, Learning Rate: 0.000887\n",
      "Epoch 6776/40000, Loss: 9.535949357086793e-05, Learning Rate: 0.000887\n",
      "Epoch 6777/40000, Loss: 3.029743311344646e-05, Learning Rate: 0.000887\n",
      "Epoch 6778/40000, Loss: 9.83911522780545e-05, Learning Rate: 0.000887\n",
      "Epoch 6779/40000, Loss: 5.780662104371004e-05, Learning Rate: 0.000887\n",
      "Epoch 6780/40000, Loss: 7.389707025140524e-05, Learning Rate: 0.000887\n",
      "Epoch 6781/40000, Loss: 9.501371823716909e-05, Learning Rate: 0.000886\n",
      "Epoch 6782/40000, Loss: 9.377330570714548e-05, Learning Rate: 0.000886\n",
      "Epoch 6783/40000, Loss: 6.202518125064671e-05, Learning Rate: 0.000886\n",
      "Epoch 6784/40000, Loss: 7.380582974292338e-05, Learning Rate: 0.000886\n",
      "Epoch 6785/40000, Loss: 5.893036723136902e-05, Learning Rate: 0.000886\n",
      "Epoch 6786/40000, Loss: 9.624714584788308e-05, Learning Rate: 0.000886\n",
      "Epoch 6787/40000, Loss: 3.179771374561824e-05, Learning Rate: 0.000886\n",
      "Epoch 6788/40000, Loss: 9.94357033050619e-05, Learning Rate: 0.000886\n",
      "Epoch 6789/40000, Loss: 0.00010627138544805348, Learning Rate: 0.000886\n",
      "Epoch 6790/40000, Loss: 7.416481821564957e-05, Learning Rate: 0.000885\n",
      "Epoch 6791/40000, Loss: 3.146098606521264e-05, Learning Rate: 0.000885\n",
      "Epoch 6792/40000, Loss: 0.00010229663166683167, Learning Rate: 0.000885\n",
      "Epoch 6793/40000, Loss: 0.0001002205754048191, Learning Rate: 0.000885\n",
      "Epoch 6794/40000, Loss: 7.162252586567774e-05, Learning Rate: 0.000885\n",
      "Epoch 6795/40000, Loss: 5.8380464906804264e-05, Learning Rate: 0.000885\n",
      "Epoch 6796/40000, Loss: 5.929578765062615e-05, Learning Rate: 0.000885\n",
      "Epoch 6797/40000, Loss: 6.0628495702985674e-05, Learning Rate: 0.000885\n",
      "Epoch 6798/40000, Loss: 6.0278191085672006e-05, Learning Rate: 0.000885\n",
      "Epoch 6799/40000, Loss: 5.963429430266842e-05, Learning Rate: 0.000884\n",
      "Epoch 6800/40000, Loss: 0.00010707958426792175, Learning Rate: 0.000884\n",
      "Epoch 6801/40000, Loss: 0.00011034185445168987, Learning Rate: 0.000884\n",
      "Epoch 6802/40000, Loss: 5.916698501096107e-05, Learning Rate: 0.000884\n",
      "Epoch 6803/40000, Loss: 6.352036871248856e-05, Learning Rate: 0.000884\n",
      "Epoch 6804/40000, Loss: 9.78084935923107e-05, Learning Rate: 0.000884\n",
      "Epoch 6805/40000, Loss: 3.212492447346449e-05, Learning Rate: 0.000884\n",
      "Epoch 6806/40000, Loss: 7.696710235904902e-05, Learning Rate: 0.000884\n",
      "Epoch 6807/40000, Loss: 7.461704080924392e-05, Learning Rate: 0.000884\n",
      "Epoch 6808/40000, Loss: 5.872396650374867e-05, Learning Rate: 0.000884\n",
      "Epoch 6809/40000, Loss: 5.832858369103633e-05, Learning Rate: 0.000883\n",
      "Epoch 6810/40000, Loss: 7.616089715156704e-05, Learning Rate: 0.000883\n",
      "Epoch 6811/40000, Loss: 6.723938713548705e-05, Learning Rate: 0.000883\n",
      "Epoch 6812/40000, Loss: 9.935763227986172e-05, Learning Rate: 0.000883\n",
      "Epoch 6813/40000, Loss: 0.00010122933599632233, Learning Rate: 0.000883\n",
      "Epoch 6814/40000, Loss: 8.089210314210504e-05, Learning Rate: 0.000883\n",
      "Epoch 6815/40000, Loss: 4.01285506086424e-05, Learning Rate: 0.000883\n",
      "Epoch 6816/40000, Loss: 7.93576255091466e-05, Learning Rate: 0.000883\n",
      "Epoch 6817/40000, Loss: 7.29533567209728e-05, Learning Rate: 0.000883\n",
      "Epoch 6818/40000, Loss: 0.0001431260461686179, Learning Rate: 0.000882\n",
      "Epoch 6819/40000, Loss: 9.1386420535855e-05, Learning Rate: 0.000882\n",
      "Epoch 6820/40000, Loss: 4.852084384765476e-05, Learning Rate: 0.000882\n",
      "Epoch 6821/40000, Loss: 0.00014132774958852679, Learning Rate: 0.000882\n",
      "Epoch 6822/40000, Loss: 0.00011656862625386566, Learning Rate: 0.000882\n",
      "Epoch 6823/40000, Loss: 0.00010949239367619157, Learning Rate: 0.000882\n",
      "Epoch 6824/40000, Loss: 4.333561810199171e-05, Learning Rate: 0.000882\n",
      "Epoch 6825/40000, Loss: 9.479423897573724e-05, Learning Rate: 0.000882\n",
      "Epoch 6826/40000, Loss: 8.28976189950481e-05, Learning Rate: 0.000882\n",
      "Epoch 6827/40000, Loss: 0.00011515377264004201, Learning Rate: 0.000882\n",
      "Epoch 6828/40000, Loss: 7.76385932113044e-05, Learning Rate: 0.000881\n",
      "Epoch 6829/40000, Loss: 7.018845644779503e-05, Learning Rate: 0.000881\n",
      "Epoch 6830/40000, Loss: 7.024659134913236e-05, Learning Rate: 0.000881\n",
      "Epoch 6831/40000, Loss: 9.20121165108867e-05, Learning Rate: 0.000881\n",
      "Epoch 6832/40000, Loss: 7.968789577716962e-05, Learning Rate: 0.000881\n",
      "Epoch 6833/40000, Loss: 9.160550689557567e-05, Learning Rate: 0.000881\n",
      "Epoch 6834/40000, Loss: 7.510012801503763e-05, Learning Rate: 0.000881\n",
      "Epoch 6835/40000, Loss: 4.727979103336111e-05, Learning Rate: 0.000881\n",
      "Epoch 6836/40000, Loss: 3.7843346945010126e-05, Learning Rate: 0.000881\n",
      "Epoch 6837/40000, Loss: 8.848123979987577e-05, Learning Rate: 0.000880\n",
      "Epoch 6838/40000, Loss: 9.640341158956289e-05, Learning Rate: 0.000880\n",
      "Epoch 6839/40000, Loss: 0.00011117206304334104, Learning Rate: 0.000880\n",
      "Epoch 6840/40000, Loss: 0.00013633606431540102, Learning Rate: 0.000880\n",
      "Epoch 6841/40000, Loss: 8.270747639471665e-05, Learning Rate: 0.000880\n",
      "Epoch 6842/40000, Loss: 0.00021942725288681686, Learning Rate: 0.000880\n",
      "Epoch 6843/40000, Loss: 0.00016565993428230286, Learning Rate: 0.000880\n",
      "Epoch 6844/40000, Loss: 0.0001093961764127016, Learning Rate: 0.000880\n",
      "Epoch 6845/40000, Loss: 0.00010532917804084718, Learning Rate: 0.000880\n",
      "Epoch 6846/40000, Loss: 0.00011298727622488514, Learning Rate: 0.000880\n",
      "Epoch 6847/40000, Loss: 0.00015415315283462405, Learning Rate: 0.000879\n",
      "Epoch 6848/40000, Loss: 0.00022470734256785363, Learning Rate: 0.000879\n",
      "Epoch 6849/40000, Loss: 0.00016790299559943378, Learning Rate: 0.000879\n",
      "Epoch 6850/40000, Loss: 9.498346480540931e-05, Learning Rate: 0.000879\n",
      "Epoch 6851/40000, Loss: 0.00014542345888912678, Learning Rate: 0.000879\n",
      "Epoch 6852/40000, Loss: 0.00010477417526999488, Learning Rate: 0.000879\n",
      "Epoch 6853/40000, Loss: 0.00012327938748057932, Learning Rate: 0.000879\n",
      "Epoch 6854/40000, Loss: 0.00017320926417596638, Learning Rate: 0.000879\n",
      "Epoch 6855/40000, Loss: 0.00010502609075047076, Learning Rate: 0.000879\n",
      "Epoch 6856/40000, Loss: 0.00011394915782148018, Learning Rate: 0.000878\n",
      "Epoch 6857/40000, Loss: 0.00014899912639521062, Learning Rate: 0.000878\n",
      "Epoch 6858/40000, Loss: 0.00016417735605500638, Learning Rate: 0.000878\n",
      "Epoch 6859/40000, Loss: 0.00010757995914900675, Learning Rate: 0.000878\n",
      "Epoch 6860/40000, Loss: 0.00014303693023975939, Learning Rate: 0.000878\n",
      "Epoch 6861/40000, Loss: 0.0001326945493929088, Learning Rate: 0.000878\n",
      "Epoch 6862/40000, Loss: 0.00014158591511659324, Learning Rate: 0.000878\n",
      "Epoch 6863/40000, Loss: 0.00019386032363399863, Learning Rate: 0.000878\n",
      "Epoch 6864/40000, Loss: 0.00011604993778746575, Learning Rate: 0.000878\n",
      "Epoch 6865/40000, Loss: 7.200648542493582e-05, Learning Rate: 0.000878\n",
      "Epoch 6866/40000, Loss: 0.00010654691141098738, Learning Rate: 0.000877\n",
      "Epoch 6867/40000, Loss: 6.367843161569908e-05, Learning Rate: 0.000877\n",
      "Epoch 6868/40000, Loss: 8.176959090633318e-05, Learning Rate: 0.000877\n",
      "Epoch 6869/40000, Loss: 7.221229316201061e-05, Learning Rate: 0.000877\n",
      "Epoch 6870/40000, Loss: 8.602779416833073e-05, Learning Rate: 0.000877\n",
      "Epoch 6871/40000, Loss: 3.523061241139658e-05, Learning Rate: 0.000877\n",
      "Epoch 6872/40000, Loss: 7.432104030158371e-05, Learning Rate: 0.000877\n",
      "Epoch 6873/40000, Loss: 7.038664625724778e-05, Learning Rate: 0.000877\n",
      "Epoch 6874/40000, Loss: 6.266817945288494e-05, Learning Rate: 0.000877\n",
      "Epoch 6875/40000, Loss: 8.043339039431885e-05, Learning Rate: 0.000876\n",
      "Epoch 6876/40000, Loss: 7.137751526897773e-05, Learning Rate: 0.000876\n",
      "Epoch 6877/40000, Loss: 7.854911382310092e-05, Learning Rate: 0.000876\n",
      "Epoch 6878/40000, Loss: 0.00010428926179884002, Learning Rate: 0.000876\n",
      "Epoch 6879/40000, Loss: 6.230580765986815e-05, Learning Rate: 0.000876\n",
      "Epoch 6880/40000, Loss: 0.00010966824629576877, Learning Rate: 0.000876\n",
      "Epoch 6881/40000, Loss: 8.021864778129384e-05, Learning Rate: 0.000876\n",
      "Epoch 6882/40000, Loss: 3.513913543429226e-05, Learning Rate: 0.000876\n",
      "Epoch 6883/40000, Loss: 7.806233043083921e-05, Learning Rate: 0.000876\n",
      "Epoch 6884/40000, Loss: 6.074862903915346e-05, Learning Rate: 0.000876\n",
      "Epoch 6885/40000, Loss: 7.492342410841957e-05, Learning Rate: 0.000875\n",
      "Epoch 6886/40000, Loss: 5.8672099839895964e-05, Learning Rate: 0.000875\n",
      "Epoch 6887/40000, Loss: 7.792343239998445e-05, Learning Rate: 0.000875\n",
      "Epoch 6888/40000, Loss: 0.00013192878395784646, Learning Rate: 0.000875\n",
      "Epoch 6889/40000, Loss: 6.416087853722274e-05, Learning Rate: 0.000875\n",
      "Epoch 6890/40000, Loss: 0.00017623684834688902, Learning Rate: 0.000875\n",
      "Epoch 6891/40000, Loss: 0.00018829747568815947, Learning Rate: 0.000875\n",
      "Epoch 6892/40000, Loss: 0.00016928314289543778, Learning Rate: 0.000875\n",
      "Epoch 6893/40000, Loss: 0.0001932411250891164, Learning Rate: 0.000875\n",
      "Epoch 6894/40000, Loss: 0.00031673681223765016, Learning Rate: 0.000874\n",
      "Epoch 6895/40000, Loss: 0.0001233716611750424, Learning Rate: 0.000874\n",
      "Epoch 6896/40000, Loss: 0.00032560661202296615, Learning Rate: 0.000874\n",
      "Epoch 6897/40000, Loss: 0.0005359468632377684, Learning Rate: 0.000874\n",
      "Epoch 6898/40000, Loss: 0.0001954576000571251, Learning Rate: 0.000874\n",
      "Epoch 6899/40000, Loss: 9.11410606931895e-05, Learning Rate: 0.000874\n",
      "Epoch 6900/40000, Loss: 0.00015277863712981343, Learning Rate: 0.000874\n",
      "Epoch 6901/40000, Loss: 0.0002442051481921226, Learning Rate: 0.000874\n",
      "Epoch 6902/40000, Loss: 0.00020604753808584064, Learning Rate: 0.000874\n",
      "Epoch 6903/40000, Loss: 5.5856620747363195e-05, Learning Rate: 0.000874\n",
      "Epoch 6904/40000, Loss: 0.00014911261678207666, Learning Rate: 0.000873\n",
      "Epoch 6905/40000, Loss: 0.000148646387970075, Learning Rate: 0.000873\n",
      "Epoch 6906/40000, Loss: 0.0001772578398231417, Learning Rate: 0.000873\n",
      "Epoch 6907/40000, Loss: 0.00012252952728886157, Learning Rate: 0.000873\n",
      "Epoch 6908/40000, Loss: 8.668247028253973e-05, Learning Rate: 0.000873\n",
      "Epoch 6909/40000, Loss: 0.00012162100756540895, Learning Rate: 0.000873\n",
      "Epoch 6910/40000, Loss: 8.91532254172489e-05, Learning Rate: 0.000873\n",
      "Epoch 6911/40000, Loss: 0.00011405320401536301, Learning Rate: 0.000873\n",
      "Epoch 6912/40000, Loss: 0.00014461948012467474, Learning Rate: 0.000873\n",
      "Epoch 6913/40000, Loss: 0.000132106855744496, Learning Rate: 0.000872\n",
      "Epoch 6914/40000, Loss: 8.124679152388126e-05, Learning Rate: 0.000872\n",
      "Epoch 6915/40000, Loss: 0.00012464881001506, Learning Rate: 0.000872\n",
      "Epoch 6916/40000, Loss: 9.36933938646689e-05, Learning Rate: 0.000872\n",
      "Epoch 6917/40000, Loss: 7.441847992595285e-05, Learning Rate: 0.000872\n",
      "Epoch 6918/40000, Loss: 0.00012907717609778047, Learning Rate: 0.000872\n",
      "Epoch 6919/40000, Loss: 0.00012358574895188212, Learning Rate: 0.000872\n",
      "Epoch 6920/40000, Loss: 8.2335842307657e-05, Learning Rate: 0.000872\n",
      "Epoch 6921/40000, Loss: 7.574136543553323e-05, Learning Rate: 0.000872\n",
      "Epoch 6922/40000, Loss: 3.8551206671399996e-05, Learning Rate: 0.000872\n",
      "Epoch 6923/40000, Loss: 6.334688077913597e-05, Learning Rate: 0.000871\n",
      "Epoch 6924/40000, Loss: 0.00010584411938907579, Learning Rate: 0.000871\n",
      "Epoch 6925/40000, Loss: 3.5700730222743005e-05, Learning Rate: 0.000871\n",
      "Epoch 6926/40000, Loss: 7.456572348019108e-05, Learning Rate: 0.000871\n",
      "Epoch 6927/40000, Loss: 7.314994581975043e-05, Learning Rate: 0.000871\n",
      "Epoch 6928/40000, Loss: 9.814196528168395e-05, Learning Rate: 0.000871\n",
      "Epoch 6929/40000, Loss: 0.00010409091191831976, Learning Rate: 0.000871\n",
      "Epoch 6930/40000, Loss: 0.0001215227457578294, Learning Rate: 0.000871\n",
      "Epoch 6931/40000, Loss: 7.357652066275477e-05, Learning Rate: 0.000871\n",
      "Epoch 6932/40000, Loss: 0.00010749208740890026, Learning Rate: 0.000870\n",
      "Epoch 6933/40000, Loss: 6.832667713752016e-05, Learning Rate: 0.000870\n",
      "Epoch 6934/40000, Loss: 0.00010198204108746722, Learning Rate: 0.000870\n",
      "Epoch 6935/40000, Loss: 8.370459545403719e-05, Learning Rate: 0.000870\n",
      "Epoch 6936/40000, Loss: 6.0949914768571034e-05, Learning Rate: 0.000870\n",
      "Epoch 6937/40000, Loss: 3.695887789945118e-05, Learning Rate: 0.000870\n",
      "Epoch 6938/40000, Loss: 3.1183601095108315e-05, Learning Rate: 0.000870\n",
      "Epoch 6939/40000, Loss: 0.00010769639629870653, Learning Rate: 0.000870\n",
      "Epoch 6940/40000, Loss: 6.717768701491877e-05, Learning Rate: 0.000870\n",
      "Epoch 6941/40000, Loss: 0.00010265184391755611, Learning Rate: 0.000870\n",
      "Epoch 6942/40000, Loss: 9.664915705798194e-05, Learning Rate: 0.000869\n",
      "Epoch 6943/40000, Loss: 3.439363354118541e-05, Learning Rate: 0.000869\n",
      "Epoch 6944/40000, Loss: 0.00010311962250852957, Learning Rate: 0.000869\n",
      "Epoch 6945/40000, Loss: 6.024998219800182e-05, Learning Rate: 0.000869\n",
      "Epoch 6946/40000, Loss: 2.966954889416229e-05, Learning Rate: 0.000869\n",
      "Epoch 6947/40000, Loss: 2.9839407943654805e-05, Learning Rate: 0.000869\n",
      "Epoch 6948/40000, Loss: 0.00010296260006725788, Learning Rate: 0.000869\n",
      "Epoch 6949/40000, Loss: 6.34889947832562e-05, Learning Rate: 0.000869\n",
      "Epoch 6950/40000, Loss: 9.897669951897115e-05, Learning Rate: 0.000869\n",
      "Epoch 6951/40000, Loss: 6.025249240337871e-05, Learning Rate: 0.000869\n",
      "Epoch 6952/40000, Loss: 7.455694867530838e-05, Learning Rate: 0.000868\n",
      "Epoch 6953/40000, Loss: 5.7275254221167415e-05, Learning Rate: 0.000868\n",
      "Epoch 6954/40000, Loss: 5.640932795358822e-05, Learning Rate: 0.000868\n",
      "Epoch 6955/40000, Loss: 9.425738244317472e-05, Learning Rate: 0.000868\n",
      "Epoch 6956/40000, Loss: 2.9613836886710487e-05, Learning Rate: 0.000868\n",
      "Epoch 6957/40000, Loss: 6.178984767757356e-05, Learning Rate: 0.000868\n",
      "Epoch 6958/40000, Loss: 9.325512655777857e-05, Learning Rate: 0.000868\n",
      "Epoch 6959/40000, Loss: 2.8758377084159292e-05, Learning Rate: 0.000868\n",
      "Epoch 6960/40000, Loss: 0.0001015192101476714, Learning Rate: 0.000868\n",
      "Epoch 6961/40000, Loss: 5.705582589143887e-05, Learning Rate: 0.000867\n",
      "Epoch 6962/40000, Loss: 2.8961732823518105e-05, Learning Rate: 0.000867\n",
      "Epoch 6963/40000, Loss: 2.8642793040489778e-05, Learning Rate: 0.000867\n",
      "Epoch 6964/40000, Loss: 9.20993261388503e-05, Learning Rate: 0.000867\n",
      "Epoch 6965/40000, Loss: 5.678494198946282e-05, Learning Rate: 0.000867\n",
      "Epoch 6966/40000, Loss: 6.03829394094646e-05, Learning Rate: 0.000867\n",
      "Epoch 6967/40000, Loss: 6.040697553544305e-05, Learning Rate: 0.000867\n",
      "Epoch 6968/40000, Loss: 0.00010140733502339572, Learning Rate: 0.000867\n",
      "Epoch 6969/40000, Loss: 7.110543083399534e-05, Learning Rate: 0.000867\n",
      "Epoch 6970/40000, Loss: 0.00010083449888043106, Learning Rate: 0.000867\n",
      "Epoch 6971/40000, Loss: 0.00010087883129017428, Learning Rate: 0.000866\n",
      "Epoch 6972/40000, Loss: 9.209226118400693e-05, Learning Rate: 0.000866\n",
      "Epoch 6973/40000, Loss: 7.221401028800756e-05, Learning Rate: 0.000866\n",
      "Epoch 6974/40000, Loss: 5.6829147069947794e-05, Learning Rate: 0.000866\n",
      "Epoch 6975/40000, Loss: 0.00010147977445740253, Learning Rate: 0.000866\n",
      "Epoch 6976/40000, Loss: 0.00010154231131309643, Learning Rate: 0.000866\n",
      "Epoch 6977/40000, Loss: 0.00010121503873961046, Learning Rate: 0.000866\n",
      "Epoch 6978/40000, Loss: 2.9232294764369726e-05, Learning Rate: 0.000866\n",
      "Epoch 6979/40000, Loss: 2.8487524105003104e-05, Learning Rate: 0.000866\n",
      "Epoch 6980/40000, Loss: 6.227532139746472e-05, Learning Rate: 0.000865\n",
      "Epoch 6981/40000, Loss: 6.33121162536554e-05, Learning Rate: 0.000865\n",
      "Epoch 6982/40000, Loss: 7.160178211051971e-05, Learning Rate: 0.000865\n",
      "Epoch 6983/40000, Loss: 7.272364746313542e-05, Learning Rate: 0.000865\n",
      "Epoch 6984/40000, Loss: 0.00010416354052722454, Learning Rate: 0.000865\n",
      "Epoch 6985/40000, Loss: 7.283358718268573e-05, Learning Rate: 0.000865\n",
      "Epoch 6986/40000, Loss: 6.267685239436105e-05, Learning Rate: 0.000865\n",
      "Epoch 6987/40000, Loss: 6.023888272466138e-05, Learning Rate: 0.000865\n",
      "Epoch 6988/40000, Loss: 5.941683048149571e-05, Learning Rate: 0.000865\n",
      "Epoch 6989/40000, Loss: 0.00010747527267085388, Learning Rate: 0.000865\n",
      "Epoch 6990/40000, Loss: 0.00010552512685535476, Learning Rate: 0.000864\n",
      "Epoch 6991/40000, Loss: 6.1016460676910356e-05, Learning Rate: 0.000864\n",
      "Epoch 6992/40000, Loss: 7.832809933461249e-05, Learning Rate: 0.000864\n",
      "Epoch 6993/40000, Loss: 9.563697676640004e-05, Learning Rate: 0.000864\n",
      "Epoch 6994/40000, Loss: 0.0001027428443194367, Learning Rate: 0.000864\n",
      "Epoch 6995/40000, Loss: 9.246937406715006e-05, Learning Rate: 0.000864\n",
      "Epoch 6996/40000, Loss: 5.7873512560036033e-05, Learning Rate: 0.000864\n",
      "Epoch 6997/40000, Loss: 2.9258480935823172e-05, Learning Rate: 0.000864\n",
      "Epoch 6998/40000, Loss: 9.40370955504477e-05, Learning Rate: 0.000864\n",
      "Epoch 6999/40000, Loss: 6.305162241915241e-05, Learning Rate: 0.000864\n",
      "Epoch 7000/40000, Loss: 9.308427979703993e-05, Learning Rate: 0.000863\n",
      "Epoch 7001/40000, Loss: 7.322557212319225e-05, Learning Rate: 0.000863\n",
      "Epoch 7002/40000, Loss: 6.326226866804063e-05, Learning Rate: 0.000863\n",
      "Epoch 7003/40000, Loss: 3.0268864065874368e-05, Learning Rate: 0.000863\n",
      "Epoch 7004/40000, Loss: 2.9238552087917924e-05, Learning Rate: 0.000863\n",
      "Epoch 7005/40000, Loss: 9.524437336949632e-05, Learning Rate: 0.000863\n",
      "Epoch 7006/40000, Loss: 2.9765033104922622e-05, Learning Rate: 0.000863\n",
      "Epoch 7007/40000, Loss: 9.549443348078057e-05, Learning Rate: 0.000863\n",
      "Epoch 7008/40000, Loss: 0.00010745757754193619, Learning Rate: 0.000863\n",
      "Epoch 7009/40000, Loss: 3.283703335910104e-05, Learning Rate: 0.000862\n",
      "Epoch 7010/40000, Loss: 5.8364577853353694e-05, Learning Rate: 0.000862\n",
      "Epoch 7011/40000, Loss: 4.010976044810377e-05, Learning Rate: 0.000862\n",
      "Epoch 7012/40000, Loss: 0.000104967737570405, Learning Rate: 0.000862\n",
      "Epoch 7013/40000, Loss: 3.352397106937133e-05, Learning Rate: 0.000862\n",
      "Epoch 7014/40000, Loss: 0.000107061758171767, Learning Rate: 0.000862\n",
      "Epoch 7015/40000, Loss: 4.289436401450075e-05, Learning Rate: 0.000862\n",
      "Epoch 7016/40000, Loss: 7.688924961257726e-05, Learning Rate: 0.000862\n",
      "Epoch 7017/40000, Loss: 7.82914794399403e-05, Learning Rate: 0.000862\n",
      "Epoch 7018/40000, Loss: 0.0001839329779613763, Learning Rate: 0.000862\n",
      "Epoch 7019/40000, Loss: 8.31984361866489e-05, Learning Rate: 0.000861\n",
      "Epoch 7020/40000, Loss: 8.501360571244732e-05, Learning Rate: 0.000861\n",
      "Epoch 7021/40000, Loss: 7.092294981703162e-05, Learning Rate: 0.000861\n",
      "Epoch 7022/40000, Loss: 7.06398714100942e-05, Learning Rate: 0.000861\n",
      "Epoch 7023/40000, Loss: 0.00011041042307624593, Learning Rate: 0.000861\n",
      "Epoch 7024/40000, Loss: 8.175347466021776e-05, Learning Rate: 0.000861\n",
      "Epoch 7025/40000, Loss: 4.077880294062197e-05, Learning Rate: 0.000861\n",
      "Epoch 7026/40000, Loss: 0.00014432737953029573, Learning Rate: 0.000861\n",
      "Epoch 7027/40000, Loss: 0.00011627898493316025, Learning Rate: 0.000861\n",
      "Epoch 7028/40000, Loss: 0.00012049802899127826, Learning Rate: 0.000861\n",
      "Epoch 7029/40000, Loss: 0.00014332322461996228, Learning Rate: 0.000860\n",
      "Epoch 7030/40000, Loss: 8.186015475075692e-05, Learning Rate: 0.000860\n",
      "Epoch 7031/40000, Loss: 9.098248847294599e-05, Learning Rate: 0.000860\n",
      "Epoch 7032/40000, Loss: 0.00012060199514962733, Learning Rate: 0.000860\n",
      "Epoch 7033/40000, Loss: 0.00020041178504470736, Learning Rate: 0.000860\n",
      "Epoch 7034/40000, Loss: 0.0001300723379245028, Learning Rate: 0.000860\n",
      "Epoch 7035/40000, Loss: 0.00011189531505806372, Learning Rate: 0.000860\n",
      "Epoch 7036/40000, Loss: 0.00011267906666034833, Learning Rate: 0.000860\n",
      "Epoch 7037/40000, Loss: 9.548373054713011e-05, Learning Rate: 0.000860\n",
      "Epoch 7038/40000, Loss: 7.24289202480577e-05, Learning Rate: 0.000859\n",
      "Epoch 7039/40000, Loss: 0.0002846206771209836, Learning Rate: 0.000859\n",
      "Epoch 7040/40000, Loss: 7.45972793083638e-05, Learning Rate: 0.000859\n",
      "Epoch 7041/40000, Loss: 0.00013916486932430416, Learning Rate: 0.000859\n",
      "Epoch 7042/40000, Loss: 0.00011467206059023738, Learning Rate: 0.000859\n",
      "Epoch 7043/40000, Loss: 0.00015966285718604922, Learning Rate: 0.000859\n",
      "Epoch 7044/40000, Loss: 0.00012777633673977107, Learning Rate: 0.000859\n",
      "Epoch 7045/40000, Loss: 0.0001365725911455229, Learning Rate: 0.000859\n",
      "Epoch 7046/40000, Loss: 7.241412095027044e-05, Learning Rate: 0.000859\n",
      "Epoch 7047/40000, Loss: 0.00012363922724034637, Learning Rate: 0.000859\n",
      "Epoch 7048/40000, Loss: 0.00011824408284155652, Learning Rate: 0.000858\n",
      "Epoch 7049/40000, Loss: 6.891624070703983e-05, Learning Rate: 0.000858\n",
      "Epoch 7050/40000, Loss: 0.00010117603960679844, Learning Rate: 0.000858\n",
      "Epoch 7051/40000, Loss: 8.081288979155943e-05, Learning Rate: 0.000858\n",
      "Epoch 7052/40000, Loss: 0.00011800255015259609, Learning Rate: 0.000858\n",
      "Epoch 7053/40000, Loss: 0.00014628810458816588, Learning Rate: 0.000858\n",
      "Epoch 7054/40000, Loss: 0.0001048075791914016, Learning Rate: 0.000858\n",
      "Epoch 7055/40000, Loss: 8.392804738832638e-05, Learning Rate: 0.000858\n",
      "Epoch 7056/40000, Loss: 0.00016098516061902046, Learning Rate: 0.000858\n",
      "Epoch 7057/40000, Loss: 0.00018864672165364027, Learning Rate: 0.000858\n",
      "Epoch 7058/40000, Loss: 0.00012202825746499002, Learning Rate: 0.000857\n",
      "Epoch 7059/40000, Loss: 0.000114376176497899, Learning Rate: 0.000857\n",
      "Epoch 7060/40000, Loss: 6.713662878610194e-05, Learning Rate: 0.000857\n",
      "Epoch 7061/40000, Loss: 8.88550202944316e-05, Learning Rate: 0.000857\n",
      "Epoch 7062/40000, Loss: 8.67067210492678e-05, Learning Rate: 0.000857\n",
      "Epoch 7063/40000, Loss: 8.831816376186907e-05, Learning Rate: 0.000857\n",
      "Epoch 7064/40000, Loss: 0.0001042832009261474, Learning Rate: 0.000857\n",
      "Epoch 7065/40000, Loss: 0.00012532793334685266, Learning Rate: 0.000857\n",
      "Epoch 7066/40000, Loss: 0.00014369988639373332, Learning Rate: 0.000857\n",
      "Epoch 7067/40000, Loss: 0.0001030051862471737, Learning Rate: 0.000856\n",
      "Epoch 7068/40000, Loss: 8.361239451915026e-05, Learning Rate: 0.000856\n",
      "Epoch 7069/40000, Loss: 0.0001366877113468945, Learning Rate: 0.000856\n",
      "Epoch 7070/40000, Loss: 0.00010983563697664067, Learning Rate: 0.000856\n",
      "Epoch 7071/40000, Loss: 0.000272541306912899, Learning Rate: 0.000856\n",
      "Epoch 7072/40000, Loss: 0.00015039830759633332, Learning Rate: 0.000856\n",
      "Epoch 7073/40000, Loss: 7.717038533883169e-05, Learning Rate: 0.000856\n",
      "Epoch 7074/40000, Loss: 8.241882460424677e-05, Learning Rate: 0.000856\n",
      "Epoch 7075/40000, Loss: 9.434280946152285e-05, Learning Rate: 0.000856\n",
      "Epoch 7076/40000, Loss: 0.00011423998512327671, Learning Rate: 0.000856\n",
      "Epoch 7077/40000, Loss: 7.295449904631823e-05, Learning Rate: 0.000855\n",
      "Epoch 7078/40000, Loss: 0.00010850442049559206, Learning Rate: 0.000855\n",
      "Epoch 7079/40000, Loss: 0.00010679969273041934, Learning Rate: 0.000855\n",
      "Epoch 7080/40000, Loss: 9.970580867957324e-05, Learning Rate: 0.000855\n",
      "Epoch 7081/40000, Loss: 3.4489617974031717e-05, Learning Rate: 0.000855\n",
      "Epoch 7082/40000, Loss: 0.0001186109147965908, Learning Rate: 0.000855\n",
      "Epoch 7083/40000, Loss: 6.826295430073515e-05, Learning Rate: 0.000855\n",
      "Epoch 7084/40000, Loss: 7.933513552416116e-05, Learning Rate: 0.000855\n",
      "Epoch 7085/40000, Loss: 7.345963967964053e-05, Learning Rate: 0.000855\n",
      "Epoch 7086/40000, Loss: 5.823511310154572e-05, Learning Rate: 0.000855\n",
      "Epoch 7087/40000, Loss: 2.911303090513684e-05, Learning Rate: 0.000854\n",
      "Epoch 7088/40000, Loss: 2.8259408281883225e-05, Learning Rate: 0.000854\n",
      "Epoch 7089/40000, Loss: 7.170885510277003e-05, Learning Rate: 0.000854\n",
      "Epoch 7090/40000, Loss: 6.188872794155031e-05, Learning Rate: 0.000854\n",
      "Epoch 7091/40000, Loss: 9.338658128399402e-05, Learning Rate: 0.000854\n",
      "Epoch 7092/40000, Loss: 5.6286742619704455e-05, Learning Rate: 0.000854\n",
      "Epoch 7093/40000, Loss: 7.034999725874513e-05, Learning Rate: 0.000854\n",
      "Epoch 7094/40000, Loss: 7.051241118460894e-05, Learning Rate: 0.000854\n",
      "Epoch 7095/40000, Loss: 6.990432302700356e-05, Learning Rate: 0.000854\n",
      "Epoch 7096/40000, Loss: 6.996789306867868e-05, Learning Rate: 0.000854\n",
      "Epoch 7097/40000, Loss: 6.013591701048426e-05, Learning Rate: 0.000853\n",
      "Epoch 7098/40000, Loss: 5.9991387388436124e-05, Learning Rate: 0.000853\n",
      "Epoch 7099/40000, Loss: 9.106065408559516e-05, Learning Rate: 0.000853\n",
      "Epoch 7100/40000, Loss: 7.051049033179879e-05, Learning Rate: 0.000853\n",
      "Epoch 7101/40000, Loss: 9.211927681462839e-05, Learning Rate: 0.000853\n",
      "Epoch 7102/40000, Loss: 2.795040563796647e-05, Learning Rate: 0.000853\n",
      "Epoch 7103/40000, Loss: 0.00010099046630784869, Learning Rate: 0.000853\n",
      "Epoch 7104/40000, Loss: 9.076925198314711e-05, Learning Rate: 0.000853\n",
      "Epoch 7105/40000, Loss: 9.069170482689515e-05, Learning Rate: 0.000853\n",
      "Epoch 7106/40000, Loss: 9.02528190636076e-05, Learning Rate: 0.000853\n",
      "Epoch 7107/40000, Loss: 0.00010042111534858122, Learning Rate: 0.000852\n",
      "Epoch 7108/40000, Loss: 5.5829350458225235e-05, Learning Rate: 0.000852\n",
      "Epoch 7109/40000, Loss: 5.912288906984031e-05, Learning Rate: 0.000852\n",
      "Epoch 7110/40000, Loss: 6.966537330299616e-05, Learning Rate: 0.000852\n",
      "Epoch 7111/40000, Loss: 6.950649549253285e-05, Learning Rate: 0.000852\n",
      "Epoch 7112/40000, Loss: 0.00010032897989731282, Learning Rate: 0.000852\n",
      "Epoch 7113/40000, Loss: 6.947164365556091e-05, Learning Rate: 0.000852\n",
      "Epoch 7114/40000, Loss: 5.92843207414262e-05, Learning Rate: 0.000852\n",
      "Epoch 7115/40000, Loss: 5.880009848624468e-05, Learning Rate: 0.000852\n",
      "Epoch 7116/40000, Loss: 9.02147003216669e-05, Learning Rate: 0.000851\n",
      "Epoch 7117/40000, Loss: 9.007297921925783e-05, Learning Rate: 0.000851\n",
      "Epoch 7118/40000, Loss: 5.8753896155394614e-05, Learning Rate: 0.000851\n",
      "Epoch 7119/40000, Loss: 6.92656758474186e-05, Learning Rate: 0.000851\n",
      "Epoch 7120/40000, Loss: 9.988115198211744e-05, Learning Rate: 0.000851\n",
      "Epoch 7121/40000, Loss: 5.520370541489683e-05, Learning Rate: 0.000851\n",
      "Epoch 7122/40000, Loss: 2.7088924980489537e-05, Learning Rate: 0.000851\n",
      "Epoch 7123/40000, Loss: 6.981840124353766e-05, Learning Rate: 0.000851\n",
      "Epoch 7124/40000, Loss: 5.914399662287906e-05, Learning Rate: 0.000851\n",
      "Epoch 7125/40000, Loss: 2.746268546616193e-05, Learning Rate: 0.000851\n",
      "Epoch 7126/40000, Loss: 5.555949610425159e-05, Learning Rate: 0.000850\n",
      "Epoch 7127/40000, Loss: 0.00010263540025334805, Learning Rate: 0.000850\n",
      "Epoch 7128/40000, Loss: 9.155405859928578e-05, Learning Rate: 0.000850\n",
      "Epoch 7129/40000, Loss: 6.996587035246193e-05, Learning Rate: 0.000850\n",
      "Epoch 7130/40000, Loss: 9.10458475118503e-05, Learning Rate: 0.000850\n",
      "Epoch 7131/40000, Loss: 6.04107954131905e-05, Learning Rate: 0.000850\n",
      "Epoch 7132/40000, Loss: 6.997475429670885e-05, Learning Rate: 0.000850\n",
      "Epoch 7133/40000, Loss: 9.222025983035564e-05, Learning Rate: 0.000850\n",
      "Epoch 7134/40000, Loss: 6.340222898870707e-05, Learning Rate: 0.000850\n",
      "Epoch 7135/40000, Loss: 7.281993021024391e-05, Learning Rate: 0.000850\n",
      "Epoch 7136/40000, Loss: 9.968433732865378e-05, Learning Rate: 0.000849\n",
      "Epoch 7137/40000, Loss: 7.614798232680187e-05, Learning Rate: 0.000849\n",
      "Epoch 7138/40000, Loss: 5.7073171774391085e-05, Learning Rate: 0.000849\n",
      "Epoch 7139/40000, Loss: 5.90754316363018e-05, Learning Rate: 0.000849\n",
      "Epoch 7140/40000, Loss: 0.00010018863395089284, Learning Rate: 0.000849\n",
      "Epoch 7141/40000, Loss: 6.874118116684258e-05, Learning Rate: 0.000849\n",
      "Epoch 7142/40000, Loss: 6.613254663534462e-05, Learning Rate: 0.000849\n",
      "Epoch 7143/40000, Loss: 0.00012927297211717814, Learning Rate: 0.000849\n",
      "Epoch 7144/40000, Loss: 7.937700138427317e-05, Learning Rate: 0.000849\n",
      "Epoch 7145/40000, Loss: 4.5251981646288186e-05, Learning Rate: 0.000849\n",
      "Epoch 7146/40000, Loss: 3.997259773313999e-05, Learning Rate: 0.000848\n",
      "Epoch 7147/40000, Loss: 0.00012344130664132535, Learning Rate: 0.000848\n",
      "Epoch 7148/40000, Loss: 5.935847730142996e-05, Learning Rate: 0.000848\n",
      "Epoch 7149/40000, Loss: 8.446321589872241e-05, Learning Rate: 0.000848\n",
      "Epoch 7150/40000, Loss: 8.619284199085087e-05, Learning Rate: 0.000848\n",
      "Epoch 7151/40000, Loss: 7.395695865852758e-05, Learning Rate: 0.000848\n",
      "Epoch 7152/40000, Loss: 4.915024328511208e-05, Learning Rate: 0.000848\n",
      "Epoch 7153/40000, Loss: 7.796237332513556e-05, Learning Rate: 0.000848\n",
      "Epoch 7154/40000, Loss: 9.556961595080793e-05, Learning Rate: 0.000848\n",
      "Epoch 7155/40000, Loss: 0.00010239533003186807, Learning Rate: 0.000848\n",
      "Epoch 7156/40000, Loss: 0.0001392596896039322, Learning Rate: 0.000847\n",
      "Epoch 7157/40000, Loss: 0.00011193643149454147, Learning Rate: 0.000847\n",
      "Epoch 7158/40000, Loss: 0.00010247447789879516, Learning Rate: 0.000847\n",
      "Epoch 7159/40000, Loss: 0.0002269092365168035, Learning Rate: 0.000847\n",
      "Epoch 7160/40000, Loss: 0.00014300181646831334, Learning Rate: 0.000847\n",
      "Epoch 7161/40000, Loss: 0.0001099987857742235, Learning Rate: 0.000847\n",
      "Epoch 7162/40000, Loss: 9.213914745487273e-05, Learning Rate: 0.000847\n",
      "Epoch 7163/40000, Loss: 9.654360474087298e-05, Learning Rate: 0.000847\n",
      "Epoch 7164/40000, Loss: 0.00012710169539786875, Learning Rate: 0.000847\n",
      "Epoch 7165/40000, Loss: 6.36128315818496e-05, Learning Rate: 0.000846\n",
      "Epoch 7166/40000, Loss: 0.00020069530000910163, Learning Rate: 0.000846\n",
      "Epoch 7167/40000, Loss: 0.00015238703053910285, Learning Rate: 0.000846\n",
      "Epoch 7168/40000, Loss: 0.0001162661865237169, Learning Rate: 0.000846\n",
      "Epoch 7169/40000, Loss: 7.474180893041193e-05, Learning Rate: 0.000846\n",
      "Epoch 7170/40000, Loss: 0.00011692993575707078, Learning Rate: 0.000846\n",
      "Epoch 7171/40000, Loss: 6.522829789901152e-05, Learning Rate: 0.000846\n",
      "Epoch 7172/40000, Loss: 7.866820669732988e-05, Learning Rate: 0.000846\n",
      "Epoch 7173/40000, Loss: 0.00010333967657061294, Learning Rate: 0.000846\n",
      "Epoch 7174/40000, Loss: 0.00010337028652429581, Learning Rate: 0.000846\n",
      "Epoch 7175/40000, Loss: 0.00010155776544706896, Learning Rate: 0.000845\n",
      "Epoch 7176/40000, Loss: 2.9904666007496417e-05, Learning Rate: 0.000845\n",
      "Epoch 7177/40000, Loss: 5.72331809962634e-05, Learning Rate: 0.000845\n",
      "Epoch 7178/40000, Loss: 5.578821219387464e-05, Learning Rate: 0.000845\n",
      "Epoch 7179/40000, Loss: 2.8436119464458898e-05, Learning Rate: 0.000845\n",
      "Epoch 7180/40000, Loss: 7.065776298986748e-05, Learning Rate: 0.000845\n",
      "Epoch 7181/40000, Loss: 3.109794488409534e-05, Learning Rate: 0.000845\n",
      "Epoch 7182/40000, Loss: 0.0001021179195959121, Learning Rate: 0.000845\n",
      "Epoch 7183/40000, Loss: 5.6210461480077356e-05, Learning Rate: 0.000845\n",
      "Epoch 7184/40000, Loss: 6.13228912698105e-05, Learning Rate: 0.000845\n",
      "Epoch 7185/40000, Loss: 2.9320446628844365e-05, Learning Rate: 0.000844\n",
      "Epoch 7186/40000, Loss: 2.7312573365634307e-05, Learning Rate: 0.000844\n",
      "Epoch 7187/40000, Loss: 0.00010344226757297292, Learning Rate: 0.000844\n",
      "Epoch 7188/40000, Loss: 2.9997199817444198e-05, Learning Rate: 0.000844\n",
      "Epoch 7189/40000, Loss: 6.490142550319433e-05, Learning Rate: 0.000844\n",
      "Epoch 7190/40000, Loss: 5.703681381419301e-05, Learning Rate: 0.000844\n",
      "Epoch 7191/40000, Loss: 6.036763807060197e-05, Learning Rate: 0.000844\n",
      "Epoch 7192/40000, Loss: 3.1662922992836684e-05, Learning Rate: 0.000844\n",
      "Epoch 7193/40000, Loss: 0.00010200712131336331, Learning Rate: 0.000844\n",
      "Epoch 7194/40000, Loss: 6.579353066626936e-05, Learning Rate: 0.000844\n",
      "Epoch 7195/40000, Loss: 9.34720374061726e-05, Learning Rate: 0.000843\n",
      "Epoch 7196/40000, Loss: 5.7273777201771736e-05, Learning Rate: 0.000843\n",
      "Epoch 7197/40000, Loss: 7.209261821117252e-05, Learning Rate: 0.000843\n",
      "Epoch 7198/40000, Loss: 6.265166302910075e-05, Learning Rate: 0.000843\n",
      "Epoch 7199/40000, Loss: 9.640469215810299e-05, Learning Rate: 0.000843\n",
      "Epoch 7200/40000, Loss: 7.507861300837249e-05, Learning Rate: 0.000843\n",
      "Epoch 7201/40000, Loss: 7.061029464239255e-05, Learning Rate: 0.000843\n",
      "Epoch 7202/40000, Loss: 0.00011801562504842877, Learning Rate: 0.000843\n",
      "Epoch 7203/40000, Loss: 0.00011106416786788031, Learning Rate: 0.000843\n",
      "Epoch 7204/40000, Loss: 9.573830175213516e-05, Learning Rate: 0.000843\n",
      "Epoch 7205/40000, Loss: 3.131126504740678e-05, Learning Rate: 0.000842\n",
      "Epoch 7206/40000, Loss: 6.305659917416051e-05, Learning Rate: 0.000842\n",
      "Epoch 7207/40000, Loss: 5.920336479903199e-05, Learning Rate: 0.000842\n",
      "Epoch 7208/40000, Loss: 7.06041173543781e-05, Learning Rate: 0.000842\n",
      "Epoch 7209/40000, Loss: 6.980820035096258e-05, Learning Rate: 0.000842\n",
      "Epoch 7210/40000, Loss: 5.740187771152705e-05, Learning Rate: 0.000842\n",
      "Epoch 7211/40000, Loss: 2.798447894747369e-05, Learning Rate: 0.000842\n",
      "Epoch 7212/40000, Loss: 2.774608219624497e-05, Learning Rate: 0.000842\n",
      "Epoch 7213/40000, Loss: 5.7008863223018125e-05, Learning Rate: 0.000842\n",
      "Epoch 7214/40000, Loss: 9.207343100570142e-05, Learning Rate: 0.000842\n",
      "Epoch 7215/40000, Loss: 9.254164615413174e-05, Learning Rate: 0.000841\n",
      "Epoch 7216/40000, Loss: 9.203415538650006e-05, Learning Rate: 0.000841\n",
      "Epoch 7217/40000, Loss: 0.0001021695279632695, Learning Rate: 0.000841\n",
      "Epoch 7218/40000, Loss: 7.032962457742542e-05, Learning Rate: 0.000841\n",
      "Epoch 7219/40000, Loss: 5.764171146438457e-05, Learning Rate: 0.000841\n",
      "Epoch 7220/40000, Loss: 5.699230678146705e-05, Learning Rate: 0.000841\n",
      "Epoch 7221/40000, Loss: 9.484546899329871e-05, Learning Rate: 0.000841\n",
      "Epoch 7222/40000, Loss: 3.076775465160608e-05, Learning Rate: 0.000841\n",
      "Epoch 7223/40000, Loss: 7.964702672325075e-05, Learning Rate: 0.000841\n",
      "Epoch 7224/40000, Loss: 0.00014696344442199916, Learning Rate: 0.000841\n",
      "Epoch 7225/40000, Loss: 0.00011299065954517573, Learning Rate: 0.000840\n",
      "Epoch 7226/40000, Loss: 8.318906475324184e-05, Learning Rate: 0.000840\n",
      "Epoch 7227/40000, Loss: 3.320737596368417e-05, Learning Rate: 0.000840\n",
      "Epoch 7228/40000, Loss: 6.919961742823943e-05, Learning Rate: 0.000840\n",
      "Epoch 7229/40000, Loss: 7.692621875321493e-05, Learning Rate: 0.000840\n",
      "Epoch 7230/40000, Loss: 3.455210026004352e-05, Learning Rate: 0.000840\n",
      "Epoch 7231/40000, Loss: 0.00010731047223089263, Learning Rate: 0.000840\n",
      "Epoch 7232/40000, Loss: 8.714612340554595e-05, Learning Rate: 0.000840\n",
      "Epoch 7233/40000, Loss: 3.6954006645828485e-05, Learning Rate: 0.000840\n",
      "Epoch 7234/40000, Loss: 0.00011152848310302943, Learning Rate: 0.000840\n",
      "Epoch 7235/40000, Loss: 8.778402843745425e-05, Learning Rate: 0.000839\n",
      "Epoch 7236/40000, Loss: 6.207567639648914e-05, Learning Rate: 0.000839\n",
      "Epoch 7237/40000, Loss: 3.211835428373888e-05, Learning Rate: 0.000839\n",
      "Epoch 7238/40000, Loss: 0.0001076888365787454, Learning Rate: 0.000839\n",
      "Epoch 7239/40000, Loss: 6.696264608763158e-05, Learning Rate: 0.000839\n",
      "Epoch 7240/40000, Loss: 8.648581570014358e-05, Learning Rate: 0.000839\n",
      "Epoch 7241/40000, Loss: 0.00012035755935357884, Learning Rate: 0.000839\n",
      "Epoch 7242/40000, Loss: 0.0001414329162798822, Learning Rate: 0.000839\n",
      "Epoch 7243/40000, Loss: 4.47528509539552e-05, Learning Rate: 0.000839\n",
      "Epoch 7244/40000, Loss: 4.7164096031337976e-05, Learning Rate: 0.000838\n",
      "Epoch 7245/40000, Loss: 6.808048055972904e-05, Learning Rate: 0.000838\n",
      "Epoch 7246/40000, Loss: 9.740581299411133e-05, Learning Rate: 0.000838\n",
      "Epoch 7247/40000, Loss: 0.00015104352496564388, Learning Rate: 0.000838\n",
      "Epoch 7248/40000, Loss: 0.0001461245701648295, Learning Rate: 0.000838\n",
      "Epoch 7249/40000, Loss: 0.0001019498668028973, Learning Rate: 0.000838\n",
      "Epoch 7250/40000, Loss: 9.12704344955273e-05, Learning Rate: 0.000838\n",
      "Epoch 7251/40000, Loss: 0.0001345271011814475, Learning Rate: 0.000838\n",
      "Epoch 7252/40000, Loss: 0.000162114214617759, Learning Rate: 0.000838\n",
      "Epoch 7253/40000, Loss: 0.00014844132238067687, Learning Rate: 0.000838\n",
      "Epoch 7254/40000, Loss: 0.00019766323384828866, Learning Rate: 0.000837\n",
      "Epoch 7255/40000, Loss: 5.683873678208329e-05, Learning Rate: 0.000837\n",
      "Epoch 7256/40000, Loss: 0.0001203625652124174, Learning Rate: 0.000837\n",
      "Epoch 7257/40000, Loss: 8.396768680540845e-05, Learning Rate: 0.000837\n",
      "Epoch 7258/40000, Loss: 0.0001131729586631991, Learning Rate: 0.000837\n",
      "Epoch 7259/40000, Loss: 6.538355228258297e-05, Learning Rate: 0.000837\n",
      "Epoch 7260/40000, Loss: 8.077388338278979e-05, Learning Rate: 0.000837\n",
      "Epoch 7261/40000, Loss: 8.458697266178206e-05, Learning Rate: 0.000837\n",
      "Epoch 7262/40000, Loss: 8.96822166396305e-05, Learning Rate: 0.000837\n",
      "Epoch 7263/40000, Loss: 9.100365423364565e-05, Learning Rate: 0.000837\n",
      "Epoch 7264/40000, Loss: 0.00012618984328582883, Learning Rate: 0.000836\n",
      "Epoch 7265/40000, Loss: 8.475309732602909e-05, Learning Rate: 0.000836\n",
      "Epoch 7266/40000, Loss: 0.00011599705612752587, Learning Rate: 0.000836\n",
      "Epoch 7267/40000, Loss: 6.413365190383047e-05, Learning Rate: 0.000836\n",
      "Epoch 7268/40000, Loss: 8.748231630306691e-05, Learning Rate: 0.000836\n",
      "Epoch 7269/40000, Loss: 0.00011067558807553723, Learning Rate: 0.000836\n",
      "Epoch 7270/40000, Loss: 0.00010415732685942203, Learning Rate: 0.000836\n",
      "Epoch 7271/40000, Loss: 8.965837332652882e-05, Learning Rate: 0.000836\n",
      "Epoch 7272/40000, Loss: 8.738126052776352e-05, Learning Rate: 0.000836\n",
      "Epoch 7273/40000, Loss: 0.00011859524238388985, Learning Rate: 0.000836\n",
      "Epoch 7274/40000, Loss: 7.91769489296712e-05, Learning Rate: 0.000835\n",
      "Epoch 7275/40000, Loss: 0.00020160377607680857, Learning Rate: 0.000835\n",
      "Epoch 7276/40000, Loss: 8.184729085769504e-05, Learning Rate: 0.000835\n",
      "Epoch 7277/40000, Loss: 5.821358718094416e-05, Learning Rate: 0.000835\n",
      "Epoch 7278/40000, Loss: 9.371958003612235e-05, Learning Rate: 0.000835\n",
      "Epoch 7279/40000, Loss: 9.600869816495106e-05, Learning Rate: 0.000835\n",
      "Epoch 7280/40000, Loss: 0.0001570481399539858, Learning Rate: 0.000835\n",
      "Epoch 7281/40000, Loss: 0.00016413901175837964, Learning Rate: 0.000835\n",
      "Epoch 7282/40000, Loss: 0.00016586665879003704, Learning Rate: 0.000835\n",
      "Epoch 7283/40000, Loss: 4.693296432378702e-05, Learning Rate: 0.000835\n",
      "Epoch 7284/40000, Loss: 0.000110925808257889, Learning Rate: 0.000834\n",
      "Epoch 7285/40000, Loss: 0.00015338321099989116, Learning Rate: 0.000834\n",
      "Epoch 7286/40000, Loss: 0.00011909787281183526, Learning Rate: 0.000834\n",
      "Epoch 7287/40000, Loss: 0.00017363062943331897, Learning Rate: 0.000834\n",
      "Epoch 7288/40000, Loss: 0.0001060770227923058, Learning Rate: 0.000834\n",
      "Epoch 7289/40000, Loss: 0.00011435127817094326, Learning Rate: 0.000834\n",
      "Epoch 7290/40000, Loss: 0.0001522362290415913, Learning Rate: 0.000834\n",
      "Epoch 7291/40000, Loss: 9.926781058311462e-05, Learning Rate: 0.000834\n",
      "Epoch 7292/40000, Loss: 0.0001647624303586781, Learning Rate: 0.000834\n",
      "Epoch 7293/40000, Loss: 0.0001104152252082713, Learning Rate: 0.000834\n",
      "Epoch 7294/40000, Loss: 7.613714114995673e-05, Learning Rate: 0.000833\n",
      "Epoch 7295/40000, Loss: 0.00010131421004189178, Learning Rate: 0.000833\n",
      "Epoch 7296/40000, Loss: 9.327596490038559e-05, Learning Rate: 0.000833\n",
      "Epoch 7297/40000, Loss: 0.00012727518333122134, Learning Rate: 0.000833\n",
      "Epoch 7298/40000, Loss: 4.099444777239114e-05, Learning Rate: 0.000833\n",
      "Epoch 7299/40000, Loss: 7.739565626252443e-05, Learning Rate: 0.000833\n",
      "Epoch 7300/40000, Loss: 9.760597458807752e-05, Learning Rate: 0.000833\n",
      "Epoch 7301/40000, Loss: 0.000112745794467628, Learning Rate: 0.000833\n",
      "Epoch 7302/40000, Loss: 0.00011670983076328412, Learning Rate: 0.000833\n",
      "Epoch 7303/40000, Loss: 0.00012468757631722838, Learning Rate: 0.000833\n",
      "Epoch 7304/40000, Loss: 7.221556006697938e-05, Learning Rate: 0.000832\n",
      "Epoch 7305/40000, Loss: 7.819195161573589e-05, Learning Rate: 0.000832\n",
      "Epoch 7306/40000, Loss: 7.344635378103703e-05, Learning Rate: 0.000832\n",
      "Epoch 7307/40000, Loss: 0.00011838271166197956, Learning Rate: 0.000832\n",
      "Epoch 7308/40000, Loss: 6.0106056480435655e-05, Learning Rate: 0.000832\n",
      "Epoch 7309/40000, Loss: 8.400284423260018e-05, Learning Rate: 0.000832\n",
      "Epoch 7310/40000, Loss: 7.478774932678789e-05, Learning Rate: 0.000832\n",
      "Epoch 7311/40000, Loss: 7.22264958312735e-05, Learning Rate: 0.000832\n",
      "Epoch 7312/40000, Loss: 0.00010455537994857877, Learning Rate: 0.000832\n",
      "Epoch 7313/40000, Loss: 7.381263276329264e-05, Learning Rate: 0.000832\n",
      "Epoch 7314/40000, Loss: 6.209505227161571e-05, Learning Rate: 0.000831\n",
      "Epoch 7315/40000, Loss: 0.00011050594912376255, Learning Rate: 0.000831\n",
      "Epoch 7316/40000, Loss: 7.224225555546582e-05, Learning Rate: 0.000831\n",
      "Epoch 7317/40000, Loss: 3.5690056392923e-05, Learning Rate: 0.000831\n",
      "Epoch 7318/40000, Loss: 0.0001315723784500733, Learning Rate: 0.000831\n",
      "Epoch 7319/40000, Loss: 0.00010098210623255, Learning Rate: 0.000831\n",
      "Epoch 7320/40000, Loss: 9.667281119618565e-05, Learning Rate: 0.000831\n",
      "Epoch 7321/40000, Loss: 0.00010486552491784096, Learning Rate: 0.000831\n",
      "Epoch 7322/40000, Loss: 3.190502684446983e-05, Learning Rate: 0.000831\n",
      "Epoch 7323/40000, Loss: 0.00010214397480012849, Learning Rate: 0.000831\n",
      "Epoch 7324/40000, Loss: 6.198192568263039e-05, Learning Rate: 0.000830\n",
      "Epoch 7325/40000, Loss: 6.994808063609526e-05, Learning Rate: 0.000830\n",
      "Epoch 7326/40000, Loss: 9.125111682806164e-05, Learning Rate: 0.000830\n",
      "Epoch 7327/40000, Loss: 7.147221185732633e-05, Learning Rate: 0.000830\n",
      "Epoch 7328/40000, Loss: 6.481357559096068e-05, Learning Rate: 0.000830\n",
      "Epoch 7329/40000, Loss: 7.577463111374527e-05, Learning Rate: 0.000830\n",
      "Epoch 7330/40000, Loss: 6.96147108101286e-05, Learning Rate: 0.000830\n",
      "Epoch 7331/40000, Loss: 9.045707702171057e-05, Learning Rate: 0.000830\n",
      "Epoch 7332/40000, Loss: 7.171984179876745e-05, Learning Rate: 0.000830\n",
      "Epoch 7333/40000, Loss: 2.798203786369413e-05, Learning Rate: 0.000830\n",
      "Epoch 7334/40000, Loss: 7.140861271182075e-05, Learning Rate: 0.000829\n",
      "Epoch 7335/40000, Loss: 2.696112460398581e-05, Learning Rate: 0.000829\n",
      "Epoch 7336/40000, Loss: 5.961564602330327e-05, Learning Rate: 0.000829\n",
      "Epoch 7337/40000, Loss: 5.8624187659006566e-05, Learning Rate: 0.000829\n",
      "Epoch 7338/40000, Loss: 5.510445043910295e-05, Learning Rate: 0.000829\n",
      "Epoch 7339/40000, Loss: 9.176642925012857e-05, Learning Rate: 0.000829\n",
      "Epoch 7340/40000, Loss: 9.86706290859729e-05, Learning Rate: 0.000829\n",
      "Epoch 7341/40000, Loss: 5.46067240065895e-05, Learning Rate: 0.000829\n",
      "Epoch 7342/40000, Loss: 6.881547597004101e-05, Learning Rate: 0.000829\n",
      "Epoch 7343/40000, Loss: 5.4752512369304895e-05, Learning Rate: 0.000829\n",
      "Epoch 7344/40000, Loss: 5.447190051199868e-05, Learning Rate: 0.000828\n",
      "Epoch 7345/40000, Loss: 9.864103049039841e-05, Learning Rate: 0.000828\n",
      "Epoch 7346/40000, Loss: 9.216593753080815e-05, Learning Rate: 0.000828\n",
      "Epoch 7347/40000, Loss: 5.4865322454134e-05, Learning Rate: 0.000828\n",
      "Epoch 7348/40000, Loss: 9.865496394922957e-05, Learning Rate: 0.000828\n",
      "Epoch 7349/40000, Loss: 5.904379941057414e-05, Learning Rate: 0.000828\n",
      "Epoch 7350/40000, Loss: 5.52107158000581e-05, Learning Rate: 0.000828\n",
      "Epoch 7351/40000, Loss: 5.4733500292059034e-05, Learning Rate: 0.000828\n",
      "Epoch 7352/40000, Loss: 9.188352851197124e-05, Learning Rate: 0.000828\n",
      "Epoch 7353/40000, Loss: 7.040308992145583e-05, Learning Rate: 0.000828\n",
      "Epoch 7354/40000, Loss: 5.548209446715191e-05, Learning Rate: 0.000828\n",
      "Epoch 7355/40000, Loss: 2.771510844468139e-05, Learning Rate: 0.000827\n",
      "Epoch 7356/40000, Loss: 5.917047383263707e-05, Learning Rate: 0.000827\n",
      "Epoch 7357/40000, Loss: 0.00010218569514108822, Learning Rate: 0.000827\n",
      "Epoch 7358/40000, Loss: 7.013971480773762e-05, Learning Rate: 0.000827\n",
      "Epoch 7359/40000, Loss: 5.8815152442548424e-05, Learning Rate: 0.000827\n",
      "Epoch 7360/40000, Loss: 6.29579953965731e-05, Learning Rate: 0.000827\n",
      "Epoch 7361/40000, Loss: 0.00010245012526866049, Learning Rate: 0.000827\n",
      "Epoch 7362/40000, Loss: 6.131015834398568e-05, Learning Rate: 0.000827\n",
      "Epoch 7363/40000, Loss: 7.000978075666353e-05, Learning Rate: 0.000827\n",
      "Epoch 7364/40000, Loss: 3.1041374313645065e-05, Learning Rate: 0.000827\n",
      "Epoch 7365/40000, Loss: 5.9126825362909585e-05, Learning Rate: 0.000826\n",
      "Epoch 7366/40000, Loss: 3.9982700400287285e-05, Learning Rate: 0.000826\n",
      "Epoch 7367/40000, Loss: 0.00010531768202781677, Learning Rate: 0.000826\n",
      "Epoch 7368/40000, Loss: 7.201026892289519e-05, Learning Rate: 0.000826\n",
      "Epoch 7369/40000, Loss: 0.00010276794637320563, Learning Rate: 0.000826\n",
      "Epoch 7370/40000, Loss: 7.314924732781947e-05, Learning Rate: 0.000826\n",
      "Epoch 7371/40000, Loss: 5.879716627532616e-05, Learning Rate: 0.000826\n",
      "Epoch 7372/40000, Loss: 7.190174801507965e-05, Learning Rate: 0.000826\n",
      "Epoch 7373/40000, Loss: 3.0629315006081015e-05, Learning Rate: 0.000826\n",
      "Epoch 7374/40000, Loss: 0.00010300484427716583, Learning Rate: 0.000826\n",
      "Epoch 7375/40000, Loss: 0.00010330716031603515, Learning Rate: 0.000825\n",
      "Epoch 7376/40000, Loss: 6.51529262540862e-05, Learning Rate: 0.000825\n",
      "Epoch 7377/40000, Loss: 0.00010162040416616946, Learning Rate: 0.000825\n",
      "Epoch 7378/40000, Loss: 7.426223601214588e-05, Learning Rate: 0.000825\n",
      "Epoch 7379/40000, Loss: 0.00011501055996632203, Learning Rate: 0.000825\n",
      "Epoch 7380/40000, Loss: 6.282916001509875e-05, Learning Rate: 0.000825\n",
      "Epoch 7381/40000, Loss: 0.0001304226607317105, Learning Rate: 0.000825\n",
      "Epoch 7382/40000, Loss: 9.133851563092321e-05, Learning Rate: 0.000825\n",
      "Epoch 7383/40000, Loss: 0.00012218292977195233, Learning Rate: 0.000825\n",
      "Epoch 7384/40000, Loss: 4.2623189074220136e-05, Learning Rate: 0.000825\n",
      "Epoch 7385/40000, Loss: 8.519952098140493e-05, Learning Rate: 0.000824\n",
      "Epoch 7386/40000, Loss: 9.064548794412985e-05, Learning Rate: 0.000824\n",
      "Epoch 7387/40000, Loss: 0.0001121312816394493, Learning Rate: 0.000824\n",
      "Epoch 7388/40000, Loss: 0.00017778851906768978, Learning Rate: 0.000824\n",
      "Epoch 7389/40000, Loss: 9.761168621480465e-05, Learning Rate: 0.000824\n",
      "Epoch 7390/40000, Loss: 0.00013612842303700745, Learning Rate: 0.000824\n",
      "Epoch 7391/40000, Loss: 5.0789560191333294e-05, Learning Rate: 0.000824\n",
      "Epoch 7392/40000, Loss: 8.811681618681177e-05, Learning Rate: 0.000824\n",
      "Epoch 7393/40000, Loss: 7.544631080236286e-05, Learning Rate: 0.000824\n",
      "Epoch 7394/40000, Loss: 9.1114743554499e-05, Learning Rate: 0.000824\n",
      "Epoch 7395/40000, Loss: 0.00014347121759783477, Learning Rate: 0.000823\n",
      "Epoch 7396/40000, Loss: 8.42577574076131e-05, Learning Rate: 0.000823\n",
      "Epoch 7397/40000, Loss: 0.00012384742149151862, Learning Rate: 0.000823\n",
      "Epoch 7398/40000, Loss: 0.00011250028182985261, Learning Rate: 0.000823\n",
      "Epoch 7399/40000, Loss: 0.00011054950300604105, Learning Rate: 0.000823\n",
      "Epoch 7400/40000, Loss: 4.427522435435094e-05, Learning Rate: 0.000823\n",
      "Epoch 7401/40000, Loss: 8.581626752857119e-05, Learning Rate: 0.000823\n",
      "Epoch 7402/40000, Loss: 0.00015687780978623778, Learning Rate: 0.000823\n",
      "Epoch 7403/40000, Loss: 5.345060708350502e-05, Learning Rate: 0.000823\n",
      "Epoch 7404/40000, Loss: 4.872642966802232e-05, Learning Rate: 0.000823\n",
      "Epoch 7405/40000, Loss: 0.00013854746066499501, Learning Rate: 0.000822\n",
      "Epoch 7406/40000, Loss: 0.00012109201634302735, Learning Rate: 0.000822\n",
      "Epoch 7407/40000, Loss: 9.282029350288212e-05, Learning Rate: 0.000822\n",
      "Epoch 7408/40000, Loss: 0.00020666749333031476, Learning Rate: 0.000822\n",
      "Epoch 7409/40000, Loss: 7.932873268146068e-05, Learning Rate: 0.000822\n",
      "Epoch 7410/40000, Loss: 0.00014699774328619242, Learning Rate: 0.000822\n",
      "Epoch 7411/40000, Loss: 0.00011993361840723082, Learning Rate: 0.000822\n",
      "Epoch 7412/40000, Loss: 9.619625052437186e-05, Learning Rate: 0.000822\n",
      "Epoch 7413/40000, Loss: 0.00011587379412958398, Learning Rate: 0.000822\n",
      "Epoch 7414/40000, Loss: 0.00011800185893662274, Learning Rate: 0.000822\n",
      "Epoch 7415/40000, Loss: 5.0014412408927456e-05, Learning Rate: 0.000821\n",
      "Epoch 7416/40000, Loss: 7.083471427904442e-05, Learning Rate: 0.000821\n",
      "Epoch 7417/40000, Loss: 0.00011290427937638015, Learning Rate: 0.000821\n",
      "Epoch 7418/40000, Loss: 6.522708281408995e-05, Learning Rate: 0.000821\n",
      "Epoch 7419/40000, Loss: 3.742850822163746e-05, Learning Rate: 0.000821\n",
      "Epoch 7420/40000, Loss: 6.471048982348293e-05, Learning Rate: 0.000821\n",
      "Epoch 7421/40000, Loss: 6.679492071270943e-05, Learning Rate: 0.000821\n",
      "Epoch 7422/40000, Loss: 9.861839498626068e-05, Learning Rate: 0.000821\n",
      "Epoch 7423/40000, Loss: 6.601207860512659e-05, Learning Rate: 0.000821\n",
      "Epoch 7424/40000, Loss: 0.0001420219923602417, Learning Rate: 0.000821\n",
      "Epoch 7425/40000, Loss: 5.8741898101288825e-05, Learning Rate: 0.000820\n",
      "Epoch 7426/40000, Loss: 0.00011441957030910999, Learning Rate: 0.000820\n",
      "Epoch 7427/40000, Loss: 6.798392860218883e-05, Learning Rate: 0.000820\n",
      "Epoch 7428/40000, Loss: 6.117879820521921e-05, Learning Rate: 0.000820\n",
      "Epoch 7429/40000, Loss: 9.958992450265214e-05, Learning Rate: 0.000820\n",
      "Epoch 7430/40000, Loss: 9.814406075747684e-05, Learning Rate: 0.000820\n",
      "Epoch 7431/40000, Loss: 6.708106957376003e-05, Learning Rate: 0.000820\n",
      "Epoch 7432/40000, Loss: 0.0001154061610577628, Learning Rate: 0.000820\n",
      "Epoch 7433/40000, Loss: 7.456781168002635e-05, Learning Rate: 0.000820\n",
      "Epoch 7434/40000, Loss: 0.00012212235014885664, Learning Rate: 0.000820\n",
      "Epoch 7435/40000, Loss: 6.0084603319410235e-05, Learning Rate: 0.000819\n",
      "Epoch 7436/40000, Loss: 8.452547626802698e-05, Learning Rate: 0.000819\n",
      "Epoch 7437/40000, Loss: 6.0066926380386576e-05, Learning Rate: 0.000819\n",
      "Epoch 7438/40000, Loss: 9.823575965128839e-05, Learning Rate: 0.000819\n",
      "Epoch 7439/40000, Loss: 7.38135349820368e-05, Learning Rate: 0.000819\n",
      "Epoch 7440/40000, Loss: 7.115364132914692e-05, Learning Rate: 0.000819\n",
      "Epoch 7441/40000, Loss: 7.830583490431309e-05, Learning Rate: 0.000819\n",
      "Epoch 7442/40000, Loss: 7.070392166497186e-05, Learning Rate: 0.000819\n",
      "Epoch 7443/40000, Loss: 0.00015533667465206236, Learning Rate: 0.000819\n",
      "Epoch 7444/40000, Loss: 9.070777741726488e-05, Learning Rate: 0.000819\n",
      "Epoch 7445/40000, Loss: 5.996451363898814e-05, Learning Rate: 0.000819\n",
      "Epoch 7446/40000, Loss: 0.0001549303124193102, Learning Rate: 0.000818\n",
      "Epoch 7447/40000, Loss: 8.384828106500208e-05, Learning Rate: 0.000818\n",
      "Epoch 7448/40000, Loss: 8.73508834047243e-05, Learning Rate: 0.000818\n",
      "Epoch 7449/40000, Loss: 0.00011137075489386916, Learning Rate: 0.000818\n",
      "Epoch 7450/40000, Loss: 7.412413833662868e-05, Learning Rate: 0.000818\n",
      "Epoch 7451/40000, Loss: 9.103609772864729e-05, Learning Rate: 0.000818\n",
      "Epoch 7452/40000, Loss: 8.851596794556826e-05, Learning Rate: 0.000818\n",
      "Epoch 7453/40000, Loss: 0.0001076863773050718, Learning Rate: 0.000818\n",
      "Epoch 7454/40000, Loss: 0.0001043239317368716, Learning Rate: 0.000818\n",
      "Epoch 7455/40000, Loss: 7.293809903785586e-05, Learning Rate: 0.000818\n",
      "Epoch 7456/40000, Loss: 6.737444346072152e-05, Learning Rate: 0.000817\n",
      "Epoch 7457/40000, Loss: 5.9702866565203294e-05, Learning Rate: 0.000817\n",
      "Epoch 7458/40000, Loss: 3.4210370358778164e-05, Learning Rate: 0.000817\n",
      "Epoch 7459/40000, Loss: 7.612062472617254e-05, Learning Rate: 0.000817\n",
      "Epoch 7460/40000, Loss: 6.509487138828263e-05, Learning Rate: 0.000817\n",
      "Epoch 7461/40000, Loss: 0.00010492101137060672, Learning Rate: 0.000817\n",
      "Epoch 7462/40000, Loss: 0.00010206767183262855, Learning Rate: 0.000817\n",
      "Epoch 7463/40000, Loss: 2.8058530006092042e-05, Learning Rate: 0.000817\n",
      "Epoch 7464/40000, Loss: 5.8585599617799744e-05, Learning Rate: 0.000817\n",
      "Epoch 7465/40000, Loss: 2.944962034234777e-05, Learning Rate: 0.000817\n",
      "Epoch 7466/40000, Loss: 2.7766989660449326e-05, Learning Rate: 0.000816\n",
      "Epoch 7467/40000, Loss: 6.94897971698083e-05, Learning Rate: 0.000816\n",
      "Epoch 7468/40000, Loss: 8.984029409475625e-05, Learning Rate: 0.000816\n",
      "Epoch 7469/40000, Loss: 8.990189962787554e-05, Learning Rate: 0.000816\n",
      "Epoch 7470/40000, Loss: 6.881602166686207e-05, Learning Rate: 0.000816\n",
      "Epoch 7471/40000, Loss: 9.121076436713338e-05, Learning Rate: 0.000816\n",
      "Epoch 7472/40000, Loss: 9.146236698143184e-05, Learning Rate: 0.000816\n",
      "Epoch 7473/40000, Loss: 6.81473029544577e-05, Learning Rate: 0.000816\n",
      "Epoch 7474/40000, Loss: 5.4487070883624256e-05, Learning Rate: 0.000816\n",
      "Epoch 7475/40000, Loss: 9.736099309520796e-05, Learning Rate: 0.000816\n",
      "Epoch 7476/40000, Loss: 8.864881237968802e-05, Learning Rate: 0.000815\n",
      "Epoch 7477/40000, Loss: 8.848113793646917e-05, Learning Rate: 0.000815\n",
      "Epoch 7478/40000, Loss: 9.921124001266435e-05, Learning Rate: 0.000815\n",
      "Epoch 7479/40000, Loss: 2.6842535589821637e-05, Learning Rate: 0.000815\n",
      "Epoch 7480/40000, Loss: 9.209405834553763e-05, Learning Rate: 0.000815\n",
      "Epoch 7481/40000, Loss: 9.092930849874392e-05, Learning Rate: 0.000815\n",
      "Epoch 7482/40000, Loss: 5.467278970172629e-05, Learning Rate: 0.000815\n",
      "Epoch 7483/40000, Loss: 6.02415093453601e-05, Learning Rate: 0.000815\n",
      "Epoch 7484/40000, Loss: 5.553019582293928e-05, Learning Rate: 0.000815\n",
      "Epoch 7485/40000, Loss: 6.657953053945675e-05, Learning Rate: 0.000815\n",
      "Epoch 7486/40000, Loss: 6.138145545264706e-05, Learning Rate: 0.000814\n",
      "Epoch 7487/40000, Loss: 2.8645939892157912e-05, Learning Rate: 0.000814\n",
      "Epoch 7488/40000, Loss: 5.481308471644297e-05, Learning Rate: 0.000814\n",
      "Epoch 7489/40000, Loss: 2.7351339667802677e-05, Learning Rate: 0.000814\n",
      "Epoch 7490/40000, Loss: 2.6112251362064853e-05, Learning Rate: 0.000814\n",
      "Epoch 7491/40000, Loss: 9.150479309028015e-05, Learning Rate: 0.000814\n",
      "Epoch 7492/40000, Loss: 2.7512094675330445e-05, Learning Rate: 0.000814\n",
      "Epoch 7493/40000, Loss: 9.074459376279265e-05, Learning Rate: 0.000814\n",
      "Epoch 7494/40000, Loss: 5.667380173690617e-05, Learning Rate: 0.000814\n",
      "Epoch 7495/40000, Loss: 0.00010137581557501107, Learning Rate: 0.000814\n",
      "Epoch 7496/40000, Loss: 6.16216057096608e-05, Learning Rate: 0.000814\n",
      "Epoch 7497/40000, Loss: 9.959863382391632e-05, Learning Rate: 0.000813\n",
      "Epoch 7498/40000, Loss: 3.156119419145398e-05, Learning Rate: 0.000813\n",
      "Epoch 7499/40000, Loss: 2.9422710213111714e-05, Learning Rate: 0.000813\n",
      "Epoch 7500/40000, Loss: 0.00010218683019047603, Learning Rate: 0.000813\n",
      "Epoch 7501/40000, Loss: 5.763679291703738e-05, Learning Rate: 0.000813\n",
      "Epoch 7502/40000, Loss: 0.00010303877934347838, Learning Rate: 0.000813\n",
      "Epoch 7503/40000, Loss: 5.708011667593382e-05, Learning Rate: 0.000813\n",
      "Epoch 7504/40000, Loss: 6.255856715142727e-05, Learning Rate: 0.000813\n",
      "Epoch 7505/40000, Loss: 9.986163058783859e-05, Learning Rate: 0.000813\n",
      "Epoch 7506/40000, Loss: 2.7622843845165335e-05, Learning Rate: 0.000813\n",
      "Epoch 7507/40000, Loss: 5.57028288312722e-05, Learning Rate: 0.000812\n",
      "Epoch 7508/40000, Loss: 6.120338366599753e-05, Learning Rate: 0.000812\n",
      "Epoch 7509/40000, Loss: 2.8817248676205054e-05, Learning Rate: 0.000812\n",
      "Epoch 7510/40000, Loss: 9.553786367177963e-05, Learning Rate: 0.000812\n",
      "Epoch 7511/40000, Loss: 0.00010112034215126187, Learning Rate: 0.000812\n",
      "Epoch 7512/40000, Loss: 6.186909740790725e-05, Learning Rate: 0.000812\n",
      "Epoch 7513/40000, Loss: 7.096585613908246e-05, Learning Rate: 0.000812\n",
      "Epoch 7514/40000, Loss: 6.95516137056984e-05, Learning Rate: 0.000812\n",
      "Epoch 7515/40000, Loss: 5.765771493315697e-05, Learning Rate: 0.000812\n",
      "Epoch 7516/40000, Loss: 5.7750010455492884e-05, Learning Rate: 0.000812\n",
      "Epoch 7517/40000, Loss: 5.5709613661747426e-05, Learning Rate: 0.000811\n",
      "Epoch 7518/40000, Loss: 5.461176988319494e-05, Learning Rate: 0.000811\n",
      "Epoch 7519/40000, Loss: 2.667012086021714e-05, Learning Rate: 0.000811\n",
      "Epoch 7520/40000, Loss: 9.198308544000611e-05, Learning Rate: 0.000811\n",
      "Epoch 7521/40000, Loss: 5.579404751188122e-05, Learning Rate: 0.000811\n",
      "Epoch 7522/40000, Loss: 0.00010646668670233339, Learning Rate: 0.000811\n",
      "Epoch 7523/40000, Loss: 2.8459675377234817e-05, Learning Rate: 0.000811\n",
      "Epoch 7524/40000, Loss: 8.060540130827576e-05, Learning Rate: 0.000811\n",
      "Epoch 7525/40000, Loss: 7.66320081311278e-05, Learning Rate: 0.000811\n",
      "Epoch 7526/40000, Loss: 6.100113023421727e-05, Learning Rate: 0.000811\n",
      "Epoch 7527/40000, Loss: 0.00010144988482352346, Learning Rate: 0.000811\n",
      "Epoch 7528/40000, Loss: 5.708695243811235e-05, Learning Rate: 0.000810\n",
      "Epoch 7529/40000, Loss: 3.0169818273861893e-05, Learning Rate: 0.000810\n",
      "Epoch 7530/40000, Loss: 0.0001060550712281838, Learning Rate: 0.000810\n",
      "Epoch 7531/40000, Loss: 6.000766734359786e-05, Learning Rate: 0.000810\n",
      "Epoch 7532/40000, Loss: 7.984320836840197e-05, Learning Rate: 0.000810\n",
      "Epoch 7533/40000, Loss: 5.889535168535076e-05, Learning Rate: 0.000810\n",
      "Epoch 7534/40000, Loss: 8.280390466097742e-05, Learning Rate: 0.000810\n",
      "Epoch 7535/40000, Loss: 9.702786337584257e-05, Learning Rate: 0.000810\n",
      "Epoch 7536/40000, Loss: 7.147524593165144e-05, Learning Rate: 0.000810\n",
      "Epoch 7537/40000, Loss: 0.00015195603191386908, Learning Rate: 0.000810\n",
      "Epoch 7538/40000, Loss: 8.274969877675176e-05, Learning Rate: 0.000809\n",
      "Epoch 7539/40000, Loss: 8.993336086859927e-05, Learning Rate: 0.000809\n",
      "Epoch 7540/40000, Loss: 0.00014205097977537662, Learning Rate: 0.000809\n",
      "Epoch 7541/40000, Loss: 6.3506537117064e-05, Learning Rate: 0.000809\n",
      "Epoch 7542/40000, Loss: 0.00012035394320264459, Learning Rate: 0.000809\n",
      "Epoch 7543/40000, Loss: 0.00011400607763789594, Learning Rate: 0.000809\n",
      "Epoch 7544/40000, Loss: 0.00011376355541869998, Learning Rate: 0.000809\n",
      "Epoch 7545/40000, Loss: 8.194871770683676e-05, Learning Rate: 0.000809\n",
      "Epoch 7546/40000, Loss: 0.00021836445375811309, Learning Rate: 0.000809\n",
      "Epoch 7547/40000, Loss: 9.090674575418234e-05, Learning Rate: 0.000809\n",
      "Epoch 7548/40000, Loss: 8.744776278035715e-05, Learning Rate: 0.000808\n",
      "Epoch 7549/40000, Loss: 0.00010936848411802202, Learning Rate: 0.000808\n",
      "Epoch 7550/40000, Loss: 0.00016948499251157045, Learning Rate: 0.000808\n",
      "Epoch 7551/40000, Loss: 0.00011130072380183265, Learning Rate: 0.000808\n",
      "Epoch 7552/40000, Loss: 0.00017197478155139834, Learning Rate: 0.000808\n",
      "Epoch 7553/40000, Loss: 8.37478437460959e-05, Learning Rate: 0.000808\n",
      "Epoch 7554/40000, Loss: 7.557611388619989e-05, Learning Rate: 0.000808\n",
      "Epoch 7555/40000, Loss: 0.00010424618812976405, Learning Rate: 0.000808\n",
      "Epoch 7556/40000, Loss: 9.703318937681615e-05, Learning Rate: 0.000808\n",
      "Epoch 7557/40000, Loss: 0.00010275447857566178, Learning Rate: 0.000808\n",
      "Epoch 7558/40000, Loss: 0.00017894644406624138, Learning Rate: 0.000807\n",
      "Epoch 7559/40000, Loss: 0.00012376578524708748, Learning Rate: 0.000807\n",
      "Epoch 7560/40000, Loss: 0.00017280070460401475, Learning Rate: 0.000807\n",
      "Epoch 7561/40000, Loss: 8.291120320791379e-05, Learning Rate: 0.000807\n",
      "Epoch 7562/40000, Loss: 5.573623639065772e-05, Learning Rate: 0.000807\n",
      "Epoch 7563/40000, Loss: 0.0001487036788603291, Learning Rate: 0.000807\n",
      "Epoch 7564/40000, Loss: 9.330574539490044e-05, Learning Rate: 0.000807\n",
      "Epoch 7565/40000, Loss: 0.00013386686623562127, Learning Rate: 0.000807\n",
      "Epoch 7566/40000, Loss: 9.740647510625422e-05, Learning Rate: 0.000807\n",
      "Epoch 7567/40000, Loss: 0.00011904697021236643, Learning Rate: 0.000807\n",
      "Epoch 7568/40000, Loss: 0.00011315839947201312, Learning Rate: 0.000807\n",
      "Epoch 7569/40000, Loss: 8.716224692761898e-05, Learning Rate: 0.000806\n",
      "Epoch 7570/40000, Loss: 0.00010757416748674586, Learning Rate: 0.000806\n",
      "Epoch 7571/40000, Loss: 6.237925117602572e-05, Learning Rate: 0.000806\n",
      "Epoch 7572/40000, Loss: 0.0001214711883221753, Learning Rate: 0.000806\n",
      "Epoch 7573/40000, Loss: 0.00011647756764432415, Learning Rate: 0.000806\n",
      "Epoch 7574/40000, Loss: 0.00010069079144159332, Learning Rate: 0.000806\n",
      "Epoch 7575/40000, Loss: 0.00013202373520471156, Learning Rate: 0.000806\n",
      "Epoch 7576/40000, Loss: 0.0001102634851122275, Learning Rate: 0.000806\n",
      "Epoch 7577/40000, Loss: 7.507829286623746e-05, Learning Rate: 0.000806\n",
      "Epoch 7578/40000, Loss: 7.848624954931438e-05, Learning Rate: 0.000806\n",
      "Epoch 7579/40000, Loss: 4.593061385094188e-05, Learning Rate: 0.000805\n",
      "Epoch 7580/40000, Loss: 6.643600499955937e-05, Learning Rate: 0.000805\n",
      "Epoch 7581/40000, Loss: 7.604898564750329e-05, Learning Rate: 0.000805\n",
      "Epoch 7582/40000, Loss: 7.544542313553393e-05, Learning Rate: 0.000805\n",
      "Epoch 7583/40000, Loss: 7.129373261705041e-05, Learning Rate: 0.000805\n",
      "Epoch 7584/40000, Loss: 3.6635439755627885e-05, Learning Rate: 0.000805\n",
      "Epoch 7585/40000, Loss: 2.930065238615498e-05, Learning Rate: 0.000805\n",
      "Epoch 7586/40000, Loss: 6.4149520767387e-05, Learning Rate: 0.000805\n",
      "Epoch 7587/40000, Loss: 5.802825035061687e-05, Learning Rate: 0.000805\n",
      "Epoch 7588/40000, Loss: 2.7276739274384454e-05, Learning Rate: 0.000805\n",
      "Epoch 7589/40000, Loss: 6.82184036122635e-05, Learning Rate: 0.000804\n",
      "Epoch 7590/40000, Loss: 5.42083871550858e-05, Learning Rate: 0.000804\n",
      "Epoch 7591/40000, Loss: 9.758927626535296e-05, Learning Rate: 0.000804\n",
      "Epoch 7592/40000, Loss: 5.428789518191479e-05, Learning Rate: 0.000804\n",
      "Epoch 7593/40000, Loss: 5.399692599894479e-05, Learning Rate: 0.000804\n",
      "Epoch 7594/40000, Loss: 5.433204205473885e-05, Learning Rate: 0.000804\n",
      "Epoch 7595/40000, Loss: 5.350754508981481e-05, Learning Rate: 0.000804\n",
      "Epoch 7596/40000, Loss: 5.3993077017366886e-05, Learning Rate: 0.000804\n",
      "Epoch 7597/40000, Loss: 8.700519538251683e-05, Learning Rate: 0.000804\n",
      "Epoch 7598/40000, Loss: 2.5905221264110878e-05, Learning Rate: 0.000804\n",
      "Epoch 7599/40000, Loss: 2.59560692938976e-05, Learning Rate: 0.000804\n",
      "Epoch 7600/40000, Loss: 5.3720181313110515e-05, Learning Rate: 0.000803\n",
      "Epoch 7601/40000, Loss: 2.580188083811663e-05, Learning Rate: 0.000803\n",
      "Epoch 7602/40000, Loss: 5.327974577085115e-05, Learning Rate: 0.000803\n",
      "Epoch 7603/40000, Loss: 5.752815559390001e-05, Learning Rate: 0.000803\n",
      "Epoch 7604/40000, Loss: 2.564580245234538e-05, Learning Rate: 0.000803\n",
      "Epoch 7605/40000, Loss: 9.665940160630271e-05, Learning Rate: 0.000803\n",
      "Epoch 7606/40000, Loss: 8.696474833413959e-05, Learning Rate: 0.000803\n",
      "Epoch 7607/40000, Loss: 9.699727525003254e-05, Learning Rate: 0.000803\n",
      "Epoch 7608/40000, Loss: 2.584721914900001e-05, Learning Rate: 0.000803\n",
      "Epoch 7609/40000, Loss: 2.575876897026319e-05, Learning Rate: 0.000803\n",
      "Epoch 7610/40000, Loss: 5.736807725043036e-05, Learning Rate: 0.000802\n",
      "Epoch 7611/40000, Loss: 5.735191371059045e-05, Learning Rate: 0.000802\n",
      "Epoch 7612/40000, Loss: 5.7276254665339366e-05, Learning Rate: 0.000802\n",
      "Epoch 7613/40000, Loss: 5.3448675316758454e-05, Learning Rate: 0.000802\n",
      "Epoch 7614/40000, Loss: 5.347140540834516e-05, Learning Rate: 0.000802\n",
      "Epoch 7615/40000, Loss: 2.5924575311364606e-05, Learning Rate: 0.000802\n",
      "Epoch 7616/40000, Loss: 2.5435623683733866e-05, Learning Rate: 0.000802\n",
      "Epoch 7617/40000, Loss: 6.660947110503912e-05, Learning Rate: 0.000802\n",
      "Epoch 7618/40000, Loss: 5.689151294063777e-05, Learning Rate: 0.000802\n",
      "Epoch 7619/40000, Loss: 2.5411569367861375e-05, Learning Rate: 0.000802\n",
      "Epoch 7620/40000, Loss: 5.663824049406685e-05, Learning Rate: 0.000802\n",
      "Epoch 7621/40000, Loss: 6.643269443884492e-05, Learning Rate: 0.000801\n",
      "Epoch 7622/40000, Loss: 8.763732330407947e-05, Learning Rate: 0.000801\n",
      "Epoch 7623/40000, Loss: 6.749537715222687e-05, Learning Rate: 0.000801\n",
      "Epoch 7624/40000, Loss: 5.453151243273169e-05, Learning Rate: 0.000801\n",
      "Epoch 7625/40000, Loss: 8.915663784136996e-05, Learning Rate: 0.000801\n",
      "Epoch 7626/40000, Loss: 5.7728830142877996e-05, Learning Rate: 0.000801\n",
      "Epoch 7627/40000, Loss: 9.972762927645817e-05, Learning Rate: 0.000801\n",
      "Epoch 7628/40000, Loss: 2.8195063350722194e-05, Learning Rate: 0.000801\n",
      "Epoch 7629/40000, Loss: 2.8182197638670914e-05, Learning Rate: 0.000801\n",
      "Epoch 7630/40000, Loss: 6.895981641719118e-05, Learning Rate: 0.000801\n",
      "Epoch 7631/40000, Loss: 3.697113425005227e-05, Learning Rate: 0.000800\n",
      "Epoch 7632/40000, Loss: 9.659720672061667e-05, Learning Rate: 0.000800\n",
      "Epoch 7633/40000, Loss: 5.537680772249587e-05, Learning Rate: 0.000800\n",
      "Epoch 7634/40000, Loss: 3.0009829060873017e-05, Learning Rate: 0.000800\n",
      "Epoch 7635/40000, Loss: 0.00010230592306470498, Learning Rate: 0.000800\n",
      "Epoch 7636/40000, Loss: 3.5165943700121716e-05, Learning Rate: 0.000800\n",
      "Epoch 7637/40000, Loss: 6.541264883708209e-05, Learning Rate: 0.000800\n",
      "Epoch 7638/40000, Loss: 5.7283399655716494e-05, Learning Rate: 0.000800\n",
      "Epoch 7639/40000, Loss: 7.852023554733023e-05, Learning Rate: 0.000800\n",
      "Epoch 7640/40000, Loss: 3.8642174331471324e-05, Learning Rate: 0.000800\n",
      "Epoch 7641/40000, Loss: 0.00011050784814869985, Learning Rate: 0.000799\n",
      "Epoch 7642/40000, Loss: 0.00010519327770452946, Learning Rate: 0.000799\n",
      "Epoch 7643/40000, Loss: 7.900239143054932e-05, Learning Rate: 0.000799\n",
      "Epoch 7644/40000, Loss: 0.00011211650416953489, Learning Rate: 0.000799\n",
      "Epoch 7645/40000, Loss: 7.454519072780386e-05, Learning Rate: 0.000799\n",
      "Epoch 7646/40000, Loss: 0.0001319219736615196, Learning Rate: 0.000799\n",
      "Epoch 7647/40000, Loss: 0.00010784639016492292, Learning Rate: 0.000799\n",
      "Epoch 7648/40000, Loss: 0.00010778236173791811, Learning Rate: 0.000799\n",
      "Epoch 7649/40000, Loss: 4.1779781895456836e-05, Learning Rate: 0.000799\n",
      "Epoch 7650/40000, Loss: 9.17610595934093e-05, Learning Rate: 0.000799\n",
      "Epoch 7651/40000, Loss: 0.0001423242356395349, Learning Rate: 0.000799\n",
      "Epoch 7652/40000, Loss: 0.00011434608313720673, Learning Rate: 0.000798\n",
      "Epoch 7653/40000, Loss: 7.038813782855868e-05, Learning Rate: 0.000798\n",
      "Epoch 7654/40000, Loss: 0.00010786467464640737, Learning Rate: 0.000798\n",
      "Epoch 7655/40000, Loss: 8.368002454517409e-05, Learning Rate: 0.000798\n",
      "Epoch 7656/40000, Loss: 8.781151700532064e-05, Learning Rate: 0.000798\n",
      "Epoch 7657/40000, Loss: 0.0001048778067342937, Learning Rate: 0.000798\n",
      "Epoch 7658/40000, Loss: 6.765082071069628e-05, Learning Rate: 0.000798\n",
      "Epoch 7659/40000, Loss: 0.00011643384641502053, Learning Rate: 0.000798\n",
      "Epoch 7660/40000, Loss: 0.00010845409997273237, Learning Rate: 0.000798\n",
      "Epoch 7661/40000, Loss: 6.272190512390807e-05, Learning Rate: 0.000798\n",
      "Epoch 7662/40000, Loss: 4.005208029411733e-05, Learning Rate: 0.000797\n",
      "Epoch 7663/40000, Loss: 0.00010252486390527338, Learning Rate: 0.000797\n",
      "Epoch 7664/40000, Loss: 7.74551008362323e-05, Learning Rate: 0.000797\n",
      "Epoch 7665/40000, Loss: 0.00011518040264490992, Learning Rate: 0.000797\n",
      "Epoch 7666/40000, Loss: 4.433307549334131e-05, Learning Rate: 0.000797\n",
      "Epoch 7667/40000, Loss: 0.00011777890176745132, Learning Rate: 0.000797\n",
      "Epoch 7668/40000, Loss: 0.00010074424790218472, Learning Rate: 0.000797\n",
      "Epoch 7669/40000, Loss: 0.00013394684356171638, Learning Rate: 0.000797\n",
      "Epoch 7670/40000, Loss: 9.518091246718541e-05, Learning Rate: 0.000797\n",
      "Epoch 7671/40000, Loss: 0.00015627394896000624, Learning Rate: 0.000797\n",
      "Epoch 7672/40000, Loss: 8.411747694481164e-05, Learning Rate: 0.000797\n",
      "Epoch 7673/40000, Loss: 4.496652400121093e-05, Learning Rate: 0.000796\n",
      "Epoch 7674/40000, Loss: 0.00012229770072735846, Learning Rate: 0.000796\n",
      "Epoch 7675/40000, Loss: 9.809080802369863e-05, Learning Rate: 0.000796\n",
      "Epoch 7676/40000, Loss: 0.00011192653619218618, Learning Rate: 0.000796\n",
      "Epoch 7677/40000, Loss: 7.151625322876498e-05, Learning Rate: 0.000796\n",
      "Epoch 7678/40000, Loss: 8.480177348246798e-05, Learning Rate: 0.000796\n",
      "Epoch 7679/40000, Loss: 4.7167748562060297e-05, Learning Rate: 0.000796\n",
      "Epoch 7680/40000, Loss: 7.578755321446806e-05, Learning Rate: 0.000796\n",
      "Epoch 7681/40000, Loss: 8.520804840372875e-05, Learning Rate: 0.000796\n",
      "Epoch 7682/40000, Loss: 9.477692219661549e-05, Learning Rate: 0.000796\n",
      "Epoch 7683/40000, Loss: 0.00016534881433472037, Learning Rate: 0.000795\n",
      "Epoch 7684/40000, Loss: 4.6176475734682754e-05, Learning Rate: 0.000795\n",
      "Epoch 7685/40000, Loss: 9.986395161831751e-05, Learning Rate: 0.000795\n",
      "Epoch 7686/40000, Loss: 0.00019300286658108234, Learning Rate: 0.000795\n",
      "Epoch 7687/40000, Loss: 0.00015924882609397173, Learning Rate: 0.000795\n",
      "Epoch 7688/40000, Loss: 8.321409404743463e-05, Learning Rate: 0.000795\n",
      "Epoch 7689/40000, Loss: 0.00015171058475971222, Learning Rate: 0.000795\n",
      "Epoch 7690/40000, Loss: 0.00013926901738159359, Learning Rate: 0.000795\n",
      "Epoch 7691/40000, Loss: 0.00019002551562152803, Learning Rate: 0.000795\n",
      "Epoch 7692/40000, Loss: 0.00015993404667824507, Learning Rate: 0.000795\n",
      "Epoch 7693/40000, Loss: 0.00011371459549991414, Learning Rate: 0.000795\n",
      "Epoch 7694/40000, Loss: 0.0001987957366509363, Learning Rate: 0.000794\n",
      "Epoch 7695/40000, Loss: 0.0002414513292023912, Learning Rate: 0.000794\n",
      "Epoch 7696/40000, Loss: 4.777443245984614e-05, Learning Rate: 0.000794\n",
      "Epoch 7697/40000, Loss: 0.0001515176409156993, Learning Rate: 0.000794\n",
      "Epoch 7698/40000, Loss: 0.0002416962233837694, Learning Rate: 0.000794\n",
      "Epoch 7699/40000, Loss: 0.00014314586587715894, Learning Rate: 0.000794\n",
      "Epoch 7700/40000, Loss: 0.00012252156739123166, Learning Rate: 0.000794\n",
      "Epoch 7701/40000, Loss: 0.0002304339432157576, Learning Rate: 0.000794\n",
      "Epoch 7702/40000, Loss: 0.00012206473184050992, Learning Rate: 0.000794\n",
      "Epoch 7703/40000, Loss: 0.0001583399425726384, Learning Rate: 0.000794\n",
      "Epoch 7704/40000, Loss: 0.00011797717888839543, Learning Rate: 0.000793\n",
      "Epoch 7705/40000, Loss: 5.240289101493545e-05, Learning Rate: 0.000793\n",
      "Epoch 7706/40000, Loss: 5.373008025344461e-05, Learning Rate: 0.000793\n",
      "Epoch 7707/40000, Loss: 0.00015278588398359716, Learning Rate: 0.000793\n",
      "Epoch 7708/40000, Loss: 0.0001196477678604424, Learning Rate: 0.000793\n",
      "Epoch 7709/40000, Loss: 0.00018427449685987085, Learning Rate: 0.000793\n",
      "Epoch 7710/40000, Loss: 7.288080087164417e-05, Learning Rate: 0.000793\n",
      "Epoch 7711/40000, Loss: 7.953554450068623e-05, Learning Rate: 0.000793\n",
      "Epoch 7712/40000, Loss: 0.00012494955444708467, Learning Rate: 0.000793\n",
      "Epoch 7713/40000, Loss: 6.728702282998711e-05, Learning Rate: 0.000793\n",
      "Epoch 7714/40000, Loss: 8.172744855983183e-05, Learning Rate: 0.000793\n",
      "Epoch 7715/40000, Loss: 7.264724263222888e-05, Learning Rate: 0.000792\n",
      "Epoch 7716/40000, Loss: 0.00010340287553844973, Learning Rate: 0.000792\n",
      "Epoch 7717/40000, Loss: 0.00012786677689291537, Learning Rate: 0.000792\n",
      "Epoch 7718/40000, Loss: 7.900777563918382e-05, Learning Rate: 0.000792\n",
      "Epoch 7719/40000, Loss: 3.2225903851212934e-05, Learning Rate: 0.000792\n",
      "Epoch 7720/40000, Loss: 9.787990711629391e-05, Learning Rate: 0.000792\n",
      "Epoch 7721/40000, Loss: 5.9801284805871546e-05, Learning Rate: 0.000792\n",
      "Epoch 7722/40000, Loss: 9.692460298538208e-05, Learning Rate: 0.000792\n",
      "Epoch 7723/40000, Loss: 6.333420606097206e-05, Learning Rate: 0.000792\n",
      "Epoch 7724/40000, Loss: 0.00012723621330223978, Learning Rate: 0.000792\n",
      "Epoch 7725/40000, Loss: 0.0001091678423108533, Learning Rate: 0.000791\n",
      "Epoch 7726/40000, Loss: 9.383270662510768e-05, Learning Rate: 0.000791\n",
      "Epoch 7727/40000, Loss: 2.8598042263183743e-05, Learning Rate: 0.000791\n",
      "Epoch 7728/40000, Loss: 0.000105563725810498, Learning Rate: 0.000791\n",
      "Epoch 7729/40000, Loss: 7.224415458040312e-05, Learning Rate: 0.000791\n",
      "Epoch 7730/40000, Loss: 0.00010443362407386303, Learning Rate: 0.000791\n",
      "Epoch 7731/40000, Loss: 0.00012082476314390078, Learning Rate: 0.000791\n",
      "Epoch 7732/40000, Loss: 6.308815500233322e-05, Learning Rate: 0.000791\n",
      "Epoch 7733/40000, Loss: 0.00011881706450367346, Learning Rate: 0.000791\n",
      "Epoch 7734/40000, Loss: 6.613418372580782e-05, Learning Rate: 0.000791\n",
      "Epoch 7735/40000, Loss: 2.9314429411897436e-05, Learning Rate: 0.000791\n",
      "Epoch 7736/40000, Loss: 6.510241655632854e-05, Learning Rate: 0.000790\n",
      "Epoch 7737/40000, Loss: 7.200070103863254e-05, Learning Rate: 0.000790\n",
      "Epoch 7738/40000, Loss: 5.5666732805548236e-05, Learning Rate: 0.000790\n",
      "Epoch 7739/40000, Loss: 6.019401189405471e-05, Learning Rate: 0.000790\n",
      "Epoch 7740/40000, Loss: 5.8352241467218846e-05, Learning Rate: 0.000790\n",
      "Epoch 7741/40000, Loss: 9.92436398519203e-05, Learning Rate: 0.000790\n",
      "Epoch 7742/40000, Loss: 5.480850086314604e-05, Learning Rate: 0.000790\n",
      "Epoch 7743/40000, Loss: 8.908398740459234e-05, Learning Rate: 0.000790\n",
      "Epoch 7744/40000, Loss: 9.607747051632032e-05, Learning Rate: 0.000790\n",
      "Epoch 7745/40000, Loss: 5.7401772210141644e-05, Learning Rate: 0.000790\n",
      "Epoch 7746/40000, Loss: 8.611677912995219e-05, Learning Rate: 0.000789\n",
      "Epoch 7747/40000, Loss: 5.291053093969822e-05, Learning Rate: 0.000789\n",
      "Epoch 7748/40000, Loss: 6.591571582248434e-05, Learning Rate: 0.000789\n",
      "Epoch 7749/40000, Loss: 5.3017014579381794e-05, Learning Rate: 0.000789\n",
      "Epoch 7750/40000, Loss: 5.2627496188506484e-05, Learning Rate: 0.000789\n",
      "Epoch 7751/40000, Loss: 5.269643952487968e-05, Learning Rate: 0.000789\n",
      "Epoch 7752/40000, Loss: 5.6331839004997164e-05, Learning Rate: 0.000789\n",
      "Epoch 7753/40000, Loss: 5.680203321389854e-05, Learning Rate: 0.000789\n",
      "Epoch 7754/40000, Loss: 6.61501835566014e-05, Learning Rate: 0.000789\n",
      "Epoch 7755/40000, Loss: 8.614706166554242e-05, Learning Rate: 0.000789\n",
      "Epoch 7756/40000, Loss: 6.61720332573168e-05, Learning Rate: 0.000789\n",
      "Epoch 7757/40000, Loss: 9.555365249980241e-05, Learning Rate: 0.000788\n",
      "Epoch 7758/40000, Loss: 8.67071867105551e-05, Learning Rate: 0.000788\n",
      "Epoch 7759/40000, Loss: 9.512269753031433e-05, Learning Rate: 0.000788\n",
      "Epoch 7760/40000, Loss: 5.322662036633119e-05, Learning Rate: 0.000788\n",
      "Epoch 7761/40000, Loss: 5.72702701902017e-05, Learning Rate: 0.000788\n",
      "Epoch 7762/40000, Loss: 5.3538664360530674e-05, Learning Rate: 0.000788\n",
      "Epoch 7763/40000, Loss: 5.3382602345664054e-05, Learning Rate: 0.000788\n",
      "Epoch 7764/40000, Loss: 5.268415407044813e-05, Learning Rate: 0.000788\n",
      "Epoch 7765/40000, Loss: 8.632412209408358e-05, Learning Rate: 0.000788\n",
      "Epoch 7766/40000, Loss: 2.55286977335345e-05, Learning Rate: 0.000788\n",
      "Epoch 7767/40000, Loss: 8.598143176641315e-05, Learning Rate: 0.000787\n",
      "Epoch 7768/40000, Loss: 5.334206798579544e-05, Learning Rate: 0.000787\n",
      "Epoch 7769/40000, Loss: 9.514553676126525e-05, Learning Rate: 0.000787\n",
      "Epoch 7770/40000, Loss: 5.674021667800844e-05, Learning Rate: 0.000787\n",
      "Epoch 7771/40000, Loss: 8.585652540205047e-05, Learning Rate: 0.000787\n",
      "Epoch 7772/40000, Loss: 9.55603682086803e-05, Learning Rate: 0.000787\n",
      "Epoch 7773/40000, Loss: 8.594740938860923e-05, Learning Rate: 0.000787\n",
      "Epoch 7774/40000, Loss: 8.58079656609334e-05, Learning Rate: 0.000787\n",
      "Epoch 7775/40000, Loss: 5.639608934870921e-05, Learning Rate: 0.000787\n",
      "Epoch 7776/40000, Loss: 6.612403376493603e-05, Learning Rate: 0.000787\n",
      "Epoch 7777/40000, Loss: 8.540344424545765e-05, Learning Rate: 0.000787\n",
      "Epoch 7778/40000, Loss: 6.68463617330417e-05, Learning Rate: 0.000786\n",
      "Epoch 7779/40000, Loss: 2.5030847609741613e-05, Learning Rate: 0.000786\n",
      "Epoch 7780/40000, Loss: 6.620718340855092e-05, Learning Rate: 0.000786\n",
      "Epoch 7781/40000, Loss: 0.000100853962067049, Learning Rate: 0.000786\n",
      "Epoch 7782/40000, Loss: 6.667480920441449e-05, Learning Rate: 0.000786\n",
      "Epoch 7783/40000, Loss: 8.742554928176105e-05, Learning Rate: 0.000786\n",
      "Epoch 7784/40000, Loss: 8.741729834582657e-05, Learning Rate: 0.000786\n",
      "Epoch 7785/40000, Loss: 5.8537265431368724e-05, Learning Rate: 0.000786\n",
      "Epoch 7786/40000, Loss: 5.786727342638187e-05, Learning Rate: 0.000786\n",
      "Epoch 7787/40000, Loss: 2.5705516236484982e-05, Learning Rate: 0.000786\n",
      "Epoch 7788/40000, Loss: 6.639471394009888e-05, Learning Rate: 0.000786\n",
      "Epoch 7789/40000, Loss: 5.754239100497216e-05, Learning Rate: 0.000785\n",
      "Epoch 7790/40000, Loss: 2.5239276510546915e-05, Learning Rate: 0.000785\n",
      "Epoch 7791/40000, Loss: 5.303649959387258e-05, Learning Rate: 0.000785\n",
      "Epoch 7792/40000, Loss: 5.2914627303835005e-05, Learning Rate: 0.000785\n",
      "Epoch 7793/40000, Loss: 6.638700870098546e-05, Learning Rate: 0.000785\n",
      "Epoch 7794/40000, Loss: 2.5671111870906316e-05, Learning Rate: 0.000785\n",
      "Epoch 7795/40000, Loss: 5.336914910003543e-05, Learning Rate: 0.000785\n",
      "Epoch 7796/40000, Loss: 8.688042726134881e-05, Learning Rate: 0.000785\n",
      "Epoch 7797/40000, Loss: 8.593264647061005e-05, Learning Rate: 0.000785\n",
      "Epoch 7798/40000, Loss: 9.571819100528955e-05, Learning Rate: 0.000785\n",
      "Epoch 7799/40000, Loss: 2.5081197236431763e-05, Learning Rate: 0.000784\n",
      "Epoch 7800/40000, Loss: 5.573645466938615e-05, Learning Rate: 0.000784\n",
      "Epoch 7801/40000, Loss: 5.487133239512332e-05, Learning Rate: 0.000784\n",
      "Epoch 7802/40000, Loss: 8.735751180211082e-05, Learning Rate: 0.000784\n",
      "Epoch 7803/40000, Loss: 8.863177936291322e-05, Learning Rate: 0.000784\n",
      "Epoch 7804/40000, Loss: 9.670246072346345e-05, Learning Rate: 0.000784\n",
      "Epoch 7805/40000, Loss: 5.5542306654388085e-05, Learning Rate: 0.000784\n",
      "Epoch 7806/40000, Loss: 9.699013753561303e-05, Learning Rate: 0.000784\n",
      "Epoch 7807/40000, Loss: 2.827301614161115e-05, Learning Rate: 0.000784\n",
      "Epoch 7808/40000, Loss: 2.912264062615577e-05, Learning Rate: 0.000784\n",
      "Epoch 7809/40000, Loss: 6.110135291237384e-05, Learning Rate: 0.000784\n",
      "Epoch 7810/40000, Loss: 0.0001018791226670146, Learning Rate: 0.000783\n",
      "Epoch 7811/40000, Loss: 6.37229168205522e-05, Learning Rate: 0.000783\n",
      "Epoch 7812/40000, Loss: 0.00010844664211617783, Learning Rate: 0.000783\n",
      "Epoch 7813/40000, Loss: 6.77240313962102e-05, Learning Rate: 0.000783\n",
      "Epoch 7814/40000, Loss: 0.00012325884017627686, Learning Rate: 0.000783\n",
      "Epoch 7815/40000, Loss: 7.701241702307016e-05, Learning Rate: 0.000783\n",
      "Epoch 7816/40000, Loss: 9.647403203416616e-05, Learning Rate: 0.000783\n",
      "Epoch 7817/40000, Loss: 0.00010972529707942158, Learning Rate: 0.000783\n",
      "Epoch 7818/40000, Loss: 6.198602932272479e-05, Learning Rate: 0.000783\n",
      "Epoch 7819/40000, Loss: 0.00010544843098614365, Learning Rate: 0.000783\n",
      "Epoch 7820/40000, Loss: 3.3521824661875144e-05, Learning Rate: 0.000782\n",
      "Epoch 7821/40000, Loss: 7.468753028661013e-05, Learning Rate: 0.000782\n",
      "Epoch 7822/40000, Loss: 9.770273754838854e-05, Learning Rate: 0.000782\n",
      "Epoch 7823/40000, Loss: 9.948344086296856e-05, Learning Rate: 0.000782\n",
      "Epoch 7824/40000, Loss: 9.659482020651922e-05, Learning Rate: 0.000782\n",
      "Epoch 7825/40000, Loss: 0.0001202329876832664, Learning Rate: 0.000782\n",
      "Epoch 7826/40000, Loss: 0.00014848938735667616, Learning Rate: 0.000782\n",
      "Epoch 7827/40000, Loss: 8.438742952421308e-05, Learning Rate: 0.000782\n",
      "Epoch 7828/40000, Loss: 9.844811575021595e-05, Learning Rate: 0.000782\n",
      "Epoch 7829/40000, Loss: 0.00015136889123823494, Learning Rate: 0.000782\n",
      "Epoch 7830/40000, Loss: 9.96147355181165e-05, Learning Rate: 0.000782\n",
      "Epoch 7831/40000, Loss: 0.00014153175288811326, Learning Rate: 0.000781\n",
      "Epoch 7832/40000, Loss: 0.0001454554294468835, Learning Rate: 0.000781\n",
      "Epoch 7833/40000, Loss: 0.0001572121400386095, Learning Rate: 0.000781\n",
      "Epoch 7834/40000, Loss: 0.00011866766726598144, Learning Rate: 0.000781\n",
      "Epoch 7835/40000, Loss: 0.00011041837569791824, Learning Rate: 0.000781\n",
      "Epoch 7836/40000, Loss: 0.00016549689462408423, Learning Rate: 0.000781\n",
      "Epoch 7837/40000, Loss: 7.967751298565418e-05, Learning Rate: 0.000781\n",
      "Epoch 7838/40000, Loss: 0.000175011096871458, Learning Rate: 0.000781\n",
      "Epoch 7839/40000, Loss: 0.00010540257790125906, Learning Rate: 0.000781\n",
      "Epoch 7840/40000, Loss: 0.000104642087535467, Learning Rate: 0.000781\n",
      "Epoch 7841/40000, Loss: 0.00010156320786336437, Learning Rate: 0.000781\n",
      "Epoch 7842/40000, Loss: 6.238577043404803e-05, Learning Rate: 0.000780\n",
      "Epoch 7843/40000, Loss: 0.00014059290697332472, Learning Rate: 0.000780\n",
      "Epoch 7844/40000, Loss: 0.00010951468721032143, Learning Rate: 0.000780\n",
      "Epoch 7845/40000, Loss: 0.00014922725677024573, Learning Rate: 0.000780\n",
      "Epoch 7846/40000, Loss: 0.00012351803889032453, Learning Rate: 0.000780\n",
      "Epoch 7847/40000, Loss: 9.128056990448385e-05, Learning Rate: 0.000780\n",
      "Epoch 7848/40000, Loss: 8.276770677184686e-05, Learning Rate: 0.000780\n",
      "Epoch 7849/40000, Loss: 0.00011504792928462848, Learning Rate: 0.000780\n",
      "Epoch 7850/40000, Loss: 8.983682346297428e-05, Learning Rate: 0.000780\n",
      "Epoch 7851/40000, Loss: 8.280679321615025e-05, Learning Rate: 0.000780\n",
      "Epoch 7852/40000, Loss: 8.894276106730103e-05, Learning Rate: 0.000780\n",
      "Epoch 7853/40000, Loss: 8.576233085477725e-05, Learning Rate: 0.000779\n",
      "Epoch 7854/40000, Loss: 0.00010349908552598208, Learning Rate: 0.000779\n",
      "Epoch 7855/40000, Loss: 0.00010513886081753299, Learning Rate: 0.000779\n",
      "Epoch 7856/40000, Loss: 9.999001485994086e-05, Learning Rate: 0.000779\n",
      "Epoch 7857/40000, Loss: 6.909287185408175e-05, Learning Rate: 0.000779\n",
      "Epoch 7858/40000, Loss: 3.1714520446257666e-05, Learning Rate: 0.000779\n",
      "Epoch 7859/40000, Loss: 6.0068341554142535e-05, Learning Rate: 0.000779\n",
      "Epoch 7860/40000, Loss: 6.843655864940956e-05, Learning Rate: 0.000779\n",
      "Epoch 7861/40000, Loss: 6.755308277206495e-05, Learning Rate: 0.000779\n",
      "Epoch 7862/40000, Loss: 7.019366603344679e-05, Learning Rate: 0.000779\n",
      "Epoch 7863/40000, Loss: 2.9331051337067038e-05, Learning Rate: 0.000778\n",
      "Epoch 7864/40000, Loss: 0.00010520715295569971, Learning Rate: 0.000778\n",
      "Epoch 7865/40000, Loss: 7.240262493724003e-05, Learning Rate: 0.000778\n",
      "Epoch 7866/40000, Loss: 6.955555727472529e-05, Learning Rate: 0.000778\n",
      "Epoch 7867/40000, Loss: 9.519766172161326e-05, Learning Rate: 0.000778\n",
      "Epoch 7868/40000, Loss: 0.00010062848741654307, Learning Rate: 0.000778\n",
      "Epoch 7869/40000, Loss: 5.890117245144211e-05, Learning Rate: 0.000778\n",
      "Epoch 7870/40000, Loss: 0.0001146513459389098, Learning Rate: 0.000778\n",
      "Epoch 7871/40000, Loss: 7.051747525110841e-05, Learning Rate: 0.000778\n",
      "Epoch 7872/40000, Loss: 9.993774438044056e-05, Learning Rate: 0.000778\n",
      "Epoch 7873/40000, Loss: 6.9429625000339e-05, Learning Rate: 0.000778\n",
      "Epoch 7874/40000, Loss: 5.644016346195713e-05, Learning Rate: 0.000777\n",
      "Epoch 7875/40000, Loss: 9.666076948633417e-05, Learning Rate: 0.000777\n",
      "Epoch 7876/40000, Loss: 6.80514276609756e-05, Learning Rate: 0.000777\n",
      "Epoch 7877/40000, Loss: 0.00010404562635812908, Learning Rate: 0.000777\n",
      "Epoch 7878/40000, Loss: 9.385751764057204e-05, Learning Rate: 0.000777\n",
      "Epoch 7879/40000, Loss: 2.824649891408626e-05, Learning Rate: 0.000777\n",
      "Epoch 7880/40000, Loss: 0.00011013435141649097, Learning Rate: 0.000777\n",
      "Epoch 7881/40000, Loss: 5.809255890198983e-05, Learning Rate: 0.000777\n",
      "Epoch 7882/40000, Loss: 6.983181810937822e-05, Learning Rate: 0.000777\n",
      "Epoch 7883/40000, Loss: 7.089799328241497e-05, Learning Rate: 0.000777\n",
      "Epoch 7884/40000, Loss: 7.002145139267668e-05, Learning Rate: 0.000777\n",
      "Epoch 7885/40000, Loss: 0.0001001482960418798, Learning Rate: 0.000776\n",
      "Epoch 7886/40000, Loss: 6.0476708313217387e-05, Learning Rate: 0.000776\n",
      "Epoch 7887/40000, Loss: 3.4090306144207716e-05, Learning Rate: 0.000776\n",
      "Epoch 7888/40000, Loss: 7.461164932465181e-05, Learning Rate: 0.000776\n",
      "Epoch 7889/40000, Loss: 6.967572699068114e-05, Learning Rate: 0.000776\n",
      "Epoch 7890/40000, Loss: 8.401209197472781e-05, Learning Rate: 0.000776\n",
      "Epoch 7891/40000, Loss: 0.00010871627455344424, Learning Rate: 0.000776\n",
      "Epoch 7892/40000, Loss: 6.044179463060573e-05, Learning Rate: 0.000776\n",
      "Epoch 7893/40000, Loss: 7.200402615126222e-05, Learning Rate: 0.000776\n",
      "Epoch 7894/40000, Loss: 3.9190017560031265e-05, Learning Rate: 0.000776\n",
      "Epoch 7895/40000, Loss: 0.00010144393309019506, Learning Rate: 0.000775\n",
      "Epoch 7896/40000, Loss: 9.311142639489844e-05, Learning Rate: 0.000775\n",
      "Epoch 7897/40000, Loss: 5.894153582630679e-05, Learning Rate: 0.000775\n",
      "Epoch 7898/40000, Loss: 9.55554933170788e-05, Learning Rate: 0.000775\n",
      "Epoch 7899/40000, Loss: 6.896514969412237e-05, Learning Rate: 0.000775\n",
      "Epoch 7900/40000, Loss: 6.802491407142952e-05, Learning Rate: 0.000775\n",
      "Epoch 7901/40000, Loss: 3.374251173227094e-05, Learning Rate: 0.000775\n",
      "Epoch 7902/40000, Loss: 7.055443711578846e-05, Learning Rate: 0.000775\n",
      "Epoch 7903/40000, Loss: 6.779137038392946e-05, Learning Rate: 0.000775\n",
      "Epoch 7904/40000, Loss: 3.3311822335235775e-05, Learning Rate: 0.000775\n",
      "Epoch 7905/40000, Loss: 0.00012557243462651968, Learning Rate: 0.000775\n",
      "Epoch 7906/40000, Loss: 0.00010741706500994042, Learning Rate: 0.000774\n",
      "Epoch 7907/40000, Loss: 3.504686173982918e-05, Learning Rate: 0.000774\n",
      "Epoch 7908/40000, Loss: 2.953318653453607e-05, Learning Rate: 0.000774\n",
      "Epoch 7909/40000, Loss: 3.085220669163391e-05, Learning Rate: 0.000774\n",
      "Epoch 7910/40000, Loss: 5.603284080279991e-05, Learning Rate: 0.000774\n",
      "Epoch 7911/40000, Loss: 2.938430588983465e-05, Learning Rate: 0.000774\n",
      "Epoch 7912/40000, Loss: 9.208655683323741e-05, Learning Rate: 0.000774\n",
      "Epoch 7913/40000, Loss: 6.700278026983142e-05, Learning Rate: 0.000774\n",
      "Epoch 7914/40000, Loss: 7.035877206362784e-05, Learning Rate: 0.000774\n",
      "Epoch 7915/40000, Loss: 0.00010109881986863911, Learning Rate: 0.000774\n",
      "Epoch 7916/40000, Loss: 7.38280505174771e-05, Learning Rate: 0.000774\n",
      "Epoch 7917/40000, Loss: 0.0001122171597671695, Learning Rate: 0.000773\n",
      "Epoch 7918/40000, Loss: 2.9125221772119403e-05, Learning Rate: 0.000773\n",
      "Epoch 7919/40000, Loss: 0.0001001827185973525, Learning Rate: 0.000773\n",
      "Epoch 7920/40000, Loss: 6.350986222969368e-05, Learning Rate: 0.000773\n",
      "Epoch 7921/40000, Loss: 6.917674909345806e-05, Learning Rate: 0.000773\n",
      "Epoch 7922/40000, Loss: 6.756497168680653e-05, Learning Rate: 0.000773\n",
      "Epoch 7923/40000, Loss: 6.568041135324165e-05, Learning Rate: 0.000773\n",
      "Epoch 7924/40000, Loss: 7.084786193445325e-05, Learning Rate: 0.000773\n",
      "Epoch 7925/40000, Loss: 5.936107481829822e-05, Learning Rate: 0.000773\n",
      "Epoch 7926/40000, Loss: 6.835559906903654e-05, Learning Rate: 0.000773\n",
      "Epoch 7927/40000, Loss: 6.373623909894377e-05, Learning Rate: 0.000773\n",
      "Epoch 7928/40000, Loss: 5.921929914620705e-05, Learning Rate: 0.000772\n",
      "Epoch 7929/40000, Loss: 6.566449883393943e-05, Learning Rate: 0.000772\n",
      "Epoch 7930/40000, Loss: 7.063568045850843e-05, Learning Rate: 0.000772\n",
      "Epoch 7931/40000, Loss: 5.7348446716787294e-05, Learning Rate: 0.000772\n",
      "Epoch 7932/40000, Loss: 9.334836795460433e-05, Learning Rate: 0.000772\n",
      "Epoch 7933/40000, Loss: 2.79372379736742e-05, Learning Rate: 0.000772\n",
      "Epoch 7934/40000, Loss: 2.6289299057680182e-05, Learning Rate: 0.000772\n",
      "Epoch 7935/40000, Loss: 6.997485616011545e-05, Learning Rate: 0.000772\n",
      "Epoch 7936/40000, Loss: 2.634063275763765e-05, Learning Rate: 0.000772\n",
      "Epoch 7937/40000, Loss: 9.175568993669003e-05, Learning Rate: 0.000772\n",
      "Epoch 7938/40000, Loss: 5.4680123867001384e-05, Learning Rate: 0.000771\n",
      "Epoch 7939/40000, Loss: 5.813657116959803e-05, Learning Rate: 0.000771\n",
      "Epoch 7940/40000, Loss: 6.746885628672317e-05, Learning Rate: 0.000771\n",
      "Epoch 7941/40000, Loss: 6.359960389090702e-05, Learning Rate: 0.000771\n",
      "Epoch 7942/40000, Loss: 8.920156687963754e-05, Learning Rate: 0.000771\n",
      "Epoch 7943/40000, Loss: 0.00010239598486805335, Learning Rate: 0.000771\n",
      "Epoch 7944/40000, Loss: 8.154585520969704e-05, Learning Rate: 0.000771\n",
      "Epoch 7945/40000, Loss: 0.00011291549890302122, Learning Rate: 0.000771\n",
      "Epoch 7946/40000, Loss: 9.51894762692973e-05, Learning Rate: 0.000771\n",
      "Epoch 7947/40000, Loss: 6.463305908255279e-05, Learning Rate: 0.000771\n",
      "Epoch 7948/40000, Loss: 8.421131497016177e-05, Learning Rate: 0.000771\n",
      "Epoch 7949/40000, Loss: 0.0001235008821822703, Learning Rate: 0.000770\n",
      "Epoch 7950/40000, Loss: 0.00010874532017624006, Learning Rate: 0.000770\n",
      "Epoch 7951/40000, Loss: 6.267495336942375e-05, Learning Rate: 0.000770\n",
      "Epoch 7952/40000, Loss: 6.70961890136823e-05, Learning Rate: 0.000770\n",
      "Epoch 7953/40000, Loss: 0.00011150178761454299, Learning Rate: 0.000770\n",
      "Epoch 7954/40000, Loss: 7.679707050556317e-05, Learning Rate: 0.000770\n",
      "Epoch 7955/40000, Loss: 7.075726171024144e-05, Learning Rate: 0.000770\n",
      "Epoch 7956/40000, Loss: 0.00011336331954225898, Learning Rate: 0.000770\n",
      "Epoch 7957/40000, Loss: 4.661977072828449e-05, Learning Rate: 0.000770\n",
      "Epoch 7958/40000, Loss: 4.354002885520458e-05, Learning Rate: 0.000770\n",
      "Epoch 7959/40000, Loss: 7.716722757322714e-05, Learning Rate: 0.000770\n",
      "Epoch 7960/40000, Loss: 0.00019290462660137564, Learning Rate: 0.000769\n",
      "Epoch 7961/40000, Loss: 0.00011303950304863974, Learning Rate: 0.000769\n",
      "Epoch 7962/40000, Loss: 0.000124254627735354, Learning Rate: 0.000769\n",
      "Epoch 7963/40000, Loss: 0.00012691305892076343, Learning Rate: 0.000769\n",
      "Epoch 7964/40000, Loss: 8.957219688454643e-05, Learning Rate: 0.000769\n",
      "Epoch 7965/40000, Loss: 4.474849265534431e-05, Learning Rate: 0.000769\n",
      "Epoch 7966/40000, Loss: 0.00011011260357918218, Learning Rate: 0.000769\n",
      "Epoch 7967/40000, Loss: 8.071789488894865e-05, Learning Rate: 0.000769\n",
      "Epoch 7968/40000, Loss: 6.03918670094572e-05, Learning Rate: 0.000769\n",
      "Epoch 7969/40000, Loss: 0.00012133632117183879, Learning Rate: 0.000769\n",
      "Epoch 7970/40000, Loss: 7.001221820246428e-05, Learning Rate: 0.000769\n",
      "Epoch 7971/40000, Loss: 0.00012999295722693205, Learning Rate: 0.000768\n",
      "Epoch 7972/40000, Loss: 0.0001058137568179518, Learning Rate: 0.000768\n",
      "Epoch 7973/40000, Loss: 9.120754839386791e-05, Learning Rate: 0.000768\n",
      "Epoch 7974/40000, Loss: 0.00012826845340896398, Learning Rate: 0.000768\n",
      "Epoch 7975/40000, Loss: 7.766108319628984e-05, Learning Rate: 0.000768\n",
      "Epoch 7976/40000, Loss: 9.137319284491241e-05, Learning Rate: 0.000768\n",
      "Epoch 7977/40000, Loss: 0.00010563066462054849, Learning Rate: 0.000768\n",
      "Epoch 7978/40000, Loss: 8.123785664793104e-05, Learning Rate: 0.000768\n",
      "Epoch 7979/40000, Loss: 3.107144948444329e-05, Learning Rate: 0.000768\n",
      "Epoch 7980/40000, Loss: 0.00010161529644392431, Learning Rate: 0.000768\n",
      "Epoch 7981/40000, Loss: 3.6083431041333824e-05, Learning Rate: 0.000768\n",
      "Epoch 7982/40000, Loss: 7.36044894438237e-05, Learning Rate: 0.000767\n",
      "Epoch 7983/40000, Loss: 8.856577187543735e-05, Learning Rate: 0.000767\n",
      "Epoch 7984/40000, Loss: 8.20524146547541e-05, Learning Rate: 0.000767\n",
      "Epoch 7985/40000, Loss: 7.98748224042356e-05, Learning Rate: 0.000767\n",
      "Epoch 7986/40000, Loss: 7.941773219499737e-05, Learning Rate: 0.000767\n",
      "Epoch 7987/40000, Loss: 0.00016387816867791116, Learning Rate: 0.000767\n",
      "Epoch 7988/40000, Loss: 7.410690886899829e-05, Learning Rate: 0.000767\n",
      "Epoch 7989/40000, Loss: 0.00011020019155694172, Learning Rate: 0.000767\n",
      "Epoch 7990/40000, Loss: 3.193968586856499e-05, Learning Rate: 0.000767\n",
      "Epoch 7991/40000, Loss: 7.578801887575537e-05, Learning Rate: 0.000767\n",
      "Epoch 7992/40000, Loss: 9.57135489443317e-05, Learning Rate: 0.000767\n",
      "Epoch 7993/40000, Loss: 7.429623656207696e-05, Learning Rate: 0.000766\n",
      "Epoch 7994/40000, Loss: 5.604461330221966e-05, Learning Rate: 0.000766\n",
      "Epoch 7995/40000, Loss: 5.4410527809523046e-05, Learning Rate: 0.000766\n",
      "Epoch 7996/40000, Loss: 0.00010240818664897233, Learning Rate: 0.000766\n",
      "Epoch 7997/40000, Loss: 0.00010045756062027067, Learning Rate: 0.000766\n",
      "Epoch 7998/40000, Loss: 9.279783989768475e-05, Learning Rate: 0.000766\n",
      "Epoch 7999/40000, Loss: 6.0745955124730244e-05, Learning Rate: 0.000766\n",
      "Epoch 8000/40000, Loss: 6.140651385067031e-05, Learning Rate: 0.000766\n",
      "Epoch 8001/40000, Loss: 0.00010018111788667738, Learning Rate: 0.000766\n",
      "Epoch 8002/40000, Loss: 9.058373689185828e-05, Learning Rate: 0.000766\n",
      "Epoch 8003/40000, Loss: 6.690845475532115e-05, Learning Rate: 0.000766\n",
      "Epoch 8004/40000, Loss: 9.172550198854879e-05, Learning Rate: 0.000765\n",
      "Epoch 8005/40000, Loss: 9.991189290303737e-05, Learning Rate: 0.000765\n",
      "Epoch 8006/40000, Loss: 6.952947296667844e-05, Learning Rate: 0.000765\n",
      "Epoch 8007/40000, Loss: 5.505866283783689e-05, Learning Rate: 0.000765\n",
      "Epoch 8008/40000, Loss: 5.6426892115268856e-05, Learning Rate: 0.000765\n",
      "Epoch 8009/40000, Loss: 7.323820318561047e-05, Learning Rate: 0.000765\n",
      "Epoch 8010/40000, Loss: 3.4002045140368864e-05, Learning Rate: 0.000765\n",
      "Epoch 8011/40000, Loss: 9.853355004452169e-05, Learning Rate: 0.000765\n",
      "Epoch 8012/40000, Loss: 6.0893973568454385e-05, Learning Rate: 0.000765\n",
      "Epoch 8013/40000, Loss: 7.337871647905558e-05, Learning Rate: 0.000765\n",
      "Epoch 8014/40000, Loss: 0.00010844435018952936, Learning Rate: 0.000764\n",
      "Epoch 8015/40000, Loss: 0.00010019874025601894, Learning Rate: 0.000764\n",
      "Epoch 8016/40000, Loss: 7.201080734375864e-05, Learning Rate: 0.000764\n",
      "Epoch 8017/40000, Loss: 4.9294845666736364e-05, Learning Rate: 0.000764\n",
      "Epoch 8018/40000, Loss: 6.54456889606081e-05, Learning Rate: 0.000764\n",
      "Epoch 8019/40000, Loss: 3.776358425966464e-05, Learning Rate: 0.000764\n",
      "Epoch 8020/40000, Loss: 9.972699626814574e-05, Learning Rate: 0.000764\n",
      "Epoch 8021/40000, Loss: 0.00010031439160229638, Learning Rate: 0.000764\n",
      "Epoch 8022/40000, Loss: 3.0088731364230625e-05, Learning Rate: 0.000764\n",
      "Epoch 8023/40000, Loss: 9.845996828516945e-05, Learning Rate: 0.000764\n",
      "Epoch 8024/40000, Loss: 9.913974645314738e-05, Learning Rate: 0.000764\n",
      "Epoch 8025/40000, Loss: 2.919796679634601e-05, Learning Rate: 0.000763\n",
      "Epoch 8026/40000, Loss: 6.952007242944092e-05, Learning Rate: 0.000763\n",
      "Epoch 8027/40000, Loss: 9.160211629932746e-05, Learning Rate: 0.000763\n",
      "Epoch 8028/40000, Loss: 0.00010108498827321455, Learning Rate: 0.000763\n",
      "Epoch 8029/40000, Loss: 0.00011939286923734471, Learning Rate: 0.000763\n",
      "Epoch 8030/40000, Loss: 6.114457210060209e-05, Learning Rate: 0.000763\n",
      "Epoch 8031/40000, Loss: 0.00010261877469019964, Learning Rate: 0.000763\n",
      "Epoch 8032/40000, Loss: 7.151300087571144e-05, Learning Rate: 0.000763\n",
      "Epoch 8033/40000, Loss: 0.0001045088647515513, Learning Rate: 0.000763\n",
      "Epoch 8034/40000, Loss: 9.822221181821078e-05, Learning Rate: 0.000763\n",
      "Epoch 8035/40000, Loss: 9.662249067332596e-05, Learning Rate: 0.000763\n",
      "Epoch 8036/40000, Loss: 3.105615178355947e-05, Learning Rate: 0.000762\n",
      "Epoch 8037/40000, Loss: 0.00010690989438444376, Learning Rate: 0.000762\n",
      "Epoch 8038/40000, Loss: 3.975759682361968e-05, Learning Rate: 0.000762\n",
      "Epoch 8039/40000, Loss: 7.176299550337717e-05, Learning Rate: 0.000762\n",
      "Epoch 8040/40000, Loss: 0.00011829945287900046, Learning Rate: 0.000762\n",
      "Epoch 8041/40000, Loss: 9.439906716579571e-05, Learning Rate: 0.000762\n",
      "Epoch 8042/40000, Loss: 0.00013051800488028675, Learning Rate: 0.000762\n",
      "Epoch 8043/40000, Loss: 8.080479165073484e-05, Learning Rate: 0.000762\n",
      "Epoch 8044/40000, Loss: 5.92823198530823e-05, Learning Rate: 0.000762\n",
      "Epoch 8045/40000, Loss: 3.410200588405132e-05, Learning Rate: 0.000762\n",
      "Epoch 8046/40000, Loss: 2.866776594601106e-05, Learning Rate: 0.000762\n",
      "Epoch 8047/40000, Loss: 6.980542821111158e-05, Learning Rate: 0.000761\n",
      "Epoch 8048/40000, Loss: 6.865568138891831e-05, Learning Rate: 0.000761\n",
      "Epoch 8049/40000, Loss: 6.543111521750689e-05, Learning Rate: 0.000761\n",
      "Epoch 8050/40000, Loss: 6.694129842799157e-05, Learning Rate: 0.000761\n",
      "Epoch 8051/40000, Loss: 5.396072083385661e-05, Learning Rate: 0.000761\n",
      "Epoch 8052/40000, Loss: 7.950725557748228e-05, Learning Rate: 0.000761\n",
      "Epoch 8053/40000, Loss: 6.276801286730915e-05, Learning Rate: 0.000761\n",
      "Epoch 8054/40000, Loss: 9.839308040682226e-05, Learning Rate: 0.000761\n",
      "Epoch 8055/40000, Loss: 7.211521005956456e-05, Learning Rate: 0.000761\n",
      "Epoch 8056/40000, Loss: 0.00010582510731182992, Learning Rate: 0.000761\n",
      "Epoch 8057/40000, Loss: 3.045875928364694e-05, Learning Rate: 0.000761\n",
      "Epoch 8058/40000, Loss: 9.070130181498826e-05, Learning Rate: 0.000760\n",
      "Epoch 8059/40000, Loss: 6.609596312046051e-05, Learning Rate: 0.000760\n",
      "Epoch 8060/40000, Loss: 0.00011765411909436807, Learning Rate: 0.000760\n",
      "Epoch 8061/40000, Loss: 3.015714537468739e-05, Learning Rate: 0.000760\n",
      "Epoch 8062/40000, Loss: 5.571599831455387e-05, Learning Rate: 0.000760\n",
      "Epoch 8063/40000, Loss: 8.886303839972243e-05, Learning Rate: 0.000760\n",
      "Epoch 8064/40000, Loss: 2.9976550649735145e-05, Learning Rate: 0.000760\n",
      "Epoch 8065/40000, Loss: 5.874292764929123e-05, Learning Rate: 0.000760\n",
      "Epoch 8066/40000, Loss: 2.8278949685045518e-05, Learning Rate: 0.000760\n",
      "Epoch 8067/40000, Loss: 5.2460836741374806e-05, Learning Rate: 0.000760\n",
      "Epoch 8068/40000, Loss: 8.813333988655359e-05, Learning Rate: 0.000760\n",
      "Epoch 8069/40000, Loss: 5.4124011512612924e-05, Learning Rate: 0.000759\n",
      "Epoch 8070/40000, Loss: 2.640187449287623e-05, Learning Rate: 0.000759\n",
      "Epoch 8071/40000, Loss: 9.595706796972081e-05, Learning Rate: 0.000759\n",
      "Epoch 8072/40000, Loss: 5.693352431990206e-05, Learning Rate: 0.000759\n",
      "Epoch 8073/40000, Loss: 8.701348269823939e-05, Learning Rate: 0.000759\n",
      "Epoch 8074/40000, Loss: 9.448725177207962e-05, Learning Rate: 0.000759\n",
      "Epoch 8075/40000, Loss: 9.445672912988812e-05, Learning Rate: 0.000759\n",
      "Epoch 8076/40000, Loss: 9.422097355127335e-05, Learning Rate: 0.000759\n",
      "Epoch 8077/40000, Loss: 5.22857517353259e-05, Learning Rate: 0.000759\n",
      "Epoch 8078/40000, Loss: 6.740375101799145e-05, Learning Rate: 0.000759\n",
      "Epoch 8079/40000, Loss: 6.320980901364237e-05, Learning Rate: 0.000759\n",
      "Epoch 8080/40000, Loss: 6.595347076654434e-05, Learning Rate: 0.000758\n",
      "Epoch 8081/40000, Loss: 9.020520519698039e-05, Learning Rate: 0.000758\n",
      "Epoch 8082/40000, Loss: 6.314137135632336e-05, Learning Rate: 0.000758\n",
      "Epoch 8083/40000, Loss: 0.00011648181680357084, Learning Rate: 0.000758\n",
      "Epoch 8084/40000, Loss: 5.609263462247327e-05, Learning Rate: 0.000758\n",
      "Epoch 8085/40000, Loss: 0.00010488987027201802, Learning Rate: 0.000758\n",
      "Epoch 8086/40000, Loss: 6.427563494071364e-05, Learning Rate: 0.000758\n",
      "Epoch 8087/40000, Loss: 0.00010232078057015315, Learning Rate: 0.000758\n",
      "Epoch 8088/40000, Loss: 3.207290137652308e-05, Learning Rate: 0.000758\n",
      "Epoch 8089/40000, Loss: 7.178355735959485e-05, Learning Rate: 0.000758\n",
      "Epoch 8090/40000, Loss: 0.00011739026376744732, Learning Rate: 0.000758\n",
      "Epoch 8091/40000, Loss: 9.290208254242316e-05, Learning Rate: 0.000757\n",
      "Epoch 8092/40000, Loss: 4.376260767458007e-05, Learning Rate: 0.000757\n",
      "Epoch 8093/40000, Loss: 7.755571277812123e-05, Learning Rate: 0.000757\n",
      "Epoch 8094/40000, Loss: 6.137922900961712e-05, Learning Rate: 0.000757\n",
      "Epoch 8095/40000, Loss: 0.00010791758541017771, Learning Rate: 0.000757\n",
      "Epoch 8096/40000, Loss: 9.958713053492829e-05, Learning Rate: 0.000757\n",
      "Epoch 8097/40000, Loss: 3.570266562746838e-05, Learning Rate: 0.000757\n",
      "Epoch 8098/40000, Loss: 5.589777720160782e-05, Learning Rate: 0.000757\n",
      "Epoch 8099/40000, Loss: 5.75804770051036e-05, Learning Rate: 0.000757\n",
      "Epoch 8100/40000, Loss: 2.9686347261304036e-05, Learning Rate: 0.000757\n",
      "Epoch 8101/40000, Loss: 2.977288022520952e-05, Learning Rate: 0.000757\n",
      "Epoch 8102/40000, Loss: 6.945973291294649e-05, Learning Rate: 0.000756\n",
      "Epoch 8103/40000, Loss: 0.00010045961244031787, Learning Rate: 0.000756\n",
      "Epoch 8104/40000, Loss: 7.572623871965334e-05, Learning Rate: 0.000756\n",
      "Epoch 8105/40000, Loss: 3.4926160878967494e-05, Learning Rate: 0.000756\n",
      "Epoch 8106/40000, Loss: 6.412639049813151e-05, Learning Rate: 0.000756\n",
      "Epoch 8107/40000, Loss: 0.00011031317990273237, Learning Rate: 0.000756\n",
      "Epoch 8108/40000, Loss: 3.431033474043943e-05, Learning Rate: 0.000756\n",
      "Epoch 8109/40000, Loss: 0.00010555281915003434, Learning Rate: 0.000756\n",
      "Epoch 8110/40000, Loss: 0.0001008564286166802, Learning Rate: 0.000756\n",
      "Epoch 8111/40000, Loss: 9.423957817489281e-05, Learning Rate: 0.000756\n",
      "Epoch 8112/40000, Loss: 5.9397407312644646e-05, Learning Rate: 0.000756\n",
      "Epoch 8113/40000, Loss: 9.934935224009678e-05, Learning Rate: 0.000755\n",
      "Epoch 8114/40000, Loss: 8.391037408728153e-05, Learning Rate: 0.000755\n",
      "Epoch 8115/40000, Loss: 6.135524745332077e-05, Learning Rate: 0.000755\n",
      "Epoch 8116/40000, Loss: 8.850123413139954e-05, Learning Rate: 0.000755\n",
      "Epoch 8117/40000, Loss: 9.896158735500649e-05, Learning Rate: 0.000755\n",
      "Epoch 8118/40000, Loss: 9.817560203373432e-05, Learning Rate: 0.000755\n",
      "Epoch 8119/40000, Loss: 3.3313186577288434e-05, Learning Rate: 0.000755\n",
      "Epoch 8120/40000, Loss: 9.260270599042997e-05, Learning Rate: 0.000755\n",
      "Epoch 8121/40000, Loss: 7.312763045774773e-05, Learning Rate: 0.000755\n",
      "Epoch 8122/40000, Loss: 3.628043850767426e-05, Learning Rate: 0.000755\n",
      "Epoch 8123/40000, Loss: 6.605515227420256e-05, Learning Rate: 0.000755\n",
      "Epoch 8124/40000, Loss: 5.8309022278990597e-05, Learning Rate: 0.000754\n",
      "Epoch 8125/40000, Loss: 3.277683208580129e-05, Learning Rate: 0.000754\n",
      "Epoch 8126/40000, Loss: 7.599432865390554e-05, Learning Rate: 0.000754\n",
      "Epoch 8127/40000, Loss: 5.9640282415784895e-05, Learning Rate: 0.000754\n",
      "Epoch 8128/40000, Loss: 5.4116560932016e-05, Learning Rate: 0.000754\n",
      "Epoch 8129/40000, Loss: 5.446207433124073e-05, Learning Rate: 0.000754\n",
      "Epoch 8130/40000, Loss: 2.894072895287536e-05, Learning Rate: 0.000754\n",
      "Epoch 8131/40000, Loss: 9.379374387208372e-05, Learning Rate: 0.000754\n",
      "Epoch 8132/40000, Loss: 2.925014450738672e-05, Learning Rate: 0.000754\n",
      "Epoch 8133/40000, Loss: 8.86616253410466e-05, Learning Rate: 0.000754\n",
      "Epoch 8134/40000, Loss: 9.754139318829402e-05, Learning Rate: 0.000754\n",
      "Epoch 8135/40000, Loss: 9.645892714615911e-05, Learning Rate: 0.000753\n",
      "Epoch 8136/40000, Loss: 2.6478815925656818e-05, Learning Rate: 0.000753\n",
      "Epoch 8137/40000, Loss: 2.5331568394904025e-05, Learning Rate: 0.000753\n",
      "Epoch 8138/40000, Loss: 2.5852546968962997e-05, Learning Rate: 0.000753\n",
      "Epoch 8139/40000, Loss: 9.59437747951597e-05, Learning Rate: 0.000753\n",
      "Epoch 8140/40000, Loss: 6.42938757664524e-05, Learning Rate: 0.000753\n",
      "Epoch 8141/40000, Loss: 5.860178134753369e-05, Learning Rate: 0.000753\n",
      "Epoch 8142/40000, Loss: 8.876435458660126e-05, Learning Rate: 0.000753\n",
      "Epoch 8143/40000, Loss: 5.311557833920233e-05, Learning Rate: 0.000753\n",
      "Epoch 8144/40000, Loss: 2.7318579668644816e-05, Learning Rate: 0.000753\n",
      "Epoch 8145/40000, Loss: 5.2973824494984e-05, Learning Rate: 0.000753\n",
      "Epoch 8146/40000, Loss: 9.173708531307057e-05, Learning Rate: 0.000752\n",
      "Epoch 8147/40000, Loss: 6.342068081721663e-05, Learning Rate: 0.000752\n",
      "Epoch 8148/40000, Loss: 5.440792301669717e-05, Learning Rate: 0.000752\n",
      "Epoch 8149/40000, Loss: 6.0653535911114886e-05, Learning Rate: 0.000752\n",
      "Epoch 8150/40000, Loss: 5.4860676755197346e-05, Learning Rate: 0.000752\n",
      "Epoch 8151/40000, Loss: 5.2984050853410736e-05, Learning Rate: 0.000752\n",
      "Epoch 8152/40000, Loss: 2.7762953322962858e-05, Learning Rate: 0.000752\n",
      "Epoch 8153/40000, Loss: 8.850565791362897e-05, Learning Rate: 0.000752\n",
      "Epoch 8154/40000, Loss: 9.169994882540777e-05, Learning Rate: 0.000752\n",
      "Epoch 8155/40000, Loss: 7.234772783704102e-05, Learning Rate: 0.000752\n",
      "Epoch 8156/40000, Loss: 6.6672197135631e-05, Learning Rate: 0.000752\n",
      "Epoch 8157/40000, Loss: 8.409334986936301e-05, Learning Rate: 0.000751\n",
      "Epoch 8158/40000, Loss: 4.8787704145070165e-05, Learning Rate: 0.000751\n",
      "Epoch 8159/40000, Loss: 3.369993646629155e-05, Learning Rate: 0.000751\n",
      "Epoch 8160/40000, Loss: 7.818704762030393e-05, Learning Rate: 0.000751\n",
      "Epoch 8161/40000, Loss: 0.00012360104301478714, Learning Rate: 0.000751\n",
      "Epoch 8162/40000, Loss: 0.0001176948644570075, Learning Rate: 0.000751\n",
      "Epoch 8163/40000, Loss: 0.00010077172191813588, Learning Rate: 0.000751\n",
      "Epoch 8164/40000, Loss: 7.452467252733186e-05, Learning Rate: 0.000751\n",
      "Epoch 8165/40000, Loss: 0.00012035819236189127, Learning Rate: 0.000751\n",
      "Epoch 8166/40000, Loss: 0.00012686919944826514, Learning Rate: 0.000751\n",
      "Epoch 8167/40000, Loss: 0.00010583462426438928, Learning Rate: 0.000751\n",
      "Epoch 8168/40000, Loss: 3.636046312749386e-05, Learning Rate: 0.000750\n",
      "Epoch 8169/40000, Loss: 5.8777135564014316e-05, Learning Rate: 0.000750\n",
      "Epoch 8170/40000, Loss: 7.208179158624262e-05, Learning Rate: 0.000750\n",
      "Epoch 8171/40000, Loss: 7.496342004742473e-05, Learning Rate: 0.000750\n",
      "Epoch 8172/40000, Loss: 6.715235213050619e-05, Learning Rate: 0.000750\n",
      "Epoch 8173/40000, Loss: 0.0001097065323847346, Learning Rate: 0.000750\n",
      "Epoch 8174/40000, Loss: 0.00011418072244850919, Learning Rate: 0.000750\n",
      "Epoch 8175/40000, Loss: 7.589157030452043e-05, Learning Rate: 0.000750\n",
      "Epoch 8176/40000, Loss: 0.00010333038517273962, Learning Rate: 0.000750\n",
      "Epoch 8177/40000, Loss: 6.909083458594978e-05, Learning Rate: 0.000750\n",
      "Epoch 8178/40000, Loss: 6.0231632232898846e-05, Learning Rate: 0.000750\n",
      "Epoch 8179/40000, Loss: 3.456622289377265e-05, Learning Rate: 0.000750\n",
      "Epoch 8180/40000, Loss: 7.798703154549003e-05, Learning Rate: 0.000749\n",
      "Epoch 8181/40000, Loss: 0.00010809570812853053, Learning Rate: 0.000749\n",
      "Epoch 8182/40000, Loss: 6.134415161795914e-05, Learning Rate: 0.000749\n",
      "Epoch 8183/40000, Loss: 5.787874761153944e-05, Learning Rate: 0.000749\n",
      "Epoch 8184/40000, Loss: 0.00010571801976766437, Learning Rate: 0.000749\n",
      "Epoch 8185/40000, Loss: 6.649248098256066e-05, Learning Rate: 0.000749\n",
      "Epoch 8186/40000, Loss: 5.7371649745618924e-05, Learning Rate: 0.000749\n",
      "Epoch 8187/40000, Loss: 9.73684509517625e-05, Learning Rate: 0.000749\n",
      "Epoch 8188/40000, Loss: 9.598290489520878e-05, Learning Rate: 0.000749\n",
      "Epoch 8189/40000, Loss: 6.72852256684564e-05, Learning Rate: 0.000749\n",
      "Epoch 8190/40000, Loss: 7.440054469043389e-05, Learning Rate: 0.000749\n",
      "Epoch 8191/40000, Loss: 8.313293074024841e-05, Learning Rate: 0.000748\n",
      "Epoch 8192/40000, Loss: 6.83386460877955e-05, Learning Rate: 0.000748\n",
      "Epoch 8193/40000, Loss: 9.254821634385735e-05, Learning Rate: 0.000748\n",
      "Epoch 8194/40000, Loss: 7.916642061900347e-05, Learning Rate: 0.000748\n",
      "Epoch 8195/40000, Loss: 0.00010079347703140229, Learning Rate: 0.000748\n",
      "Epoch 8196/40000, Loss: 5.487704765982926e-05, Learning Rate: 0.000748\n",
      "Epoch 8197/40000, Loss: 2.962609869427979e-05, Learning Rate: 0.000748\n",
      "Epoch 8198/40000, Loss: 0.0001025925885187462, Learning Rate: 0.000748\n",
      "Epoch 8199/40000, Loss: 0.00010086187103297561, Learning Rate: 0.000748\n",
      "Epoch 8200/40000, Loss: 7.555731281172484e-05, Learning Rate: 0.000748\n",
      "Epoch 8201/40000, Loss: 6.859264249214903e-05, Learning Rate: 0.000748\n",
      "Epoch 8202/40000, Loss: 0.00010515895701246336, Learning Rate: 0.000747\n",
      "Epoch 8203/40000, Loss: 7.343559991568327e-05, Learning Rate: 0.000747\n",
      "Epoch 8204/40000, Loss: 0.00010519326315261424, Learning Rate: 0.000747\n",
      "Epoch 8205/40000, Loss: 5.992924707243219e-05, Learning Rate: 0.000747\n",
      "Epoch 8206/40000, Loss: 5.527679968508892e-05, Learning Rate: 0.000747\n",
      "Epoch 8207/40000, Loss: 3.220390499336645e-05, Learning Rate: 0.000747\n",
      "Epoch 8208/40000, Loss: 7.252974319271743e-05, Learning Rate: 0.000747\n",
      "Epoch 8209/40000, Loss: 5.648722071782686e-05, Learning Rate: 0.000747\n",
      "Epoch 8210/40000, Loss: 0.00014995069068390876, Learning Rate: 0.000747\n",
      "Epoch 8211/40000, Loss: 3.972326885559596e-05, Learning Rate: 0.000747\n",
      "Epoch 8212/40000, Loss: 6.110365211497992e-05, Learning Rate: 0.000747\n",
      "Epoch 8213/40000, Loss: 9.094476990867406e-05, Learning Rate: 0.000746\n",
      "Epoch 8214/40000, Loss: 6.699371442664415e-05, Learning Rate: 0.000746\n",
      "Epoch 8215/40000, Loss: 4.7828812967054546e-05, Learning Rate: 0.000746\n",
      "Epoch 8216/40000, Loss: 8.663507469464093e-05, Learning Rate: 0.000746\n",
      "Epoch 8217/40000, Loss: 5.99935301579535e-05, Learning Rate: 0.000746\n",
      "Epoch 8218/40000, Loss: 0.00011997051478829235, Learning Rate: 0.000746\n",
      "Epoch 8219/40000, Loss: 0.00012207514373585582, Learning Rate: 0.000746\n",
      "Epoch 8220/40000, Loss: 0.00011722366616595536, Learning Rate: 0.000746\n",
      "Epoch 8221/40000, Loss: 6.859035056550056e-05, Learning Rate: 0.000746\n",
      "Epoch 8222/40000, Loss: 0.00011475451901787892, Learning Rate: 0.000746\n",
      "Epoch 8223/40000, Loss: 8.398803765885532e-05, Learning Rate: 0.000746\n",
      "Epoch 8224/40000, Loss: 0.00012639211490750313, Learning Rate: 0.000745\n",
      "Epoch 8225/40000, Loss: 0.00011762529902625829, Learning Rate: 0.000745\n",
      "Epoch 8226/40000, Loss: 0.00011447874567238614, Learning Rate: 0.000745\n",
      "Epoch 8227/40000, Loss: 8.246308425441384e-05, Learning Rate: 0.000745\n",
      "Epoch 8228/40000, Loss: 7.81549169914797e-05, Learning Rate: 0.000745\n",
      "Epoch 8229/40000, Loss: 3.572525383788161e-05, Learning Rate: 0.000745\n",
      "Epoch 8230/40000, Loss: 7.608610030729324e-05, Learning Rate: 0.000745\n",
      "Epoch 8231/40000, Loss: 7.717240805504844e-05, Learning Rate: 0.000745\n",
      "Epoch 8232/40000, Loss: 0.00011123022704850882, Learning Rate: 0.000745\n",
      "Epoch 8233/40000, Loss: 2.902969572460279e-05, Learning Rate: 0.000745\n",
      "Epoch 8234/40000, Loss: 0.00010570550512056798, Learning Rate: 0.000745\n",
      "Epoch 8235/40000, Loss: 6.753739580744877e-05, Learning Rate: 0.000744\n",
      "Epoch 8236/40000, Loss: 0.00013627219595946372, Learning Rate: 0.000744\n",
      "Epoch 8237/40000, Loss: 5.6185821449616924e-05, Learning Rate: 0.000744\n",
      "Epoch 8238/40000, Loss: 5.737653555115685e-05, Learning Rate: 0.000744\n",
      "Epoch 8239/40000, Loss: 6.579458568012342e-05, Learning Rate: 0.000744\n",
      "Epoch 8240/40000, Loss: 7.601147808600217e-05, Learning Rate: 0.000744\n",
      "Epoch 8241/40000, Loss: 5.4850708693265915e-05, Learning Rate: 0.000744\n",
      "Epoch 8242/40000, Loss: 7.688869663979858e-05, Learning Rate: 0.000744\n",
      "Epoch 8243/40000, Loss: 7.97231841715984e-05, Learning Rate: 0.000744\n",
      "Epoch 8244/40000, Loss: 8.168045314960182e-05, Learning Rate: 0.000744\n",
      "Epoch 8245/40000, Loss: 6.674300675513223e-05, Learning Rate: 0.000744\n",
      "Epoch 8246/40000, Loss: 5.29178068973124e-05, Learning Rate: 0.000744\n",
      "Epoch 8247/40000, Loss: 7.373878179350868e-05, Learning Rate: 0.000743\n",
      "Epoch 8248/40000, Loss: 6.908460636623204e-05, Learning Rate: 0.000743\n",
      "Epoch 8249/40000, Loss: 9.793184290174395e-05, Learning Rate: 0.000743\n",
      "Epoch 8250/40000, Loss: 8.500162221025676e-05, Learning Rate: 0.000743\n",
      "Epoch 8251/40000, Loss: 4.057471232954413e-05, Learning Rate: 0.000743\n",
      "Epoch 8252/40000, Loss: 6.648204725934193e-05, Learning Rate: 0.000743\n",
      "Epoch 8253/40000, Loss: 3.89272827305831e-05, Learning Rate: 0.000743\n",
      "Epoch 8254/40000, Loss: 4.371031536720693e-05, Learning Rate: 0.000743\n",
      "Epoch 8255/40000, Loss: 0.00010381070023868233, Learning Rate: 0.000743\n",
      "Epoch 8256/40000, Loss: 6.171713175717741e-05, Learning Rate: 0.000743\n",
      "Epoch 8257/40000, Loss: 0.00010360109445173293, Learning Rate: 0.000743\n",
      "Epoch 8258/40000, Loss: 5.547600449062884e-05, Learning Rate: 0.000742\n",
      "Epoch 8259/40000, Loss: 7.028214895399287e-05, Learning Rate: 0.000742\n",
      "Epoch 8260/40000, Loss: 6.702751852571964e-05, Learning Rate: 0.000742\n",
      "Epoch 8261/40000, Loss: 9.945588681148365e-05, Learning Rate: 0.000742\n",
      "Epoch 8262/40000, Loss: 7.057673792587593e-05, Learning Rate: 0.000742\n",
      "Epoch 8263/40000, Loss: 0.000106481158582028, Learning Rate: 0.000742\n",
      "Epoch 8264/40000, Loss: 0.00010033301805378869, Learning Rate: 0.000742\n",
      "Epoch 8265/40000, Loss: 6.444862083299085e-05, Learning Rate: 0.000742\n",
      "Epoch 8266/40000, Loss: 6.811337516410276e-05, Learning Rate: 0.000742\n",
      "Epoch 8267/40000, Loss: 9.102418698603287e-05, Learning Rate: 0.000742\n",
      "Epoch 8268/40000, Loss: 5.9223944845143706e-05, Learning Rate: 0.000742\n",
      "Epoch 8269/40000, Loss: 7.229795301100239e-05, Learning Rate: 0.000741\n",
      "Epoch 8270/40000, Loss: 6.76476483931765e-05, Learning Rate: 0.000741\n",
      "Epoch 8271/40000, Loss: 7.505674147978425e-05, Learning Rate: 0.000741\n",
      "Epoch 8272/40000, Loss: 6.319887324934825e-05, Learning Rate: 0.000741\n",
      "Epoch 8273/40000, Loss: 0.00010888032556977123, Learning Rate: 0.000741\n",
      "Epoch 8274/40000, Loss: 3.013593413925264e-05, Learning Rate: 0.000741\n",
      "Epoch 8275/40000, Loss: 6.963616760913283e-05, Learning Rate: 0.000741\n",
      "Epoch 8276/40000, Loss: 7.733302481938154e-05, Learning Rate: 0.000741\n",
      "Epoch 8277/40000, Loss: 9.567511006025597e-05, Learning Rate: 0.000741\n",
      "Epoch 8278/40000, Loss: 3.7692589103244245e-05, Learning Rate: 0.000741\n",
      "Epoch 8279/40000, Loss: 3.546484003891237e-05, Learning Rate: 0.000741\n",
      "Epoch 8280/40000, Loss: 5.73749202885665e-05, Learning Rate: 0.000740\n",
      "Epoch 8281/40000, Loss: 0.00011234664270887151, Learning Rate: 0.000740\n",
      "Epoch 8282/40000, Loss: 0.00010840221511898562, Learning Rate: 0.000740\n",
      "Epoch 8283/40000, Loss: 0.00011208148498553783, Learning Rate: 0.000740\n",
      "Epoch 8284/40000, Loss: 0.0002188080979976803, Learning Rate: 0.000740\n",
      "Epoch 8285/40000, Loss: 7.219919643830508e-05, Learning Rate: 0.000740\n",
      "Epoch 8286/40000, Loss: 0.00012442171282600611, Learning Rate: 0.000740\n",
      "Epoch 8287/40000, Loss: 0.00019264714501332492, Learning Rate: 0.000740\n",
      "Epoch 8288/40000, Loss: 9.705386764835566e-05, Learning Rate: 0.000740\n",
      "Epoch 8289/40000, Loss: 0.00022249440371524543, Learning Rate: 0.000740\n",
      "Epoch 8290/40000, Loss: 6.427837070077658e-05, Learning Rate: 0.000740\n",
      "Epoch 8291/40000, Loss: 6.84979313518852e-05, Learning Rate: 0.000739\n",
      "Epoch 8292/40000, Loss: 0.00010894336446654052, Learning Rate: 0.000739\n",
      "Epoch 8293/40000, Loss: 8.17662148620002e-05, Learning Rate: 0.000739\n",
      "Epoch 8294/40000, Loss: 7.949866267153993e-05, Learning Rate: 0.000739\n",
      "Epoch 8295/40000, Loss: 0.00021011600620113313, Learning Rate: 0.000739\n",
      "Epoch 8296/40000, Loss: 0.00013094587484374642, Learning Rate: 0.000739\n",
      "Epoch 8297/40000, Loss: 0.00011962369171669707, Learning Rate: 0.000739\n",
      "Epoch 8298/40000, Loss: 9.325627615908161e-05, Learning Rate: 0.000739\n",
      "Epoch 8299/40000, Loss: 0.0001000577030936256, Learning Rate: 0.000739\n",
      "Epoch 8300/40000, Loss: 0.00012744305422529578, Learning Rate: 0.000739\n",
      "Epoch 8301/40000, Loss: 4.0534610889153555e-05, Learning Rate: 0.000739\n",
      "Epoch 8302/40000, Loss: 6.891174416523427e-05, Learning Rate: 0.000739\n",
      "Epoch 8303/40000, Loss: 5.949583282927051e-05, Learning Rate: 0.000738\n",
      "Epoch 8304/40000, Loss: 4.14392925449647e-05, Learning Rate: 0.000738\n",
      "Epoch 8305/40000, Loss: 6.781594129279256e-05, Learning Rate: 0.000738\n",
      "Epoch 8306/40000, Loss: 6.67451968183741e-05, Learning Rate: 0.000738\n",
      "Epoch 8307/40000, Loss: 6.176514580147341e-05, Learning Rate: 0.000738\n",
      "Epoch 8308/40000, Loss: 3.092124097747728e-05, Learning Rate: 0.000738\n",
      "Epoch 8309/40000, Loss: 5.5230757425306365e-05, Learning Rate: 0.000738\n",
      "Epoch 8310/40000, Loss: 9.707068966235965e-05, Learning Rate: 0.000738\n",
      "Epoch 8311/40000, Loss: 8.896584040485322e-05, Learning Rate: 0.000738\n",
      "Epoch 8312/40000, Loss: 6.611992284888402e-05, Learning Rate: 0.000738\n",
      "Epoch 8313/40000, Loss: 5.684160714736208e-05, Learning Rate: 0.000738\n",
      "Epoch 8314/40000, Loss: 2.6174971935688518e-05, Learning Rate: 0.000737\n",
      "Epoch 8315/40000, Loss: 9.408263576915488e-05, Learning Rate: 0.000737\n",
      "Epoch 8316/40000, Loss: 2.6639987481758e-05, Learning Rate: 0.000737\n",
      "Epoch 8317/40000, Loss: 0.00010405311331851408, Learning Rate: 0.000737\n",
      "Epoch 8318/40000, Loss: 2.568974559835624e-05, Learning Rate: 0.000737\n",
      "Epoch 8319/40000, Loss: 9.44572311709635e-05, Learning Rate: 0.000737\n",
      "Epoch 8320/40000, Loss: 6.324008427327499e-05, Learning Rate: 0.000737\n",
      "Epoch 8321/40000, Loss: 5.103801959194243e-05, Learning Rate: 0.000737\n",
      "Epoch 8322/40000, Loss: 5.115303065394983e-05, Learning Rate: 0.000737\n",
      "Epoch 8323/40000, Loss: 5.4818683565827087e-05, Learning Rate: 0.000737\n",
      "Epoch 8324/40000, Loss: 8.387729758396745e-05, Learning Rate: 0.000737\n",
      "Epoch 8325/40000, Loss: 5.727600000682287e-05, Learning Rate: 0.000736\n",
      "Epoch 8326/40000, Loss: 6.235813634702936e-05, Learning Rate: 0.000736\n",
      "Epoch 8327/40000, Loss: 6.248501449590549e-05, Learning Rate: 0.000736\n",
      "Epoch 8328/40000, Loss: 2.4763248802628368e-05, Learning Rate: 0.000736\n",
      "Epoch 8329/40000, Loss: 9.349930041935295e-05, Learning Rate: 0.000736\n",
      "Epoch 8330/40000, Loss: 4.9958322051679716e-05, Learning Rate: 0.000736\n",
      "Epoch 8331/40000, Loss: 8.234341657953337e-05, Learning Rate: 0.000736\n",
      "Epoch 8332/40000, Loss: 6.247426790650934e-05, Learning Rate: 0.000736\n",
      "Epoch 8333/40000, Loss: 2.372718881815672e-05, Learning Rate: 0.000736\n",
      "Epoch 8334/40000, Loss: 5.444791531772353e-05, Learning Rate: 0.000736\n",
      "Epoch 8335/40000, Loss: 8.19991109892726e-05, Learning Rate: 0.000736\n",
      "Epoch 8336/40000, Loss: 5.511937342816964e-05, Learning Rate: 0.000736\n",
      "Epoch 8337/40000, Loss: 8.345234527951106e-05, Learning Rate: 0.000735\n",
      "Epoch 8338/40000, Loss: 9.158685861621052e-05, Learning Rate: 0.000735\n",
      "Epoch 8339/40000, Loss: 2.532972939661704e-05, Learning Rate: 0.000735\n",
      "Epoch 8340/40000, Loss: 2.4598832169431262e-05, Learning Rate: 0.000735\n",
      "Epoch 8341/40000, Loss: 9.475620026933029e-05, Learning Rate: 0.000735\n",
      "Epoch 8342/40000, Loss: 5.453858830151148e-05, Learning Rate: 0.000735\n",
      "Epoch 8343/40000, Loss: 7.571320747956634e-05, Learning Rate: 0.000735\n",
      "Epoch 8344/40000, Loss: 6.13822994637303e-05, Learning Rate: 0.000735\n",
      "Epoch 8345/40000, Loss: 2.52968766289996e-05, Learning Rate: 0.000735\n",
      "Epoch 8346/40000, Loss: 5.644483826472424e-05, Learning Rate: 0.000735\n",
      "Epoch 8347/40000, Loss: 6.168278196128085e-05, Learning Rate: 0.000735\n",
      "Epoch 8348/40000, Loss: 5.997714106342755e-05, Learning Rate: 0.000734\n",
      "Epoch 8349/40000, Loss: 8.747854735702276e-05, Learning Rate: 0.000734\n",
      "Epoch 8350/40000, Loss: 6.811161438236013e-05, Learning Rate: 0.000734\n",
      "Epoch 8351/40000, Loss: 6.64872641209513e-05, Learning Rate: 0.000734\n",
      "Epoch 8352/40000, Loss: 6.646841211477295e-05, Learning Rate: 0.000734\n",
      "Epoch 8353/40000, Loss: 7.60102120693773e-05, Learning Rate: 0.000734\n",
      "Epoch 8354/40000, Loss: 6.0259415477048606e-05, Learning Rate: 0.000734\n",
      "Epoch 8355/40000, Loss: 8.934671495808288e-05, Learning Rate: 0.000734\n",
      "Epoch 8356/40000, Loss: 6.894065882079303e-05, Learning Rate: 0.000734\n",
      "Epoch 8357/40000, Loss: 3.146389644825831e-05, Learning Rate: 0.000734\n",
      "Epoch 8358/40000, Loss: 3.418151391088031e-05, Learning Rate: 0.000734\n",
      "Epoch 8359/40000, Loss: 0.00010161566751776263, Learning Rate: 0.000733\n",
      "Epoch 8360/40000, Loss: 6.676591874565929e-05, Learning Rate: 0.000733\n",
      "Epoch 8361/40000, Loss: 9.776819933904335e-05, Learning Rate: 0.000733\n",
      "Epoch 8362/40000, Loss: 5.716081795981154e-05, Learning Rate: 0.000733\n",
      "Epoch 8363/40000, Loss: 3.054290573345497e-05, Learning Rate: 0.000733\n",
      "Epoch 8364/40000, Loss: 6.433425733121112e-05, Learning Rate: 0.000733\n",
      "Epoch 8365/40000, Loss: 0.00010351877426728606, Learning Rate: 0.000733\n",
      "Epoch 8366/40000, Loss: 5.946522651356645e-05, Learning Rate: 0.000733\n",
      "Epoch 8367/40000, Loss: 9.305575804319233e-05, Learning Rate: 0.000733\n",
      "Epoch 8368/40000, Loss: 9.258428326575086e-05, Learning Rate: 0.000733\n",
      "Epoch 8369/40000, Loss: 9.606008097762242e-05, Learning Rate: 0.000733\n",
      "Epoch 8370/40000, Loss: 5.525568485609256e-05, Learning Rate: 0.000733\n",
      "Epoch 8371/40000, Loss: 6.969219975871965e-05, Learning Rate: 0.000732\n",
      "Epoch 8372/40000, Loss: 6.303837290033698e-05, Learning Rate: 0.000732\n",
      "Epoch 8373/40000, Loss: 0.00010904949886025861, Learning Rate: 0.000732\n",
      "Epoch 8374/40000, Loss: 6.604104419238865e-05, Learning Rate: 0.000732\n",
      "Epoch 8375/40000, Loss: 9.775173384696245e-05, Learning Rate: 0.000732\n",
      "Epoch 8376/40000, Loss: 8.406997949350625e-05, Learning Rate: 0.000732\n",
      "Epoch 8377/40000, Loss: 3.851195651805028e-05, Learning Rate: 0.000732\n",
      "Epoch 8378/40000, Loss: 9.784848953131586e-05, Learning Rate: 0.000732\n",
      "Epoch 8379/40000, Loss: 9.195998427458107e-05, Learning Rate: 0.000732\n",
      "Epoch 8380/40000, Loss: 6.617190229007974e-05, Learning Rate: 0.000732\n",
      "Epoch 8381/40000, Loss: 2.9562608688138425e-05, Learning Rate: 0.000732\n",
      "Epoch 8382/40000, Loss: 6.945410132175311e-05, Learning Rate: 0.000731\n",
      "Epoch 8383/40000, Loss: 7.120712689356878e-05, Learning Rate: 0.000731\n",
      "Epoch 8384/40000, Loss: 6.136461161077023e-05, Learning Rate: 0.000731\n",
      "Epoch 8385/40000, Loss: 6.97918439982459e-05, Learning Rate: 0.000731\n",
      "Epoch 8386/40000, Loss: 2.791116276057437e-05, Learning Rate: 0.000731\n",
      "Epoch 8387/40000, Loss: 9.360826516058296e-05, Learning Rate: 0.000731\n",
      "Epoch 8388/40000, Loss: 9.135630534728989e-05, Learning Rate: 0.000731\n",
      "Epoch 8389/40000, Loss: 6.670880247838795e-05, Learning Rate: 0.000731\n",
      "Epoch 8390/40000, Loss: 6.107195076765493e-05, Learning Rate: 0.000731\n",
      "Epoch 8391/40000, Loss: 9.423375013284385e-05, Learning Rate: 0.000731\n",
      "Epoch 8392/40000, Loss: 9.512694668956101e-05, Learning Rate: 0.000731\n",
      "Epoch 8393/40000, Loss: 8.487556624459103e-05, Learning Rate: 0.000731\n",
      "Epoch 8394/40000, Loss: 9.595810115570202e-05, Learning Rate: 0.000730\n",
      "Epoch 8395/40000, Loss: 2.6320463803131133e-05, Learning Rate: 0.000730\n",
      "Epoch 8396/40000, Loss: 5.893741763429716e-05, Learning Rate: 0.000730\n",
      "Epoch 8397/40000, Loss: 6.800921983085573e-05, Learning Rate: 0.000730\n",
      "Epoch 8398/40000, Loss: 5.4230862588156015e-05, Learning Rate: 0.000730\n",
      "Epoch 8399/40000, Loss: 3.133221252937801e-05, Learning Rate: 0.000730\n",
      "Epoch 8400/40000, Loss: 0.00012439095007721335, Learning Rate: 0.000730\n",
      "Epoch 8401/40000, Loss: 0.00010672016651369631, Learning Rate: 0.000730\n",
      "Epoch 8402/40000, Loss: 9.840805432759225e-05, Learning Rate: 0.000730\n",
      "Epoch 8403/40000, Loss: 5.5608543334528804e-05, Learning Rate: 0.000730\n",
      "Epoch 8404/40000, Loss: 3.5723132896237075e-05, Learning Rate: 0.000730\n",
      "Epoch 8405/40000, Loss: 6.395819946192205e-05, Learning Rate: 0.000729\n",
      "Epoch 8406/40000, Loss: 7.343046308960766e-05, Learning Rate: 0.000729\n",
      "Epoch 8407/40000, Loss: 7.292431109817699e-05, Learning Rate: 0.000729\n",
      "Epoch 8408/40000, Loss: 6.701532402075827e-05, Learning Rate: 0.000729\n",
      "Epoch 8409/40000, Loss: 7.634161011083052e-05, Learning Rate: 0.000729\n",
      "Epoch 8410/40000, Loss: 0.00013531441800296307, Learning Rate: 0.000729\n",
      "Epoch 8411/40000, Loss: 0.00011047778389183804, Learning Rate: 0.000729\n",
      "Epoch 8412/40000, Loss: 0.00011236033606110141, Learning Rate: 0.000729\n",
      "Epoch 8413/40000, Loss: 5.569504719460383e-05, Learning Rate: 0.000729\n",
      "Epoch 8414/40000, Loss: 0.00012309364683460444, Learning Rate: 0.000729\n",
      "Epoch 8415/40000, Loss: 0.00013364682672545314, Learning Rate: 0.000729\n",
      "Epoch 8416/40000, Loss: 4.377995355753228e-05, Learning Rate: 0.000728\n",
      "Epoch 8417/40000, Loss: 8.201611490221694e-05, Learning Rate: 0.000728\n",
      "Epoch 8418/40000, Loss: 9.275248885387555e-05, Learning Rate: 0.000728\n",
      "Epoch 8419/40000, Loss: 5.6536511692684144e-05, Learning Rate: 0.000728\n",
      "Epoch 8420/40000, Loss: 8.654841803945601e-05, Learning Rate: 0.000728\n",
      "Epoch 8421/40000, Loss: 0.00010804877820191905, Learning Rate: 0.000728\n",
      "Epoch 8422/40000, Loss: 5.397860149969347e-05, Learning Rate: 0.000728\n",
      "Epoch 8423/40000, Loss: 6.618377665290609e-05, Learning Rate: 0.000728\n",
      "Epoch 8424/40000, Loss: 8.895810606190935e-05, Learning Rate: 0.000728\n",
      "Epoch 8425/40000, Loss: 0.00012328990851528943, Learning Rate: 0.000728\n",
      "Epoch 8426/40000, Loss: 0.00013438973110169172, Learning Rate: 0.000728\n",
      "Epoch 8427/40000, Loss: 7.616752554895356e-05, Learning Rate: 0.000728\n",
      "Epoch 8428/40000, Loss: 4.2026273149531335e-05, Learning Rate: 0.000727\n",
      "Epoch 8429/40000, Loss: 6.0268961533438414e-05, Learning Rate: 0.000727\n",
      "Epoch 8430/40000, Loss: 7.326508057303727e-05, Learning Rate: 0.000727\n",
      "Epoch 8431/40000, Loss: 5.741069981013425e-05, Learning Rate: 0.000727\n",
      "Epoch 8432/40000, Loss: 3.877374911098741e-05, Learning Rate: 0.000727\n",
      "Epoch 8433/40000, Loss: 4.2654548451537266e-05, Learning Rate: 0.000727\n",
      "Epoch 8434/40000, Loss: 6.832124199718237e-05, Learning Rate: 0.000727\n",
      "Epoch 8435/40000, Loss: 9.763440175447613e-05, Learning Rate: 0.000727\n",
      "Epoch 8436/40000, Loss: 6.741067772964016e-05, Learning Rate: 0.000727\n",
      "Epoch 8437/40000, Loss: 5.487133239512332e-05, Learning Rate: 0.000727\n",
      "Epoch 8438/40000, Loss: 7.804525375831872e-05, Learning Rate: 0.000727\n",
      "Epoch 8439/40000, Loss: 9.800223779166117e-05, Learning Rate: 0.000726\n",
      "Epoch 8440/40000, Loss: 9.71403787843883e-05, Learning Rate: 0.000726\n",
      "Epoch 8441/40000, Loss: 7.729754725005478e-05, Learning Rate: 0.000726\n",
      "Epoch 8442/40000, Loss: 0.00012605462688952684, Learning Rate: 0.000726\n",
      "Epoch 8443/40000, Loss: 0.00012091641838196665, Learning Rate: 0.000726\n",
      "Epoch 8444/40000, Loss: 0.0001338915608357638, Learning Rate: 0.000726\n",
      "Epoch 8445/40000, Loss: 0.00015441430150531232, Learning Rate: 0.000726\n",
      "Epoch 8446/40000, Loss: 0.0001472682342864573, Learning Rate: 0.000726\n",
      "Epoch 8447/40000, Loss: 8.40012653497979e-05, Learning Rate: 0.000726\n",
      "Epoch 8448/40000, Loss: 0.0001486474648118019, Learning Rate: 0.000726\n",
      "Epoch 8449/40000, Loss: 0.0003983662754762918, Learning Rate: 0.000726\n",
      "Epoch 8450/40000, Loss: 0.00015268634888343513, Learning Rate: 0.000726\n",
      "Epoch 8451/40000, Loss: 0.0001456803292967379, Learning Rate: 0.000725\n",
      "Epoch 8452/40000, Loss: 0.00037016274291090667, Learning Rate: 0.000725\n",
      "Epoch 8453/40000, Loss: 0.00011939577962039039, Learning Rate: 0.000725\n",
      "Epoch 8454/40000, Loss: 0.0001517709606559947, Learning Rate: 0.000725\n",
      "Epoch 8455/40000, Loss: 7.731586811132729e-05, Learning Rate: 0.000725\n",
      "Epoch 8456/40000, Loss: 9.806029265746474e-05, Learning Rate: 0.000725\n",
      "Epoch 8457/40000, Loss: 0.00013039525947533548, Learning Rate: 0.000725\n",
      "Epoch 8458/40000, Loss: 0.00017244832997675985, Learning Rate: 0.000725\n",
      "Epoch 8459/40000, Loss: 0.00013045669766142964, Learning Rate: 0.000725\n",
      "Epoch 8460/40000, Loss: 5.396075721364468e-05, Learning Rate: 0.000725\n",
      "Epoch 8461/40000, Loss: 7.717745029367507e-05, Learning Rate: 0.000725\n",
      "Epoch 8462/40000, Loss: 7.208992610685527e-05, Learning Rate: 0.000724\n",
      "Epoch 8463/40000, Loss: 8.78916762303561e-05, Learning Rate: 0.000724\n",
      "Epoch 8464/40000, Loss: 7.768163050059229e-05, Learning Rate: 0.000724\n",
      "Epoch 8465/40000, Loss: 6.90998713253066e-05, Learning Rate: 0.000724\n",
      "Epoch 8466/40000, Loss: 2.793546809698455e-05, Learning Rate: 0.000724\n",
      "Epoch 8467/40000, Loss: 4.8621921450830996e-05, Learning Rate: 0.000724\n",
      "Epoch 8468/40000, Loss: 9.525137284072116e-05, Learning Rate: 0.000724\n",
      "Epoch 8469/40000, Loss: 8.915497164707631e-05, Learning Rate: 0.000724\n",
      "Epoch 8470/40000, Loss: 7.04091798979789e-05, Learning Rate: 0.000724\n",
      "Epoch 8471/40000, Loss: 9.504293848294765e-05, Learning Rate: 0.000724\n",
      "Epoch 8472/40000, Loss: 0.00011112365609733388, Learning Rate: 0.000724\n",
      "Epoch 8473/40000, Loss: 9.325981955043972e-05, Learning Rate: 0.000724\n",
      "Epoch 8474/40000, Loss: 7.01306780683808e-05, Learning Rate: 0.000723\n",
      "Epoch 8475/40000, Loss: 8.823270763969049e-05, Learning Rate: 0.000723\n",
      "Epoch 8476/40000, Loss: 3.0781135137658566e-05, Learning Rate: 0.000723\n",
      "Epoch 8477/40000, Loss: 9.44833445828408e-05, Learning Rate: 0.000723\n",
      "Epoch 8478/40000, Loss: 6.299438973655924e-05, Learning Rate: 0.000723\n",
      "Epoch 8479/40000, Loss: 5.551603317144327e-05, Learning Rate: 0.000723\n",
      "Epoch 8480/40000, Loss: 6.104237399995327e-05, Learning Rate: 0.000723\n",
      "Epoch 8481/40000, Loss: 6.427329208236188e-05, Learning Rate: 0.000723\n",
      "Epoch 8482/40000, Loss: 9.423997107660398e-05, Learning Rate: 0.000723\n",
      "Epoch 8483/40000, Loss: 6.365408626152202e-05, Learning Rate: 0.000723\n",
      "Epoch 8484/40000, Loss: 3.385284435353242e-05, Learning Rate: 0.000723\n",
      "Epoch 8485/40000, Loss: 7.928060222184286e-05, Learning Rate: 0.000722\n",
      "Epoch 8486/40000, Loss: 5.9620757383527234e-05, Learning Rate: 0.000722\n",
      "Epoch 8487/40000, Loss: 6.994220893830061e-05, Learning Rate: 0.000722\n",
      "Epoch 8488/40000, Loss: 8.961280400399119e-05, Learning Rate: 0.000722\n",
      "Epoch 8489/40000, Loss: 5.7517805544193834e-05, Learning Rate: 0.000722\n",
      "Epoch 8490/40000, Loss: 9.672204760136083e-05, Learning Rate: 0.000722\n",
      "Epoch 8491/40000, Loss: 3.332807682454586e-05, Learning Rate: 0.000722\n",
      "Epoch 8492/40000, Loss: 9.254946053260937e-05, Learning Rate: 0.000722\n",
      "Epoch 8493/40000, Loss: 2.826528543664608e-05, Learning Rate: 0.000722\n",
      "Epoch 8494/40000, Loss: 5.1148090278729796e-05, Learning Rate: 0.000722\n",
      "Epoch 8495/40000, Loss: 2.4849541659932584e-05, Learning Rate: 0.000722\n",
      "Epoch 8496/40000, Loss: 2.436209615552798e-05, Learning Rate: 0.000722\n",
      "Epoch 8497/40000, Loss: 2.6618874471751042e-05, Learning Rate: 0.000721\n",
      "Epoch 8498/40000, Loss: 2.4128448785631917e-05, Learning Rate: 0.000721\n",
      "Epoch 8499/40000, Loss: 2.5687208108138293e-05, Learning Rate: 0.000721\n",
      "Epoch 8500/40000, Loss: 0.00013715711247641593, Learning Rate: 0.000721\n",
      "Epoch 8501/40000, Loss: 0.0001006299935397692, Learning Rate: 0.000721\n",
      "Epoch 8502/40000, Loss: 5.9058787883259356e-05, Learning Rate: 0.000721\n",
      "Epoch 8503/40000, Loss: 9.359203977510333e-05, Learning Rate: 0.000721\n",
      "Epoch 8504/40000, Loss: 5.0318682042416185e-05, Learning Rate: 0.000721\n",
      "Epoch 8505/40000, Loss: 6.288121949182823e-05, Learning Rate: 0.000721\n",
      "Epoch 8506/40000, Loss: 5.14091007062234e-05, Learning Rate: 0.000721\n",
      "Epoch 8507/40000, Loss: 5.045984289608896e-05, Learning Rate: 0.000721\n",
      "Epoch 8508/40000, Loss: 2.4241673600045033e-05, Learning Rate: 0.000720\n",
      "Epoch 8509/40000, Loss: 4.970597001374699e-05, Learning Rate: 0.000720\n",
      "Epoch 8510/40000, Loss: 9.20859893085435e-05, Learning Rate: 0.000720\n",
      "Epoch 8511/40000, Loss: 6.190953718032688e-05, Learning Rate: 0.000720\n",
      "Epoch 8512/40000, Loss: 6.149270484456792e-05, Learning Rate: 0.000720\n",
      "Epoch 8513/40000, Loss: 5.6609802413731813e-05, Learning Rate: 0.000720\n",
      "Epoch 8514/40000, Loss: 9.176055755233392e-05, Learning Rate: 0.000720\n",
      "Epoch 8515/40000, Loss: 6.26307082711719e-05, Learning Rate: 0.000720\n",
      "Epoch 8516/40000, Loss: 8.166174666257575e-05, Learning Rate: 0.000720\n",
      "Epoch 8517/40000, Loss: 9.135737491305918e-05, Learning Rate: 0.000720\n",
      "Epoch 8518/40000, Loss: 9.077713912120089e-05, Learning Rate: 0.000720\n",
      "Epoch 8519/40000, Loss: 8.18261833046563e-05, Learning Rate: 0.000720\n",
      "Epoch 8520/40000, Loss: 9.2975766165182e-05, Learning Rate: 0.000719\n",
      "Epoch 8521/40000, Loss: 4.9647886044112965e-05, Learning Rate: 0.000719\n",
      "Epoch 8522/40000, Loss: 2.3996224626898766e-05, Learning Rate: 0.000719\n",
      "Epoch 8523/40000, Loss: 6.178230978548527e-05, Learning Rate: 0.000719\n",
      "Epoch 8524/40000, Loss: 9.019733988679945e-05, Learning Rate: 0.000719\n",
      "Epoch 8525/40000, Loss: 5.609741492662579e-05, Learning Rate: 0.000719\n",
      "Epoch 8526/40000, Loss: 5.547163891606033e-05, Learning Rate: 0.000719\n",
      "Epoch 8527/40000, Loss: 5.510223491000943e-05, Learning Rate: 0.000719\n",
      "Epoch 8528/40000, Loss: 4.9082042096415535e-05, Learning Rate: 0.000719\n",
      "Epoch 8529/40000, Loss: 6.162928184494376e-05, Learning Rate: 0.000719\n",
      "Epoch 8530/40000, Loss: 5.020410753786564e-05, Learning Rate: 0.000719\n",
      "Epoch 8531/40000, Loss: 4.901915235677734e-05, Learning Rate: 0.000719\n",
      "Epoch 8532/40000, Loss: 2.3324879293795675e-05, Learning Rate: 0.000718\n",
      "Epoch 8533/40000, Loss: 6.136130105005577e-05, Learning Rate: 0.000718\n",
      "Epoch 8534/40000, Loss: 2.374331234022975e-05, Learning Rate: 0.000718\n",
      "Epoch 8535/40000, Loss: 8.983073348645121e-05, Learning Rate: 0.000718\n",
      "Epoch 8536/40000, Loss: 8.074704237515107e-05, Learning Rate: 0.000718\n",
      "Epoch 8537/40000, Loss: 8.963113214122131e-05, Learning Rate: 0.000718\n",
      "Epoch 8538/40000, Loss: 5.5159380281111225e-05, Learning Rate: 0.000718\n",
      "Epoch 8539/40000, Loss: 6.135718285804614e-05, Learning Rate: 0.000718\n",
      "Epoch 8540/40000, Loss: 9.163189679384232e-05, Learning Rate: 0.000718\n",
      "Epoch 8541/40000, Loss: 5.4660773457726464e-05, Learning Rate: 0.000718\n",
      "Epoch 8542/40000, Loss: 5.441432222141884e-05, Learning Rate: 0.000718\n",
      "Epoch 8543/40000, Loss: 8.973779040388763e-05, Learning Rate: 0.000717\n",
      "Epoch 8544/40000, Loss: 2.3620006686542183e-05, Learning Rate: 0.000717\n",
      "Epoch 8545/40000, Loss: 6.162274803500623e-05, Learning Rate: 0.000717\n",
      "Epoch 8546/40000, Loss: 9.262102685170248e-05, Learning Rate: 0.000717\n",
      "Epoch 8547/40000, Loss: 5.6106862757587805e-05, Learning Rate: 0.000717\n",
      "Epoch 8548/40000, Loss: 4.9006786866812035e-05, Learning Rate: 0.000717\n",
      "Epoch 8549/40000, Loss: 8.193936082534492e-05, Learning Rate: 0.000717\n",
      "Epoch 8550/40000, Loss: 9.043739555636421e-05, Learning Rate: 0.000717\n",
      "Epoch 8551/40000, Loss: 2.351378498133272e-05, Learning Rate: 0.000717\n",
      "Epoch 8552/40000, Loss: 6.126255902927369e-05, Learning Rate: 0.000717\n",
      "Epoch 8553/40000, Loss: 8.081256237346679e-05, Learning Rate: 0.000717\n",
      "Epoch 8554/40000, Loss: 5.471651456900872e-05, Learning Rate: 0.000717\n",
      "Epoch 8555/40000, Loss: 8.071864431258291e-05, Learning Rate: 0.000716\n",
      "Epoch 8556/40000, Loss: 5.560813951888122e-05, Learning Rate: 0.000716\n",
      "Epoch 8557/40000, Loss: 5.5185199016705155e-05, Learning Rate: 0.000716\n",
      "Epoch 8558/40000, Loss: 6.153935100883245e-05, Learning Rate: 0.000716\n",
      "Epoch 8559/40000, Loss: 4.885812813881785e-05, Learning Rate: 0.000716\n",
      "Epoch 8560/40000, Loss: 6.166420644149184e-05, Learning Rate: 0.000716\n",
      "Epoch 8561/40000, Loss: 5.965388845652342e-05, Learning Rate: 0.000716\n",
      "Epoch 8562/40000, Loss: 6.211665458977222e-05, Learning Rate: 0.000716\n",
      "Epoch 8563/40000, Loss: 8.472382614854723e-05, Learning Rate: 0.000716\n",
      "Epoch 8564/40000, Loss: 9.117653826251626e-05, Learning Rate: 0.000716\n",
      "Epoch 8565/40000, Loss: 5.1138933486072347e-05, Learning Rate: 0.000716\n",
      "Epoch 8566/40000, Loss: 6.531076360261068e-05, Learning Rate: 0.000715\n",
      "Epoch 8567/40000, Loss: 9.146588126895949e-05, Learning Rate: 0.000715\n",
      "Epoch 8568/40000, Loss: 0.00013407415826804936, Learning Rate: 0.000715\n",
      "Epoch 8569/40000, Loss: 7.927302067400888e-05, Learning Rate: 0.000715\n",
      "Epoch 8570/40000, Loss: 4.419046672410332e-05, Learning Rate: 0.000715\n",
      "Epoch 8571/40000, Loss: 3.457951243035495e-05, Learning Rate: 0.000715\n",
      "Epoch 8572/40000, Loss: 6.618730549234897e-05, Learning Rate: 0.000715\n",
      "Epoch 8573/40000, Loss: 6.608459807466716e-05, Learning Rate: 0.000715\n",
      "Epoch 8574/40000, Loss: 0.0001227164757438004, Learning Rate: 0.000715\n",
      "Epoch 8575/40000, Loss: 0.00012162735947640613, Learning Rate: 0.000715\n",
      "Epoch 8576/40000, Loss: 6.932752876309678e-05, Learning Rate: 0.000715\n",
      "Epoch 8577/40000, Loss: 6.265752017498016e-05, Learning Rate: 0.000715\n",
      "Epoch 8578/40000, Loss: 4.511084989644587e-05, Learning Rate: 0.000714\n",
      "Epoch 8579/40000, Loss: 0.00010936845501419157, Learning Rate: 0.000714\n",
      "Epoch 8580/40000, Loss: 7.511573494412005e-05, Learning Rate: 0.000714\n",
      "Epoch 8581/40000, Loss: 0.00010405688226455823, Learning Rate: 0.000714\n",
      "Epoch 8582/40000, Loss: 6.444160680985078e-05, Learning Rate: 0.000714\n",
      "Epoch 8583/40000, Loss: 5.4734839068260044e-05, Learning Rate: 0.000714\n",
      "Epoch 8584/40000, Loss: 6.488454528152943e-05, Learning Rate: 0.000714\n",
      "Epoch 8585/40000, Loss: 9.6049094281625e-05, Learning Rate: 0.000714\n",
      "Epoch 8586/40000, Loss: 5.965767923044041e-05, Learning Rate: 0.000714\n",
      "Epoch 8587/40000, Loss: 6.773170025553554e-05, Learning Rate: 0.000714\n",
      "Epoch 8588/40000, Loss: 9.715845953905955e-05, Learning Rate: 0.000714\n",
      "Epoch 8589/40000, Loss: 9.01770472410135e-05, Learning Rate: 0.000714\n",
      "Epoch 8590/40000, Loss: 6.774435314582661e-05, Learning Rate: 0.000713\n",
      "Epoch 8591/40000, Loss: 6.560437759617344e-05, Learning Rate: 0.000713\n",
      "Epoch 8592/40000, Loss: 6.511054380098358e-05, Learning Rate: 0.000713\n",
      "Epoch 8593/40000, Loss: 0.00011023063416359946, Learning Rate: 0.000713\n",
      "Epoch 8594/40000, Loss: 9.341628174297512e-05, Learning Rate: 0.000713\n",
      "Epoch 8595/40000, Loss: 9.152856364380568e-05, Learning Rate: 0.000713\n",
      "Epoch 8596/40000, Loss: 7.257321703946218e-05, Learning Rate: 0.000713\n",
      "Epoch 8597/40000, Loss: 6.004375609336421e-05, Learning Rate: 0.000713\n",
      "Epoch 8598/40000, Loss: 0.00010552516323514283, Learning Rate: 0.000713\n",
      "Epoch 8599/40000, Loss: 9.369821782456711e-05, Learning Rate: 0.000713\n",
      "Epoch 8600/40000, Loss: 6.542903429362923e-05, Learning Rate: 0.000713\n",
      "Epoch 8601/40000, Loss: 4.29365063610021e-05, Learning Rate: 0.000712\n",
      "Epoch 8602/40000, Loss: 2.9500604796339758e-05, Learning Rate: 0.000712\n",
      "Epoch 8603/40000, Loss: 6.229717837413773e-05, Learning Rate: 0.000712\n",
      "Epoch 8604/40000, Loss: 9.725587005959824e-05, Learning Rate: 0.000712\n",
      "Epoch 8605/40000, Loss: 9.399705595569685e-05, Learning Rate: 0.000712\n",
      "Epoch 8606/40000, Loss: 6.802049756515771e-05, Learning Rate: 0.000712\n",
      "Epoch 8607/40000, Loss: 9.883657185127959e-05, Learning Rate: 0.000712\n",
      "Epoch 8608/40000, Loss: 6.619743362534791e-05, Learning Rate: 0.000712\n",
      "Epoch 8609/40000, Loss: 3.2585325243417174e-05, Learning Rate: 0.000712\n",
      "Epoch 8610/40000, Loss: 3.0360943128471263e-05, Learning Rate: 0.000712\n",
      "Epoch 8611/40000, Loss: 9.78428652160801e-05, Learning Rate: 0.000712\n",
      "Epoch 8612/40000, Loss: 5.296196468407288e-05, Learning Rate: 0.000712\n",
      "Epoch 8613/40000, Loss: 0.00012765986321028322, Learning Rate: 0.000711\n",
      "Epoch 8614/40000, Loss: 5.5541819165227935e-05, Learning Rate: 0.000711\n",
      "Epoch 8615/40000, Loss: 6.443213351303712e-05, Learning Rate: 0.000711\n",
      "Epoch 8616/40000, Loss: 8.884525595931336e-05, Learning Rate: 0.000711\n",
      "Epoch 8617/40000, Loss: 3.243081664550118e-05, Learning Rate: 0.000711\n",
      "Epoch 8618/40000, Loss: 5.688972305506468e-05, Learning Rate: 0.000711\n",
      "Epoch 8619/40000, Loss: 9.583136852597818e-05, Learning Rate: 0.000711\n",
      "Epoch 8620/40000, Loss: 9.354531357530504e-05, Learning Rate: 0.000711\n",
      "Epoch 8621/40000, Loss: 6.71114758006297e-05, Learning Rate: 0.000711\n",
      "Epoch 8622/40000, Loss: 6.150630360934883e-05, Learning Rate: 0.000711\n",
      "Epoch 8623/40000, Loss: 8.603117021266371e-05, Learning Rate: 0.000711\n",
      "Epoch 8624/40000, Loss: 0.00010474021109985188, Learning Rate: 0.000711\n",
      "Epoch 8625/40000, Loss: 0.00010887727694353089, Learning Rate: 0.000710\n",
      "Epoch 8626/40000, Loss: 6.608813419006765e-05, Learning Rate: 0.000710\n",
      "Epoch 8627/40000, Loss: 2.8006099455524236e-05, Learning Rate: 0.000710\n",
      "Epoch 8628/40000, Loss: 9.475680417381227e-05, Learning Rate: 0.000710\n",
      "Epoch 8629/40000, Loss: 9.854890231508762e-05, Learning Rate: 0.000710\n",
      "Epoch 8630/40000, Loss: 7.151084719225764e-05, Learning Rate: 0.000710\n",
      "Epoch 8631/40000, Loss: 9.285907435696572e-05, Learning Rate: 0.000710\n",
      "Epoch 8632/40000, Loss: 8.514889486832544e-05, Learning Rate: 0.000710\n",
      "Epoch 8633/40000, Loss: 6.170209962874651e-05, Learning Rate: 0.000710\n",
      "Epoch 8634/40000, Loss: 2.9361392080318183e-05, Learning Rate: 0.000710\n",
      "Epoch 8635/40000, Loss: 5.275991134112701e-05, Learning Rate: 0.000710\n",
      "Epoch 8636/40000, Loss: 0.00012511743989307433, Learning Rate: 0.000710\n",
      "Epoch 8637/40000, Loss: 0.00010184609709540382, Learning Rate: 0.000709\n",
      "Epoch 8638/40000, Loss: 0.00018813734641298652, Learning Rate: 0.000709\n",
      "Epoch 8639/40000, Loss: 7.198011735454202e-05, Learning Rate: 0.000709\n",
      "Epoch 8640/40000, Loss: 0.0001372587139485404, Learning Rate: 0.000709\n",
      "Epoch 8641/40000, Loss: 0.00010491417197044939, Learning Rate: 0.000709\n",
      "Epoch 8642/40000, Loss: 0.00012687141133937985, Learning Rate: 0.000709\n",
      "Epoch 8643/40000, Loss: 0.00010454333096276969, Learning Rate: 0.000709\n",
      "Epoch 8644/40000, Loss: 0.00016992748714983463, Learning Rate: 0.000709\n",
      "Epoch 8645/40000, Loss: 7.620685937581584e-05, Learning Rate: 0.000709\n",
      "Epoch 8646/40000, Loss: 0.00011592908413149416, Learning Rate: 0.000709\n",
      "Epoch 8647/40000, Loss: 0.00015703596000093967, Learning Rate: 0.000709\n",
      "Epoch 8648/40000, Loss: 9.250037692254409e-05, Learning Rate: 0.000708\n",
      "Epoch 8649/40000, Loss: 0.00010067479888675734, Learning Rate: 0.000708\n",
      "Epoch 8650/40000, Loss: 0.00023776346642989665, Learning Rate: 0.000708\n",
      "Epoch 8651/40000, Loss: 8.499152318108827e-05, Learning Rate: 0.000708\n",
      "Epoch 8652/40000, Loss: 0.00011634709517238662, Learning Rate: 0.000708\n",
      "Epoch 8653/40000, Loss: 6.085963104851544e-05, Learning Rate: 0.000708\n",
      "Epoch 8654/40000, Loss: 7.872425339883193e-05, Learning Rate: 0.000708\n",
      "Epoch 8655/40000, Loss: 5.125550160300918e-05, Learning Rate: 0.000708\n",
      "Epoch 8656/40000, Loss: 0.00011436388012953103, Learning Rate: 0.000708\n",
      "Epoch 8657/40000, Loss: 0.0001084366231225431, Learning Rate: 0.000708\n",
      "Epoch 8658/40000, Loss: 0.00010689499322324991, Learning Rate: 0.000708\n",
      "Epoch 8659/40000, Loss: 6.649210990872234e-05, Learning Rate: 0.000708\n",
      "Epoch 8660/40000, Loss: 3.699235458043404e-05, Learning Rate: 0.000707\n",
      "Epoch 8661/40000, Loss: 6.879222200950608e-05, Learning Rate: 0.000707\n",
      "Epoch 8662/40000, Loss: 5.6522691011196e-05, Learning Rate: 0.000707\n",
      "Epoch 8663/40000, Loss: 7.205329166026786e-05, Learning Rate: 0.000707\n",
      "Epoch 8664/40000, Loss: 8.985507884062827e-05, Learning Rate: 0.000707\n",
      "Epoch 8665/40000, Loss: 0.00011169326171511784, Learning Rate: 0.000707\n",
      "Epoch 8666/40000, Loss: 9.412311919732019e-05, Learning Rate: 0.000707\n",
      "Epoch 8667/40000, Loss: 9.164849325316027e-05, Learning Rate: 0.000707\n",
      "Epoch 8668/40000, Loss: 9.117218723986298e-05, Learning Rate: 0.000707\n",
      "Epoch 8669/40000, Loss: 6.969281821511686e-05, Learning Rate: 0.000707\n",
      "Epoch 8670/40000, Loss: 9.679989307187498e-05, Learning Rate: 0.000707\n",
      "Epoch 8671/40000, Loss: 2.926559500338044e-05, Learning Rate: 0.000707\n",
      "Epoch 8672/40000, Loss: 9.756101644597948e-05, Learning Rate: 0.000706\n",
      "Epoch 8673/40000, Loss: 6.267372373258695e-05, Learning Rate: 0.000706\n",
      "Epoch 8674/40000, Loss: 6.622706860071048e-05, Learning Rate: 0.000706\n",
      "Epoch 8675/40000, Loss: 3.9421411202056333e-05, Learning Rate: 0.000706\n",
      "Epoch 8676/40000, Loss: 7.307233317987993e-05, Learning Rate: 0.000706\n",
      "Epoch 8677/40000, Loss: 6.101025428506546e-05, Learning Rate: 0.000706\n",
      "Epoch 8678/40000, Loss: 5.735165541409515e-05, Learning Rate: 0.000706\n",
      "Epoch 8679/40000, Loss: 9.132512786891311e-05, Learning Rate: 0.000706\n",
      "Epoch 8680/40000, Loss: 6.701065285596997e-05, Learning Rate: 0.000706\n",
      "Epoch 8681/40000, Loss: 2.89484960376285e-05, Learning Rate: 0.000706\n",
      "Epoch 8682/40000, Loss: 2.7500153009896167e-05, Learning Rate: 0.000706\n",
      "Epoch 8683/40000, Loss: 2.500264236005023e-05, Learning Rate: 0.000706\n",
      "Epoch 8684/40000, Loss: 5.374659667722881e-05, Learning Rate: 0.000705\n",
      "Epoch 8685/40000, Loss: 2.5449617169215344e-05, Learning Rate: 0.000705\n",
      "Epoch 8686/40000, Loss: 6.250845763133839e-05, Learning Rate: 0.000705\n",
      "Epoch 8687/40000, Loss: 5.766577669419348e-05, Learning Rate: 0.000705\n",
      "Epoch 8688/40000, Loss: 4.997102587367408e-05, Learning Rate: 0.000705\n",
      "Epoch 8689/40000, Loss: 5.51799894310534e-05, Learning Rate: 0.000705\n",
      "Epoch 8690/40000, Loss: 5.449033415061422e-05, Learning Rate: 0.000705\n",
      "Epoch 8691/40000, Loss: 5.5582146160304546e-05, Learning Rate: 0.000705\n",
      "Epoch 8692/40000, Loss: 9.1065390734002e-05, Learning Rate: 0.000705\n",
      "Epoch 8693/40000, Loss: 9.072218381334096e-05, Learning Rate: 0.000705\n",
      "Epoch 8694/40000, Loss: 6.238826608750969e-05, Learning Rate: 0.000705\n",
      "Epoch 8695/40000, Loss: 8.483953570248559e-05, Learning Rate: 0.000705\n",
      "Epoch 8696/40000, Loss: 6.314768688753247e-05, Learning Rate: 0.000704\n",
      "Epoch 8697/40000, Loss: 9.154788858722895e-05, Learning Rate: 0.000704\n",
      "Epoch 8698/40000, Loss: 2.743295772233978e-05, Learning Rate: 0.000704\n",
      "Epoch 8699/40000, Loss: 8.947284368332475e-05, Learning Rate: 0.000704\n",
      "Epoch 8700/40000, Loss: 2.922154453699477e-05, Learning Rate: 0.000704\n",
      "Epoch 8701/40000, Loss: 9.4641036412213e-05, Learning Rate: 0.000704\n",
      "Epoch 8702/40000, Loss: 6.30044232821092e-05, Learning Rate: 0.000704\n",
      "Epoch 8703/40000, Loss: 6.214708264451474e-05, Learning Rate: 0.000704\n",
      "Epoch 8704/40000, Loss: 6.130685505922884e-05, Learning Rate: 0.000704\n",
      "Epoch 8705/40000, Loss: 5.4496376833412796e-05, Learning Rate: 0.000704\n",
      "Epoch 8706/40000, Loss: 9.175876766676083e-05, Learning Rate: 0.000704\n",
      "Epoch 8707/40000, Loss: 2.3992402930161916e-05, Learning Rate: 0.000703\n",
      "Epoch 8708/40000, Loss: 6.165012018755078e-05, Learning Rate: 0.000703\n",
      "Epoch 8709/40000, Loss: 5.768231494585052e-05, Learning Rate: 0.000703\n",
      "Epoch 8710/40000, Loss: 8.593602251494303e-05, Learning Rate: 0.000703\n",
      "Epoch 8711/40000, Loss: 5.070280167274177e-05, Learning Rate: 0.000703\n",
      "Epoch 8712/40000, Loss: 4.94796440762002e-05, Learning Rate: 0.000703\n",
      "Epoch 8713/40000, Loss: 8.926997543312609e-05, Learning Rate: 0.000703\n",
      "Epoch 8714/40000, Loss: 2.4330063752131537e-05, Learning Rate: 0.000703\n",
      "Epoch 8715/40000, Loss: 8.895498467609286e-05, Learning Rate: 0.000703\n",
      "Epoch 8716/40000, Loss: 6.170601409394294e-05, Learning Rate: 0.000703\n",
      "Epoch 8717/40000, Loss: 6.0642778407782316e-05, Learning Rate: 0.000703\n",
      "Epoch 8718/40000, Loss: 5.310425694915466e-05, Learning Rate: 0.000703\n",
      "Epoch 8719/40000, Loss: 5.291130582918413e-05, Learning Rate: 0.000702\n",
      "Epoch 8720/40000, Loss: 4.792671461473219e-05, Learning Rate: 0.000702\n",
      "Epoch 8721/40000, Loss: 2.2917751266504638e-05, Learning Rate: 0.000702\n",
      "Epoch 8722/40000, Loss: 7.957882189657539e-05, Learning Rate: 0.000702\n",
      "Epoch 8723/40000, Loss: 2.2933832951821387e-05, Learning Rate: 0.000702\n",
      "Epoch 8724/40000, Loss: 2.2522079234477133e-05, Learning Rate: 0.000702\n",
      "Epoch 8725/40000, Loss: 2.2649446691502817e-05, Learning Rate: 0.000702\n",
      "Epoch 8726/40000, Loss: 4.8169171350309625e-05, Learning Rate: 0.000702\n",
      "Epoch 8727/40000, Loss: 5.365337710827589e-05, Learning Rate: 0.000702\n",
      "Epoch 8728/40000, Loss: 5.3595427743857726e-05, Learning Rate: 0.000702\n",
      "Epoch 8729/40000, Loss: 6.032137025613338e-05, Learning Rate: 0.000702\n",
      "Epoch 8730/40000, Loss: 6.0331905842758715e-05, Learning Rate: 0.000702\n",
      "Epoch 8731/40000, Loss: 5.091907223686576e-05, Learning Rate: 0.000701\n",
      "Epoch 8732/40000, Loss: 5.4469623137265444e-05, Learning Rate: 0.000701\n",
      "Epoch 8733/40000, Loss: 4.89592130179517e-05, Learning Rate: 0.000701\n",
      "Epoch 8734/40000, Loss: 8.170298679033294e-05, Learning Rate: 0.000701\n",
      "Epoch 8735/40000, Loss: 5.5849090131232515e-05, Learning Rate: 0.000701\n",
      "Epoch 8736/40000, Loss: 4.9794904043665156e-05, Learning Rate: 0.000701\n",
      "Epoch 8737/40000, Loss: 4.938436541124247e-05, Learning Rate: 0.000701\n",
      "Epoch 8738/40000, Loss: 6.27928675385192e-05, Learning Rate: 0.000701\n",
      "Epoch 8739/40000, Loss: 9.248573769582435e-05, Learning Rate: 0.000701\n",
      "Epoch 8740/40000, Loss: 8.169237844413146e-05, Learning Rate: 0.000701\n",
      "Epoch 8741/40000, Loss: 9.0511777671054e-05, Learning Rate: 0.000701\n",
      "Epoch 8742/40000, Loss: 4.930648719891906e-05, Learning Rate: 0.000701\n",
      "Epoch 8743/40000, Loss: 8.956486999522895e-05, Learning Rate: 0.000700\n",
      "Epoch 8744/40000, Loss: 9.125886572292075e-05, Learning Rate: 0.000700\n",
      "Epoch 8745/40000, Loss: 6.143398059066385e-05, Learning Rate: 0.000700\n",
      "Epoch 8746/40000, Loss: 9.129969112109393e-05, Learning Rate: 0.000700\n",
      "Epoch 8747/40000, Loss: 4.978573269909248e-05, Learning Rate: 0.000700\n",
      "Epoch 8748/40000, Loss: 4.99193774885498e-05, Learning Rate: 0.000700\n",
      "Epoch 8749/40000, Loss: 9.334736387245357e-05, Learning Rate: 0.000700\n",
      "Epoch 8750/40000, Loss: 9.128846431849524e-05, Learning Rate: 0.000700\n",
      "Epoch 8751/40000, Loss: 5.2209004934411496e-05, Learning Rate: 0.000700\n",
      "Epoch 8752/40000, Loss: 2.417220821371302e-05, Learning Rate: 0.000700\n",
      "Epoch 8753/40000, Loss: 2.480680268490687e-05, Learning Rate: 0.000700\n",
      "Epoch 8754/40000, Loss: 5.86591413593851e-05, Learning Rate: 0.000700\n",
      "Epoch 8755/40000, Loss: 2.9481130695785396e-05, Learning Rate: 0.000699\n",
      "Epoch 8756/40000, Loss: 7.55940200178884e-05, Learning Rate: 0.000699\n",
      "Epoch 8757/40000, Loss: 9.744946873979643e-05, Learning Rate: 0.000699\n",
      "Epoch 8758/40000, Loss: 7.345356425503269e-05, Learning Rate: 0.000699\n",
      "Epoch 8759/40000, Loss: 6.950017268536612e-05, Learning Rate: 0.000699\n",
      "Epoch 8760/40000, Loss: 9.370892075821757e-05, Learning Rate: 0.000699\n",
      "Epoch 8761/40000, Loss: 3.367540193721652e-05, Learning Rate: 0.000699\n",
      "Epoch 8762/40000, Loss: 3.196505713276565e-05, Learning Rate: 0.000699\n",
      "Epoch 8763/40000, Loss: 0.00010209988977294415, Learning Rate: 0.000699\n",
      "Epoch 8764/40000, Loss: 6.35373653494753e-05, Learning Rate: 0.000699\n",
      "Epoch 8765/40000, Loss: 6.76838681101799e-05, Learning Rate: 0.000699\n",
      "Epoch 8766/40000, Loss: 8.61674634506926e-05, Learning Rate: 0.000699\n",
      "Epoch 8767/40000, Loss: 0.0001137534054578282, Learning Rate: 0.000698\n",
      "Epoch 8768/40000, Loss: 8.025167335290462e-05, Learning Rate: 0.000698\n",
      "Epoch 8769/40000, Loss: 0.00010580808884697035, Learning Rate: 0.000698\n",
      "Epoch 8770/40000, Loss: 8.894025813788176e-05, Learning Rate: 0.000698\n",
      "Epoch 8771/40000, Loss: 7.975456537678838e-05, Learning Rate: 0.000698\n",
      "Epoch 8772/40000, Loss: 9.444781608181074e-05, Learning Rate: 0.000698\n",
      "Epoch 8773/40000, Loss: 0.00016319623682647943, Learning Rate: 0.000698\n",
      "Epoch 8774/40000, Loss: 9.087087528314441e-05, Learning Rate: 0.000698\n",
      "Epoch 8775/40000, Loss: 0.00014784805534873158, Learning Rate: 0.000698\n",
      "Epoch 8776/40000, Loss: 0.00013128241698723286, Learning Rate: 0.000698\n",
      "Epoch 8777/40000, Loss: 0.00012202365178382024, Learning Rate: 0.000698\n",
      "Epoch 8778/40000, Loss: 7.617936353199184e-05, Learning Rate: 0.000698\n",
      "Epoch 8779/40000, Loss: 7.606513827340677e-05, Learning Rate: 0.000697\n",
      "Epoch 8780/40000, Loss: 8.292352868011221e-05, Learning Rate: 0.000697\n",
      "Epoch 8781/40000, Loss: 9.907176718115807e-05, Learning Rate: 0.000697\n",
      "Epoch 8782/40000, Loss: 0.0001330458326265216, Learning Rate: 0.000697\n",
      "Epoch 8783/40000, Loss: 8.266289660241455e-05, Learning Rate: 0.000697\n",
      "Epoch 8784/40000, Loss: 0.0001808913511922583, Learning Rate: 0.000697\n",
      "Epoch 8785/40000, Loss: 7.146412826841697e-05, Learning Rate: 0.000697\n",
      "Epoch 8786/40000, Loss: 0.00010539872891968116, Learning Rate: 0.000697\n",
      "Epoch 8787/40000, Loss: 0.00010920156637439504, Learning Rate: 0.000697\n",
      "Epoch 8788/40000, Loss: 7.350039231823757e-05, Learning Rate: 0.000697\n",
      "Epoch 8789/40000, Loss: 0.00025862499023787677, Learning Rate: 0.000697\n",
      "Epoch 8790/40000, Loss: 0.0001333374239038676, Learning Rate: 0.000697\n",
      "Epoch 8791/40000, Loss: 0.0001420979097019881, Learning Rate: 0.000696\n",
      "Epoch 8792/40000, Loss: 0.00014627151540480554, Learning Rate: 0.000696\n",
      "Epoch 8793/40000, Loss: 9.115783905144781e-05, Learning Rate: 0.000696\n",
      "Epoch 8794/40000, Loss: 0.0001302909222431481, Learning Rate: 0.000696\n",
      "Epoch 8795/40000, Loss: 7.232375355670229e-05, Learning Rate: 0.000696\n",
      "Epoch 8796/40000, Loss: 0.00014103113790042698, Learning Rate: 0.000696\n",
      "Epoch 8797/40000, Loss: 5.8186949900118634e-05, Learning Rate: 0.000696\n",
      "Epoch 8798/40000, Loss: 7.755445403745398e-05, Learning Rate: 0.000696\n",
      "Epoch 8799/40000, Loss: 3.5295262932777405e-05, Learning Rate: 0.000696\n",
      "Epoch 8800/40000, Loss: 5.814257019665092e-05, Learning Rate: 0.000696\n",
      "Epoch 8801/40000, Loss: 0.00010480704077053815, Learning Rate: 0.000696\n",
      "Epoch 8802/40000, Loss: 3.1763232982484624e-05, Learning Rate: 0.000696\n",
      "Epoch 8803/40000, Loss: 6.572515121661127e-05, Learning Rate: 0.000695\n",
      "Epoch 8804/40000, Loss: 9.475786646362394e-05, Learning Rate: 0.000695\n",
      "Epoch 8805/40000, Loss: 2.5715198717080057e-05, Learning Rate: 0.000695\n",
      "Epoch 8806/40000, Loss: 6.0195769037818536e-05, Learning Rate: 0.000695\n",
      "Epoch 8807/40000, Loss: 6.355283403536305e-05, Learning Rate: 0.000695\n",
      "Epoch 8808/40000, Loss: 5.241624603513628e-05, Learning Rate: 0.000695\n",
      "Epoch 8809/40000, Loss: 5.0386217480991036e-05, Learning Rate: 0.000695\n",
      "Epoch 8810/40000, Loss: 5.9859354223590344e-05, Learning Rate: 0.000695\n",
      "Epoch 8811/40000, Loss: 2.702871097426396e-05, Learning Rate: 0.000695\n",
      "Epoch 8812/40000, Loss: 6.355677760438994e-05, Learning Rate: 0.000695\n",
      "Epoch 8813/40000, Loss: 6.379148544510826e-05, Learning Rate: 0.000695\n",
      "Epoch 8814/40000, Loss: 9.529757517157122e-05, Learning Rate: 0.000695\n",
      "Epoch 8815/40000, Loss: 9.52233313000761e-05, Learning Rate: 0.000694\n",
      "Epoch 8816/40000, Loss: 5.94579687458463e-05, Learning Rate: 0.000694\n",
      "Epoch 8817/40000, Loss: 2.9918879590695724e-05, Learning Rate: 0.000694\n",
      "Epoch 8818/40000, Loss: 5.6814085837686434e-05, Learning Rate: 0.000694\n",
      "Epoch 8819/40000, Loss: 5.939346374361776e-05, Learning Rate: 0.000694\n",
      "Epoch 8820/40000, Loss: 8.2926191680599e-05, Learning Rate: 0.000694\n",
      "Epoch 8821/40000, Loss: 6.230697181308642e-05, Learning Rate: 0.000694\n",
      "Epoch 8822/40000, Loss: 5.772351141786203e-05, Learning Rate: 0.000694\n",
      "Epoch 8823/40000, Loss: 9.074702393263578e-05, Learning Rate: 0.000694\n",
      "Epoch 8824/40000, Loss: 6.309532909654081e-05, Learning Rate: 0.000694\n",
      "Epoch 8825/40000, Loss: 6.181706703500822e-05, Learning Rate: 0.000694\n",
      "Epoch 8826/40000, Loss: 8.368325507035479e-05, Learning Rate: 0.000694\n",
      "Epoch 8827/40000, Loss: 4.930940849590115e-05, Learning Rate: 0.000693\n",
      "Epoch 8828/40000, Loss: 8.923620771383867e-05, Learning Rate: 0.000693\n",
      "Epoch 8829/40000, Loss: 6.219633360160515e-05, Learning Rate: 0.000693\n",
      "Epoch 8830/40000, Loss: 8.26827745186165e-05, Learning Rate: 0.000693\n",
      "Epoch 8831/40000, Loss: 5.712650454370305e-05, Learning Rate: 0.000693\n",
      "Epoch 8832/40000, Loss: 6.170503911562264e-05, Learning Rate: 0.000693\n",
      "Epoch 8833/40000, Loss: 6.0606365877902135e-05, Learning Rate: 0.000693\n",
      "Epoch 8834/40000, Loss: 2.763536940619815e-05, Learning Rate: 0.000693\n",
      "Epoch 8835/40000, Loss: 8.666637586429715e-05, Learning Rate: 0.000693\n",
      "Epoch 8836/40000, Loss: 4.998654912924394e-05, Learning Rate: 0.000693\n",
      "Epoch 8837/40000, Loss: 4.933480522595346e-05, Learning Rate: 0.000693\n",
      "Epoch 8838/40000, Loss: 6.374669465003535e-05, Learning Rate: 0.000693\n",
      "Epoch 8839/40000, Loss: 8.851722668623552e-05, Learning Rate: 0.000692\n",
      "Epoch 8840/40000, Loss: 9.257223427994177e-05, Learning Rate: 0.000692\n",
      "Epoch 8841/40000, Loss: 0.00014283505151979625, Learning Rate: 0.000692\n",
      "Epoch 8842/40000, Loss: 2.9401344363577664e-05, Learning Rate: 0.000692\n",
      "Epoch 8843/40000, Loss: 6.397233664756641e-05, Learning Rate: 0.000692\n",
      "Epoch 8844/40000, Loss: 3.128532262053341e-05, Learning Rate: 0.000692\n",
      "Epoch 8845/40000, Loss: 5.8113771956413984e-05, Learning Rate: 0.000692\n",
      "Epoch 8846/40000, Loss: 6.242722884053364e-05, Learning Rate: 0.000692\n",
      "Epoch 8847/40000, Loss: 6.821089482400566e-05, Learning Rate: 0.000692\n",
      "Epoch 8848/40000, Loss: 9.164661605609581e-05, Learning Rate: 0.000692\n",
      "Epoch 8849/40000, Loss: 9.841383143793792e-05, Learning Rate: 0.000692\n",
      "Epoch 8850/40000, Loss: 9.194380982080474e-05, Learning Rate: 0.000692\n",
      "Epoch 8851/40000, Loss: 4.9504807975608855e-05, Learning Rate: 0.000691\n",
      "Epoch 8852/40000, Loss: 8.939247345551848e-05, Learning Rate: 0.000691\n",
      "Epoch 8853/40000, Loss: 2.344920540053863e-05, Learning Rate: 0.000691\n",
      "Epoch 8854/40000, Loss: 9.716682689031586e-05, Learning Rate: 0.000691\n",
      "Epoch 8855/40000, Loss: 5.6049848353723064e-05, Learning Rate: 0.000691\n",
      "Epoch 8856/40000, Loss: 5.5620337661821395e-05, Learning Rate: 0.000691\n",
      "Epoch 8857/40000, Loss: 6.231251609278843e-05, Learning Rate: 0.000691\n",
      "Epoch 8858/40000, Loss: 8.508881728630513e-05, Learning Rate: 0.000691\n",
      "Epoch 8859/40000, Loss: 8.568577322876081e-05, Learning Rate: 0.000691\n",
      "Epoch 8860/40000, Loss: 9.964858327293769e-05, Learning Rate: 0.000691\n",
      "Epoch 8861/40000, Loss: 6.529718666570261e-05, Learning Rate: 0.000691\n",
      "Epoch 8862/40000, Loss: 2.6433121092850342e-05, Learning Rate: 0.000691\n",
      "Epoch 8863/40000, Loss: 9.210746065946296e-05, Learning Rate: 0.000690\n",
      "Epoch 8864/40000, Loss: 9.077019785763696e-05, Learning Rate: 0.000690\n",
      "Epoch 8865/40000, Loss: 9.062782191904262e-05, Learning Rate: 0.000690\n",
      "Epoch 8866/40000, Loss: 8.959073602454737e-05, Learning Rate: 0.000690\n",
      "Epoch 8867/40000, Loss: 2.993079397128895e-05, Learning Rate: 0.000690\n",
      "Epoch 8868/40000, Loss: 6.853076774859801e-05, Learning Rate: 0.000690\n",
      "Epoch 8869/40000, Loss: 6.683978426735848e-05, Learning Rate: 0.000690\n",
      "Epoch 8870/40000, Loss: 5.563996819546446e-05, Learning Rate: 0.000690\n",
      "Epoch 8871/40000, Loss: 0.00011425989941926673, Learning Rate: 0.000690\n",
      "Epoch 8872/40000, Loss: 0.00010208015009993687, Learning Rate: 0.000690\n",
      "Epoch 8873/40000, Loss: 5.319822230376303e-05, Learning Rate: 0.000690\n",
      "Epoch 8874/40000, Loss: 9.245256660506129e-05, Learning Rate: 0.000690\n",
      "Epoch 8875/40000, Loss: 6.09618509770371e-05, Learning Rate: 0.000689\n",
      "Epoch 8876/40000, Loss: 2.6856718250201084e-05, Learning Rate: 0.000689\n",
      "Epoch 8877/40000, Loss: 5.582331141340546e-05, Learning Rate: 0.000689\n",
      "Epoch 8878/40000, Loss: 4.100370279047638e-05, Learning Rate: 0.000689\n",
      "Epoch 8879/40000, Loss: 9.841458086157218e-05, Learning Rate: 0.000689\n",
      "Epoch 8880/40000, Loss: 9.523369953967631e-05, Learning Rate: 0.000689\n",
      "Epoch 8881/40000, Loss: 3.5445544199319556e-05, Learning Rate: 0.000689\n",
      "Epoch 8882/40000, Loss: 9.565295476932079e-05, Learning Rate: 0.000689\n",
      "Epoch 8883/40000, Loss: 8.502749551553279e-05, Learning Rate: 0.000689\n",
      "Epoch 8884/40000, Loss: 0.00012178564793430269, Learning Rate: 0.000689\n",
      "Epoch 8885/40000, Loss: 0.0001611444167792797, Learning Rate: 0.000689\n",
      "Epoch 8886/40000, Loss: 6.213637243490666e-05, Learning Rate: 0.000689\n",
      "Epoch 8887/40000, Loss: 8.793768938630819e-05, Learning Rate: 0.000688\n",
      "Epoch 8888/40000, Loss: 0.00011107786122011021, Learning Rate: 0.000688\n",
      "Epoch 8889/40000, Loss: 6.208024569787085e-05, Learning Rate: 0.000688\n",
      "Epoch 8890/40000, Loss: 7.735246617812663e-05, Learning Rate: 0.000688\n",
      "Epoch 8891/40000, Loss: 0.0001178429156425409, Learning Rate: 0.000688\n",
      "Epoch 8892/40000, Loss: 0.00011206439376110211, Learning Rate: 0.000688\n",
      "Epoch 8893/40000, Loss: 7.678685506107286e-05, Learning Rate: 0.000688\n",
      "Epoch 8894/40000, Loss: 7.071293657645583e-05, Learning Rate: 0.000688\n",
      "Epoch 8895/40000, Loss: 9.643787780078128e-05, Learning Rate: 0.000688\n",
      "Epoch 8896/40000, Loss: 0.00010593031765893102, Learning Rate: 0.000688\n",
      "Epoch 8897/40000, Loss: 6.443564780056477e-05, Learning Rate: 0.000688\n",
      "Epoch 8898/40000, Loss: 7.382652984233573e-05, Learning Rate: 0.000688\n",
      "Epoch 8899/40000, Loss: 7.090225699357688e-05, Learning Rate: 0.000687\n",
      "Epoch 8900/40000, Loss: 9.760382090462372e-05, Learning Rate: 0.000687\n",
      "Epoch 8901/40000, Loss: 6.075959390727803e-05, Learning Rate: 0.000687\n",
      "Epoch 8902/40000, Loss: 5.235883509158157e-05, Learning Rate: 0.000687\n",
      "Epoch 8903/40000, Loss: 0.00021200577612034976, Learning Rate: 0.000687\n",
      "Epoch 8904/40000, Loss: 0.00010952019511023536, Learning Rate: 0.000687\n",
      "Epoch 8905/40000, Loss: 3.268873115302995e-05, Learning Rate: 0.000687\n",
      "Epoch 8906/40000, Loss: 0.00011136401735711843, Learning Rate: 0.000687\n",
      "Epoch 8907/40000, Loss: 8.143001468852162e-05, Learning Rate: 0.000687\n",
      "Epoch 8908/40000, Loss: 2.9991817427799106e-05, Learning Rate: 0.000687\n",
      "Epoch 8909/40000, Loss: 0.00010209056199528277, Learning Rate: 0.000687\n",
      "Epoch 8910/40000, Loss: 4.2766019760165364e-05, Learning Rate: 0.000687\n",
      "Epoch 8911/40000, Loss: 5.3298015700420365e-05, Learning Rate: 0.000686\n",
      "Epoch 8912/40000, Loss: 0.00010377074795542285, Learning Rate: 0.000686\n",
      "Epoch 8913/40000, Loss: 0.0001017770409816876, Learning Rate: 0.000686\n",
      "Epoch 8914/40000, Loss: 5.3590487368637696e-05, Learning Rate: 0.000686\n",
      "Epoch 8915/40000, Loss: 7.837508019292727e-05, Learning Rate: 0.000686\n",
      "Epoch 8916/40000, Loss: 6.498248694697395e-05, Learning Rate: 0.000686\n",
      "Epoch 8917/40000, Loss: 3.941807881346904e-05, Learning Rate: 0.000686\n",
      "Epoch 8918/40000, Loss: 9.746856812853366e-05, Learning Rate: 0.000686\n",
      "Epoch 8919/40000, Loss: 5.054119537817314e-05, Learning Rate: 0.000686\n",
      "Epoch 8920/40000, Loss: 6.304573616944253e-05, Learning Rate: 0.000686\n",
      "Epoch 8921/40000, Loss: 6.145628867670894e-05, Learning Rate: 0.000686\n",
      "Epoch 8922/40000, Loss: 5.1936498493887484e-05, Learning Rate: 0.000686\n",
      "Epoch 8923/40000, Loss: 6.349726754706353e-05, Learning Rate: 0.000685\n",
      "Epoch 8924/40000, Loss: 5.601765587925911e-05, Learning Rate: 0.000685\n",
      "Epoch 8925/40000, Loss: 9.236670302925631e-05, Learning Rate: 0.000685\n",
      "Epoch 8926/40000, Loss: 6.181596108945087e-05, Learning Rate: 0.000685\n",
      "Epoch 8927/40000, Loss: 9.829490591073409e-05, Learning Rate: 0.000685\n",
      "Epoch 8928/40000, Loss: 5.4384061513701454e-05, Learning Rate: 0.000685\n",
      "Epoch 8929/40000, Loss: 2.4007309548323974e-05, Learning Rate: 0.000685\n",
      "Epoch 8930/40000, Loss: 4.888286275672726e-05, Learning Rate: 0.000685\n",
      "Epoch 8931/40000, Loss: 8.193548273993656e-05, Learning Rate: 0.000685\n",
      "Epoch 8932/40000, Loss: 2.429088817734737e-05, Learning Rate: 0.000685\n",
      "Epoch 8933/40000, Loss: 8.925762085709721e-05, Learning Rate: 0.000685\n",
      "Epoch 8934/40000, Loss: 8.875621279003099e-05, Learning Rate: 0.000685\n",
      "Epoch 8935/40000, Loss: 2.347309055039659e-05, Learning Rate: 0.000685\n",
      "Epoch 8936/40000, Loss: 9.081408643396571e-05, Learning Rate: 0.000684\n",
      "Epoch 8937/40000, Loss: 8.864949631970376e-05, Learning Rate: 0.000684\n",
      "Epoch 8938/40000, Loss: 5.5692977184662595e-05, Learning Rate: 0.000684\n",
      "Epoch 8939/40000, Loss: 4.874168007518165e-05, Learning Rate: 0.000684\n",
      "Epoch 8940/40000, Loss: 4.844581781071611e-05, Learning Rate: 0.000684\n",
      "Epoch 8941/40000, Loss: 4.776879359269515e-05, Learning Rate: 0.000684\n",
      "Epoch 8942/40000, Loss: 8.864619303494692e-05, Learning Rate: 0.000684\n",
      "Epoch 8943/40000, Loss: 7.350953092100099e-05, Learning Rate: 0.000684\n",
      "Epoch 8944/40000, Loss: 6.327781011350453e-05, Learning Rate: 0.000684\n",
      "Epoch 8945/40000, Loss: 4.830031321034767e-05, Learning Rate: 0.000684\n",
      "Epoch 8946/40000, Loss: 9.021953883348033e-05, Learning Rate: 0.000684\n",
      "Epoch 8947/40000, Loss: 8.031976904021576e-05, Learning Rate: 0.000684\n",
      "Epoch 8948/40000, Loss: 5.3314390243031085e-05, Learning Rate: 0.000683\n",
      "Epoch 8949/40000, Loss: 7.934243330964819e-05, Learning Rate: 0.000683\n",
      "Epoch 8950/40000, Loss: 6.073508848203346e-05, Learning Rate: 0.000683\n",
      "Epoch 8951/40000, Loss: 4.794607593794353e-05, Learning Rate: 0.000683\n",
      "Epoch 8952/40000, Loss: 5.3957464842824265e-05, Learning Rate: 0.000683\n",
      "Epoch 8953/40000, Loss: 8.542579598724842e-05, Learning Rate: 0.000683\n",
      "Epoch 8954/40000, Loss: 2.4181885237339884e-05, Learning Rate: 0.000683\n",
      "Epoch 8955/40000, Loss: 2.3831842554500327e-05, Learning Rate: 0.000683\n",
      "Epoch 8956/40000, Loss: 2.5168708816636354e-05, Learning Rate: 0.000683\n",
      "Epoch 8957/40000, Loss: 5.57480234419927e-05, Learning Rate: 0.000683\n",
      "Epoch 8958/40000, Loss: 8.949842595029622e-05, Learning Rate: 0.000683\n",
      "Epoch 8959/40000, Loss: 6.458832649514079e-05, Learning Rate: 0.000683\n",
      "Epoch 8960/40000, Loss: 2.3849417630117387e-05, Learning Rate: 0.000682\n",
      "Epoch 8961/40000, Loss: 6.630827556364238e-05, Learning Rate: 0.000682\n",
      "Epoch 8962/40000, Loss: 6.807550380472094e-05, Learning Rate: 0.000682\n",
      "Epoch 8963/40000, Loss: 2.5678828023956157e-05, Learning Rate: 0.000682\n",
      "Epoch 8964/40000, Loss: 9.343212150270119e-05, Learning Rate: 0.000682\n",
      "Epoch 8965/40000, Loss: 5.728285759687424e-05, Learning Rate: 0.000682\n",
      "Epoch 8966/40000, Loss: 2.5173394533339888e-05, Learning Rate: 0.000682\n",
      "Epoch 8967/40000, Loss: 9.185900125885382e-05, Learning Rate: 0.000682\n",
      "Epoch 8968/40000, Loss: 6.116128497524187e-05, Learning Rate: 0.000682\n",
      "Epoch 8969/40000, Loss: 8.858169167069718e-05, Learning Rate: 0.000682\n",
      "Epoch 8970/40000, Loss: 6.446951738325879e-05, Learning Rate: 0.000682\n",
      "Epoch 8971/40000, Loss: 6.2062572396826e-05, Learning Rate: 0.000682\n",
      "Epoch 8972/40000, Loss: 3.5847846447722986e-05, Learning Rate: 0.000681\n",
      "Epoch 8973/40000, Loss: 9.136951121035963e-05, Learning Rate: 0.000681\n",
      "Epoch 8974/40000, Loss: 5.420215893536806e-05, Learning Rate: 0.000681\n",
      "Epoch 8975/40000, Loss: 0.00012916591367684305, Learning Rate: 0.000681\n",
      "Epoch 8976/40000, Loss: 7.74439176893793e-05, Learning Rate: 0.000681\n",
      "Epoch 8977/40000, Loss: 5.979876732453704e-05, Learning Rate: 0.000681\n",
      "Epoch 8978/40000, Loss: 0.00010438967001391575, Learning Rate: 0.000681\n",
      "Epoch 8979/40000, Loss: 7.79118126956746e-05, Learning Rate: 0.000681\n",
      "Epoch 8980/40000, Loss: 3.059346636291593e-05, Learning Rate: 0.000681\n",
      "Epoch 8981/40000, Loss: 2.7483401936478913e-05, Learning Rate: 0.000681\n",
      "Epoch 8982/40000, Loss: 5.659933958668262e-05, Learning Rate: 0.000681\n",
      "Epoch 8983/40000, Loss: 9.652399603510275e-05, Learning Rate: 0.000681\n",
      "Epoch 8984/40000, Loss: 8.667776273796335e-05, Learning Rate: 0.000680\n",
      "Epoch 8985/40000, Loss: 6.0750346165150404e-05, Learning Rate: 0.000680\n",
      "Epoch 8986/40000, Loss: 5.736910316045396e-05, Learning Rate: 0.000680\n",
      "Epoch 8987/40000, Loss: 5.168392090126872e-05, Learning Rate: 0.000680\n",
      "Epoch 8988/40000, Loss: 5.731733836000785e-05, Learning Rate: 0.000680\n",
      "Epoch 8989/40000, Loss: 5.648802107316442e-05, Learning Rate: 0.000680\n",
      "Epoch 8990/40000, Loss: 6.311678589554504e-05, Learning Rate: 0.000680\n",
      "Epoch 8991/40000, Loss: 6.893719546496868e-05, Learning Rate: 0.000680\n",
      "Epoch 8992/40000, Loss: 7.34663408366032e-05, Learning Rate: 0.000680\n",
      "Epoch 8993/40000, Loss: 8.873243496054783e-05, Learning Rate: 0.000680\n",
      "Epoch 8994/40000, Loss: 8.639137377031147e-05, Learning Rate: 0.000680\n",
      "Epoch 8995/40000, Loss: 7.01472454238683e-05, Learning Rate: 0.000680\n",
      "Epoch 8996/40000, Loss: 6.503977783722803e-05, Learning Rate: 0.000680\n",
      "Epoch 8997/40000, Loss: 5.0384791393298656e-05, Learning Rate: 0.000679\n",
      "Epoch 8998/40000, Loss: 8.981427527032793e-05, Learning Rate: 0.000679\n",
      "Epoch 8999/40000, Loss: 2.464772478560917e-05, Learning Rate: 0.000679\n",
      "Epoch 9000/40000, Loss: 8.495560905430466e-05, Learning Rate: 0.000679\n",
      "Epoch 9001/40000, Loss: 6.195328023750335e-05, Learning Rate: 0.000679\n",
      "Epoch 9002/40000, Loss: 8.994793461170048e-05, Learning Rate: 0.000679\n",
      "Epoch 9003/40000, Loss: 8.375962352147326e-05, Learning Rate: 0.000679\n",
      "Epoch 9004/40000, Loss: 4.918298509437591e-05, Learning Rate: 0.000679\n",
      "Epoch 9005/40000, Loss: 5.882684126845561e-05, Learning Rate: 0.000679\n",
      "Epoch 9006/40000, Loss: 2.4789049348328263e-05, Learning Rate: 0.000679\n",
      "Epoch 9007/40000, Loss: 5.8185421949019656e-05, Learning Rate: 0.000679\n",
      "Epoch 9008/40000, Loss: 9.11578827071935e-05, Learning Rate: 0.000679\n",
      "Epoch 9009/40000, Loss: 2.873882658604998e-05, Learning Rate: 0.000678\n",
      "Epoch 9010/40000, Loss: 8.538468682672828e-05, Learning Rate: 0.000678\n",
      "Epoch 9011/40000, Loss: 6.351176125463098e-05, Learning Rate: 0.000678\n",
      "Epoch 9012/40000, Loss: 8.316213643411174e-05, Learning Rate: 0.000678\n",
      "Epoch 9013/40000, Loss: 2.8930542612215504e-05, Learning Rate: 0.000678\n",
      "Epoch 9014/40000, Loss: 9.388466423843056e-05, Learning Rate: 0.000678\n",
      "Epoch 9015/40000, Loss: 9.39126402954571e-05, Learning Rate: 0.000678\n",
      "Epoch 9016/40000, Loss: 4.06929939344991e-05, Learning Rate: 0.000678\n",
      "Epoch 9017/40000, Loss: 4.599573730956763e-05, Learning Rate: 0.000678\n",
      "Epoch 9018/40000, Loss: 9.389487240696326e-05, Learning Rate: 0.000678\n",
      "Epoch 9019/40000, Loss: 0.00012535462155938148, Learning Rate: 0.000678\n",
      "Epoch 9020/40000, Loss: 8.963621075963601e-05, Learning Rate: 0.000678\n",
      "Epoch 9021/40000, Loss: 6.459309224737808e-05, Learning Rate: 0.000677\n",
      "Epoch 9022/40000, Loss: 9.24327177926898e-05, Learning Rate: 0.000677\n",
      "Epoch 9023/40000, Loss: 7.400214235531166e-05, Learning Rate: 0.000677\n",
      "Epoch 9024/40000, Loss: 0.00013526270049624145, Learning Rate: 0.000677\n",
      "Epoch 9025/40000, Loss: 9.486314957030118e-05, Learning Rate: 0.000677\n",
      "Epoch 9026/40000, Loss: 0.00014409786672331393, Learning Rate: 0.000677\n",
      "Epoch 9027/40000, Loss: 0.00018648267723619938, Learning Rate: 0.000677\n",
      "Epoch 9028/40000, Loss: 0.0001670489291427657, Learning Rate: 0.000677\n",
      "Epoch 9029/40000, Loss: 0.00012790760956704617, Learning Rate: 0.000677\n",
      "Epoch 9030/40000, Loss: 6.18573249084875e-05, Learning Rate: 0.000677\n",
      "Epoch 9031/40000, Loss: 0.0001629565958864987, Learning Rate: 0.000677\n",
      "Epoch 9032/40000, Loss: 0.00015973846893757582, Learning Rate: 0.000677\n",
      "Epoch 9033/40000, Loss: 0.00010870966070797294, Learning Rate: 0.000676\n",
      "Epoch 9034/40000, Loss: 0.00016050750855356455, Learning Rate: 0.000676\n",
      "Epoch 9035/40000, Loss: 0.00012044964387314394, Learning Rate: 0.000676\n",
      "Epoch 9036/40000, Loss: 0.00017577025573700666, Learning Rate: 0.000676\n",
      "Epoch 9037/40000, Loss: 6.810545892221853e-05, Learning Rate: 0.000676\n",
      "Epoch 9038/40000, Loss: 0.00014815844770055264, Learning Rate: 0.000676\n",
      "Epoch 9039/40000, Loss: 0.00011323345097480342, Learning Rate: 0.000676\n",
      "Epoch 9040/40000, Loss: 0.00011611937952693552, Learning Rate: 0.000676\n",
      "Epoch 9041/40000, Loss: 0.00012886940385214984, Learning Rate: 0.000676\n",
      "Epoch 9042/40000, Loss: 0.00012773553316947073, Learning Rate: 0.000676\n",
      "Epoch 9043/40000, Loss: 0.00010592211765469983, Learning Rate: 0.000676\n",
      "Epoch 9044/40000, Loss: 8.555227395845577e-05, Learning Rate: 0.000676\n",
      "Epoch 9045/40000, Loss: 0.00011473261838546023, Learning Rate: 0.000676\n",
      "Epoch 9046/40000, Loss: 5.71444834349677e-05, Learning Rate: 0.000675\n",
      "Epoch 9047/40000, Loss: 5.790151044493541e-05, Learning Rate: 0.000675\n",
      "Epoch 9048/40000, Loss: 7.426006777677685e-05, Learning Rate: 0.000675\n",
      "Epoch 9049/40000, Loss: 6.183746154420078e-05, Learning Rate: 0.000675\n",
      "Epoch 9050/40000, Loss: 2.828708238666877e-05, Learning Rate: 0.000675\n",
      "Epoch 9051/40000, Loss: 9.404635784449056e-05, Learning Rate: 0.000675\n",
      "Epoch 9052/40000, Loss: 9.04325206647627e-05, Learning Rate: 0.000675\n",
      "Epoch 9053/40000, Loss: 7.309541979338974e-05, Learning Rate: 0.000675\n",
      "Epoch 9054/40000, Loss: 9.455218241782859e-05, Learning Rate: 0.000675\n",
      "Epoch 9055/40000, Loss: 8.384160901186988e-05, Learning Rate: 0.000675\n",
      "Epoch 9056/40000, Loss: 8.706338121555746e-05, Learning Rate: 0.000675\n",
      "Epoch 9057/40000, Loss: 6.836272223154083e-05, Learning Rate: 0.000675\n",
      "Epoch 9058/40000, Loss: 2.9846823963453062e-05, Learning Rate: 0.000674\n",
      "Epoch 9059/40000, Loss: 6.300885434029624e-05, Learning Rate: 0.000674\n",
      "Epoch 9060/40000, Loss: 3.0742445233045146e-05, Learning Rate: 0.000674\n",
      "Epoch 9061/40000, Loss: 9.879829303827137e-05, Learning Rate: 0.000674\n",
      "Epoch 9062/40000, Loss: 8.742298086872324e-05, Learning Rate: 0.000674\n",
      "Epoch 9063/40000, Loss: 7.625667785760015e-05, Learning Rate: 0.000674\n",
      "Epoch 9064/40000, Loss: 4.9452908569946885e-05, Learning Rate: 0.000674\n",
      "Epoch 9065/40000, Loss: 8.71032098075375e-05, Learning Rate: 0.000674\n",
      "Epoch 9066/40000, Loss: 6.81892634020187e-05, Learning Rate: 0.000674\n",
      "Epoch 9067/40000, Loss: 2.8063508580089547e-05, Learning Rate: 0.000674\n",
      "Epoch 9068/40000, Loss: 9.010711801238358e-05, Learning Rate: 0.000674\n",
      "Epoch 9069/40000, Loss: 2.8078849936719052e-05, Learning Rate: 0.000674\n",
      "Epoch 9070/40000, Loss: 5.0459158956073225e-05, Learning Rate: 0.000674\n",
      "Epoch 9071/40000, Loss: 9.401581337442622e-05, Learning Rate: 0.000673\n",
      "Epoch 9072/40000, Loss: 6.507198850158602e-05, Learning Rate: 0.000673\n",
      "Epoch 9073/40000, Loss: 6.228852726053447e-05, Learning Rate: 0.000673\n",
      "Epoch 9074/40000, Loss: 9.828613110585138e-05, Learning Rate: 0.000673\n",
      "Epoch 9075/40000, Loss: 5.492916170624085e-05, Learning Rate: 0.000673\n",
      "Epoch 9076/40000, Loss: 2.4829094400047325e-05, Learning Rate: 0.000673\n",
      "Epoch 9077/40000, Loss: 5.3722586017102e-05, Learning Rate: 0.000673\n",
      "Epoch 9078/40000, Loss: 6.0454669437604025e-05, Learning Rate: 0.000673\n",
      "Epoch 9079/40000, Loss: 5.3497173212235793e-05, Learning Rate: 0.000673\n",
      "Epoch 9080/40000, Loss: 8.126568718580529e-05, Learning Rate: 0.000673\n",
      "Epoch 9081/40000, Loss: 4.7956207708921283e-05, Learning Rate: 0.000673\n",
      "Epoch 9082/40000, Loss: 9.127028897637501e-05, Learning Rate: 0.000673\n",
      "Epoch 9083/40000, Loss: 5.996805339236744e-05, Learning Rate: 0.000672\n",
      "Epoch 9084/40000, Loss: 8.102888386929408e-05, Learning Rate: 0.000672\n",
      "Epoch 9085/40000, Loss: 6.193411536514759e-05, Learning Rate: 0.000672\n",
      "Epoch 9086/40000, Loss: 8.872522448655218e-05, Learning Rate: 0.000672\n",
      "Epoch 9087/40000, Loss: 4.781155803357251e-05, Learning Rate: 0.000672\n",
      "Epoch 9088/40000, Loss: 4.714401438832283e-05, Learning Rate: 0.000672\n",
      "Epoch 9089/40000, Loss: 8.003034599823877e-05, Learning Rate: 0.000672\n",
      "Epoch 9090/40000, Loss: 8.920186519389972e-05, Learning Rate: 0.000672\n",
      "Epoch 9091/40000, Loss: 5.3646843298338354e-05, Learning Rate: 0.000672\n",
      "Epoch 9092/40000, Loss: 4.7654324589530006e-05, Learning Rate: 0.000672\n",
      "Epoch 9093/40000, Loss: 8.633245306555182e-05, Learning Rate: 0.000672\n",
      "Epoch 9094/40000, Loss: 8.030903700273484e-05, Learning Rate: 0.000672\n",
      "Epoch 9095/40000, Loss: 5.4509262554347515e-05, Learning Rate: 0.000671\n",
      "Epoch 9096/40000, Loss: 9.002501610666513e-05, Learning Rate: 0.000671\n",
      "Epoch 9097/40000, Loss: 4.841501868213527e-05, Learning Rate: 0.000671\n",
      "Epoch 9098/40000, Loss: 4.7500529035460204e-05, Learning Rate: 0.000671\n",
      "Epoch 9099/40000, Loss: 5.2731644245795906e-05, Learning Rate: 0.000671\n",
      "Epoch 9100/40000, Loss: 5.998262713546865e-05, Learning Rate: 0.000671\n",
      "Epoch 9101/40000, Loss: 2.2902366254129447e-05, Learning Rate: 0.000671\n",
      "Epoch 9102/40000, Loss: 8.741848432691768e-05, Learning Rate: 0.000671\n",
      "Epoch 9103/40000, Loss: 7.861241465434432e-05, Learning Rate: 0.000671\n",
      "Epoch 9104/40000, Loss: 6.032310557202436e-05, Learning Rate: 0.000671\n",
      "Epoch 9105/40000, Loss: 2.270221375511028e-05, Learning Rate: 0.000671\n",
      "Epoch 9106/40000, Loss: 4.69787119072862e-05, Learning Rate: 0.000671\n",
      "Epoch 9107/40000, Loss: 2.2281843484961428e-05, Learning Rate: 0.000671\n",
      "Epoch 9108/40000, Loss: 8.726192027097568e-05, Learning Rate: 0.000670\n",
      "Epoch 9109/40000, Loss: 7.925782119855285e-05, Learning Rate: 0.000670\n",
      "Epoch 9110/40000, Loss: 4.692727088695392e-05, Learning Rate: 0.000670\n",
      "Epoch 9111/40000, Loss: 5.199762017582543e-05, Learning Rate: 0.000670\n",
      "Epoch 9112/40000, Loss: 8.752182475291193e-05, Learning Rate: 0.000670\n",
      "Epoch 9113/40000, Loss: 7.824546628398821e-05, Learning Rate: 0.000670\n",
      "Epoch 9114/40000, Loss: 5.199560837354511e-05, Learning Rate: 0.000670\n",
      "Epoch 9115/40000, Loss: 5.128281190991402e-05, Learning Rate: 0.000670\n",
      "Epoch 9116/40000, Loss: 7.77054374339059e-05, Learning Rate: 0.000670\n",
      "Epoch 9117/40000, Loss: 5.907699596718885e-05, Learning Rate: 0.000670\n",
      "Epoch 9118/40000, Loss: 5.156963015906513e-05, Learning Rate: 0.000670\n",
      "Epoch 9119/40000, Loss: 4.6883422328392044e-05, Learning Rate: 0.000670\n",
      "Epoch 9120/40000, Loss: 2.2254669602261856e-05, Learning Rate: 0.000669\n",
      "Epoch 9121/40000, Loss: 5.173899626242928e-05, Learning Rate: 0.000669\n",
      "Epoch 9122/40000, Loss: 7.780361920595169e-05, Learning Rate: 0.000669\n",
      "Epoch 9123/40000, Loss: 8.675229764776304e-05, Learning Rate: 0.000669\n",
      "Epoch 9124/40000, Loss: 5.190247611608356e-05, Learning Rate: 0.000669\n",
      "Epoch 9125/40000, Loss: 7.805080531397834e-05, Learning Rate: 0.000669\n",
      "Epoch 9126/40000, Loss: 8.66021218826063e-05, Learning Rate: 0.000669\n",
      "Epoch 9127/40000, Loss: 7.977389759616926e-05, Learning Rate: 0.000669\n",
      "Epoch 9128/40000, Loss: 9.101135219680145e-05, Learning Rate: 0.000669\n",
      "Epoch 9129/40000, Loss: 8.922267443267629e-05, Learning Rate: 0.000669\n",
      "Epoch 9130/40000, Loss: 8.731196430744603e-05, Learning Rate: 0.000669\n",
      "Epoch 9131/40000, Loss: 5.246783257462084e-05, Learning Rate: 0.000669\n",
      "Epoch 9132/40000, Loss: 5.958892506896518e-05, Learning Rate: 0.000669\n",
      "Epoch 9133/40000, Loss: 8.714575960766524e-05, Learning Rate: 0.000668\n",
      "Epoch 9134/40000, Loss: 5.210766539676115e-05, Learning Rate: 0.000668\n",
      "Epoch 9135/40000, Loss: 8.702249033376575e-05, Learning Rate: 0.000668\n",
      "Epoch 9136/40000, Loss: 5.329847044777125e-05, Learning Rate: 0.000668\n",
      "Epoch 9137/40000, Loss: 5.966365279164165e-05, Learning Rate: 0.000668\n",
      "Epoch 9138/40000, Loss: 7.783793262206018e-05, Learning Rate: 0.000668\n",
      "Epoch 9139/40000, Loss: 6.044135079719126e-05, Learning Rate: 0.000668\n",
      "Epoch 9140/40000, Loss: 8.74995457706973e-05, Learning Rate: 0.000668\n",
      "Epoch 9141/40000, Loss: 5.931969644734636e-05, Learning Rate: 0.000668\n",
      "Epoch 9142/40000, Loss: 5.257088196231052e-05, Learning Rate: 0.000668\n",
      "Epoch 9143/40000, Loss: 8.698265446582809e-05, Learning Rate: 0.000668\n",
      "Epoch 9144/40000, Loss: 8.673882257426158e-05, Learning Rate: 0.000668\n",
      "Epoch 9145/40000, Loss: 5.221843093750067e-05, Learning Rate: 0.000667\n",
      "Epoch 9146/40000, Loss: 2.241058427898679e-05, Learning Rate: 0.000667\n",
      "Epoch 9147/40000, Loss: 5.919032992096618e-05, Learning Rate: 0.000667\n",
      "Epoch 9148/40000, Loss: 5.920477269683033e-05, Learning Rate: 0.000667\n",
      "Epoch 9149/40000, Loss: 2.2076937966630794e-05, Learning Rate: 0.000667\n",
      "Epoch 9150/40000, Loss: 7.784950867062435e-05, Learning Rate: 0.000667\n",
      "Epoch 9151/40000, Loss: 4.6653731260448694e-05, Learning Rate: 0.000667\n",
      "Epoch 9152/40000, Loss: 7.88609468145296e-05, Learning Rate: 0.000667\n",
      "Epoch 9153/40000, Loss: 7.810282841091976e-05, Learning Rate: 0.000667\n",
      "Epoch 9154/40000, Loss: 5.244810745352879e-05, Learning Rate: 0.000667\n",
      "Epoch 9155/40000, Loss: 4.673030343838036e-05, Learning Rate: 0.000667\n",
      "Epoch 9156/40000, Loss: 4.722268204204738e-05, Learning Rate: 0.000667\n",
      "Epoch 9157/40000, Loss: 7.857140735723078e-05, Learning Rate: 0.000667\n",
      "Epoch 9158/40000, Loss: 2.4027989638852887e-05, Learning Rate: 0.000666\n",
      "Epoch 9159/40000, Loss: 7.907608232926577e-05, Learning Rate: 0.000666\n",
      "Epoch 9160/40000, Loss: 2.2783493477618322e-05, Learning Rate: 0.000666\n",
      "Epoch 9161/40000, Loss: 8.168592466972768e-05, Learning Rate: 0.000666\n",
      "Epoch 9162/40000, Loss: 3.069360536755994e-05, Learning Rate: 0.000666\n",
      "Epoch 9163/40000, Loss: 5.744529335061088e-05, Learning Rate: 0.000666\n",
      "Epoch 9164/40000, Loss: 5.548309127334505e-05, Learning Rate: 0.000666\n",
      "Epoch 9165/40000, Loss: 6.311769539024681e-05, Learning Rate: 0.000666\n",
      "Epoch 9166/40000, Loss: 9.181898349197581e-05, Learning Rate: 0.000666\n",
      "Epoch 9167/40000, Loss: 3.271227978984825e-05, Learning Rate: 0.000666\n",
      "Epoch 9168/40000, Loss: 9.752437472343445e-05, Learning Rate: 0.000666\n",
      "Epoch 9169/40000, Loss: 6.70399604132399e-05, Learning Rate: 0.000666\n",
      "Epoch 9170/40000, Loss: 0.0001012553257169202, Learning Rate: 0.000665\n",
      "Epoch 9171/40000, Loss: 9.007450717035681e-05, Learning Rate: 0.000665\n",
      "Epoch 9172/40000, Loss: 5.901090844417922e-05, Learning Rate: 0.000665\n",
      "Epoch 9173/40000, Loss: 5.775996396550909e-05, Learning Rate: 0.000665\n",
      "Epoch 9174/40000, Loss: 0.00011489240569062531, Learning Rate: 0.000665\n",
      "Epoch 9175/40000, Loss: 8.97110512596555e-05, Learning Rate: 0.000665\n",
      "Epoch 9176/40000, Loss: 3.299659147160128e-05, Learning Rate: 0.000665\n",
      "Epoch 9177/40000, Loss: 6.478688737843186e-05, Learning Rate: 0.000665\n",
      "Epoch 9178/40000, Loss: 6.425171886803582e-05, Learning Rate: 0.000665\n",
      "Epoch 9179/40000, Loss: 9.944837802322581e-05, Learning Rate: 0.000665\n",
      "Epoch 9180/40000, Loss: 5.932516069151461e-05, Learning Rate: 0.000665\n",
      "Epoch 9181/40000, Loss: 0.00010889988334383816, Learning Rate: 0.000665\n",
      "Epoch 9182/40000, Loss: 5.392414459493011e-05, Learning Rate: 0.000665\n",
      "Epoch 9183/40000, Loss: 8.320172491949052e-05, Learning Rate: 0.000664\n",
      "Epoch 9184/40000, Loss: 9.699736983748153e-05, Learning Rate: 0.000664\n",
      "Epoch 9185/40000, Loss: 0.00010946985275950283, Learning Rate: 0.000664\n",
      "Epoch 9186/40000, Loss: 5.914606663282029e-05, Learning Rate: 0.000664\n",
      "Epoch 9187/40000, Loss: 8.389817958232015e-05, Learning Rate: 0.000664\n",
      "Epoch 9188/40000, Loss: 3.6221841583028436e-05, Learning Rate: 0.000664\n",
      "Epoch 9189/40000, Loss: 3.6493463994702324e-05, Learning Rate: 0.000664\n",
      "Epoch 9190/40000, Loss: 6.764832505723462e-05, Learning Rate: 0.000664\n",
      "Epoch 9191/40000, Loss: 5.7728619140107185e-05, Learning Rate: 0.000664\n",
      "Epoch 9192/40000, Loss: 8.810844883555546e-05, Learning Rate: 0.000664\n",
      "Epoch 9193/40000, Loss: 8.633857942186296e-05, Learning Rate: 0.000664\n",
      "Epoch 9194/40000, Loss: 2.6441022782819346e-05, Learning Rate: 0.000664\n",
      "Epoch 9195/40000, Loss: 2.4428582037216984e-05, Learning Rate: 0.000663\n",
      "Epoch 9196/40000, Loss: 9.433581726625562e-05, Learning Rate: 0.000663\n",
      "Epoch 9197/40000, Loss: 5.049143874202855e-05, Learning Rate: 0.000663\n",
      "Epoch 9198/40000, Loss: 2.529270204831846e-05, Learning Rate: 0.000663\n",
      "Epoch 9199/40000, Loss: 6.123637285782024e-05, Learning Rate: 0.000663\n",
      "Epoch 9200/40000, Loss: 5.068846803624183e-05, Learning Rate: 0.000663\n",
      "Epoch 9201/40000, Loss: 5.1956172683276236e-05, Learning Rate: 0.000663\n",
      "Epoch 9202/40000, Loss: 7.112455932656303e-05, Learning Rate: 0.000663\n",
      "Epoch 9203/40000, Loss: 5.978125045658089e-05, Learning Rate: 0.000663\n",
      "Epoch 9204/40000, Loss: 0.00011759977496694773, Learning Rate: 0.000663\n",
      "Epoch 9205/40000, Loss: 3.085705975536257e-05, Learning Rate: 0.000663\n",
      "Epoch 9206/40000, Loss: 0.00010787704377435148, Learning Rate: 0.000663\n",
      "Epoch 9207/40000, Loss: 6.665061664534733e-05, Learning Rate: 0.000663\n",
      "Epoch 9208/40000, Loss: 5.273252463666722e-05, Learning Rate: 0.000662\n",
      "Epoch 9209/40000, Loss: 6.991963891778141e-05, Learning Rate: 0.000662\n",
      "Epoch 9210/40000, Loss: 3.8241272704908624e-05, Learning Rate: 0.000662\n",
      "Epoch 9211/40000, Loss: 0.00011689819802995771, Learning Rate: 0.000662\n",
      "Epoch 9212/40000, Loss: 7.398751040454954e-05, Learning Rate: 0.000662\n",
      "Epoch 9213/40000, Loss: 0.00011072689812863246, Learning Rate: 0.000662\n",
      "Epoch 9214/40000, Loss: 8.922497363528237e-05, Learning Rate: 0.000662\n",
      "Epoch 9215/40000, Loss: 3.560230470611714e-05, Learning Rate: 0.000662\n",
      "Epoch 9216/40000, Loss: 8.003975381143391e-05, Learning Rate: 0.000662\n",
      "Epoch 9217/40000, Loss: 6.302850670181215e-05, Learning Rate: 0.000662\n",
      "Epoch 9218/40000, Loss: 5.617500937660225e-05, Learning Rate: 0.000662\n",
      "Epoch 9219/40000, Loss: 6.52281814836897e-05, Learning Rate: 0.000662\n",
      "Epoch 9220/40000, Loss: 3.712004399858415e-05, Learning Rate: 0.000661\n",
      "Epoch 9221/40000, Loss: 6.775421934435144e-05, Learning Rate: 0.000661\n",
      "Epoch 9222/40000, Loss: 5.319359479472041e-05, Learning Rate: 0.000661\n",
      "Epoch 9223/40000, Loss: 6.903075700392947e-05, Learning Rate: 0.000661\n",
      "Epoch 9224/40000, Loss: 0.00013222826237324625, Learning Rate: 0.000661\n",
      "Epoch 9225/40000, Loss: 7.587522850371897e-05, Learning Rate: 0.000661\n",
      "Epoch 9226/40000, Loss: 3.319552342873067e-05, Learning Rate: 0.000661\n",
      "Epoch 9227/40000, Loss: 0.000120256379886996, Learning Rate: 0.000661\n",
      "Epoch 9228/40000, Loss: 6.208699778653681e-05, Learning Rate: 0.000661\n",
      "Epoch 9229/40000, Loss: 0.00012444978347048163, Learning Rate: 0.000661\n",
      "Epoch 9230/40000, Loss: 8.552215876989067e-05, Learning Rate: 0.000661\n",
      "Epoch 9231/40000, Loss: 9.863549348665401e-05, Learning Rate: 0.000661\n",
      "Epoch 9232/40000, Loss: 9.361042611999437e-05, Learning Rate: 0.000661\n",
      "Epoch 9233/40000, Loss: 6.528814265038818e-05, Learning Rate: 0.000660\n",
      "Epoch 9234/40000, Loss: 5.599241558229551e-05, Learning Rate: 0.000660\n",
      "Epoch 9235/40000, Loss: 2.798714194796048e-05, Learning Rate: 0.000660\n",
      "Epoch 9236/40000, Loss: 6.599451444344595e-05, Learning Rate: 0.000660\n",
      "Epoch 9237/40000, Loss: 6.104072235757485e-05, Learning Rate: 0.000660\n",
      "Epoch 9238/40000, Loss: 2.591983684396837e-05, Learning Rate: 0.000660\n",
      "Epoch 9239/40000, Loss: 2.4130426027113572e-05, Learning Rate: 0.000660\n",
      "Epoch 9240/40000, Loss: 8.342788351001218e-05, Learning Rate: 0.000660\n",
      "Epoch 9241/40000, Loss: 5.822363164043054e-05, Learning Rate: 0.000660\n",
      "Epoch 9242/40000, Loss: 8.017304207896814e-05, Learning Rate: 0.000660\n",
      "Epoch 9243/40000, Loss: 7.954213651828468e-05, Learning Rate: 0.000660\n",
      "Epoch 9244/40000, Loss: 6.028697680449113e-05, Learning Rate: 0.000660\n",
      "Epoch 9245/40000, Loss: 5.191534728510305e-05, Learning Rate: 0.000660\n",
      "Epoch 9246/40000, Loss: 2.3699170924373902e-05, Learning Rate: 0.000659\n",
      "Epoch 9247/40000, Loss: 6.0188365750946105e-05, Learning Rate: 0.000659\n",
      "Epoch 9248/40000, Loss: 8.597933629062027e-05, Learning Rate: 0.000659\n",
      "Epoch 9249/40000, Loss: 6.243217649171129e-05, Learning Rate: 0.000659\n",
      "Epoch 9250/40000, Loss: 2.4797831429168582e-05, Learning Rate: 0.000659\n",
      "Epoch 9251/40000, Loss: 5.0839367759181187e-05, Learning Rate: 0.000659\n",
      "Epoch 9252/40000, Loss: 4.8473841161467135e-05, Learning Rate: 0.000659\n",
      "Epoch 9253/40000, Loss: 2.751876672846265e-05, Learning Rate: 0.000659\n",
      "Epoch 9254/40000, Loss: 2.4526560082449578e-05, Learning Rate: 0.000659\n",
      "Epoch 9255/40000, Loss: 5.1429495215415955e-05, Learning Rate: 0.000659\n",
      "Epoch 9256/40000, Loss: 6.126333028078079e-05, Learning Rate: 0.000659\n",
      "Epoch 9257/40000, Loss: 5.992500518914312e-05, Learning Rate: 0.000659\n",
      "Epoch 9258/40000, Loss: 2.5719054974615574e-05, Learning Rate: 0.000658\n",
      "Epoch 9259/40000, Loss: 2.686946390895173e-05, Learning Rate: 0.000658\n",
      "Epoch 9260/40000, Loss: 5.496148514794186e-05, Learning Rate: 0.000658\n",
      "Epoch 9261/40000, Loss: 5.3542127716355026e-05, Learning Rate: 0.000658\n",
      "Epoch 9262/40000, Loss: 4.8114969104062766e-05, Learning Rate: 0.000658\n",
      "Epoch 9263/40000, Loss: 8.884451381163672e-05, Learning Rate: 0.000658\n",
      "Epoch 9264/40000, Loss: 8.722668280825019e-05, Learning Rate: 0.000658\n",
      "Epoch 9265/40000, Loss: 5.575754403253086e-05, Learning Rate: 0.000658\n",
      "Epoch 9266/40000, Loss: 9.15526325115934e-05, Learning Rate: 0.000658\n",
      "Epoch 9267/40000, Loss: 2.4046414182521403e-05, Learning Rate: 0.000658\n",
      "Epoch 9268/40000, Loss: 5.996345862513408e-05, Learning Rate: 0.000658\n",
      "Epoch 9269/40000, Loss: 9.076687274500728e-05, Learning Rate: 0.000658\n",
      "Epoch 9270/40000, Loss: 4.9740279791876674e-05, Learning Rate: 0.000658\n",
      "Epoch 9271/40000, Loss: 4.7496938350377604e-05, Learning Rate: 0.000657\n",
      "Epoch 9272/40000, Loss: 2.556596155045554e-05, Learning Rate: 0.000657\n",
      "Epoch 9273/40000, Loss: 2.419642623863183e-05, Learning Rate: 0.000657\n",
      "Epoch 9274/40000, Loss: 5.3627052693627775e-05, Learning Rate: 0.000657\n",
      "Epoch 9275/40000, Loss: 8.071128104347736e-05, Learning Rate: 0.000657\n",
      "Epoch 9276/40000, Loss: 7.977506902534515e-05, Learning Rate: 0.000657\n",
      "Epoch 9277/40000, Loss: 6.006228431942873e-05, Learning Rate: 0.000657\n",
      "Epoch 9278/40000, Loss: 5.480043910210952e-05, Learning Rate: 0.000657\n",
      "Epoch 9279/40000, Loss: 7.958625064929947e-05, Learning Rate: 0.000657\n",
      "Epoch 9280/40000, Loss: 5.2046463679289445e-05, Learning Rate: 0.000657\n",
      "Epoch 9281/40000, Loss: 8.703846833668649e-05, Learning Rate: 0.000657\n",
      "Epoch 9282/40000, Loss: 2.2741009161109105e-05, Learning Rate: 0.000657\n",
      "Epoch 9283/40000, Loss: 8.83129469002597e-05, Learning Rate: 0.000657\n",
      "Epoch 9284/40000, Loss: 2.8694192224065773e-05, Learning Rate: 0.000656\n",
      "Epoch 9285/40000, Loss: 3.0066621548030525e-05, Learning Rate: 0.000656\n",
      "Epoch 9286/40000, Loss: 2.883901834138669e-05, Learning Rate: 0.000656\n",
      "Epoch 9287/40000, Loss: 9.099452290683985e-05, Learning Rate: 0.000656\n",
      "Epoch 9288/40000, Loss: 6.220009527169168e-05, Learning Rate: 0.000656\n",
      "Epoch 9289/40000, Loss: 6.672939343843609e-05, Learning Rate: 0.000656\n",
      "Epoch 9290/40000, Loss: 9.594307630322874e-05, Learning Rate: 0.000656\n",
      "Epoch 9291/40000, Loss: 0.00028397212736308575, Learning Rate: 0.000656\n",
      "Epoch 9292/40000, Loss: 7.224064029287547e-05, Learning Rate: 0.000656\n",
      "Epoch 9293/40000, Loss: 6.692174792988226e-05, Learning Rate: 0.000656\n",
      "Epoch 9294/40000, Loss: 6.904043402755633e-05, Learning Rate: 0.000656\n",
      "Epoch 9295/40000, Loss: 7.133144390536472e-05, Learning Rate: 0.000656\n",
      "Epoch 9296/40000, Loss: 9.820986451813951e-05, Learning Rate: 0.000655\n",
      "Epoch 9297/40000, Loss: 6.634846795350313e-05, Learning Rate: 0.000655\n",
      "Epoch 9298/40000, Loss: 5.9767800848931074e-05, Learning Rate: 0.000655\n",
      "Epoch 9299/40000, Loss: 0.00010798472794704139, Learning Rate: 0.000655\n",
      "Epoch 9300/40000, Loss: 3.893091343343258e-05, Learning Rate: 0.000655\n",
      "Epoch 9301/40000, Loss: 0.0002179028670070693, Learning Rate: 0.000655\n",
      "Epoch 9302/40000, Loss: 6.41533188172616e-05, Learning Rate: 0.000655\n",
      "Epoch 9303/40000, Loss: 5.4131589422468096e-05, Learning Rate: 0.000655\n",
      "Epoch 9304/40000, Loss: 3.921096140402369e-05, Learning Rate: 0.000655\n",
      "Epoch 9305/40000, Loss: 7.777227438054979e-05, Learning Rate: 0.000655\n",
      "Epoch 9306/40000, Loss: 0.00014259757881518453, Learning Rate: 0.000655\n",
      "Epoch 9307/40000, Loss: 7.21560645615682e-05, Learning Rate: 0.000655\n",
      "Epoch 9308/40000, Loss: 7.399648893624544e-05, Learning Rate: 0.000655\n",
      "Epoch 9309/40000, Loss: 6.22962397756055e-05, Learning Rate: 0.000654\n",
      "Epoch 9310/40000, Loss: 9.268087887903675e-05, Learning Rate: 0.000654\n",
      "Epoch 9311/40000, Loss: 5.311964559950866e-05, Learning Rate: 0.000654\n",
      "Epoch 9312/40000, Loss: 5.345289173419587e-05, Learning Rate: 0.000654\n",
      "Epoch 9313/40000, Loss: 0.00010070495773106813, Learning Rate: 0.000654\n",
      "Epoch 9314/40000, Loss: 8.030897879507393e-05, Learning Rate: 0.000654\n",
      "Epoch 9315/40000, Loss: 0.00010009593825088814, Learning Rate: 0.000654\n",
      "Epoch 9316/40000, Loss: 0.00010786839266074821, Learning Rate: 0.000654\n",
      "Epoch 9317/40000, Loss: 6.249315629247576e-05, Learning Rate: 0.000654\n",
      "Epoch 9318/40000, Loss: 3.373462095623836e-05, Learning Rate: 0.000654\n",
      "Epoch 9319/40000, Loss: 5.141923975315876e-05, Learning Rate: 0.000654\n",
      "Epoch 9320/40000, Loss: 6.266070704441518e-05, Learning Rate: 0.000654\n",
      "Epoch 9321/40000, Loss: 2.735126508923713e-05, Learning Rate: 0.000654\n",
      "Epoch 9322/40000, Loss: 6.143009522929788e-05, Learning Rate: 0.000653\n",
      "Epoch 9323/40000, Loss: 4.898610495729372e-05, Learning Rate: 0.000653\n",
      "Epoch 9324/40000, Loss: 2.3528322344645858e-05, Learning Rate: 0.000653\n",
      "Epoch 9325/40000, Loss: 4.943519161315635e-05, Learning Rate: 0.000653\n",
      "Epoch 9326/40000, Loss: 5.294942093314603e-05, Learning Rate: 0.000653\n",
      "Epoch 9327/40000, Loss: 5.0486476538935676e-05, Learning Rate: 0.000653\n",
      "Epoch 9328/40000, Loss: 4.779579830938019e-05, Learning Rate: 0.000653\n",
      "Epoch 9329/40000, Loss: 7.866758824093267e-05, Learning Rate: 0.000653\n",
      "Epoch 9330/40000, Loss: 6.1795100918971e-05, Learning Rate: 0.000653\n",
      "Epoch 9331/40000, Loss: 8.789959974819794e-05, Learning Rate: 0.000653\n",
      "Epoch 9332/40000, Loss: 4.741337397717871e-05, Learning Rate: 0.000653\n",
      "Epoch 9333/40000, Loss: 6.024054528097622e-05, Learning Rate: 0.000653\n",
      "Epoch 9334/40000, Loss: 5.2213879826013e-05, Learning Rate: 0.000653\n",
      "Epoch 9335/40000, Loss: 7.765180635033175e-05, Learning Rate: 0.000652\n",
      "Epoch 9336/40000, Loss: 5.897339360672049e-05, Learning Rate: 0.000652\n",
      "Epoch 9337/40000, Loss: 8.563974552089348e-05, Learning Rate: 0.000652\n",
      "Epoch 9338/40000, Loss: 2.22173985093832e-05, Learning Rate: 0.000652\n",
      "Epoch 9339/40000, Loss: 5.871787288924679e-05, Learning Rate: 0.000652\n",
      "Epoch 9340/40000, Loss: 5.132270962349139e-05, Learning Rate: 0.000652\n",
      "Epoch 9341/40000, Loss: 5.110476558911614e-05, Learning Rate: 0.000652\n",
      "Epoch 9342/40000, Loss: 2.2515805540024303e-05, Learning Rate: 0.000652\n",
      "Epoch 9343/40000, Loss: 5.878839147044346e-05, Learning Rate: 0.000652\n",
      "Epoch 9344/40000, Loss: 2.1998139345669188e-05, Learning Rate: 0.000652\n",
      "Epoch 9345/40000, Loss: 5.122961738379672e-05, Learning Rate: 0.000652\n",
      "Epoch 9346/40000, Loss: 4.63991891592741e-05, Learning Rate: 0.000652\n",
      "Epoch 9347/40000, Loss: 5.281819903757423e-05, Learning Rate: 0.000651\n",
      "Epoch 9348/40000, Loss: 5.3163257689448074e-05, Learning Rate: 0.000651\n",
      "Epoch 9349/40000, Loss: 4.76914610771928e-05, Learning Rate: 0.000651\n",
      "Epoch 9350/40000, Loss: 2.2389087462215684e-05, Learning Rate: 0.000651\n",
      "Epoch 9351/40000, Loss: 7.764789916109294e-05, Learning Rate: 0.000651\n",
      "Epoch 9352/40000, Loss: 7.765106420265511e-05, Learning Rate: 0.000651\n",
      "Epoch 9353/40000, Loss: 5.840603989781812e-05, Learning Rate: 0.000651\n",
      "Epoch 9354/40000, Loss: 7.930239371489733e-05, Learning Rate: 0.000651\n",
      "Epoch 9355/40000, Loss: 7.767765055177733e-05, Learning Rate: 0.000651\n",
      "Epoch 9356/40000, Loss: 6.0349244449753314e-05, Learning Rate: 0.000651\n",
      "Epoch 9357/40000, Loss: 2.216879147454165e-05, Learning Rate: 0.000651\n",
      "Epoch 9358/40000, Loss: 8.404914115089923e-05, Learning Rate: 0.000651\n",
      "Epoch 9359/40000, Loss: 4.716832336271182e-05, Learning Rate: 0.000651\n",
      "Epoch 9360/40000, Loss: 5.472264456329867e-05, Learning Rate: 0.000650\n",
      "Epoch 9361/40000, Loss: 8.00572379375808e-05, Learning Rate: 0.000650\n",
      "Epoch 9362/40000, Loss: 4.830986654269509e-05, Learning Rate: 0.000650\n",
      "Epoch 9363/40000, Loss: 4.752126187668182e-05, Learning Rate: 0.000650\n",
      "Epoch 9364/40000, Loss: 8.302779315272346e-05, Learning Rate: 0.000650\n",
      "Epoch 9365/40000, Loss: 9.13515905267559e-05, Learning Rate: 0.000650\n",
      "Epoch 9366/40000, Loss: 5.624376717605628e-05, Learning Rate: 0.000650\n",
      "Epoch 9367/40000, Loss: 5.072252679383382e-05, Learning Rate: 0.000650\n",
      "Epoch 9368/40000, Loss: 5.384310134104453e-05, Learning Rate: 0.000650\n",
      "Epoch 9369/40000, Loss: 4.714728856924921e-05, Learning Rate: 0.000650\n",
      "Epoch 9370/40000, Loss: 8.11754580354318e-05, Learning Rate: 0.000650\n",
      "Epoch 9371/40000, Loss: 5.1699065807042643e-05, Learning Rate: 0.000650\n",
      "Epoch 9372/40000, Loss: 9.167980897473171e-05, Learning Rate: 0.000650\n",
      "Epoch 9373/40000, Loss: 3.489164373604581e-05, Learning Rate: 0.000649\n",
      "Epoch 9374/40000, Loss: 6.345505244098604e-05, Learning Rate: 0.000649\n",
      "Epoch 9375/40000, Loss: 5.2230709115974605e-05, Learning Rate: 0.000649\n",
      "Epoch 9376/40000, Loss: 6.892866804264486e-05, Learning Rate: 0.000649\n",
      "Epoch 9377/40000, Loss: 0.00010071953147416934, Learning Rate: 0.000649\n",
      "Epoch 9378/40000, Loss: 6.76070194458589e-05, Learning Rate: 0.000649\n",
      "Epoch 9379/40000, Loss: 5.6946817494463176e-05, Learning Rate: 0.000649\n",
      "Epoch 9380/40000, Loss: 0.00010290151840308681, Learning Rate: 0.000649\n",
      "Epoch 9381/40000, Loss: 6.883816240588203e-05, Learning Rate: 0.000649\n",
      "Epoch 9382/40000, Loss: 0.00010166793072130531, Learning Rate: 0.000649\n",
      "Epoch 9383/40000, Loss: 7.824502245057374e-05, Learning Rate: 0.000649\n",
      "Epoch 9384/40000, Loss: 9.79535179794766e-05, Learning Rate: 0.000649\n",
      "Epoch 9385/40000, Loss: 6.624564412049949e-05, Learning Rate: 0.000649\n",
      "Epoch 9386/40000, Loss: 9.344203863292933e-05, Learning Rate: 0.000648\n",
      "Epoch 9387/40000, Loss: 0.00015103627811186016, Learning Rate: 0.000648\n",
      "Epoch 9388/40000, Loss: 0.00010661444684956223, Learning Rate: 0.000648\n",
      "Epoch 9389/40000, Loss: 8.887831791071221e-05, Learning Rate: 0.000648\n",
      "Epoch 9390/40000, Loss: 8.962668653111905e-05, Learning Rate: 0.000648\n",
      "Epoch 9391/40000, Loss: 6.724266131641343e-05, Learning Rate: 0.000648\n",
      "Epoch 9392/40000, Loss: 6.722246325807646e-05, Learning Rate: 0.000648\n",
      "Epoch 9393/40000, Loss: 7.328912761295214e-05, Learning Rate: 0.000648\n",
      "Epoch 9394/40000, Loss: 5.834356488776393e-05, Learning Rate: 0.000648\n",
      "Epoch 9395/40000, Loss: 8.972323848865926e-05, Learning Rate: 0.000648\n",
      "Epoch 9396/40000, Loss: 7.009090040810406e-05, Learning Rate: 0.000648\n",
      "Epoch 9397/40000, Loss: 6.213531014509499e-05, Learning Rate: 0.000648\n",
      "Epoch 9398/40000, Loss: 3.944683339796029e-05, Learning Rate: 0.000648\n",
      "Epoch 9399/40000, Loss: 6.465408660005778e-05, Learning Rate: 0.000647\n",
      "Epoch 9400/40000, Loss: 4.4002095819450915e-05, Learning Rate: 0.000647\n",
      "Epoch 9401/40000, Loss: 8.62801680341363e-05, Learning Rate: 0.000647\n",
      "Epoch 9402/40000, Loss: 6.074375778553076e-05, Learning Rate: 0.000647\n",
      "Epoch 9403/40000, Loss: 8.541122952010483e-05, Learning Rate: 0.000647\n",
      "Epoch 9404/40000, Loss: 6.000233770464547e-05, Learning Rate: 0.000647\n",
      "Epoch 9405/40000, Loss: 2.482849777152296e-05, Learning Rate: 0.000647\n",
      "Epoch 9406/40000, Loss: 7.970022852532566e-05, Learning Rate: 0.000647\n",
      "Epoch 9407/40000, Loss: 5.399586007115431e-05, Learning Rate: 0.000647\n",
      "Epoch 9408/40000, Loss: 4.7427289246115834e-05, Learning Rate: 0.000647\n",
      "Epoch 9409/40000, Loss: 8.249277016147971e-05, Learning Rate: 0.000647\n",
      "Epoch 9410/40000, Loss: 5.3072966693434864e-05, Learning Rate: 0.000647\n",
      "Epoch 9411/40000, Loss: 5.192615572013892e-05, Learning Rate: 0.000646\n",
      "Epoch 9412/40000, Loss: 5.881407923880033e-05, Learning Rate: 0.000646\n",
      "Epoch 9413/40000, Loss: 2.2395053747459315e-05, Learning Rate: 0.000646\n",
      "Epoch 9414/40000, Loss: 5.953184518148191e-05, Learning Rate: 0.000646\n",
      "Epoch 9415/40000, Loss: 2.2436792278313078e-05, Learning Rate: 0.000646\n",
      "Epoch 9416/40000, Loss: 4.75134547741618e-05, Learning Rate: 0.000646\n",
      "Epoch 9417/40000, Loss: 5.9682377468561754e-05, Learning Rate: 0.000646\n",
      "Epoch 9418/40000, Loss: 5.8721347159007564e-05, Learning Rate: 0.000646\n",
      "Epoch 9419/40000, Loss: 4.8150559450732544e-05, Learning Rate: 0.000646\n",
      "Epoch 9420/40000, Loss: 2.552813930378761e-05, Learning Rate: 0.000646\n",
      "Epoch 9421/40000, Loss: 4.8628789954818785e-05, Learning Rate: 0.000646\n",
      "Epoch 9422/40000, Loss: 4.690518107963726e-05, Learning Rate: 0.000646\n",
      "Epoch 9423/40000, Loss: 5.1476665248628706e-05, Learning Rate: 0.000646\n",
      "Epoch 9424/40000, Loss: 5.900669930269942e-05, Learning Rate: 0.000645\n",
      "Epoch 9425/40000, Loss: 2.2020078176865354e-05, Learning Rate: 0.000645\n",
      "Epoch 9426/40000, Loss: 5.257137672742829e-05, Learning Rate: 0.000645\n",
      "Epoch 9427/40000, Loss: 8.696242730366066e-05, Learning Rate: 0.000645\n",
      "Epoch 9428/40000, Loss: 5.904264617129229e-05, Learning Rate: 0.000645\n",
      "Epoch 9429/40000, Loss: 2.294614569109399e-05, Learning Rate: 0.000645\n",
      "Epoch 9430/40000, Loss: 2.4935519832069986e-05, Learning Rate: 0.000645\n",
      "Epoch 9431/40000, Loss: 7.944350363686681e-05, Learning Rate: 0.000645\n",
      "Epoch 9432/40000, Loss: 8.82053209352307e-05, Learning Rate: 0.000645\n",
      "Epoch 9433/40000, Loss: 5.956967652309686e-05, Learning Rate: 0.000645\n",
      "Epoch 9434/40000, Loss: 8.771553257247433e-05, Learning Rate: 0.000645\n",
      "Epoch 9435/40000, Loss: 6.544021016452461e-05, Learning Rate: 0.000645\n",
      "Epoch 9436/40000, Loss: 2.4473632947774604e-05, Learning Rate: 0.000645\n",
      "Epoch 9437/40000, Loss: 8.872643229551613e-05, Learning Rate: 0.000644\n",
      "Epoch 9438/40000, Loss: 8.097655518213287e-05, Learning Rate: 0.000644\n",
      "Epoch 9439/40000, Loss: 2.3995706214918755e-05, Learning Rate: 0.000644\n",
      "Epoch 9440/40000, Loss: 5.111207428853959e-05, Learning Rate: 0.000644\n",
      "Epoch 9441/40000, Loss: 5.5357737437589094e-05, Learning Rate: 0.000644\n",
      "Epoch 9442/40000, Loss: 6.173157453304157e-05, Learning Rate: 0.000644\n",
      "Epoch 9443/40000, Loss: 2.7216072339797392e-05, Learning Rate: 0.000644\n",
      "Epoch 9444/40000, Loss: 5.8814010117203e-05, Learning Rate: 0.000644\n",
      "Epoch 9445/40000, Loss: 6.412623770302162e-05, Learning Rate: 0.000644\n",
      "Epoch 9446/40000, Loss: 6.151696288725361e-05, Learning Rate: 0.000644\n",
      "Epoch 9447/40000, Loss: 5.43071364518255e-05, Learning Rate: 0.000644\n",
      "Epoch 9448/40000, Loss: 5.976041211397387e-05, Learning Rate: 0.000644\n",
      "Epoch 9449/40000, Loss: 5.8891575463349e-05, Learning Rate: 0.000644\n",
      "Epoch 9450/40000, Loss: 6.283078255364671e-05, Learning Rate: 0.000643\n",
      "Epoch 9451/40000, Loss: 3.0501980290864594e-05, Learning Rate: 0.000643\n",
      "Epoch 9452/40000, Loss: 6.924359331605956e-05, Learning Rate: 0.000643\n",
      "Epoch 9453/40000, Loss: 7.995477790245786e-05, Learning Rate: 0.000643\n",
      "Epoch 9454/40000, Loss: 2.9634746169904247e-05, Learning Rate: 0.000643\n",
      "Epoch 9455/40000, Loss: 6.143748032627627e-05, Learning Rate: 0.000643\n",
      "Epoch 9456/40000, Loss: 4.821128095500171e-05, Learning Rate: 0.000643\n",
      "Epoch 9457/40000, Loss: 8.459738455712795e-05, Learning Rate: 0.000643\n",
      "Epoch 9458/40000, Loss: 9.009370842250064e-05, Learning Rate: 0.000643\n",
      "Epoch 9459/40000, Loss: 6.187416875036433e-05, Learning Rate: 0.000643\n",
      "Epoch 9460/40000, Loss: 8.852417522575706e-05, Learning Rate: 0.000643\n",
      "Epoch 9461/40000, Loss: 6.238073547137901e-05, Learning Rate: 0.000643\n",
      "Epoch 9462/40000, Loss: 5.9385387430666015e-05, Learning Rate: 0.000643\n",
      "Epoch 9463/40000, Loss: 2.4919067072914913e-05, Learning Rate: 0.000642\n",
      "Epoch 9464/40000, Loss: 2.5628873117966577e-05, Learning Rate: 0.000642\n",
      "Epoch 9465/40000, Loss: 8.787284605205059e-05, Learning Rate: 0.000642\n",
      "Epoch 9466/40000, Loss: 6.831713108113036e-05, Learning Rate: 0.000642\n",
      "Epoch 9467/40000, Loss: 3.032439053640701e-05, Learning Rate: 0.000642\n",
      "Epoch 9468/40000, Loss: 7.057742186589167e-05, Learning Rate: 0.000642\n",
      "Epoch 9469/40000, Loss: 3.323873897898011e-05, Learning Rate: 0.000642\n",
      "Epoch 9470/40000, Loss: 3.072626714129001e-05, Learning Rate: 0.000642\n",
      "Epoch 9471/40000, Loss: 9.69333341345191e-05, Learning Rate: 0.000642\n",
      "Epoch 9472/40000, Loss: 6.338366074487567e-05, Learning Rate: 0.000642\n",
      "Epoch 9473/40000, Loss: 0.0001000870979623869, Learning Rate: 0.000642\n",
      "Epoch 9474/40000, Loss: 9.632490400690585e-05, Learning Rate: 0.000642\n",
      "Epoch 9475/40000, Loss: 2.7961494197370484e-05, Learning Rate: 0.000642\n",
      "Epoch 9476/40000, Loss: 9.522752952761948e-05, Learning Rate: 0.000641\n",
      "Epoch 9477/40000, Loss: 9.419202251592651e-05, Learning Rate: 0.000641\n",
      "Epoch 9478/40000, Loss: 6.351785850711167e-05, Learning Rate: 0.000641\n",
      "Epoch 9479/40000, Loss: 9.791531192604452e-05, Learning Rate: 0.000641\n",
      "Epoch 9480/40000, Loss: 6.372711504809558e-05, Learning Rate: 0.000641\n",
      "Epoch 9481/40000, Loss: 0.00010522836964810267, Learning Rate: 0.000641\n",
      "Epoch 9482/40000, Loss: 6.789246981497854e-05, Learning Rate: 0.000641\n",
      "Epoch 9483/40000, Loss: 5.731416604248807e-05, Learning Rate: 0.000641\n",
      "Epoch 9484/40000, Loss: 6.92200701450929e-05, Learning Rate: 0.000641\n",
      "Epoch 9485/40000, Loss: 0.00011256226571276784, Learning Rate: 0.000641\n",
      "Epoch 9486/40000, Loss: 6.556467269547284e-05, Learning Rate: 0.000641\n",
      "Epoch 9487/40000, Loss: 0.0001752894459059462, Learning Rate: 0.000641\n",
      "Epoch 9488/40000, Loss: 8.19410415715538e-05, Learning Rate: 0.000641\n",
      "Epoch 9489/40000, Loss: 0.00014428645954467356, Learning Rate: 0.000640\n",
      "Epoch 9490/40000, Loss: 0.0002208072692155838, Learning Rate: 0.000640\n",
      "Epoch 9491/40000, Loss: 8.811895531835034e-05, Learning Rate: 0.000640\n",
      "Epoch 9492/40000, Loss: 7.648710015928373e-05, Learning Rate: 0.000640\n",
      "Epoch 9493/40000, Loss: 0.00011176887346664444, Learning Rate: 0.000640\n",
      "Epoch 9494/40000, Loss: 9.213628072757274e-05, Learning Rate: 0.000640\n",
      "Epoch 9495/40000, Loss: 0.00012229513959027827, Learning Rate: 0.000640\n",
      "Epoch 9496/40000, Loss: 0.00010889563418459147, Learning Rate: 0.000640\n",
      "Epoch 9497/40000, Loss: 6.722563557559624e-05, Learning Rate: 0.000640\n",
      "Epoch 9498/40000, Loss: 3.832971924566664e-05, Learning Rate: 0.000640\n",
      "Epoch 9499/40000, Loss: 0.00010351809760322794, Learning Rate: 0.000640\n",
      "Epoch 9500/40000, Loss: 5.094724474474788e-05, Learning Rate: 0.000640\n",
      "Epoch 9501/40000, Loss: 9.904849866870791e-05, Learning Rate: 0.000640\n",
      "Epoch 9502/40000, Loss: 9.854620293481275e-05, Learning Rate: 0.000639\n",
      "Epoch 9503/40000, Loss: 8.519492985215038e-05, Learning Rate: 0.000639\n",
      "Epoch 9504/40000, Loss: 4.877047103946097e-05, Learning Rate: 0.000639\n",
      "Epoch 9505/40000, Loss: 9.267565474146977e-05, Learning Rate: 0.000639\n",
      "Epoch 9506/40000, Loss: 9.217245678883046e-05, Learning Rate: 0.000639\n",
      "Epoch 9507/40000, Loss: 6.027557537890971e-05, Learning Rate: 0.000639\n",
      "Epoch 9508/40000, Loss: 5.186546331970021e-05, Learning Rate: 0.000639\n",
      "Epoch 9509/40000, Loss: 4.686961983679794e-05, Learning Rate: 0.000639\n",
      "Epoch 9510/40000, Loss: 2.2786050976719707e-05, Learning Rate: 0.000639\n",
      "Epoch 9511/40000, Loss: 5.383207098930143e-05, Learning Rate: 0.000639\n",
      "Epoch 9512/40000, Loss: 5.3897172620054334e-05, Learning Rate: 0.000639\n",
      "Epoch 9513/40000, Loss: 2.284812399011571e-05, Learning Rate: 0.000639\n",
      "Epoch 9514/40000, Loss: 7.775404083076864e-05, Learning Rate: 0.000639\n",
      "Epoch 9515/40000, Loss: 8.69503419380635e-05, Learning Rate: 0.000638\n",
      "Epoch 9516/40000, Loss: 4.627496673492715e-05, Learning Rate: 0.000638\n",
      "Epoch 9517/40000, Loss: 8.17692416603677e-05, Learning Rate: 0.000638\n",
      "Epoch 9518/40000, Loss: 5.904415957047604e-05, Learning Rate: 0.000638\n",
      "Epoch 9519/40000, Loss: 2.2809548681834713e-05, Learning Rate: 0.000638\n",
      "Epoch 9520/40000, Loss: 7.90864069131203e-05, Learning Rate: 0.000638\n",
      "Epoch 9521/40000, Loss: 2.6233396056341007e-05, Learning Rate: 0.000638\n",
      "Epoch 9522/40000, Loss: 5.654602136928588e-05, Learning Rate: 0.000638\n",
      "Epoch 9523/40000, Loss: 8.714922296348959e-05, Learning Rate: 0.000638\n",
      "Epoch 9524/40000, Loss: 8.717972377780825e-05, Learning Rate: 0.000638\n",
      "Epoch 9525/40000, Loss: 5.9001085901400074e-05, Learning Rate: 0.000638\n",
      "Epoch 9526/40000, Loss: 5.933622014708817e-05, Learning Rate: 0.000638\n",
      "Epoch 9527/40000, Loss: 6.337974627967924e-05, Learning Rate: 0.000638\n",
      "Epoch 9528/40000, Loss: 6.208493141457438e-05, Learning Rate: 0.000637\n",
      "Epoch 9529/40000, Loss: 5.60229345865082e-05, Learning Rate: 0.000637\n",
      "Epoch 9530/40000, Loss: 8.42119989101775e-05, Learning Rate: 0.000637\n",
      "Epoch 9531/40000, Loss: 6.296050560194999e-05, Learning Rate: 0.000637\n",
      "Epoch 9532/40000, Loss: 5.4165553592611104e-05, Learning Rate: 0.000637\n",
      "Epoch 9533/40000, Loss: 5.293253343552351e-05, Learning Rate: 0.000637\n",
      "Epoch 9534/40000, Loss: 2.3972606868483126e-05, Learning Rate: 0.000637\n",
      "Epoch 9535/40000, Loss: 5.5082175094867125e-05, Learning Rate: 0.000637\n",
      "Epoch 9536/40000, Loss: 8.212298416765407e-05, Learning Rate: 0.000637\n",
      "Epoch 9537/40000, Loss: 2.5282024580519646e-05, Learning Rate: 0.000637\n",
      "Epoch 9538/40000, Loss: 6.077583384467289e-05, Learning Rate: 0.000637\n",
      "Epoch 9539/40000, Loss: 8.048605377553031e-05, Learning Rate: 0.000637\n",
      "Epoch 9540/40000, Loss: 5.256557051325217e-05, Learning Rate: 0.000637\n",
      "Epoch 9541/40000, Loss: 8.556836110074073e-05, Learning Rate: 0.000636\n",
      "Epoch 9542/40000, Loss: 6.059365477995016e-05, Learning Rate: 0.000636\n",
      "Epoch 9543/40000, Loss: 7.981766975717619e-05, Learning Rate: 0.000636\n",
      "Epoch 9544/40000, Loss: 5.417841748567298e-05, Learning Rate: 0.000636\n",
      "Epoch 9545/40000, Loss: 8.616104605607688e-05, Learning Rate: 0.000636\n",
      "Epoch 9546/40000, Loss: 5.157958366908133e-05, Learning Rate: 0.000636\n",
      "Epoch 9547/40000, Loss: 5.894638889003545e-05, Learning Rate: 0.000636\n",
      "Epoch 9548/40000, Loss: 5.481189873535186e-05, Learning Rate: 0.000636\n",
      "Epoch 9549/40000, Loss: 8.050503674894571e-05, Learning Rate: 0.000636\n",
      "Epoch 9550/40000, Loss: 8.639386942377314e-05, Learning Rate: 0.000636\n",
      "Epoch 9551/40000, Loss: 5.166801929590292e-05, Learning Rate: 0.000636\n",
      "Epoch 9552/40000, Loss: 4.5846398279536515e-05, Learning Rate: 0.000636\n",
      "Epoch 9553/40000, Loss: 8.84405235410668e-05, Learning Rate: 0.000636\n",
      "Epoch 9554/40000, Loss: 8.732122660148889e-05, Learning Rate: 0.000635\n",
      "Epoch 9555/40000, Loss: 5.9585479903034866e-05, Learning Rate: 0.000635\n",
      "Epoch 9556/40000, Loss: 7.773606921546161e-05, Learning Rate: 0.000635\n",
      "Epoch 9557/40000, Loss: 6.327102892100811e-05, Learning Rate: 0.000635\n",
      "Epoch 9558/40000, Loss: 5.1733186410274357e-05, Learning Rate: 0.000635\n",
      "Epoch 9559/40000, Loss: 5.120853893458843e-05, Learning Rate: 0.000635\n",
      "Epoch 9560/40000, Loss: 5.913816130487248e-05, Learning Rate: 0.000635\n",
      "Epoch 9561/40000, Loss: 4.5744236558675766e-05, Learning Rate: 0.000635\n",
      "Epoch 9562/40000, Loss: 8.427968714386225e-05, Learning Rate: 0.000635\n",
      "Epoch 9563/40000, Loss: 6.024944377713837e-05, Learning Rate: 0.000635\n",
      "Epoch 9564/40000, Loss: 4.6012017264729366e-05, Learning Rate: 0.000635\n",
      "Epoch 9565/40000, Loss: 2.333419251954183e-05, Learning Rate: 0.000635\n",
      "Epoch 9566/40000, Loss: 6.204746023286134e-05, Learning Rate: 0.000635\n",
      "Epoch 9567/40000, Loss: 2.7665919333230704e-05, Learning Rate: 0.000635\n",
      "Epoch 9568/40000, Loss: 2.4940491130109876e-05, Learning Rate: 0.000634\n",
      "Epoch 9569/40000, Loss: 8.712623093742877e-05, Learning Rate: 0.000634\n",
      "Epoch 9570/40000, Loss: 5.22181398991961e-05, Learning Rate: 0.000634\n",
      "Epoch 9571/40000, Loss: 7.858471508370712e-05, Learning Rate: 0.000634\n",
      "Epoch 9572/40000, Loss: 5.5845801398390904e-05, Learning Rate: 0.000634\n",
      "Epoch 9573/40000, Loss: 9.504421905148774e-05, Learning Rate: 0.000634\n",
      "Epoch 9574/40000, Loss: 9.226690599462017e-05, Learning Rate: 0.000634\n",
      "Epoch 9575/40000, Loss: 6.260343798203394e-05, Learning Rate: 0.000634\n",
      "Epoch 9576/40000, Loss: 2.5444071070523933e-05, Learning Rate: 0.000634\n",
      "Epoch 9577/40000, Loss: 4.810910832020454e-05, Learning Rate: 0.000634\n",
      "Epoch 9578/40000, Loss: 6.000000939820893e-05, Learning Rate: 0.000634\n",
      "Epoch 9579/40000, Loss: 5.8609304687706754e-05, Learning Rate: 0.000634\n",
      "Epoch 9580/40000, Loss: 4.754625479108654e-05, Learning Rate: 0.000634\n",
      "Epoch 9581/40000, Loss: 8.872500620782375e-05, Learning Rate: 0.000633\n",
      "Epoch 9582/40000, Loss: 5.8688536228146404e-05, Learning Rate: 0.000633\n",
      "Epoch 9583/40000, Loss: 5.1742295909207314e-05, Learning Rate: 0.000633\n",
      "Epoch 9584/40000, Loss: 5.979590423521586e-05, Learning Rate: 0.000633\n",
      "Epoch 9585/40000, Loss: 5.181009692023508e-05, Learning Rate: 0.000633\n",
      "Epoch 9586/40000, Loss: 2.4059374482021667e-05, Learning Rate: 0.000633\n",
      "Epoch 9587/40000, Loss: 8.569759665988386e-05, Learning Rate: 0.000633\n",
      "Epoch 9588/40000, Loss: 8.341202919837087e-05, Learning Rate: 0.000633\n",
      "Epoch 9589/40000, Loss: 2.3184633391792886e-05, Learning Rate: 0.000633\n",
      "Epoch 9590/40000, Loss: 8.096775127341971e-05, Learning Rate: 0.000633\n",
      "Epoch 9591/40000, Loss: 9.041131852427498e-05, Learning Rate: 0.000633\n",
      "Epoch 9592/40000, Loss: 8.713827992323786e-05, Learning Rate: 0.000633\n",
      "Epoch 9593/40000, Loss: 2.4683511583134532e-05, Learning Rate: 0.000633\n",
      "Epoch 9594/40000, Loss: 4.751262895297259e-05, Learning Rate: 0.000632\n",
      "Epoch 9595/40000, Loss: 2.59752559941262e-05, Learning Rate: 0.000632\n",
      "Epoch 9596/40000, Loss: 9.199074702337384e-05, Learning Rate: 0.000632\n",
      "Epoch 9597/40000, Loss: 8.619907021056861e-05, Learning Rate: 0.000632\n",
      "Epoch 9598/40000, Loss: 9.005895117297769e-05, Learning Rate: 0.000632\n",
      "Epoch 9599/40000, Loss: 8.443769183941185e-05, Learning Rate: 0.000632\n",
      "Epoch 9600/40000, Loss: 2.6368274120613933e-05, Learning Rate: 0.000632\n",
      "Epoch 9601/40000, Loss: 6.548118108185008e-05, Learning Rate: 0.000632\n",
      "Epoch 9602/40000, Loss: 2.3690108719165437e-05, Learning Rate: 0.000632\n",
      "Epoch 9603/40000, Loss: 2.4195542209781706e-05, Learning Rate: 0.000632\n",
      "Epoch 9604/40000, Loss: 6.761314580217004e-05, Learning Rate: 0.000632\n",
      "Epoch 9605/40000, Loss: 2.3856864572735503e-05, Learning Rate: 0.000632\n",
      "Epoch 9606/40000, Loss: 4.7967390855774283e-05, Learning Rate: 0.000632\n",
      "Epoch 9607/40000, Loss: 7.83525247243233e-05, Learning Rate: 0.000631\n",
      "Epoch 9608/40000, Loss: 8.936513040680438e-05, Learning Rate: 0.000631\n",
      "Epoch 9609/40000, Loss: 6.28654524916783e-05, Learning Rate: 0.000631\n",
      "Epoch 9610/40000, Loss: 4.741168231703341e-05, Learning Rate: 0.000631\n",
      "Epoch 9611/40000, Loss: 7.241222920129076e-05, Learning Rate: 0.000631\n",
      "Epoch 9612/40000, Loss: 5.242975748842582e-05, Learning Rate: 0.000631\n",
      "Epoch 9613/40000, Loss: 0.0001316692359978333, Learning Rate: 0.000631\n",
      "Epoch 9614/40000, Loss: 9.142235649051145e-05, Learning Rate: 0.000631\n",
      "Epoch 9615/40000, Loss: 8.623418398201466e-05, Learning Rate: 0.000631\n",
      "Epoch 9616/40000, Loss: 3.023515637323726e-05, Learning Rate: 0.000631\n",
      "Epoch 9617/40000, Loss: 5.8291559980716556e-05, Learning Rate: 0.000631\n",
      "Epoch 9618/40000, Loss: 6.65547777316533e-05, Learning Rate: 0.000631\n",
      "Epoch 9619/40000, Loss: 7.27301012375392e-05, Learning Rate: 0.000631\n",
      "Epoch 9620/40000, Loss: 0.00010693688818719238, Learning Rate: 0.000630\n",
      "Epoch 9621/40000, Loss: 6.0293790738796815e-05, Learning Rate: 0.000630\n",
      "Epoch 9622/40000, Loss: 6.849606870673597e-05, Learning Rate: 0.000630\n",
      "Epoch 9623/40000, Loss: 6.252083403524011e-05, Learning Rate: 0.000630\n",
      "Epoch 9624/40000, Loss: 6.421318539651111e-05, Learning Rate: 0.000630\n",
      "Epoch 9625/40000, Loss: 3.217020639567636e-05, Learning Rate: 0.000630\n",
      "Epoch 9626/40000, Loss: 7.448728138115257e-05, Learning Rate: 0.000630\n",
      "Epoch 9627/40000, Loss: 0.00010731867223512381, Learning Rate: 0.000630\n",
      "Epoch 9628/40000, Loss: 9.534400305710733e-05, Learning Rate: 0.000630\n",
      "Epoch 9629/40000, Loss: 0.00010592074249871075, Learning Rate: 0.000630\n",
      "Epoch 9630/40000, Loss: 3.150092015857808e-05, Learning Rate: 0.000630\n",
      "Epoch 9631/40000, Loss: 0.0001243325968971476, Learning Rate: 0.000630\n",
      "Epoch 9632/40000, Loss: 0.00010021394700743258, Learning Rate: 0.000630\n",
      "Epoch 9633/40000, Loss: 3.174472294631414e-05, Learning Rate: 0.000630\n",
      "Epoch 9634/40000, Loss: 6.729253072990105e-05, Learning Rate: 0.000629\n",
      "Epoch 9635/40000, Loss: 6.430641951737925e-05, Learning Rate: 0.000629\n",
      "Epoch 9636/40000, Loss: 5.180813968763687e-05, Learning Rate: 0.000629\n",
      "Epoch 9637/40000, Loss: 9.286896965932101e-05, Learning Rate: 0.000629\n",
      "Epoch 9638/40000, Loss: 6.177321483846754e-05, Learning Rate: 0.000629\n",
      "Epoch 9639/40000, Loss: 0.00010539284266997129, Learning Rate: 0.000629\n",
      "Epoch 9640/40000, Loss: 8.718895696802065e-05, Learning Rate: 0.000629\n",
      "Epoch 9641/40000, Loss: 5.337012044037692e-05, Learning Rate: 0.000629\n",
      "Epoch 9642/40000, Loss: 7.335266855079681e-05, Learning Rate: 0.000629\n",
      "Epoch 9643/40000, Loss: 6.224281969480217e-05, Learning Rate: 0.000629\n",
      "Epoch 9644/40000, Loss: 6.791658233851194e-05, Learning Rate: 0.000629\n",
      "Epoch 9645/40000, Loss: 9.672196756582707e-05, Learning Rate: 0.000629\n",
      "Epoch 9646/40000, Loss: 2.5768567866180092e-05, Learning Rate: 0.000629\n",
      "Epoch 9647/40000, Loss: 3.291194298071787e-05, Learning Rate: 0.000628\n",
      "Epoch 9648/40000, Loss: 8.607681229477748e-05, Learning Rate: 0.000628\n",
      "Epoch 9649/40000, Loss: 9.89120890153572e-05, Learning Rate: 0.000628\n",
      "Epoch 9650/40000, Loss: 6.948134250706062e-05, Learning Rate: 0.000628\n",
      "Epoch 9651/40000, Loss: 8.482588600600138e-05, Learning Rate: 0.000628\n",
      "Epoch 9652/40000, Loss: 7.310927321668714e-05, Learning Rate: 0.000628\n",
      "Epoch 9653/40000, Loss: 0.00012019404675811529, Learning Rate: 0.000628\n",
      "Epoch 9654/40000, Loss: 0.00010722123988671228, Learning Rate: 0.000628\n",
      "Epoch 9655/40000, Loss: 3.690319863380864e-05, Learning Rate: 0.000628\n",
      "Epoch 9656/40000, Loss: 7.662506686756387e-05, Learning Rate: 0.000628\n",
      "Epoch 9657/40000, Loss: 6.585381925106049e-05, Learning Rate: 0.000628\n",
      "Epoch 9658/40000, Loss: 6.542589835589752e-05, Learning Rate: 0.000628\n",
      "Epoch 9659/40000, Loss: 4.816204818780534e-05, Learning Rate: 0.000628\n",
      "Epoch 9660/40000, Loss: 8.969142800197005e-05, Learning Rate: 0.000627\n",
      "Epoch 9661/40000, Loss: 6.361563282553107e-05, Learning Rate: 0.000627\n",
      "Epoch 9662/40000, Loss: 7.366847421508282e-05, Learning Rate: 0.000627\n",
      "Epoch 9663/40000, Loss: 6.989585381234065e-05, Learning Rate: 0.000627\n",
      "Epoch 9664/40000, Loss: 0.00014970605843700469, Learning Rate: 0.000627\n",
      "Epoch 9665/40000, Loss: 5.737549508921802e-05, Learning Rate: 0.000627\n",
      "Epoch 9666/40000, Loss: 7.049137639114633e-05, Learning Rate: 0.000627\n",
      "Epoch 9667/40000, Loss: 6.357310485327616e-05, Learning Rate: 0.000627\n",
      "Epoch 9668/40000, Loss: 6.890977238072082e-05, Learning Rate: 0.000627\n",
      "Epoch 9669/40000, Loss: 3.1772164220456034e-05, Learning Rate: 0.000627\n",
      "Epoch 9670/40000, Loss: 9.78010066319257e-05, Learning Rate: 0.000627\n",
      "Epoch 9671/40000, Loss: 7.114498293958604e-05, Learning Rate: 0.000627\n",
      "Epoch 9672/40000, Loss: 3.0105784389888868e-05, Learning Rate: 0.000627\n",
      "Epoch 9673/40000, Loss: 0.0001046997494995594, Learning Rate: 0.000626\n",
      "Epoch 9674/40000, Loss: 7.522689702454954e-05, Learning Rate: 0.000626\n",
      "Epoch 9675/40000, Loss: 6.891074735904112e-05, Learning Rate: 0.000626\n",
      "Epoch 9676/40000, Loss: 0.00010011975246015936, Learning Rate: 0.000626\n",
      "Epoch 9677/40000, Loss: 8.869313023751602e-05, Learning Rate: 0.000626\n",
      "Epoch 9678/40000, Loss: 0.00017913573537953198, Learning Rate: 0.000626\n",
      "Epoch 9679/40000, Loss: 6.368668982759118e-05, Learning Rate: 0.000626\n",
      "Epoch 9680/40000, Loss: 0.00020395069441292435, Learning Rate: 0.000626\n",
      "Epoch 9681/40000, Loss: 0.00011722439376171678, Learning Rate: 0.000626\n",
      "Epoch 9682/40000, Loss: 5.419727312983014e-05, Learning Rate: 0.000626\n",
      "Epoch 9683/40000, Loss: 7.140631350921467e-05, Learning Rate: 0.000626\n",
      "Epoch 9684/40000, Loss: 9.915475675370544e-05, Learning Rate: 0.000626\n",
      "Epoch 9685/40000, Loss: 7.090849248925224e-05, Learning Rate: 0.000626\n",
      "Epoch 9686/40000, Loss: 9.859391138888896e-05, Learning Rate: 0.000626\n",
      "Epoch 9687/40000, Loss: 7.173883204814047e-05, Learning Rate: 0.000625\n",
      "Epoch 9688/40000, Loss: 6.19093916611746e-05, Learning Rate: 0.000625\n",
      "Epoch 9689/40000, Loss: 3.898017530445941e-05, Learning Rate: 0.000625\n",
      "Epoch 9690/40000, Loss: 3.273480979260057e-05, Learning Rate: 0.000625\n",
      "Epoch 9691/40000, Loss: 6.493698310805485e-05, Learning Rate: 0.000625\n",
      "Epoch 9692/40000, Loss: 5.036607035435736e-05, Learning Rate: 0.000625\n",
      "Epoch 9693/40000, Loss: 7.085393008310348e-05, Learning Rate: 0.000625\n",
      "Epoch 9694/40000, Loss: 9.248573041986674e-05, Learning Rate: 0.000625\n",
      "Epoch 9695/40000, Loss: 3.7795187381561846e-05, Learning Rate: 0.000625\n",
      "Epoch 9696/40000, Loss: 5.6227523600682616e-05, Learning Rate: 0.000625\n",
      "Epoch 9697/40000, Loss: 8.840417285682634e-05, Learning Rate: 0.000625\n",
      "Epoch 9698/40000, Loss: 6.190717977005988e-05, Learning Rate: 0.000625\n",
      "Epoch 9699/40000, Loss: 0.00010842381743714213, Learning Rate: 0.000625\n",
      "Epoch 9700/40000, Loss: 9.644289821153507e-05, Learning Rate: 0.000624\n",
      "Epoch 9701/40000, Loss: 9.371437772642821e-05, Learning Rate: 0.000624\n",
      "Epoch 9702/40000, Loss: 5.0037830078508705e-05, Learning Rate: 0.000624\n",
      "Epoch 9703/40000, Loss: 2.660683821886778e-05, Learning Rate: 0.000624\n",
      "Epoch 9704/40000, Loss: 9.276914352085441e-05, Learning Rate: 0.000624\n",
      "Epoch 9705/40000, Loss: 5.9629539464367554e-05, Learning Rate: 0.000624\n",
      "Epoch 9706/40000, Loss: 5.8475532568991184e-05, Learning Rate: 0.000624\n",
      "Epoch 9707/40000, Loss: 2.7714126190403476e-05, Learning Rate: 0.000624\n",
      "Epoch 9708/40000, Loss: 4.805103890248574e-05, Learning Rate: 0.000624\n",
      "Epoch 9709/40000, Loss: 8.578447159379721e-05, Learning Rate: 0.000624\n",
      "Epoch 9710/40000, Loss: 5.701253394363448e-05, Learning Rate: 0.000624\n",
      "Epoch 9711/40000, Loss: 4.670565249398351e-05, Learning Rate: 0.000624\n",
      "Epoch 9712/40000, Loss: 7.73644496803172e-05, Learning Rate: 0.000624\n",
      "Epoch 9713/40000, Loss: 5.883616904611699e-05, Learning Rate: 0.000623\n",
      "Epoch 9714/40000, Loss: 5.149027856532484e-05, Learning Rate: 0.000623\n",
      "Epoch 9715/40000, Loss: 5.109560879645869e-05, Learning Rate: 0.000623\n",
      "Epoch 9716/40000, Loss: 2.2664356947643682e-05, Learning Rate: 0.000623\n",
      "Epoch 9717/40000, Loss: 8.562971925130114e-05, Learning Rate: 0.000623\n",
      "Epoch 9718/40000, Loss: 2.5454159185755998e-05, Learning Rate: 0.000623\n",
      "Epoch 9719/40000, Loss: 8.573377999709919e-05, Learning Rate: 0.000623\n",
      "Epoch 9720/40000, Loss: 7.875380106270313e-05, Learning Rate: 0.000623\n",
      "Epoch 9721/40000, Loss: 7.815739081706852e-05, Learning Rate: 0.000623\n",
      "Epoch 9722/40000, Loss: 6.379871774697676e-05, Learning Rate: 0.000623\n",
      "Epoch 9723/40000, Loss: 5.484904977492988e-05, Learning Rate: 0.000623\n",
      "Epoch 9724/40000, Loss: 6.020660657668486e-05, Learning Rate: 0.000623\n",
      "Epoch 9725/40000, Loss: 8.575626998208463e-05, Learning Rate: 0.000623\n",
      "Epoch 9726/40000, Loss: 6.566450610989705e-05, Learning Rate: 0.000623\n",
      "Epoch 9727/40000, Loss: 5.6077755289152265e-05, Learning Rate: 0.000622\n",
      "Epoch 9728/40000, Loss: 0.00010700405255192891, Learning Rate: 0.000622\n",
      "Epoch 9729/40000, Loss: 6.823069998063147e-05, Learning Rate: 0.000622\n",
      "Epoch 9730/40000, Loss: 2.8533366275951266e-05, Learning Rate: 0.000622\n",
      "Epoch 9731/40000, Loss: 5.923713979427703e-05, Learning Rate: 0.000622\n",
      "Epoch 9732/40000, Loss: 6.424699677154422e-05, Learning Rate: 0.000622\n",
      "Epoch 9733/40000, Loss: 3.184748493367806e-05, Learning Rate: 0.000622\n",
      "Epoch 9734/40000, Loss: 8.797504415269941e-05, Learning Rate: 0.000622\n",
      "Epoch 9735/40000, Loss: 9.381285781273618e-05, Learning Rate: 0.000622\n",
      "Epoch 9736/40000, Loss: 5.200001032790169e-05, Learning Rate: 0.000622\n",
      "Epoch 9737/40000, Loss: 6.347912130877376e-05, Learning Rate: 0.000622\n",
      "Epoch 9738/40000, Loss: 5.679177775164135e-05, Learning Rate: 0.000622\n",
      "Epoch 9739/40000, Loss: 2.3171542125055566e-05, Learning Rate: 0.000622\n",
      "Epoch 9740/40000, Loss: 6.0499765822896734e-05, Learning Rate: 0.000621\n",
      "Epoch 9741/40000, Loss: 5.889658132218756e-05, Learning Rate: 0.000621\n",
      "Epoch 9742/40000, Loss: 2.3712142137810588e-05, Learning Rate: 0.000621\n",
      "Epoch 9743/40000, Loss: 2.3530434191343375e-05, Learning Rate: 0.000621\n",
      "Epoch 9744/40000, Loss: 2.1878033294342458e-05, Learning Rate: 0.000621\n",
      "Epoch 9745/40000, Loss: 5.855289055034518e-05, Learning Rate: 0.000621\n",
      "Epoch 9746/40000, Loss: 2.2754691599402577e-05, Learning Rate: 0.000621\n",
      "Epoch 9747/40000, Loss: 7.75699081714265e-05, Learning Rate: 0.000621\n",
      "Epoch 9748/40000, Loss: 8.993196388473734e-05, Learning Rate: 0.000621\n",
      "Epoch 9749/40000, Loss: 2.3836439140723087e-05, Learning Rate: 0.000621\n",
      "Epoch 9750/40000, Loss: 5.941418930888176e-05, Learning Rate: 0.000621\n",
      "Epoch 9751/40000, Loss: 6.0009762819390744e-05, Learning Rate: 0.000621\n",
      "Epoch 9752/40000, Loss: 7.674928929191083e-05, Learning Rate: 0.000621\n",
      "Epoch 9753/40000, Loss: 7.559660298284143e-05, Learning Rate: 0.000621\n",
      "Epoch 9754/40000, Loss: 8.766412793193012e-05, Learning Rate: 0.000620\n",
      "Epoch 9755/40000, Loss: 2.1915418983553536e-05, Learning Rate: 0.000620\n",
      "Epoch 9756/40000, Loss: 7.817940058885142e-05, Learning Rate: 0.000620\n",
      "Epoch 9757/40000, Loss: 7.589528831886128e-05, Learning Rate: 0.000620\n",
      "Epoch 9758/40000, Loss: 5.1312599680386484e-05, Learning Rate: 0.000620\n",
      "Epoch 9759/40000, Loss: 2.210522143286653e-05, Learning Rate: 0.000620\n",
      "Epoch 9760/40000, Loss: 2.2038893803255633e-05, Learning Rate: 0.000620\n",
      "Epoch 9761/40000, Loss: 5.042592965764925e-05, Learning Rate: 0.000620\n",
      "Epoch 9762/40000, Loss: 2.1225805539870635e-05, Learning Rate: 0.000620\n",
      "Epoch 9763/40000, Loss: 5.7018820371013135e-05, Learning Rate: 0.000620\n",
      "Epoch 9764/40000, Loss: 8.399245416512713e-05, Learning Rate: 0.000620\n",
      "Epoch 9765/40000, Loss: 5.709170363843441e-05, Learning Rate: 0.000620\n",
      "Epoch 9766/40000, Loss: 4.94407067890279e-05, Learning Rate: 0.000620\n",
      "Epoch 9767/40000, Loss: 5.809072536067106e-05, Learning Rate: 0.000619\n",
      "Epoch 9768/40000, Loss: 2.1170299078221433e-05, Learning Rate: 0.000619\n",
      "Epoch 9769/40000, Loss: 5.771109135821462e-05, Learning Rate: 0.000619\n",
      "Epoch 9770/40000, Loss: 4.504121534409933e-05, Learning Rate: 0.000619\n",
      "Epoch 9771/40000, Loss: 4.4802884076489136e-05, Learning Rate: 0.000619\n",
      "Epoch 9772/40000, Loss: 2.086003587464802e-05, Learning Rate: 0.000619\n",
      "Epoch 9773/40000, Loss: 2.1299470972735435e-05, Learning Rate: 0.000619\n",
      "Epoch 9774/40000, Loss: 2.1033532902947627e-05, Learning Rate: 0.000619\n",
      "Epoch 9775/40000, Loss: 5.8045297919306904e-05, Learning Rate: 0.000619\n",
      "Epoch 9776/40000, Loss: 4.5068813051329926e-05, Learning Rate: 0.000619\n",
      "Epoch 9777/40000, Loss: 6.064200715627521e-05, Learning Rate: 0.000619\n",
      "Epoch 9778/40000, Loss: 2.1418793039629236e-05, Learning Rate: 0.000619\n",
      "Epoch 9779/40000, Loss: 2.0950934413122013e-05, Learning Rate: 0.000619\n",
      "Epoch 9780/40000, Loss: 4.488438935368322e-05, Learning Rate: 0.000618\n",
      "Epoch 9781/40000, Loss: 4.476903995964676e-05, Learning Rate: 0.000618\n",
      "Epoch 9782/40000, Loss: 8.462595724267885e-05, Learning Rate: 0.000618\n",
      "Epoch 9783/40000, Loss: 5.786167093901895e-05, Learning Rate: 0.000618\n",
      "Epoch 9784/40000, Loss: 5.0309787184232846e-05, Learning Rate: 0.000618\n",
      "Epoch 9785/40000, Loss: 7.751565863145515e-05, Learning Rate: 0.000618\n",
      "Epoch 9786/40000, Loss: 5.473992132465355e-05, Learning Rate: 0.000618\n",
      "Epoch 9787/40000, Loss: 2.8453816412365995e-05, Learning Rate: 0.000618\n",
      "Epoch 9788/40000, Loss: 6.219182250788435e-05, Learning Rate: 0.000618\n",
      "Epoch 9789/40000, Loss: 4.921259460388683e-05, Learning Rate: 0.000618\n",
      "Epoch 9790/40000, Loss: 5.963369403616525e-05, Learning Rate: 0.000618\n",
      "Epoch 9791/40000, Loss: 4.873138095717877e-05, Learning Rate: 0.000618\n",
      "Epoch 9792/40000, Loss: 8.503178833052516e-05, Learning Rate: 0.000618\n",
      "Epoch 9793/40000, Loss: 7.870481204008684e-05, Learning Rate: 0.000618\n",
      "Epoch 9794/40000, Loss: 4.94758460263256e-05, Learning Rate: 0.000617\n",
      "Epoch 9795/40000, Loss: 8.626526687294245e-05, Learning Rate: 0.000617\n",
      "Epoch 9796/40000, Loss: 8.157570846378803e-05, Learning Rate: 0.000617\n",
      "Epoch 9797/40000, Loss: 6.699768709950149e-05, Learning Rate: 0.000617\n",
      "Epoch 9798/40000, Loss: 5.447135481517762e-05, Learning Rate: 0.000617\n",
      "Epoch 9799/40000, Loss: 5.924459037487395e-05, Learning Rate: 0.000617\n",
      "Epoch 9800/40000, Loss: 2.9725344575126655e-05, Learning Rate: 0.000617\n",
      "Epoch 9801/40000, Loss: 5.357568807085045e-05, Learning Rate: 0.000617\n",
      "Epoch 9802/40000, Loss: 9.625689563108608e-05, Learning Rate: 0.000617\n",
      "Epoch 9803/40000, Loss: 9.584603685652837e-05, Learning Rate: 0.000617\n",
      "Epoch 9804/40000, Loss: 6.704662519041449e-05, Learning Rate: 0.000617\n",
      "Epoch 9805/40000, Loss: 5.000757664674893e-05, Learning Rate: 0.000617\n",
      "Epoch 9806/40000, Loss: 9.714096086099744e-05, Learning Rate: 0.000617\n",
      "Epoch 9807/40000, Loss: 6.886202754685655e-05, Learning Rate: 0.000616\n",
      "Epoch 9808/40000, Loss: 5.1262810302432626e-05, Learning Rate: 0.000616\n",
      "Epoch 9809/40000, Loss: 9.439382120035589e-05, Learning Rate: 0.000616\n",
      "Epoch 9810/40000, Loss: 6.152229616418481e-05, Learning Rate: 0.000616\n",
      "Epoch 9811/40000, Loss: 6.60435325698927e-05, Learning Rate: 0.000616\n",
      "Epoch 9812/40000, Loss: 2.9758115488220938e-05, Learning Rate: 0.000616\n",
      "Epoch 9813/40000, Loss: 9.504114132141694e-05, Learning Rate: 0.000616\n",
      "Epoch 9814/40000, Loss: 3.008392195624765e-05, Learning Rate: 0.000616\n",
      "Epoch 9815/40000, Loss: 2.668355591595173e-05, Learning Rate: 0.000616\n",
      "Epoch 9816/40000, Loss: 5.687091106665321e-05, Learning Rate: 0.000616\n",
      "Epoch 9817/40000, Loss: 0.00013283912267070264, Learning Rate: 0.000616\n",
      "Epoch 9818/40000, Loss: 8.054548379732296e-05, Learning Rate: 0.000616\n",
      "Epoch 9819/40000, Loss: 0.0001198294703499414, Learning Rate: 0.000616\n",
      "Epoch 9820/40000, Loss: 9.569213580107316e-05, Learning Rate: 0.000616\n",
      "Epoch 9821/40000, Loss: 0.0001259295822819695, Learning Rate: 0.000615\n",
      "Epoch 9822/40000, Loss: 3.847886182484217e-05, Learning Rate: 0.000615\n",
      "Epoch 9823/40000, Loss: 6.319108069874346e-05, Learning Rate: 0.000615\n",
      "Epoch 9824/40000, Loss: 7.93699364294298e-05, Learning Rate: 0.000615\n",
      "Epoch 9825/40000, Loss: 3.390529309399426e-05, Learning Rate: 0.000615\n",
      "Epoch 9826/40000, Loss: 2.7833873900817707e-05, Learning Rate: 0.000615\n",
      "Epoch 9827/40000, Loss: 6.06806788709946e-05, Learning Rate: 0.000615\n",
      "Epoch 9828/40000, Loss: 6.444205064326525e-05, Learning Rate: 0.000615\n",
      "Epoch 9829/40000, Loss: 2.5972492949222215e-05, Learning Rate: 0.000615\n",
      "Epoch 9830/40000, Loss: 0.00011170747166033834, Learning Rate: 0.000615\n",
      "Epoch 9831/40000, Loss: 6.935782585060224e-05, Learning Rate: 0.000615\n",
      "Epoch 9832/40000, Loss: 5.20665489602834e-05, Learning Rate: 0.000615\n",
      "Epoch 9833/40000, Loss: 7.214861398097128e-05, Learning Rate: 0.000615\n",
      "Epoch 9834/40000, Loss: 0.00011528173490660265, Learning Rate: 0.000615\n",
      "Epoch 9835/40000, Loss: 5.9119956858921796e-05, Learning Rate: 0.000614\n",
      "Epoch 9836/40000, Loss: 9.482097084401175e-05, Learning Rate: 0.000614\n",
      "Epoch 9837/40000, Loss: 5.970288475509733e-05, Learning Rate: 0.000614\n",
      "Epoch 9838/40000, Loss: 0.00010882058268180117, Learning Rate: 0.000614\n",
      "Epoch 9839/40000, Loss: 8.92839307198301e-05, Learning Rate: 0.000614\n",
      "Epoch 9840/40000, Loss: 4.8308924306184053e-05, Learning Rate: 0.000614\n",
      "Epoch 9841/40000, Loss: 4.592777986545116e-05, Learning Rate: 0.000614\n",
      "Epoch 9842/40000, Loss: 4.6869787183823064e-05, Learning Rate: 0.000614\n",
      "Epoch 9843/40000, Loss: 6.03947919444181e-05, Learning Rate: 0.000614\n",
      "Epoch 9844/40000, Loss: 8.817506750347093e-05, Learning Rate: 0.000614\n",
      "Epoch 9845/40000, Loss: 7.839254976715893e-05, Learning Rate: 0.000614\n",
      "Epoch 9846/40000, Loss: 6.0921218391740695e-05, Learning Rate: 0.000614\n",
      "Epoch 9847/40000, Loss: 8.105533925117925e-05, Learning Rate: 0.000614\n",
      "Epoch 9848/40000, Loss: 4.7342877223854885e-05, Learning Rate: 0.000613\n",
      "Epoch 9849/40000, Loss: 9.500236774329096e-05, Learning Rate: 0.000613\n",
      "Epoch 9850/40000, Loss: 8.787671686150134e-05, Learning Rate: 0.000613\n",
      "Epoch 9851/40000, Loss: 5.4802698286948726e-05, Learning Rate: 0.000613\n",
      "Epoch 9852/40000, Loss: 4.763544711750001e-05, Learning Rate: 0.000613\n",
      "Epoch 9853/40000, Loss: 6.738593947375193e-05, Learning Rate: 0.000613\n",
      "Epoch 9854/40000, Loss: 5.084739677840844e-05, Learning Rate: 0.000613\n",
      "Epoch 9855/40000, Loss: 3.5152574128005654e-05, Learning Rate: 0.000613\n",
      "Epoch 9856/40000, Loss: 6.763466080883518e-05, Learning Rate: 0.000613\n",
      "Epoch 9857/40000, Loss: 3.537799420882948e-05, Learning Rate: 0.000613\n",
      "Epoch 9858/40000, Loss: 6.34204552625306e-05, Learning Rate: 0.000613\n",
      "Epoch 9859/40000, Loss: 8.503485150868073e-05, Learning Rate: 0.000613\n",
      "Epoch 9860/40000, Loss: 7.980989175848663e-05, Learning Rate: 0.000613\n",
      "Epoch 9861/40000, Loss: 5.715341467293911e-05, Learning Rate: 0.000613\n",
      "Epoch 9862/40000, Loss: 2.4737555577303283e-05, Learning Rate: 0.000612\n",
      "Epoch 9863/40000, Loss: 4.931275543640368e-05, Learning Rate: 0.000612\n",
      "Epoch 9864/40000, Loss: 8.851727761793882e-05, Learning Rate: 0.000612\n",
      "Epoch 9865/40000, Loss: 4.965694097336382e-05, Learning Rate: 0.000612\n",
      "Epoch 9866/40000, Loss: 6.258583016460761e-05, Learning Rate: 0.000612\n",
      "Epoch 9867/40000, Loss: 8.834716572891921e-05, Learning Rate: 0.000612\n",
      "Epoch 9868/40000, Loss: 8.610564691480249e-05, Learning Rate: 0.000612\n",
      "Epoch 9869/40000, Loss: 6.0697668232023716e-05, Learning Rate: 0.000612\n",
      "Epoch 9870/40000, Loss: 9.005500032799318e-05, Learning Rate: 0.000612\n",
      "Epoch 9871/40000, Loss: 5.8002027799375355e-05, Learning Rate: 0.000612\n",
      "Epoch 9872/40000, Loss: 8.847986464388669e-05, Learning Rate: 0.000612\n",
      "Epoch 9873/40000, Loss: 8.768346015131101e-05, Learning Rate: 0.000612\n",
      "Epoch 9874/40000, Loss: 8.534175140084699e-05, Learning Rate: 0.000612\n",
      "Epoch 9875/40000, Loss: 5.684643110726029e-05, Learning Rate: 0.000611\n",
      "Epoch 9876/40000, Loss: 5.385552503867075e-05, Learning Rate: 0.000611\n",
      "Epoch 9877/40000, Loss: 8.609352516941726e-05, Learning Rate: 0.000611\n",
      "Epoch 9878/40000, Loss: 5.8921239542542025e-05, Learning Rate: 0.000611\n",
      "Epoch 9879/40000, Loss: 4.855652150581591e-05, Learning Rate: 0.000611\n",
      "Epoch 9880/40000, Loss: 2.3106160369934514e-05, Learning Rate: 0.000611\n",
      "Epoch 9881/40000, Loss: 8.399993384955451e-05, Learning Rate: 0.000611\n",
      "Epoch 9882/40000, Loss: 2.7531561499927193e-05, Learning Rate: 0.000611\n",
      "Epoch 9883/40000, Loss: 2.4484023015247658e-05, Learning Rate: 0.000611\n",
      "Epoch 9884/40000, Loss: 8.154984243446961e-05, Learning Rate: 0.000611\n",
      "Epoch 9885/40000, Loss: 5.533281364478171e-05, Learning Rate: 0.000611\n",
      "Epoch 9886/40000, Loss: 6.0637376009253785e-05, Learning Rate: 0.000611\n",
      "Epoch 9887/40000, Loss: 4.78073452541139e-05, Learning Rate: 0.000611\n",
      "Epoch 9888/40000, Loss: 2.6790208721649833e-05, Learning Rate: 0.000611\n",
      "Epoch 9889/40000, Loss: 5.3912634029984474e-05, Learning Rate: 0.000610\n",
      "Epoch 9890/40000, Loss: 5.430695091490634e-05, Learning Rate: 0.000610\n",
      "Epoch 9891/40000, Loss: 8.414055628236383e-05, Learning Rate: 0.000610\n",
      "Epoch 9892/40000, Loss: 6.689201109111309e-05, Learning Rate: 0.000610\n",
      "Epoch 9893/40000, Loss: 9.274822514271364e-05, Learning Rate: 0.000610\n",
      "Epoch 9894/40000, Loss: 6.50507427053526e-05, Learning Rate: 0.000610\n",
      "Epoch 9895/40000, Loss: 6.665564433205873e-05, Learning Rate: 0.000610\n",
      "Epoch 9896/40000, Loss: 9.796499216463417e-05, Learning Rate: 0.000610\n",
      "Epoch 9897/40000, Loss: 6.984538777032867e-05, Learning Rate: 0.000610\n",
      "Epoch 9898/40000, Loss: 6.191256397869438e-05, Learning Rate: 0.000610\n",
      "Epoch 9899/40000, Loss: 8.5521096480079e-05, Learning Rate: 0.000610\n",
      "Epoch 9900/40000, Loss: 8.32273653941229e-05, Learning Rate: 0.000610\n",
      "Epoch 9901/40000, Loss: 2.803932875394821e-05, Learning Rate: 0.000610\n",
      "Epoch 9902/40000, Loss: 5.172323653823696e-05, Learning Rate: 0.000610\n",
      "Epoch 9903/40000, Loss: 4.41900483565405e-05, Learning Rate: 0.000609\n",
      "Epoch 9904/40000, Loss: 9.033091191668063e-05, Learning Rate: 0.000609\n",
      "Epoch 9905/40000, Loss: 6.807175668654963e-05, Learning Rate: 0.000609\n",
      "Epoch 9906/40000, Loss: 9.361941192764789e-05, Learning Rate: 0.000609\n",
      "Epoch 9907/40000, Loss: 2.607813621580135e-05, Learning Rate: 0.000609\n",
      "Epoch 9908/40000, Loss: 9.166049858322367e-05, Learning Rate: 0.000609\n",
      "Epoch 9909/40000, Loss: 0.00011733518476830795, Learning Rate: 0.000609\n",
      "Epoch 9910/40000, Loss: 0.00010359072621213272, Learning Rate: 0.000609\n",
      "Epoch 9911/40000, Loss: 0.0002122845471603796, Learning Rate: 0.000609\n",
      "Epoch 9912/40000, Loss: 3.9689639379503205e-05, Learning Rate: 0.000609\n",
      "Epoch 9913/40000, Loss: 0.0001057981135090813, Learning Rate: 0.000609\n",
      "Epoch 9914/40000, Loss: 9.950876847142354e-05, Learning Rate: 0.000609\n",
      "Epoch 9915/40000, Loss: 9.60609395406209e-05, Learning Rate: 0.000609\n",
      "Epoch 9916/40000, Loss: 8.768117550062016e-05, Learning Rate: 0.000608\n",
      "Epoch 9917/40000, Loss: 5.83742767048534e-05, Learning Rate: 0.000608\n",
      "Epoch 9918/40000, Loss: 9.43779741646722e-05, Learning Rate: 0.000608\n",
      "Epoch 9919/40000, Loss: 4.914953024126589e-05, Learning Rate: 0.000608\n",
      "Epoch 9920/40000, Loss: 6.319806561805308e-05, Learning Rate: 0.000608\n",
      "Epoch 9921/40000, Loss: 7.98036708147265e-05, Learning Rate: 0.000608\n",
      "Epoch 9922/40000, Loss: 7.743920286884531e-05, Learning Rate: 0.000608\n",
      "Epoch 9923/40000, Loss: 4.8545382014708593e-05, Learning Rate: 0.000608\n",
      "Epoch 9924/40000, Loss: 2.4607876184745692e-05, Learning Rate: 0.000608\n",
      "Epoch 9925/40000, Loss: 2.3372202122118324e-05, Learning Rate: 0.000608\n",
      "Epoch 9926/40000, Loss: 7.805865607224405e-05, Learning Rate: 0.000608\n",
      "Epoch 9927/40000, Loss: 5.35590952495113e-05, Learning Rate: 0.000608\n",
      "Epoch 9928/40000, Loss: 8.902716217562556e-05, Learning Rate: 0.000608\n",
      "Epoch 9929/40000, Loss: 8.30436110845767e-05, Learning Rate: 0.000608\n",
      "Epoch 9930/40000, Loss: 9.616816532798111e-05, Learning Rate: 0.000607\n",
      "Epoch 9931/40000, Loss: 5.7626868510851637e-05, Learning Rate: 0.000607\n",
      "Epoch 9932/40000, Loss: 4.8191050154855475e-05, Learning Rate: 0.000607\n",
      "Epoch 9933/40000, Loss: 6.084763663238846e-05, Learning Rate: 0.000607\n",
      "Epoch 9934/40000, Loss: 8.315689774462953e-05, Learning Rate: 0.000607\n",
      "Epoch 9935/40000, Loss: 5.8692563470685855e-05, Learning Rate: 0.000607\n",
      "Epoch 9936/40000, Loss: 4.991238165530376e-05, Learning Rate: 0.000607\n",
      "Epoch 9937/40000, Loss: 3.050832856388297e-05, Learning Rate: 0.000607\n",
      "Epoch 9938/40000, Loss: 0.00010104334796778858, Learning Rate: 0.000607\n",
      "Epoch 9939/40000, Loss: 0.00012959509331267327, Learning Rate: 0.000607\n",
      "Epoch 9940/40000, Loss: 7.515055767726153e-05, Learning Rate: 0.000607\n",
      "Epoch 9941/40000, Loss: 0.00013475766172632575, Learning Rate: 0.000607\n",
      "Epoch 9942/40000, Loss: 0.00030177118605934083, Learning Rate: 0.000607\n",
      "Epoch 9943/40000, Loss: 0.00016972767480183393, Learning Rate: 0.000607\n",
      "Epoch 9944/40000, Loss: 0.00010196516814175993, Learning Rate: 0.000606\n",
      "Epoch 9945/40000, Loss: 0.00017798518820200115, Learning Rate: 0.000606\n",
      "Epoch 9946/40000, Loss: 8.083518332568929e-05, Learning Rate: 0.000606\n",
      "Epoch 9947/40000, Loss: 8.872540638549253e-05, Learning Rate: 0.000606\n",
      "Epoch 9948/40000, Loss: 0.00013302717707119882, Learning Rate: 0.000606\n",
      "Epoch 9949/40000, Loss: 0.00013172255421523005, Learning Rate: 0.000606\n",
      "Epoch 9950/40000, Loss: 6.791995110688731e-05, Learning Rate: 0.000606\n",
      "Epoch 9951/40000, Loss: 6.779500108677894e-05, Learning Rate: 0.000606\n",
      "Epoch 9952/40000, Loss: 8.071836055023596e-05, Learning Rate: 0.000606\n",
      "Epoch 9953/40000, Loss: 6.269389996305108e-05, Learning Rate: 0.000606\n",
      "Epoch 9954/40000, Loss: 2.791108090605121e-05, Learning Rate: 0.000606\n",
      "Epoch 9955/40000, Loss: 6.90358501742594e-05, Learning Rate: 0.000606\n",
      "Epoch 9956/40000, Loss: 3.432079392950982e-05, Learning Rate: 0.000606\n",
      "Epoch 9957/40000, Loss: 6.390190537786111e-05, Learning Rate: 0.000605\n",
      "Epoch 9958/40000, Loss: 6.226437108125538e-05, Learning Rate: 0.000605\n",
      "Epoch 9959/40000, Loss: 7.990092854015529e-05, Learning Rate: 0.000605\n",
      "Epoch 9960/40000, Loss: 9.472720557823777e-05, Learning Rate: 0.000605\n",
      "Epoch 9961/40000, Loss: 5.644743941957131e-05, Learning Rate: 0.000605\n",
      "Epoch 9962/40000, Loss: 5.987038093735464e-05, Learning Rate: 0.000605\n",
      "Epoch 9963/40000, Loss: 5.531062197405845e-05, Learning Rate: 0.000605\n",
      "Epoch 9964/40000, Loss: 4.610372707247734e-05, Learning Rate: 0.000605\n",
      "Epoch 9965/40000, Loss: 5.9067180700367317e-05, Learning Rate: 0.000605\n",
      "Epoch 9966/40000, Loss: 5.838791184942238e-05, Learning Rate: 0.000605\n",
      "Epoch 9967/40000, Loss: 2.119991768267937e-05, Learning Rate: 0.000605\n",
      "Epoch 9968/40000, Loss: 4.5579996367450804e-05, Learning Rate: 0.000605\n",
      "Epoch 9969/40000, Loss: 2.2118576453067362e-05, Learning Rate: 0.000605\n",
      "Epoch 9970/40000, Loss: 5.1274520956212655e-05, Learning Rate: 0.000605\n",
      "Epoch 9971/40000, Loss: 5.766247340943664e-05, Learning Rate: 0.000604\n",
      "Epoch 9972/40000, Loss: 4.495332177611999e-05, Learning Rate: 0.000604\n",
      "Epoch 9973/40000, Loss: 4.508256824919954e-05, Learning Rate: 0.000604\n",
      "Epoch 9974/40000, Loss: 4.4594184146262705e-05, Learning Rate: 0.000604\n",
      "Epoch 9975/40000, Loss: 8.347690891241655e-05, Learning Rate: 0.000604\n",
      "Epoch 9976/40000, Loss: 4.5111730287317187e-05, Learning Rate: 0.000604\n",
      "Epoch 9977/40000, Loss: 8.388621063204482e-05, Learning Rate: 0.000604\n",
      "Epoch 9978/40000, Loss: 4.4540571252582595e-05, Learning Rate: 0.000604\n",
      "Epoch 9979/40000, Loss: 5.747967952629551e-05, Learning Rate: 0.000604\n",
      "Epoch 9980/40000, Loss: 7.484894740628079e-05, Learning Rate: 0.000604\n",
      "Epoch 9981/40000, Loss: 4.949545837007463e-05, Learning Rate: 0.000604\n",
      "Epoch 9982/40000, Loss: 5.731909550377168e-05, Learning Rate: 0.000604\n",
      "Epoch 9983/40000, Loss: 7.43491982575506e-05, Learning Rate: 0.000604\n",
      "Epoch 9984/40000, Loss: 2.1017194740124978e-05, Learning Rate: 0.000604\n",
      "Epoch 9985/40000, Loss: 7.52159467083402e-05, Learning Rate: 0.000603\n",
      "Epoch 9986/40000, Loss: 4.9663573008729145e-05, Learning Rate: 0.000603\n",
      "Epoch 9987/40000, Loss: 5.7206667406717315e-05, Learning Rate: 0.000603\n",
      "Epoch 9988/40000, Loss: 5.708750904886983e-05, Learning Rate: 0.000603\n",
      "Epoch 9989/40000, Loss: 5.7017983635887504e-05, Learning Rate: 0.000603\n",
      "Epoch 9990/40000, Loss: 2.0908906662953086e-05, Learning Rate: 0.000603\n",
      "Epoch 9991/40000, Loss: 8.319537300849333e-05, Learning Rate: 0.000603\n",
      "Epoch 9992/40000, Loss: 8.312329009640962e-05, Learning Rate: 0.000603\n",
      "Epoch 9993/40000, Loss: 7.382845797110349e-05, Learning Rate: 0.000603\n",
      "Epoch 9994/40000, Loss: 7.37377122277394e-05, Learning Rate: 0.000603\n",
      "Epoch 9995/40000, Loss: 7.35851499484852e-05, Learning Rate: 0.000603\n",
      "Epoch 9996/40000, Loss: 4.9293856136500835e-05, Learning Rate: 0.000603\n",
      "Epoch 9997/40000, Loss: 4.4270738726481795e-05, Learning Rate: 0.000603\n",
      "Epoch 9998/40000, Loss: 7.358830771408975e-05, Learning Rate: 0.000603\n",
      "Epoch 9999/40000, Loss: 8.385739056393504e-05, Learning Rate: 0.000602\n",
      "Epoch 10000/40000, Loss: 2.0833476810366847e-05, Learning Rate: 0.000602\n",
      "Epoch 10001/40000, Loss: 4.443836951395497e-05, Learning Rate: 0.000602\n",
      "Epoch 10002/40000, Loss: 7.651487248949707e-05, Learning Rate: 0.000602\n",
      "Epoch 10003/40000, Loss: 2.09289210033603e-05, Learning Rate: 0.000602\n",
      "Epoch 10004/40000, Loss: 4.404773426358588e-05, Learning Rate: 0.000602\n",
      "Epoch 10005/40000, Loss: 5.6966058764373884e-05, Learning Rate: 0.000602\n",
      "Epoch 10006/40000, Loss: 5.670665268553421e-05, Learning Rate: 0.000602\n",
      "Epoch 10007/40000, Loss: 8.373251330340281e-05, Learning Rate: 0.000602\n",
      "Epoch 10008/40000, Loss: 4.9064226914197206e-05, Learning Rate: 0.000602\n",
      "Epoch 10009/40000, Loss: 5.708956086891703e-05, Learning Rate: 0.000602\n",
      "Epoch 10010/40000, Loss: 4.4393913412932307e-05, Learning Rate: 0.000602\n",
      "Epoch 10011/40000, Loss: 5.6828634114935994e-05, Learning Rate: 0.000602\n",
      "Epoch 10012/40000, Loss: 8.28782794997096e-05, Learning Rate: 0.000602\n",
      "Epoch 10013/40000, Loss: 4.905491368845105e-05, Learning Rate: 0.000601\n",
      "Epoch 10014/40000, Loss: 8.302785863634199e-05, Learning Rate: 0.000601\n",
      "Epoch 10015/40000, Loss: 4.982275640941225e-05, Learning Rate: 0.000601\n",
      "Epoch 10016/40000, Loss: 8.326736133312806e-05, Learning Rate: 0.000601\n",
      "Epoch 10017/40000, Loss: 2.1129118977114558e-05, Learning Rate: 0.000601\n",
      "Epoch 10018/40000, Loss: 5.698618042515591e-05, Learning Rate: 0.000601\n",
      "Epoch 10019/40000, Loss: 8.364066161448136e-05, Learning Rate: 0.000601\n",
      "Epoch 10020/40000, Loss: 5.789579518022947e-05, Learning Rate: 0.000601\n",
      "Epoch 10021/40000, Loss: 5.061179763288237e-05, Learning Rate: 0.000601\n",
      "Epoch 10022/40000, Loss: 2.151807893824298e-05, Learning Rate: 0.000601\n",
      "Epoch 10023/40000, Loss: 5.6969445722643286e-05, Learning Rate: 0.000601\n",
      "Epoch 10024/40000, Loss: 2.1127747459104285e-05, Learning Rate: 0.000601\n",
      "Epoch 10025/40000, Loss: 8.359945786651224e-05, Learning Rate: 0.000601\n",
      "Epoch 10026/40000, Loss: 2.08416095119901e-05, Learning Rate: 0.000601\n",
      "Epoch 10027/40000, Loss: 5.675925422110595e-05, Learning Rate: 0.000600\n",
      "Epoch 10028/40000, Loss: 5.6821241741999984e-05, Learning Rate: 0.000600\n",
      "Epoch 10029/40000, Loss: 5.668809171766043e-05, Learning Rate: 0.000600\n",
      "Epoch 10030/40000, Loss: 4.943351450492628e-05, Learning Rate: 0.000600\n",
      "Epoch 10031/40000, Loss: 4.953926691086963e-05, Learning Rate: 0.000600\n",
      "Epoch 10032/40000, Loss: 4.494281893130392e-05, Learning Rate: 0.000600\n",
      "Epoch 10033/40000, Loss: 5.073059583082795e-05, Learning Rate: 0.000600\n",
      "Epoch 10034/40000, Loss: 7.508546696044505e-05, Learning Rate: 0.000600\n",
      "Epoch 10035/40000, Loss: 7.535272015957162e-05, Learning Rate: 0.000600\n",
      "Epoch 10036/40000, Loss: 5.656164285028353e-05, Learning Rate: 0.000600\n",
      "Epoch 10037/40000, Loss: 3.903153265127912e-05, Learning Rate: 0.000600\n",
      "Epoch 10038/40000, Loss: 6.249477883102372e-05, Learning Rate: 0.000600\n",
      "Epoch 10039/40000, Loss: 5.365909237298183e-05, Learning Rate: 0.000600\n",
      "Epoch 10040/40000, Loss: 8.229954255511984e-05, Learning Rate: 0.000599\n",
      "Epoch 10041/40000, Loss: 5.607095590676181e-05, Learning Rate: 0.000599\n",
      "Epoch 10042/40000, Loss: 7.920983625808731e-05, Learning Rate: 0.000599\n",
      "Epoch 10043/40000, Loss: 8.863974653650075e-05, Learning Rate: 0.000599\n",
      "Epoch 10044/40000, Loss: 8.747633546590805e-05, Learning Rate: 0.000599\n",
      "Epoch 10045/40000, Loss: 8.51200966280885e-05, Learning Rate: 0.000599\n",
      "Epoch 10046/40000, Loss: 4.607021764968522e-05, Learning Rate: 0.000599\n",
      "Epoch 10047/40000, Loss: 7.81323469709605e-05, Learning Rate: 0.000599\n",
      "Epoch 10048/40000, Loss: 5.321417120285332e-05, Learning Rate: 0.000599\n",
      "Epoch 10049/40000, Loss: 4.528989302343689e-05, Learning Rate: 0.000599\n",
      "Epoch 10050/40000, Loss: 8.060743130045012e-05, Learning Rate: 0.000599\n",
      "Epoch 10051/40000, Loss: 8.873312617652118e-05, Learning Rate: 0.000599\n",
      "Epoch 10052/40000, Loss: 8.693378913449124e-05, Learning Rate: 0.000599\n",
      "Epoch 10053/40000, Loss: 2.535040221118834e-05, Learning Rate: 0.000599\n",
      "Epoch 10054/40000, Loss: 5.874123962712474e-05, Learning Rate: 0.000598\n",
      "Epoch 10055/40000, Loss: 8.708277891855687e-05, Learning Rate: 0.000598\n",
      "Epoch 10056/40000, Loss: 7.941913645481691e-05, Learning Rate: 0.000598\n",
      "Epoch 10057/40000, Loss: 5.2868956117890775e-05, Learning Rate: 0.000598\n",
      "Epoch 10058/40000, Loss: 6.105270585976541e-05, Learning Rate: 0.000598\n",
      "Epoch 10059/40000, Loss: 6.260308146011084e-05, Learning Rate: 0.000598\n",
      "Epoch 10060/40000, Loss: 6.077509533497505e-05, Learning Rate: 0.000598\n",
      "Epoch 10061/40000, Loss: 9.946687350748107e-05, Learning Rate: 0.000598\n",
      "Epoch 10062/40000, Loss: 9.426112956134602e-05, Learning Rate: 0.000598\n",
      "Epoch 10063/40000, Loss: 9.223814413417131e-05, Learning Rate: 0.000598\n",
      "Epoch 10064/40000, Loss: 5.0475944590289146e-05, Learning Rate: 0.000598\n",
      "Epoch 10065/40000, Loss: 8.395688200835139e-05, Learning Rate: 0.000598\n",
      "Epoch 10066/40000, Loss: 8.019403321668506e-05, Learning Rate: 0.000598\n",
      "Epoch 10067/40000, Loss: 7.346580241573974e-05, Learning Rate: 0.000598\n",
      "Epoch 10068/40000, Loss: 5.240308746579103e-05, Learning Rate: 0.000597\n",
      "Epoch 10069/40000, Loss: 9.399923146702349e-05, Learning Rate: 0.000597\n",
      "Epoch 10070/40000, Loss: 9.870195935945958e-05, Learning Rate: 0.000597\n",
      "Epoch 10071/40000, Loss: 7.266519969562069e-05, Learning Rate: 0.000597\n",
      "Epoch 10072/40000, Loss: 0.00010056953760795295, Learning Rate: 0.000597\n",
      "Epoch 10073/40000, Loss: 0.0001445767848053947, Learning Rate: 0.000597\n",
      "Epoch 10074/40000, Loss: 5.8419816923560575e-05, Learning Rate: 0.000597\n",
      "Epoch 10075/40000, Loss: 9.982594201574102e-05, Learning Rate: 0.000597\n",
      "Epoch 10076/40000, Loss: 9.952500113286078e-05, Learning Rate: 0.000597\n",
      "Epoch 10077/40000, Loss: 8.594715473009273e-05, Learning Rate: 0.000597\n",
      "Epoch 10078/40000, Loss: 4.96790416946169e-05, Learning Rate: 0.000597\n",
      "Epoch 10079/40000, Loss: 4.8062833229778334e-05, Learning Rate: 0.000597\n",
      "Epoch 10080/40000, Loss: 5.882291225134395e-05, Learning Rate: 0.000597\n",
      "Epoch 10081/40000, Loss: 5.443607733468525e-05, Learning Rate: 0.000597\n",
      "Epoch 10082/40000, Loss: 4.628115857485682e-05, Learning Rate: 0.000596\n",
      "Epoch 10083/40000, Loss: 6.132861017249525e-05, Learning Rate: 0.000596\n",
      "Epoch 10084/40000, Loss: 5.2147686801617965e-05, Learning Rate: 0.000596\n",
      "Epoch 10085/40000, Loss: 5.866114952368662e-05, Learning Rate: 0.000596\n",
      "Epoch 10086/40000, Loss: 6.094803393352777e-05, Learning Rate: 0.000596\n",
      "Epoch 10087/40000, Loss: 5.4765849199611694e-05, Learning Rate: 0.000596\n",
      "Epoch 10088/40000, Loss: 8.333702862728387e-05, Learning Rate: 0.000596\n",
      "Epoch 10089/40000, Loss: 8.483290002914146e-05, Learning Rate: 0.000596\n",
      "Epoch 10090/40000, Loss: 5.592435263679363e-05, Learning Rate: 0.000596\n",
      "Epoch 10091/40000, Loss: 7.922566146589816e-05, Learning Rate: 0.000596\n",
      "Epoch 10092/40000, Loss: 7.75623120716773e-05, Learning Rate: 0.000596\n",
      "Epoch 10093/40000, Loss: 9.295613563153893e-05, Learning Rate: 0.000596\n",
      "Epoch 10094/40000, Loss: 8.319935295730829e-05, Learning Rate: 0.000596\n",
      "Epoch 10095/40000, Loss: 5.910987965762615e-05, Learning Rate: 0.000596\n",
      "Epoch 10096/40000, Loss: 2.8098831535317004e-05, Learning Rate: 0.000595\n",
      "Epoch 10097/40000, Loss: 5.930931365583092e-05, Learning Rate: 0.000595\n",
      "Epoch 10098/40000, Loss: 5.793404488940723e-05, Learning Rate: 0.000595\n",
      "Epoch 10099/40000, Loss: 5.694330684491433e-05, Learning Rate: 0.000595\n",
      "Epoch 10100/40000, Loss: 5.1963030273327604e-05, Learning Rate: 0.000595\n",
      "Epoch 10101/40000, Loss: 5.9389629313955083e-05, Learning Rate: 0.000595\n",
      "Epoch 10102/40000, Loss: 8.055732905631885e-05, Learning Rate: 0.000595\n",
      "Epoch 10103/40000, Loss: 5.4732197895646095e-05, Learning Rate: 0.000595\n",
      "Epoch 10104/40000, Loss: 4.841024201596156e-05, Learning Rate: 0.000595\n",
      "Epoch 10105/40000, Loss: 2.784120079013519e-05, Learning Rate: 0.000595\n",
      "Epoch 10106/40000, Loss: 9.408456389792264e-05, Learning Rate: 0.000595\n",
      "Epoch 10107/40000, Loss: 5.5820655688876286e-05, Learning Rate: 0.000595\n",
      "Epoch 10108/40000, Loss: 5.998063352308236e-05, Learning Rate: 0.000595\n",
      "Epoch 10109/40000, Loss: 2.3984339350135997e-05, Learning Rate: 0.000595\n",
      "Epoch 10110/40000, Loss: 9.535314165987074e-05, Learning Rate: 0.000594\n",
      "Epoch 10111/40000, Loss: 5.032562694395892e-05, Learning Rate: 0.000594\n",
      "Epoch 10112/40000, Loss: 3.27297966578044e-05, Learning Rate: 0.000594\n",
      "Epoch 10113/40000, Loss: 3.387982724234462e-05, Learning Rate: 0.000594\n",
      "Epoch 10114/40000, Loss: 3.4295844670850784e-05, Learning Rate: 0.000594\n",
      "Epoch 10115/40000, Loss: 2.6186979084741324e-05, Learning Rate: 0.000594\n",
      "Epoch 10116/40000, Loss: 8.923363202484325e-05, Learning Rate: 0.000594\n",
      "Epoch 10117/40000, Loss: 4.315321712056175e-05, Learning Rate: 0.000594\n",
      "Epoch 10118/40000, Loss: 0.0001016281676129438, Learning Rate: 0.000594\n",
      "Epoch 10119/40000, Loss: 0.00010303538874723017, Learning Rate: 0.000594\n",
      "Epoch 10120/40000, Loss: 0.00010379204468335956, Learning Rate: 0.000594\n",
      "Epoch 10121/40000, Loss: 9.801948181120679e-05, Learning Rate: 0.000594\n",
      "Epoch 10122/40000, Loss: 6.744161510141566e-05, Learning Rate: 0.000594\n",
      "Epoch 10123/40000, Loss: 5.2714683988597244e-05, Learning Rate: 0.000594\n",
      "Epoch 10124/40000, Loss: 0.00010833624401129782, Learning Rate: 0.000593\n",
      "Epoch 10125/40000, Loss: 6.414199742721394e-05, Learning Rate: 0.000593\n",
      "Epoch 10126/40000, Loss: 6.350562034640461e-05, Learning Rate: 0.000593\n",
      "Epoch 10127/40000, Loss: 0.0001271553337574005, Learning Rate: 0.000593\n",
      "Epoch 10128/40000, Loss: 8.025626448215917e-05, Learning Rate: 0.000593\n",
      "Epoch 10129/40000, Loss: 8.125591557472944e-05, Learning Rate: 0.000593\n",
      "Epoch 10130/40000, Loss: 8.276966400444508e-05, Learning Rate: 0.000593\n",
      "Epoch 10131/40000, Loss: 0.00011445149721112102, Learning Rate: 0.000593\n",
      "Epoch 10132/40000, Loss: 7.419388566631824e-05, Learning Rate: 0.000593\n",
      "Epoch 10133/40000, Loss: 0.00011397536582080647, Learning Rate: 0.000593\n",
      "Epoch 10134/40000, Loss: 6.921520252944902e-05, Learning Rate: 0.000593\n",
      "Epoch 10135/40000, Loss: 0.00010628454765537754, Learning Rate: 0.000593\n",
      "Epoch 10136/40000, Loss: 8.153633098118007e-05, Learning Rate: 0.000593\n",
      "Epoch 10137/40000, Loss: 5.964048614259809e-05, Learning Rate: 0.000593\n",
      "Epoch 10138/40000, Loss: 6.412808579625562e-05, Learning Rate: 0.000592\n",
      "Epoch 10139/40000, Loss: 9.272144961869344e-05, Learning Rate: 0.000592\n",
      "Epoch 10140/40000, Loss: 8.34947859402746e-05, Learning Rate: 0.000592\n",
      "Epoch 10141/40000, Loss: 4.961014201398939e-05, Learning Rate: 0.000592\n",
      "Epoch 10142/40000, Loss: 2.643106199684553e-05, Learning Rate: 0.000592\n",
      "Epoch 10143/40000, Loss: 8.63102832227014e-05, Learning Rate: 0.000592\n",
      "Epoch 10144/40000, Loss: 5.0874845328507945e-05, Learning Rate: 0.000592\n",
      "Epoch 10145/40000, Loss: 8.359785715583712e-05, Learning Rate: 0.000592\n",
      "Epoch 10146/40000, Loss: 9.711106395116076e-05, Learning Rate: 0.000592\n",
      "Epoch 10147/40000, Loss: 8.040732791414484e-05, Learning Rate: 0.000592\n",
      "Epoch 10148/40000, Loss: 6.449764623539522e-05, Learning Rate: 0.000592\n",
      "Epoch 10149/40000, Loss: 6.128952372819185e-05, Learning Rate: 0.000592\n",
      "Epoch 10150/40000, Loss: 2.5642286345828325e-05, Learning Rate: 0.000592\n",
      "Epoch 10151/40000, Loss: 2.5495612135273404e-05, Learning Rate: 0.000592\n",
      "Epoch 10152/40000, Loss: 2.5388340873178095e-05, Learning Rate: 0.000591\n",
      "Epoch 10153/40000, Loss: 5.759091436630115e-05, Learning Rate: 0.000591\n",
      "Epoch 10154/40000, Loss: 8.382476516999304e-05, Learning Rate: 0.000591\n",
      "Epoch 10155/40000, Loss: 3.727732473635115e-05, Learning Rate: 0.000591\n",
      "Epoch 10156/40000, Loss: 6.0900365497218445e-05, Learning Rate: 0.000591\n",
      "Epoch 10157/40000, Loss: 6.66434716549702e-05, Learning Rate: 0.000591\n",
      "Epoch 10158/40000, Loss: 7.176328654168174e-05, Learning Rate: 0.000591\n",
      "Epoch 10159/40000, Loss: 4.8470101319253445e-05, Learning Rate: 0.000591\n",
      "Epoch 10160/40000, Loss: 0.00010031079000327736, Learning Rate: 0.000591\n",
      "Epoch 10161/40000, Loss: 9.841448627412319e-05, Learning Rate: 0.000591\n",
      "Epoch 10162/40000, Loss: 5.4103809816297144e-05, Learning Rate: 0.000591\n",
      "Epoch 10163/40000, Loss: 8.560264541301876e-05, Learning Rate: 0.000591\n",
      "Epoch 10164/40000, Loss: 6.047482747817412e-05, Learning Rate: 0.000591\n",
      "Epoch 10165/40000, Loss: 4.9449106882093474e-05, Learning Rate: 0.000591\n",
      "Epoch 10166/40000, Loss: 5.8945457567460835e-05, Learning Rate: 0.000591\n",
      "Epoch 10167/40000, Loss: 5.187619899515994e-05, Learning Rate: 0.000590\n",
      "Epoch 10168/40000, Loss: 4.565548078971915e-05, Learning Rate: 0.000590\n",
      "Epoch 10169/40000, Loss: 5.5292737670242786e-05, Learning Rate: 0.000590\n",
      "Epoch 10170/40000, Loss: 7.636424561496824e-05, Learning Rate: 0.000590\n",
      "Epoch 10171/40000, Loss: 4.5978442358318716e-05, Learning Rate: 0.000590\n",
      "Epoch 10172/40000, Loss: 8.499294926878065e-05, Learning Rate: 0.000590\n",
      "Epoch 10173/40000, Loss: 4.939969221595675e-05, Learning Rate: 0.000590\n",
      "Epoch 10174/40000, Loss: 8.746633102418855e-05, Learning Rate: 0.000590\n",
      "Epoch 10175/40000, Loss: 5.7256540458183736e-05, Learning Rate: 0.000590\n",
      "Epoch 10176/40000, Loss: 4.472594810067676e-05, Learning Rate: 0.000590\n",
      "Epoch 10177/40000, Loss: 2.101009158650413e-05, Learning Rate: 0.000590\n",
      "Epoch 10178/40000, Loss: 4.428727334016003e-05, Learning Rate: 0.000590\n",
      "Epoch 10179/40000, Loss: 4.3578227632679045e-05, Learning Rate: 0.000590\n",
      "Epoch 10180/40000, Loss: 7.334693509619683e-05, Learning Rate: 0.000590\n",
      "Epoch 10181/40000, Loss: 4.881040513282642e-05, Learning Rate: 0.000589\n",
      "Epoch 10182/40000, Loss: 8.239556336775422e-05, Learning Rate: 0.000589\n",
      "Epoch 10183/40000, Loss: 8.210265514208004e-05, Learning Rate: 0.000589\n",
      "Epoch 10184/40000, Loss: 4.864202855969779e-05, Learning Rate: 0.000589\n",
      "Epoch 10185/40000, Loss: 5.566308391280472e-05, Learning Rate: 0.000589\n",
      "Epoch 10186/40000, Loss: 8.242335024988279e-05, Learning Rate: 0.000589\n",
      "Epoch 10187/40000, Loss: 4.371931208879687e-05, Learning Rate: 0.000589\n",
      "Epoch 10188/40000, Loss: 2.0986892195651308e-05, Learning Rate: 0.000589\n",
      "Epoch 10189/40000, Loss: 7.263720181072131e-05, Learning Rate: 0.000589\n",
      "Epoch 10190/40000, Loss: 4.438460382516496e-05, Learning Rate: 0.000589\n",
      "Epoch 10191/40000, Loss: 7.308290514629334e-05, Learning Rate: 0.000589\n",
      "Epoch 10192/40000, Loss: 4.548382275970653e-05, Learning Rate: 0.000589\n",
      "Epoch 10193/40000, Loss: 7.713045488344505e-05, Learning Rate: 0.000589\n",
      "Epoch 10194/40000, Loss: 7.580708188470453e-05, Learning Rate: 0.000589\n",
      "Epoch 10195/40000, Loss: 4.525207623373717e-05, Learning Rate: 0.000588\n",
      "Epoch 10196/40000, Loss: 5.0326736527495086e-05, Learning Rate: 0.000588\n",
      "Epoch 10197/40000, Loss: 7.766062481096014e-05, Learning Rate: 0.000588\n",
      "Epoch 10198/40000, Loss: 8.728329703444615e-05, Learning Rate: 0.000588\n",
      "Epoch 10199/40000, Loss: 4.968323264620267e-05, Learning Rate: 0.000588\n",
      "Epoch 10200/40000, Loss: 4.712317604571581e-05, Learning Rate: 0.000588\n",
      "Epoch 10201/40000, Loss: 8.566941687604412e-05, Learning Rate: 0.000588\n",
      "Epoch 10202/40000, Loss: 8.118596451822668e-05, Learning Rate: 0.000588\n",
      "Epoch 10203/40000, Loss: 2.1922140149399638e-05, Learning Rate: 0.000588\n",
      "Epoch 10204/40000, Loss: 2.2326647012960166e-05, Learning Rate: 0.000588\n",
      "Epoch 10205/40000, Loss: 4.634373908629641e-05, Learning Rate: 0.000588\n",
      "Epoch 10206/40000, Loss: 5.586981205851771e-05, Learning Rate: 0.000588\n",
      "Epoch 10207/40000, Loss: 7.817912410246208e-05, Learning Rate: 0.000588\n",
      "Epoch 10208/40000, Loss: 4.5515949750551954e-05, Learning Rate: 0.000588\n",
      "Epoch 10209/40000, Loss: 2.175500958401244e-05, Learning Rate: 0.000587\n",
      "Epoch 10210/40000, Loss: 5.647680154652335e-05, Learning Rate: 0.000587\n",
      "Epoch 10211/40000, Loss: 8.409177826251835e-05, Learning Rate: 0.000587\n",
      "Epoch 10212/40000, Loss: 7.400617323582992e-05, Learning Rate: 0.000587\n",
      "Epoch 10213/40000, Loss: 4.9551843403605744e-05, Learning Rate: 0.000587\n",
      "Epoch 10214/40000, Loss: 2.142213998013176e-05, Learning Rate: 0.000587\n",
      "Epoch 10215/40000, Loss: 2.0961302652722225e-05, Learning Rate: 0.000587\n",
      "Epoch 10216/40000, Loss: 8.357835031347349e-05, Learning Rate: 0.000587\n",
      "Epoch 10217/40000, Loss: 5.21996189490892e-05, Learning Rate: 0.000587\n",
      "Epoch 10218/40000, Loss: 5.161849549040198e-05, Learning Rate: 0.000587\n",
      "Epoch 10219/40000, Loss: 2.139663411071524e-05, Learning Rate: 0.000587\n",
      "Epoch 10220/40000, Loss: 4.43150638602674e-05, Learning Rate: 0.000587\n",
      "Epoch 10221/40000, Loss: 7.44533899705857e-05, Learning Rate: 0.000587\n",
      "Epoch 10222/40000, Loss: 8.646511560073122e-05, Learning Rate: 0.000587\n",
      "Epoch 10223/40000, Loss: 2.1745268895756453e-05, Learning Rate: 0.000586\n",
      "Epoch 10224/40000, Loss: 4.576741048367694e-05, Learning Rate: 0.000586\n",
      "Epoch 10225/40000, Loss: 4.4766984501620755e-05, Learning Rate: 0.000586\n",
      "Epoch 10226/40000, Loss: 5.583139500231482e-05, Learning Rate: 0.000586\n",
      "Epoch 10227/40000, Loss: 8.35909231682308e-05, Learning Rate: 0.000586\n",
      "Epoch 10228/40000, Loss: 4.998359509045258e-05, Learning Rate: 0.000586\n",
      "Epoch 10229/40000, Loss: 7.607496809214354e-05, Learning Rate: 0.000586\n",
      "Epoch 10230/40000, Loss: 4.751075175590813e-05, Learning Rate: 0.000586\n",
      "Epoch 10231/40000, Loss: 7.670070772292092e-05, Learning Rate: 0.000586\n",
      "Epoch 10232/40000, Loss: 2.2520525817526504e-05, Learning Rate: 0.000586\n",
      "Epoch 10233/40000, Loss: 5.6672706705285236e-05, Learning Rate: 0.000586\n",
      "Epoch 10234/40000, Loss: 7.417528104269877e-05, Learning Rate: 0.000586\n",
      "Epoch 10235/40000, Loss: 8.436023199465126e-05, Learning Rate: 0.000586\n",
      "Epoch 10236/40000, Loss: 5.6797172874212265e-05, Learning Rate: 0.000586\n",
      "Epoch 10237/40000, Loss: 2.297526953043416e-05, Learning Rate: 0.000585\n",
      "Epoch 10238/40000, Loss: 7.680189446546137e-05, Learning Rate: 0.000585\n",
      "Epoch 10239/40000, Loss: 4.6198365453165025e-05, Learning Rate: 0.000585\n",
      "Epoch 10240/40000, Loss: 8.272732520708814e-05, Learning Rate: 0.000585\n",
      "Epoch 10241/40000, Loss: 2.72928191407118e-05, Learning Rate: 0.000585\n",
      "Epoch 10242/40000, Loss: 0.00010047806426882744, Learning Rate: 0.000585\n",
      "Epoch 10243/40000, Loss: 8.657445869175717e-05, Learning Rate: 0.000585\n",
      "Epoch 10244/40000, Loss: 5.515111843124032e-05, Learning Rate: 0.000585\n",
      "Epoch 10245/40000, Loss: 5.745117232436314e-05, Learning Rate: 0.000585\n",
      "Epoch 10246/40000, Loss: 5.041539407102391e-05, Learning Rate: 0.000585\n",
      "Epoch 10247/40000, Loss: 5.741558561567217e-05, Learning Rate: 0.000585\n",
      "Epoch 10248/40000, Loss: 9.898010466713458e-05, Learning Rate: 0.000585\n",
      "Epoch 10249/40000, Loss: 5.1992701628478244e-05, Learning Rate: 0.000585\n",
      "Epoch 10250/40000, Loss: 9.008116467157379e-05, Learning Rate: 0.000585\n",
      "Epoch 10251/40000, Loss: 4.6899080189177766e-05, Learning Rate: 0.000585\n",
      "Epoch 10252/40000, Loss: 9.719394438434392e-05, Learning Rate: 0.000584\n",
      "Epoch 10253/40000, Loss: 5.586591942119412e-05, Learning Rate: 0.000584\n",
      "Epoch 10254/40000, Loss: 8.983645238913596e-05, Learning Rate: 0.000584\n",
      "Epoch 10255/40000, Loss: 8.343951776623726e-05, Learning Rate: 0.000584\n",
      "Epoch 10256/40000, Loss: 4.900144995190203e-05, Learning Rate: 0.000584\n",
      "Epoch 10257/40000, Loss: 5.8589375839801505e-05, Learning Rate: 0.000584\n",
      "Epoch 10258/40000, Loss: 8.270464604720473e-05, Learning Rate: 0.000584\n",
      "Epoch 10259/40000, Loss: 5.995218816678971e-05, Learning Rate: 0.000584\n",
      "Epoch 10260/40000, Loss: 7.113195897545666e-05, Learning Rate: 0.000584\n",
      "Epoch 10261/40000, Loss: 0.0001011049171211198, Learning Rate: 0.000584\n",
      "Epoch 10262/40000, Loss: 0.00010128440044354647, Learning Rate: 0.000584\n",
      "Epoch 10263/40000, Loss: 0.0002363709209021181, Learning Rate: 0.000584\n",
      "Epoch 10264/40000, Loss: 6.780395051464438e-05, Learning Rate: 0.000584\n",
      "Epoch 10265/40000, Loss: 0.00010653975914465263, Learning Rate: 0.000584\n",
      "Epoch 10266/40000, Loss: 7.082098454702646e-05, Learning Rate: 0.000583\n",
      "Epoch 10267/40000, Loss: 6.627594848396257e-05, Learning Rate: 0.000583\n",
      "Epoch 10268/40000, Loss: 9.503606997895986e-05, Learning Rate: 0.000583\n",
      "Epoch 10269/40000, Loss: 5.651472383760847e-05, Learning Rate: 0.000583\n",
      "Epoch 10270/40000, Loss: 4.969708970747888e-05, Learning Rate: 0.000583\n",
      "Epoch 10271/40000, Loss: 2.7776090064435266e-05, Learning Rate: 0.000583\n",
      "Epoch 10272/40000, Loss: 8.71466618264094e-05, Learning Rate: 0.000583\n",
      "Epoch 10273/40000, Loss: 3.9523631130577996e-05, Learning Rate: 0.000583\n",
      "Epoch 10274/40000, Loss: 8.839365909807384e-05, Learning Rate: 0.000583\n",
      "Epoch 10275/40000, Loss: 0.0001019993142108433, Learning Rate: 0.000583\n",
      "Epoch 10276/40000, Loss: 9.381426207255572e-05, Learning Rate: 0.000583\n",
      "Epoch 10277/40000, Loss: 9.135253640124574e-05, Learning Rate: 0.000583\n",
      "Epoch 10278/40000, Loss: 0.0001104294860851951, Learning Rate: 0.000583\n",
      "Epoch 10279/40000, Loss: 3.456381455180235e-05, Learning Rate: 0.000583\n",
      "Epoch 10280/40000, Loss: 0.00011631983943516389, Learning Rate: 0.000582\n",
      "Epoch 10281/40000, Loss: 9.510469681117684e-05, Learning Rate: 0.000582\n",
      "Epoch 10282/40000, Loss: 4.109077053726651e-05, Learning Rate: 0.000582\n",
      "Epoch 10283/40000, Loss: 3.089108213316649e-05, Learning Rate: 0.000582\n",
      "Epoch 10284/40000, Loss: 9.962663898477331e-05, Learning Rate: 0.000582\n",
      "Epoch 10285/40000, Loss: 3.271182504249737e-05, Learning Rate: 0.000582\n",
      "Epoch 10286/40000, Loss: 6.661845691269264e-05, Learning Rate: 0.000582\n",
      "Epoch 10287/40000, Loss: 5.8957928558811545e-05, Learning Rate: 0.000582\n",
      "Epoch 10288/40000, Loss: 4.9627709813648835e-05, Learning Rate: 0.000582\n",
      "Epoch 10289/40000, Loss: 2.4342092729057185e-05, Learning Rate: 0.000582\n",
      "Epoch 10290/40000, Loss: 8.28668853500858e-05, Learning Rate: 0.000582\n",
      "Epoch 10291/40000, Loss: 7.970091246534139e-05, Learning Rate: 0.000582\n",
      "Epoch 10292/40000, Loss: 6.00353887421079e-05, Learning Rate: 0.000582\n",
      "Epoch 10293/40000, Loss: 8.745765080675483e-05, Learning Rate: 0.000582\n",
      "Epoch 10294/40000, Loss: 5.407807111623697e-05, Learning Rate: 0.000581\n",
      "Epoch 10295/40000, Loss: 5.044385397923179e-05, Learning Rate: 0.000581\n",
      "Epoch 10296/40000, Loss: 4.6090273826848716e-05, Learning Rate: 0.000581\n",
      "Epoch 10297/40000, Loss: 2.3791519197402522e-05, Learning Rate: 0.000581\n",
      "Epoch 10298/40000, Loss: 2.2656653527519666e-05, Learning Rate: 0.000581\n",
      "Epoch 10299/40000, Loss: 2.2209105736692436e-05, Learning Rate: 0.000581\n",
      "Epoch 10300/40000, Loss: 2.1441537683131173e-05, Learning Rate: 0.000581\n",
      "Epoch 10301/40000, Loss: 5.4426604037871584e-05, Learning Rate: 0.000581\n",
      "Epoch 10302/40000, Loss: 5.1501607231330127e-05, Learning Rate: 0.000581\n",
      "Epoch 10303/40000, Loss: 5.732817226089537e-05, Learning Rate: 0.000581\n",
      "Epoch 10304/40000, Loss: 8.627851639175788e-05, Learning Rate: 0.000581\n",
      "Epoch 10305/40000, Loss: 2.154723188141361e-05, Learning Rate: 0.000581\n",
      "Epoch 10306/40000, Loss: 7.865680527174845e-05, Learning Rate: 0.000581\n",
      "Epoch 10307/40000, Loss: 5.349099956220016e-05, Learning Rate: 0.000581\n",
      "Epoch 10308/40000, Loss: 4.4941971282241866e-05, Learning Rate: 0.000581\n",
      "Epoch 10309/40000, Loss: 8.501583215547726e-05, Learning Rate: 0.000580\n",
      "Epoch 10310/40000, Loss: 7.531505252700299e-05, Learning Rate: 0.000580\n",
      "Epoch 10311/40000, Loss: 5.237925870460458e-05, Learning Rate: 0.000580\n",
      "Epoch 10312/40000, Loss: 5.965672244201414e-05, Learning Rate: 0.000580\n",
      "Epoch 10313/40000, Loss: 7.367586658801883e-05, Learning Rate: 0.000580\n",
      "Epoch 10314/40000, Loss: 4.681782957050018e-05, Learning Rate: 0.000580\n",
      "Epoch 10315/40000, Loss: 7.681779243284836e-05, Learning Rate: 0.000580\n",
      "Epoch 10316/40000, Loss: 5.714439248549752e-05, Learning Rate: 0.000580\n",
      "Epoch 10317/40000, Loss: 8.435436757281423e-05, Learning Rate: 0.000580\n",
      "Epoch 10318/40000, Loss: 2.433592453598976e-05, Learning Rate: 0.000580\n",
      "Epoch 10319/40000, Loss: 5.560224235523492e-05, Learning Rate: 0.000580\n",
      "Epoch 10320/40000, Loss: 6.166330422274768e-05, Learning Rate: 0.000580\n",
      "Epoch 10321/40000, Loss: 4.525074837147258e-05, Learning Rate: 0.000580\n",
      "Epoch 10322/40000, Loss: 7.451547571690753e-05, Learning Rate: 0.000580\n",
      "Epoch 10323/40000, Loss: 5.150129436515272e-05, Learning Rate: 0.000579\n",
      "Epoch 10324/40000, Loss: 8.44061651150696e-05, Learning Rate: 0.000579\n",
      "Epoch 10325/40000, Loss: 7.575316703878343e-05, Learning Rate: 0.000579\n",
      "Epoch 10326/40000, Loss: 9.377788956044242e-05, Learning Rate: 0.000579\n",
      "Epoch 10327/40000, Loss: 6.45604741293937e-05, Learning Rate: 0.000579\n",
      "Epoch 10328/40000, Loss: 5.6254644732689485e-05, Learning Rate: 0.000579\n",
      "Epoch 10329/40000, Loss: 9.06668501556851e-05, Learning Rate: 0.000579\n",
      "Epoch 10330/40000, Loss: 4.76018431072589e-05, Learning Rate: 0.000579\n",
      "Epoch 10331/40000, Loss: 0.00010638868843670934, Learning Rate: 0.000579\n",
      "Epoch 10332/40000, Loss: 5.3518411732511595e-05, Learning Rate: 0.000579\n",
      "Epoch 10333/40000, Loss: 9.499395673628896e-05, Learning Rate: 0.000579\n",
      "Epoch 10334/40000, Loss: 6.418496195692569e-05, Learning Rate: 0.000579\n",
      "Epoch 10335/40000, Loss: 5.500646147993393e-05, Learning Rate: 0.000579\n",
      "Epoch 10336/40000, Loss: 9.134376159636304e-05, Learning Rate: 0.000579\n",
      "Epoch 10337/40000, Loss: 4.8089590563904494e-05, Learning Rate: 0.000579\n",
      "Epoch 10338/40000, Loss: 3.041397940251045e-05, Learning Rate: 0.000578\n",
      "Epoch 10339/40000, Loss: 8.930472540669143e-05, Learning Rate: 0.000578\n",
      "Epoch 10340/40000, Loss: 3.427673436817713e-05, Learning Rate: 0.000578\n",
      "Epoch 10341/40000, Loss: 9.237397171091288e-05, Learning Rate: 0.000578\n",
      "Epoch 10342/40000, Loss: 6.543526251334697e-05, Learning Rate: 0.000578\n",
      "Epoch 10343/40000, Loss: 0.00012990030518267304, Learning Rate: 0.000578\n",
      "Epoch 10344/40000, Loss: 9.40439203986898e-05, Learning Rate: 0.000578\n",
      "Epoch 10345/40000, Loss: 0.00013509132259059697, Learning Rate: 0.000578\n",
      "Epoch 10346/40000, Loss: 3.3503347367513925e-05, Learning Rate: 0.000578\n",
      "Epoch 10347/40000, Loss: 0.0001062156370608136, Learning Rate: 0.000578\n",
      "Epoch 10348/40000, Loss: 5.407311618910171e-05, Learning Rate: 0.000578\n",
      "Epoch 10349/40000, Loss: 4.78740876133088e-05, Learning Rate: 0.000578\n",
      "Epoch 10350/40000, Loss: 6.219048373168334e-05, Learning Rate: 0.000578\n",
      "Epoch 10351/40000, Loss: 9.609833068680018e-05, Learning Rate: 0.000578\n",
      "Epoch 10352/40000, Loss: 2.8615624614758417e-05, Learning Rate: 0.000577\n",
      "Epoch 10353/40000, Loss: 4.794914275407791e-05, Learning Rate: 0.000577\n",
      "Epoch 10354/40000, Loss: 6.6384207457304e-05, Learning Rate: 0.000577\n",
      "Epoch 10355/40000, Loss: 6.443620804930106e-05, Learning Rate: 0.000577\n",
      "Epoch 10356/40000, Loss: 5.057496309746057e-05, Learning Rate: 0.000577\n",
      "Epoch 10357/40000, Loss: 5.904329373151995e-05, Learning Rate: 0.000577\n",
      "Epoch 10358/40000, Loss: 6.989293615333736e-05, Learning Rate: 0.000577\n",
      "Epoch 10359/40000, Loss: 6.07755318924319e-05, Learning Rate: 0.000577\n",
      "Epoch 10360/40000, Loss: 5.720467015635222e-05, Learning Rate: 0.000577\n",
      "Epoch 10361/40000, Loss: 8.318891923408955e-05, Learning Rate: 0.000577\n",
      "Epoch 10362/40000, Loss: 7.930218271212652e-05, Learning Rate: 0.000577\n",
      "Epoch 10363/40000, Loss: 6.046304406481795e-05, Learning Rate: 0.000577\n",
      "Epoch 10364/40000, Loss: 4.976483978680335e-05, Learning Rate: 0.000577\n",
      "Epoch 10365/40000, Loss: 2.8059577743988484e-05, Learning Rate: 0.000577\n",
      "Epoch 10366/40000, Loss: 7.736423140158877e-05, Learning Rate: 0.000576\n",
      "Epoch 10367/40000, Loss: 4.8254620196530595e-05, Learning Rate: 0.000576\n",
      "Epoch 10368/40000, Loss: 5.767498805653304e-05, Learning Rate: 0.000576\n",
      "Epoch 10369/40000, Loss: 2.317046710231807e-05, Learning Rate: 0.000576\n",
      "Epoch 10370/40000, Loss: 5.131988655193709e-05, Learning Rate: 0.000576\n",
      "Epoch 10371/40000, Loss: 5.6542525271652266e-05, Learning Rate: 0.000576\n",
      "Epoch 10372/40000, Loss: 8.380376675631851e-05, Learning Rate: 0.000576\n",
      "Epoch 10373/40000, Loss: 4.425474617164582e-05, Learning Rate: 0.000576\n",
      "Epoch 10374/40000, Loss: 4.368431109469384e-05, Learning Rate: 0.000576\n",
      "Epoch 10375/40000, Loss: 5.569994755205698e-05, Learning Rate: 0.000576\n",
      "Epoch 10376/40000, Loss: 4.323218308854848e-05, Learning Rate: 0.000576\n",
      "Epoch 10377/40000, Loss: 7.351910608122125e-05, Learning Rate: 0.000576\n",
      "Epoch 10378/40000, Loss: 2.186649726354517e-05, Learning Rate: 0.000576\n",
      "Epoch 10379/40000, Loss: 5.2612685976782814e-05, Learning Rate: 0.000576\n",
      "Epoch 10380/40000, Loss: 5.908598905079998e-05, Learning Rate: 0.000576\n",
      "Epoch 10381/40000, Loss: 2.2373078536475077e-05, Learning Rate: 0.000575\n",
      "Epoch 10382/40000, Loss: 8.507534221280366e-05, Learning Rate: 0.000575\n",
      "Epoch 10383/40000, Loss: 2.2533356968779117e-05, Learning Rate: 0.000575\n",
      "Epoch 10384/40000, Loss: 5.390947626437992e-05, Learning Rate: 0.000575\n",
      "Epoch 10385/40000, Loss: 5.106418757350184e-05, Learning Rate: 0.000575\n",
      "Epoch 10386/40000, Loss: 8.348820119863376e-05, Learning Rate: 0.000575\n",
      "Epoch 10387/40000, Loss: 2.3498480004491284e-05, Learning Rate: 0.000575\n",
      "Epoch 10388/40000, Loss: 2.0878233044641092e-05, Learning Rate: 0.000575\n",
      "Epoch 10389/40000, Loss: 7.575761992484331e-05, Learning Rate: 0.000575\n",
      "Epoch 10390/40000, Loss: 2.2852180336485617e-05, Learning Rate: 0.000575\n",
      "Epoch 10391/40000, Loss: 5.000711826141924e-05, Learning Rate: 0.000575\n",
      "Epoch 10392/40000, Loss: 8.482681005261838e-05, Learning Rate: 0.000575\n",
      "Epoch 10393/40000, Loss: 5.106403477839194e-05, Learning Rate: 0.000575\n",
      "Epoch 10394/40000, Loss: 5.957363828201778e-05, Learning Rate: 0.000575\n",
      "Epoch 10395/40000, Loss: 6.856143590994179e-05, Learning Rate: 0.000574\n",
      "Epoch 10396/40000, Loss: 5.2847579354420304e-05, Learning Rate: 0.000574\n",
      "Epoch 10397/40000, Loss: 6.763516284991056e-05, Learning Rate: 0.000574\n",
      "Epoch 10398/40000, Loss: 5.9607184084597975e-05, Learning Rate: 0.000574\n",
      "Epoch 10399/40000, Loss: 0.00010888542601605877, Learning Rate: 0.000574\n",
      "Epoch 10400/40000, Loss: 7.321765588130802e-05, Learning Rate: 0.000574\n",
      "Epoch 10401/40000, Loss: 9.060562297236174e-05, Learning Rate: 0.000574\n",
      "Epoch 10402/40000, Loss: 7.079777424223721e-05, Learning Rate: 0.000574\n",
      "Epoch 10403/40000, Loss: 5.616017006104812e-05, Learning Rate: 0.000574\n",
      "Epoch 10404/40000, Loss: 2.863597728719469e-05, Learning Rate: 0.000574\n",
      "Epoch 10405/40000, Loss: 9.583052451489493e-05, Learning Rate: 0.000574\n",
      "Epoch 10406/40000, Loss: 8.598226122558117e-05, Learning Rate: 0.000574\n",
      "Epoch 10407/40000, Loss: 6.175219459692016e-05, Learning Rate: 0.000574\n",
      "Epoch 10408/40000, Loss: 6.562760972883552e-05, Learning Rate: 0.000574\n",
      "Epoch 10409/40000, Loss: 7.054480374790728e-05, Learning Rate: 0.000574\n",
      "Epoch 10410/40000, Loss: 5.02041402796749e-05, Learning Rate: 0.000573\n",
      "Epoch 10411/40000, Loss: 9.603141370462254e-05, Learning Rate: 0.000573\n",
      "Epoch 10412/40000, Loss: 6.627136463066563e-05, Learning Rate: 0.000573\n",
      "Epoch 10413/40000, Loss: 2.867156763386447e-05, Learning Rate: 0.000573\n",
      "Epoch 10414/40000, Loss: 8.550834172638133e-05, Learning Rate: 0.000573\n",
      "Epoch 10415/40000, Loss: 8.370449359063059e-05, Learning Rate: 0.000573\n",
      "Epoch 10416/40000, Loss: 2.9810202249791473e-05, Learning Rate: 0.000573\n",
      "Epoch 10417/40000, Loss: 2.7739093638956547e-05, Learning Rate: 0.000573\n",
      "Epoch 10418/40000, Loss: 4.537383938441053e-05, Learning Rate: 0.000573\n",
      "Epoch 10419/40000, Loss: 6.352494528982788e-05, Learning Rate: 0.000573\n",
      "Epoch 10420/40000, Loss: 5.6801462051225826e-05, Learning Rate: 0.000573\n",
      "Epoch 10421/40000, Loss: 2.6392704967292957e-05, Learning Rate: 0.000573\n",
      "Epoch 10422/40000, Loss: 0.00010171333269681782, Learning Rate: 0.000573\n",
      "Epoch 10423/40000, Loss: 4.959592843079008e-05, Learning Rate: 0.000573\n",
      "Epoch 10424/40000, Loss: 7.188337622210383e-05, Learning Rate: 0.000572\n",
      "Epoch 10425/40000, Loss: 9.51866022660397e-05, Learning Rate: 0.000572\n",
      "Epoch 10426/40000, Loss: 3.344005381222814e-05, Learning Rate: 0.000572\n",
      "Epoch 10427/40000, Loss: 9.240579674951732e-05, Learning Rate: 0.000572\n",
      "Epoch 10428/40000, Loss: 5.7891473261406645e-05, Learning Rate: 0.000572\n",
      "Epoch 10429/40000, Loss: 3.2248099159915e-05, Learning Rate: 0.000572\n",
      "Epoch 10430/40000, Loss: 5.9298949054209515e-05, Learning Rate: 0.000572\n",
      "Epoch 10431/40000, Loss: 5.9012105339206755e-05, Learning Rate: 0.000572\n",
      "Epoch 10432/40000, Loss: 8.804542449070141e-05, Learning Rate: 0.000572\n",
      "Epoch 10433/40000, Loss: 5.6910903367679566e-05, Learning Rate: 0.000572\n",
      "Epoch 10434/40000, Loss: 7.931968866614625e-05, Learning Rate: 0.000572\n",
      "Epoch 10435/40000, Loss: 7.760946755297482e-05, Learning Rate: 0.000572\n",
      "Epoch 10436/40000, Loss: 4.9723617848940194e-05, Learning Rate: 0.000572\n",
      "Epoch 10437/40000, Loss: 8.822207018965855e-05, Learning Rate: 0.000572\n",
      "Epoch 10438/40000, Loss: 8.513991633662954e-05, Learning Rate: 0.000572\n",
      "Epoch 10439/40000, Loss: 7.473998994100839e-05, Learning Rate: 0.000571\n",
      "Epoch 10440/40000, Loss: 9.427566692465916e-05, Learning Rate: 0.000571\n",
      "Epoch 10441/40000, Loss: 2.2793134121457115e-05, Learning Rate: 0.000571\n",
      "Epoch 10442/40000, Loss: 5.676543150912039e-05, Learning Rate: 0.000571\n",
      "Epoch 10443/40000, Loss: 8.801003423286602e-05, Learning Rate: 0.000571\n",
      "Epoch 10444/40000, Loss: 5.579504795605317e-05, Learning Rate: 0.000571\n",
      "Epoch 10445/40000, Loss: 4.9753776693250984e-05, Learning Rate: 0.000571\n",
      "Epoch 10446/40000, Loss: 8.351781434612349e-05, Learning Rate: 0.000571\n",
      "Epoch 10447/40000, Loss: 7.677548273932189e-05, Learning Rate: 0.000571\n",
      "Epoch 10448/40000, Loss: 2.261482586618513e-05, Learning Rate: 0.000571\n",
      "Epoch 10449/40000, Loss: 0.00010264458251185715, Learning Rate: 0.000571\n",
      "Epoch 10450/40000, Loss: 5.819279977004044e-05, Learning Rate: 0.000571\n",
      "Epoch 10451/40000, Loss: 4.680421625380404e-05, Learning Rate: 0.000571\n",
      "Epoch 10452/40000, Loss: 8.336374594364315e-05, Learning Rate: 0.000571\n",
      "Epoch 10453/40000, Loss: 5.196328856982291e-05, Learning Rate: 0.000571\n",
      "Epoch 10454/40000, Loss: 4.559510125545785e-05, Learning Rate: 0.000570\n",
      "Epoch 10455/40000, Loss: 4.452165012480691e-05, Learning Rate: 0.000570\n",
      "Epoch 10456/40000, Loss: 5.6822354963514954e-05, Learning Rate: 0.000570\n",
      "Epoch 10457/40000, Loss: 5.573003363679163e-05, Learning Rate: 0.000570\n",
      "Epoch 10458/40000, Loss: 2.5433331757085398e-05, Learning Rate: 0.000570\n",
      "Epoch 10459/40000, Loss: 5.504637010744773e-05, Learning Rate: 0.000570\n",
      "Epoch 10460/40000, Loss: 5.959279587841593e-05, Learning Rate: 0.000570\n",
      "Epoch 10461/40000, Loss: 7.564573752461001e-05, Learning Rate: 0.000570\n",
      "Epoch 10462/40000, Loss: 7.289731001947075e-05, Learning Rate: 0.000570\n",
      "Epoch 10463/40000, Loss: 4.4754975533578545e-05, Learning Rate: 0.000570\n",
      "Epoch 10464/40000, Loss: 4.3986095988657326e-05, Learning Rate: 0.000570\n",
      "Epoch 10465/40000, Loss: 5.3505515097640455e-05, Learning Rate: 0.000570\n",
      "Epoch 10466/40000, Loss: 5.6930995924631134e-05, Learning Rate: 0.000570\n",
      "Epoch 10467/40000, Loss: 6.824751471867785e-05, Learning Rate: 0.000570\n",
      "Epoch 10468/40000, Loss: 2.4679058697074652e-05, Learning Rate: 0.000569\n",
      "Epoch 10469/40000, Loss: 5.593522291746922e-05, Learning Rate: 0.000569\n",
      "Epoch 10470/40000, Loss: 4.455355883692391e-05, Learning Rate: 0.000569\n",
      "Epoch 10471/40000, Loss: 7.487070979550481e-05, Learning Rate: 0.000569\n",
      "Epoch 10472/40000, Loss: 7.406191434711218e-05, Learning Rate: 0.000569\n",
      "Epoch 10473/40000, Loss: 5.2514515118673444e-05, Learning Rate: 0.000569\n",
      "Epoch 10474/40000, Loss: 5.263635830488056e-05, Learning Rate: 0.000569\n",
      "Epoch 10475/40000, Loss: 5.573955422732979e-05, Learning Rate: 0.000569\n",
      "Epoch 10476/40000, Loss: 8.642077591503039e-05, Learning Rate: 0.000569\n",
      "Epoch 10477/40000, Loss: 5.846807835041545e-05, Learning Rate: 0.000569\n",
      "Epoch 10478/40000, Loss: 8.236638677772135e-05, Learning Rate: 0.000569\n",
      "Epoch 10479/40000, Loss: 5.477039303514175e-05, Learning Rate: 0.000569\n",
      "Epoch 10480/40000, Loss: 2.5306697352789342e-05, Learning Rate: 0.000569\n",
      "Epoch 10481/40000, Loss: 5.680759568349458e-05, Learning Rate: 0.000569\n",
      "Epoch 10482/40000, Loss: 8.675502613186836e-05, Learning Rate: 0.000569\n",
      "Epoch 10483/40000, Loss: 5.4636893764836714e-05, Learning Rate: 0.000568\n",
      "Epoch 10484/40000, Loss: 9.963024058379233e-05, Learning Rate: 0.000568\n",
      "Epoch 10485/40000, Loss: 9.700388181954622e-05, Learning Rate: 0.000568\n",
      "Epoch 10486/40000, Loss: 5.122402581037022e-05, Learning Rate: 0.000568\n",
      "Epoch 10487/40000, Loss: 6.057998689357191e-05, Learning Rate: 0.000568\n",
      "Epoch 10488/40000, Loss: 2.452324770274572e-05, Learning Rate: 0.000568\n",
      "Epoch 10489/40000, Loss: 2.243485869257711e-05, Learning Rate: 0.000568\n",
      "Epoch 10490/40000, Loss: 4.922756852465682e-05, Learning Rate: 0.000568\n",
      "Epoch 10491/40000, Loss: 6.259060319280252e-05, Learning Rate: 0.000568\n",
      "Epoch 10492/40000, Loss: 8.759833144722506e-05, Learning Rate: 0.000568\n",
      "Epoch 10493/40000, Loss: 9.12933173822239e-05, Learning Rate: 0.000568\n",
      "Epoch 10494/40000, Loss: 6.265240517677739e-05, Learning Rate: 0.000568\n",
      "Epoch 10495/40000, Loss: 4.1785675421124324e-05, Learning Rate: 0.000568\n",
      "Epoch 10496/40000, Loss: 2.9834463930455968e-05, Learning Rate: 0.000568\n",
      "Epoch 10497/40000, Loss: 9.661827061790973e-05, Learning Rate: 0.000568\n",
      "Epoch 10498/40000, Loss: 8.583833550801501e-05, Learning Rate: 0.000567\n",
      "Epoch 10499/40000, Loss: 9.603070793673396e-05, Learning Rate: 0.000567\n",
      "Epoch 10500/40000, Loss: 0.00011936104419874027, Learning Rate: 0.000567\n",
      "Epoch 10501/40000, Loss: 8.1110410974361e-05, Learning Rate: 0.000567\n",
      "Epoch 10502/40000, Loss: 5.289026375976391e-05, Learning Rate: 0.000567\n",
      "Epoch 10503/40000, Loss: 9.039411816047505e-05, Learning Rate: 0.000567\n",
      "Epoch 10504/40000, Loss: 5.6965305702760816e-05, Learning Rate: 0.000567\n",
      "Epoch 10505/40000, Loss: 6.147778913145885e-05, Learning Rate: 0.000567\n",
      "Epoch 10506/40000, Loss: 6.0431364545365795e-05, Learning Rate: 0.000567\n",
      "Epoch 10507/40000, Loss: 5.402204624260776e-05, Learning Rate: 0.000567\n",
      "Epoch 10508/40000, Loss: 6.539228343171999e-05, Learning Rate: 0.000567\n",
      "Epoch 10509/40000, Loss: 0.00013499349006451666, Learning Rate: 0.000567\n",
      "Epoch 10510/40000, Loss: 6.181722710607573e-05, Learning Rate: 0.000567\n",
      "Epoch 10511/40000, Loss: 2.8185546398162842e-05, Learning Rate: 0.000567\n",
      "Epoch 10512/40000, Loss: 8.611693192506209e-05, Learning Rate: 0.000566\n",
      "Epoch 10513/40000, Loss: 7.6924407039769e-05, Learning Rate: 0.000566\n",
      "Epoch 10514/40000, Loss: 7.35321591491811e-05, Learning Rate: 0.000566\n",
      "Epoch 10515/40000, Loss: 8.770170825300738e-05, Learning Rate: 0.000566\n",
      "Epoch 10516/40000, Loss: 5.785235043731518e-05, Learning Rate: 0.000566\n",
      "Epoch 10517/40000, Loss: 5.054816938354634e-05, Learning Rate: 0.000566\n",
      "Epoch 10518/40000, Loss: 7.589824963361025e-05, Learning Rate: 0.000566\n",
      "Epoch 10519/40000, Loss: 7.424058276228607e-05, Learning Rate: 0.000566\n",
      "Epoch 10520/40000, Loss: 5.3664854931412265e-05, Learning Rate: 0.000566\n",
      "Epoch 10521/40000, Loss: 7.457723404513672e-05, Learning Rate: 0.000566\n",
      "Epoch 10522/40000, Loss: 8.334970334544778e-05, Learning Rate: 0.000566\n",
      "Epoch 10523/40000, Loss: 5.3606850997311994e-05, Learning Rate: 0.000566\n",
      "Epoch 10524/40000, Loss: 8.006377902347594e-05, Learning Rate: 0.000566\n",
      "Epoch 10525/40000, Loss: 2.3249624064192176e-05, Learning Rate: 0.000566\n",
      "Epoch 10526/40000, Loss: 0.00011468839511508122, Learning Rate: 0.000566\n",
      "Epoch 10527/40000, Loss: 4.816270666196942e-05, Learning Rate: 0.000565\n",
      "Epoch 10528/40000, Loss: 4.439024269231595e-05, Learning Rate: 0.000565\n",
      "Epoch 10529/40000, Loss: 5.3554776968667284e-05, Learning Rate: 0.000565\n",
      "Epoch 10530/40000, Loss: 4.621863627107814e-05, Learning Rate: 0.000565\n",
      "Epoch 10531/40000, Loss: 2.258615495520644e-05, Learning Rate: 0.000565\n",
      "Epoch 10532/40000, Loss: 4.985579289495945e-05, Learning Rate: 0.000565\n",
      "Epoch 10533/40000, Loss: 4.412220005178824e-05, Learning Rate: 0.000565\n",
      "Epoch 10534/40000, Loss: 6.099138772697188e-05, Learning Rate: 0.000565\n",
      "Epoch 10535/40000, Loss: 4.436811650521122e-05, Learning Rate: 0.000565\n",
      "Epoch 10536/40000, Loss: 4.430805711308494e-05, Learning Rate: 0.000565\n",
      "Epoch 10537/40000, Loss: 8.332869037985802e-05, Learning Rate: 0.000565\n",
      "Epoch 10538/40000, Loss: 5.569674976868555e-05, Learning Rate: 0.000565\n",
      "Epoch 10539/40000, Loss: 5.571884321398102e-05, Learning Rate: 0.000565\n",
      "Epoch 10540/40000, Loss: 4.8966958274831995e-05, Learning Rate: 0.000565\n",
      "Epoch 10541/40000, Loss: 7.385920616798103e-05, Learning Rate: 0.000565\n",
      "Epoch 10542/40000, Loss: 4.389008608995937e-05, Learning Rate: 0.000564\n",
      "Epoch 10543/40000, Loss: 8.469432941637933e-05, Learning Rate: 0.000564\n",
      "Epoch 10544/40000, Loss: 5.660963142872788e-05, Learning Rate: 0.000564\n",
      "Epoch 10545/40000, Loss: 7.528857531724498e-05, Learning Rate: 0.000564\n",
      "Epoch 10546/40000, Loss: 2.2756605176255107e-05, Learning Rate: 0.000564\n",
      "Epoch 10547/40000, Loss: 4.5965185563545674e-05, Learning Rate: 0.000564\n",
      "Epoch 10548/40000, Loss: 4.576509309117682e-05, Learning Rate: 0.000564\n",
      "Epoch 10549/40000, Loss: 5.6990160373970866e-05, Learning Rate: 0.000564\n",
      "Epoch 10550/40000, Loss: 5.712230267818086e-05, Learning Rate: 0.000564\n",
      "Epoch 10551/40000, Loss: 5.051292828284204e-05, Learning Rate: 0.000564\n",
      "Epoch 10552/40000, Loss: 5.882461118744686e-05, Learning Rate: 0.000564\n",
      "Epoch 10553/40000, Loss: 4.468995030038059e-05, Learning Rate: 0.000564\n",
      "Epoch 10554/40000, Loss: 4.5550874347100034e-05, Learning Rate: 0.000564\n",
      "Epoch 10555/40000, Loss: 5.4402371461037546e-05, Learning Rate: 0.000564\n",
      "Epoch 10556/40000, Loss: 3.352424755576067e-05, Learning Rate: 0.000564\n",
      "Epoch 10557/40000, Loss: 7.871584966778755e-05, Learning Rate: 0.000563\n",
      "Epoch 10558/40000, Loss: 5.4286829254124314e-05, Learning Rate: 0.000563\n",
      "Epoch 10559/40000, Loss: 7.068578997859731e-05, Learning Rate: 0.000563\n",
      "Epoch 10560/40000, Loss: 8.8200242316816e-05, Learning Rate: 0.000563\n",
      "Epoch 10561/40000, Loss: 0.00011276208533672616, Learning Rate: 0.000563\n",
      "Epoch 10562/40000, Loss: 3.9141606976045296e-05, Learning Rate: 0.000563\n",
      "Epoch 10563/40000, Loss: 7.04914637026377e-05, Learning Rate: 0.000563\n",
      "Epoch 10564/40000, Loss: 7.76091983425431e-05, Learning Rate: 0.000563\n",
      "Epoch 10565/40000, Loss: 0.0001223817525897175, Learning Rate: 0.000563\n",
      "Epoch 10566/40000, Loss: 0.00015107366198208183, Learning Rate: 0.000563\n",
      "Epoch 10567/40000, Loss: 3.757259401027113e-05, Learning Rate: 0.000563\n",
      "Epoch 10568/40000, Loss: 9.914915426634252e-05, Learning Rate: 0.000563\n",
      "Epoch 10569/40000, Loss: 7.317190465983003e-05, Learning Rate: 0.000563\n",
      "Epoch 10570/40000, Loss: 7.814721175236627e-05, Learning Rate: 0.000563\n",
      "Epoch 10571/40000, Loss: 6.166339881019667e-05, Learning Rate: 0.000562\n",
      "Epoch 10572/40000, Loss: 9.527181828161702e-05, Learning Rate: 0.000562\n",
      "Epoch 10573/40000, Loss: 3.056703644688241e-05, Learning Rate: 0.000562\n",
      "Epoch 10574/40000, Loss: 0.00015350818284787238, Learning Rate: 0.000562\n",
      "Epoch 10575/40000, Loss: 9.004736057249829e-05, Learning Rate: 0.000562\n",
      "Epoch 10576/40000, Loss: 5.980101559543982e-05, Learning Rate: 0.000562\n",
      "Epoch 10577/40000, Loss: 4.856686791754328e-05, Learning Rate: 0.000562\n",
      "Epoch 10578/40000, Loss: 2.573188248788938e-05, Learning Rate: 0.000562\n",
      "Epoch 10579/40000, Loss: 5.9661011619027704e-05, Learning Rate: 0.000562\n",
      "Epoch 10580/40000, Loss: 9.103037882596254e-05, Learning Rate: 0.000562\n",
      "Epoch 10581/40000, Loss: 4.645678563974798e-05, Learning Rate: 0.000562\n",
      "Epoch 10582/40000, Loss: 7.40070900064893e-05, Learning Rate: 0.000562\n",
      "Epoch 10583/40000, Loss: 5.431169847724959e-05, Learning Rate: 0.000562\n",
      "Epoch 10584/40000, Loss: 2.2663743948214687e-05, Learning Rate: 0.000562\n",
      "Epoch 10585/40000, Loss: 2.1538919099839404e-05, Learning Rate: 0.000562\n",
      "Epoch 10586/40000, Loss: 4.8340356443077326e-05, Learning Rate: 0.000561\n",
      "Epoch 10587/40000, Loss: 4.4734297262039036e-05, Learning Rate: 0.000561\n",
      "Epoch 10588/40000, Loss: 5.4681011533830315e-05, Learning Rate: 0.000561\n",
      "Epoch 10589/40000, Loss: 5.1864382839994505e-05, Learning Rate: 0.000561\n",
      "Epoch 10590/40000, Loss: 8.322451321873814e-05, Learning Rate: 0.000561\n",
      "Epoch 10591/40000, Loss: 6.447584746638313e-05, Learning Rate: 0.000561\n",
      "Epoch 10592/40000, Loss: 8.388897549593821e-05, Learning Rate: 0.000561\n",
      "Epoch 10593/40000, Loss: 6.935639976290986e-05, Learning Rate: 0.000561\n",
      "Epoch 10594/40000, Loss: 8.273977437056601e-05, Learning Rate: 0.000561\n",
      "Epoch 10595/40000, Loss: 4.343078035162762e-05, Learning Rate: 0.000561\n",
      "Epoch 10596/40000, Loss: 5.1990911742905155e-05, Learning Rate: 0.000561\n",
      "Epoch 10597/40000, Loss: 8.163031452568248e-05, Learning Rate: 0.000561\n",
      "Epoch 10598/40000, Loss: 4.489609636948444e-05, Learning Rate: 0.000561\n",
      "Epoch 10599/40000, Loss: 2.04285497602541e-05, Learning Rate: 0.000561\n",
      "Epoch 10600/40000, Loss: 5.5011259973980486e-05, Learning Rate: 0.000561\n",
      "Epoch 10601/40000, Loss: 4.290935612516478e-05, Learning Rate: 0.000560\n",
      "Epoch 10602/40000, Loss: 2.016751022893004e-05, Learning Rate: 0.000560\n",
      "Epoch 10603/40000, Loss: 2.141571167157963e-05, Learning Rate: 0.000560\n",
      "Epoch 10604/40000, Loss: 2.1972349713905714e-05, Learning Rate: 0.000560\n",
      "Epoch 10605/40000, Loss: 5.45284929103218e-05, Learning Rate: 0.000560\n",
      "Epoch 10606/40000, Loss: 7.179834210546687e-05, Learning Rate: 0.000560\n",
      "Epoch 10607/40000, Loss: 5.4257645388133824e-05, Learning Rate: 0.000560\n",
      "Epoch 10608/40000, Loss: 4.887442264589481e-05, Learning Rate: 0.000560\n",
      "Epoch 10609/40000, Loss: 5.427110954769887e-05, Learning Rate: 0.000560\n",
      "Epoch 10610/40000, Loss: 4.3348387407604605e-05, Learning Rate: 0.000560\n",
      "Epoch 10611/40000, Loss: 7.096412446117029e-05, Learning Rate: 0.000560\n",
      "Epoch 10612/40000, Loss: 7.141116657294333e-05, Learning Rate: 0.000560\n",
      "Epoch 10613/40000, Loss: 7.230178744066507e-05, Learning Rate: 0.000560\n",
      "Epoch 10614/40000, Loss: 4.336956772021949e-05, Learning Rate: 0.000560\n",
      "Epoch 10615/40000, Loss: 5.443298505269922e-05, Learning Rate: 0.000560\n",
      "Epoch 10616/40000, Loss: 4.405413710628636e-05, Learning Rate: 0.000559\n",
      "Epoch 10617/40000, Loss: 5.428367876447737e-05, Learning Rate: 0.000559\n",
      "Epoch 10618/40000, Loss: 8.22703295852989e-05, Learning Rate: 0.000559\n",
      "Epoch 10619/40000, Loss: 2.039924220298417e-05, Learning Rate: 0.000559\n",
      "Epoch 10620/40000, Loss: 2.0485600543906912e-05, Learning Rate: 0.000559\n",
      "Epoch 10621/40000, Loss: 5.4987431212794036e-05, Learning Rate: 0.000559\n",
      "Epoch 10622/40000, Loss: 2.0283227058826014e-05, Learning Rate: 0.000559\n",
      "Epoch 10623/40000, Loss: 8.429250738117844e-05, Learning Rate: 0.000559\n",
      "Epoch 10624/40000, Loss: 2.1397672753664665e-05, Learning Rate: 0.000559\n",
      "Epoch 10625/40000, Loss: 4.402118793223053e-05, Learning Rate: 0.000559\n",
      "Epoch 10626/40000, Loss: 2.12545146496268e-05, Learning Rate: 0.000559\n",
      "Epoch 10627/40000, Loss: 5.007581421523355e-05, Learning Rate: 0.000559\n",
      "Epoch 10628/40000, Loss: 5.499688631971367e-05, Learning Rate: 0.000559\n",
      "Epoch 10629/40000, Loss: 4.346567948232405e-05, Learning Rate: 0.000559\n",
      "Epoch 10630/40000, Loss: 5.744564259657636e-05, Learning Rate: 0.000559\n",
      "Epoch 10631/40000, Loss: 2.2960990463616326e-05, Learning Rate: 0.000558\n",
      "Epoch 10632/40000, Loss: 8.518422691849992e-05, Learning Rate: 0.000558\n",
      "Epoch 10633/40000, Loss: 5.706778756575659e-05, Learning Rate: 0.000558\n",
      "Epoch 10634/40000, Loss: 5.1734390581259504e-05, Learning Rate: 0.000558\n",
      "Epoch 10635/40000, Loss: 6.503971962956712e-05, Learning Rate: 0.000558\n",
      "Epoch 10636/40000, Loss: 2.3445907572750002e-05, Learning Rate: 0.000558\n",
      "Epoch 10637/40000, Loss: 5.876798968529329e-05, Learning Rate: 0.000558\n",
      "Epoch 10638/40000, Loss: 5.061187766841613e-05, Learning Rate: 0.000558\n",
      "Epoch 10639/40000, Loss: 7.80025584390387e-05, Learning Rate: 0.000558\n",
      "Epoch 10640/40000, Loss: 4.486381294555031e-05, Learning Rate: 0.000558\n",
      "Epoch 10641/40000, Loss: 2.195066917920485e-05, Learning Rate: 0.000558\n",
      "Epoch 10642/40000, Loss: 5.787460759165697e-05, Learning Rate: 0.000558\n",
      "Epoch 10643/40000, Loss: 3.0198627428035252e-05, Learning Rate: 0.000558\n",
      "Epoch 10644/40000, Loss: 5.49086689716205e-05, Learning Rate: 0.000558\n",
      "Epoch 10645/40000, Loss: 5.266071457299404e-05, Learning Rate: 0.000558\n",
      "Epoch 10646/40000, Loss: 4.5403805415844545e-05, Learning Rate: 0.000557\n",
      "Epoch 10647/40000, Loss: 2.483485877746716e-05, Learning Rate: 0.000557\n",
      "Epoch 10648/40000, Loss: 5.709056495106779e-05, Learning Rate: 0.000557\n",
      "Epoch 10649/40000, Loss: 8.222378528444096e-05, Learning Rate: 0.000557\n",
      "Epoch 10650/40000, Loss: 6.097068762755953e-05, Learning Rate: 0.000557\n",
      "Epoch 10651/40000, Loss: 2.801551454467699e-05, Learning Rate: 0.000557\n",
      "Epoch 10652/40000, Loss: 8.955688099376857e-05, Learning Rate: 0.000557\n",
      "Epoch 10653/40000, Loss: 4.6429668145719916e-05, Learning Rate: 0.000557\n",
      "Epoch 10654/40000, Loss: 7.978794019436464e-05, Learning Rate: 0.000557\n",
      "Epoch 10655/40000, Loss: 5.807965135318227e-05, Learning Rate: 0.000557\n",
      "Epoch 10656/40000, Loss: 4.8643763875588775e-05, Learning Rate: 0.000557\n",
      "Epoch 10657/40000, Loss: 5.619041621685028e-05, Learning Rate: 0.000557\n",
      "Epoch 10658/40000, Loss: 5.5132411944214255e-05, Learning Rate: 0.000557\n",
      "Epoch 10659/40000, Loss: 4.845000148634426e-05, Learning Rate: 0.000557\n",
      "Epoch 10660/40000, Loss: 2.5114895834121853e-05, Learning Rate: 0.000557\n",
      "Epoch 10661/40000, Loss: 9.094785491470248e-05, Learning Rate: 0.000556\n",
      "Epoch 10662/40000, Loss: 7.763078610878438e-05, Learning Rate: 0.000556\n",
      "Epoch 10663/40000, Loss: 2.451946602377575e-05, Learning Rate: 0.000556\n",
      "Epoch 10664/40000, Loss: 2.662215229065623e-05, Learning Rate: 0.000556\n",
      "Epoch 10665/40000, Loss: 2.6040048396680504e-05, Learning Rate: 0.000556\n",
      "Epoch 10666/40000, Loss: 0.00010760851000668481, Learning Rate: 0.000556\n",
      "Epoch 10667/40000, Loss: 5.919127579545602e-05, Learning Rate: 0.000556\n",
      "Epoch 10668/40000, Loss: 2.5838447982096113e-05, Learning Rate: 0.000556\n",
      "Epoch 10669/40000, Loss: 0.0001327687787124887, Learning Rate: 0.000556\n",
      "Epoch 10670/40000, Loss: 5.953274012426846e-05, Learning Rate: 0.000556\n",
      "Epoch 10671/40000, Loss: 9.428832709090784e-05, Learning Rate: 0.000556\n",
      "Epoch 10672/40000, Loss: 5.020928438170813e-05, Learning Rate: 0.000556\n",
      "Epoch 10673/40000, Loss: 5.319901538314298e-05, Learning Rate: 0.000556\n",
      "Epoch 10674/40000, Loss: 2.7386678993934765e-05, Learning Rate: 0.000556\n",
      "Epoch 10675/40000, Loss: 8.515866647940129e-05, Learning Rate: 0.000556\n",
      "Epoch 10676/40000, Loss: 6.495594425359741e-05, Learning Rate: 0.000555\n",
      "Epoch 10677/40000, Loss: 5.385224358178675e-05, Learning Rate: 0.000555\n",
      "Epoch 10678/40000, Loss: 3.037838905584067e-05, Learning Rate: 0.000555\n",
      "Epoch 10679/40000, Loss: 0.00010281062714057043, Learning Rate: 0.000555\n",
      "Epoch 10680/40000, Loss: 9.014915121952072e-05, Learning Rate: 0.000555\n",
      "Epoch 10681/40000, Loss: 8.781789802014828e-05, Learning Rate: 0.000555\n",
      "Epoch 10682/40000, Loss: 2.5003115297295153e-05, Learning Rate: 0.000555\n",
      "Epoch 10683/40000, Loss: 8.646847709314898e-05, Learning Rate: 0.000555\n",
      "Epoch 10684/40000, Loss: 6.31187140243128e-05, Learning Rate: 0.000555\n",
      "Epoch 10685/40000, Loss: 2.463039527356159e-05, Learning Rate: 0.000555\n",
      "Epoch 10686/40000, Loss: 7.557905337307602e-05, Learning Rate: 0.000555\n",
      "Epoch 10687/40000, Loss: 4.5512220822274685e-05, Learning Rate: 0.000555\n",
      "Epoch 10688/40000, Loss: 5.501320993062109e-05, Learning Rate: 0.000555\n",
      "Epoch 10689/40000, Loss: 7.324518810492009e-05, Learning Rate: 0.000555\n",
      "Epoch 10690/40000, Loss: 4.372786497697234e-05, Learning Rate: 0.000555\n",
      "Epoch 10691/40000, Loss: 7.45238721719943e-05, Learning Rate: 0.000554\n",
      "Epoch 10692/40000, Loss: 5.4988337069517e-05, Learning Rate: 0.000554\n",
      "Epoch 10693/40000, Loss: 7.055910100461915e-05, Learning Rate: 0.000554\n",
      "Epoch 10694/40000, Loss: 2.11812603083672e-05, Learning Rate: 0.000554\n",
      "Epoch 10695/40000, Loss: 8.099952538032085e-05, Learning Rate: 0.000554\n",
      "Epoch 10696/40000, Loss: 2.1714455215260386e-05, Learning Rate: 0.000554\n",
      "Epoch 10697/40000, Loss: 7.029785047052428e-05, Learning Rate: 0.000554\n",
      "Epoch 10698/40000, Loss: 4.8146532208193094e-05, Learning Rate: 0.000554\n",
      "Epoch 10699/40000, Loss: 7.021379133220762e-05, Learning Rate: 0.000554\n",
      "Epoch 10700/40000, Loss: 4.853442806052044e-05, Learning Rate: 0.000554\n",
      "Epoch 10701/40000, Loss: 5.370633880374953e-05, Learning Rate: 0.000554\n",
      "Epoch 10702/40000, Loss: 4.4420510675990954e-05, Learning Rate: 0.000554\n",
      "Epoch 10703/40000, Loss: 8.124165469780564e-05, Learning Rate: 0.000554\n",
      "Epoch 10704/40000, Loss: 8.09432240203023e-05, Learning Rate: 0.000554\n",
      "Epoch 10705/40000, Loss: 4.4407199311535805e-05, Learning Rate: 0.000554\n",
      "Epoch 10706/40000, Loss: 8.40013162815012e-05, Learning Rate: 0.000553\n",
      "Epoch 10707/40000, Loss: 5.347783007891849e-05, Learning Rate: 0.000553\n",
      "Epoch 10708/40000, Loss: 5.322927609086037e-05, Learning Rate: 0.000553\n",
      "Epoch 10709/40000, Loss: 8.229542436311021e-05, Learning Rate: 0.000553\n",
      "Epoch 10710/40000, Loss: 7.089679274940863e-05, Learning Rate: 0.000553\n",
      "Epoch 10711/40000, Loss: 4.8385078116552904e-05, Learning Rate: 0.000553\n",
      "Epoch 10712/40000, Loss: 7.035165617708117e-05, Learning Rate: 0.000553\n",
      "Epoch 10713/40000, Loss: 8.260294998763129e-05, Learning Rate: 0.000553\n",
      "Epoch 10714/40000, Loss: 5.0265505706192926e-05, Learning Rate: 0.000553\n",
      "Epoch 10715/40000, Loss: 2.0617193513317034e-05, Learning Rate: 0.000553\n",
      "Epoch 10716/40000, Loss: 7.080091017996892e-05, Learning Rate: 0.000553\n",
      "Epoch 10717/40000, Loss: 8.345525566255674e-05, Learning Rate: 0.000553\n",
      "Epoch 10718/40000, Loss: 2.220966234744992e-05, Learning Rate: 0.000553\n",
      "Epoch 10719/40000, Loss: 4.990492743672803e-05, Learning Rate: 0.000553\n",
      "Epoch 10720/40000, Loss: 5.384914402384311e-05, Learning Rate: 0.000553\n",
      "Epoch 10721/40000, Loss: 2.4929480787250213e-05, Learning Rate: 0.000552\n",
      "Epoch 10722/40000, Loss: 5.8289326261729e-05, Learning Rate: 0.000552\n",
      "Epoch 10723/40000, Loss: 5.4111274948809296e-05, Learning Rate: 0.000552\n",
      "Epoch 10724/40000, Loss: 5.5475771659985185e-05, Learning Rate: 0.000552\n",
      "Epoch 10725/40000, Loss: 9.133340063272044e-05, Learning Rate: 0.000552\n",
      "Epoch 10726/40000, Loss: 4.8890316975302994e-05, Learning Rate: 0.000552\n",
      "Epoch 10727/40000, Loss: 4.65191260445863e-05, Learning Rate: 0.000552\n",
      "Epoch 10728/40000, Loss: 0.00010669352195691317, Learning Rate: 0.000552\n",
      "Epoch 10729/40000, Loss: 8.898127998691052e-05, Learning Rate: 0.000552\n",
      "Epoch 10730/40000, Loss: 4.131724563194439e-05, Learning Rate: 0.000552\n",
      "Epoch 10731/40000, Loss: 0.0001167564550996758, Learning Rate: 0.000552\n",
      "Epoch 10732/40000, Loss: 0.00012230021820869297, Learning Rate: 0.000552\n",
      "Epoch 10733/40000, Loss: 7.299600110854954e-05, Learning Rate: 0.000552\n",
      "Epoch 10734/40000, Loss: 0.0002590317162685096, Learning Rate: 0.000552\n",
      "Epoch 10735/40000, Loss: 0.00012147106463089585, Learning Rate: 0.000552\n",
      "Epoch 10736/40000, Loss: 0.00010609914170345291, Learning Rate: 0.000551\n",
      "Epoch 10737/40000, Loss: 9.50306566664949e-05, Learning Rate: 0.000551\n",
      "Epoch 10738/40000, Loss: 6.287163705565035e-05, Learning Rate: 0.000551\n",
      "Epoch 10739/40000, Loss: 5.5905693443492055e-05, Learning Rate: 0.000551\n",
      "Epoch 10740/40000, Loss: 9.34641866479069e-05, Learning Rate: 0.000551\n",
      "Epoch 10741/40000, Loss: 9.252231393475085e-05, Learning Rate: 0.000551\n",
      "Epoch 10742/40000, Loss: 5.813449024572037e-05, Learning Rate: 0.000551\n",
      "Epoch 10743/40000, Loss: 9.540246537653729e-05, Learning Rate: 0.000551\n",
      "Epoch 10744/40000, Loss: 7.81694398028776e-05, Learning Rate: 0.000551\n",
      "Epoch 10745/40000, Loss: 6.100511382101104e-05, Learning Rate: 0.000551\n",
      "Epoch 10746/40000, Loss: 5.67679489904549e-05, Learning Rate: 0.000551\n",
      "Epoch 10747/40000, Loss: 5.6048269470920786e-05, Learning Rate: 0.000551\n",
      "Epoch 10748/40000, Loss: 4.6012883103685454e-05, Learning Rate: 0.000551\n",
      "Epoch 10749/40000, Loss: 2.246448457299266e-05, Learning Rate: 0.000551\n",
      "Epoch 10750/40000, Loss: 5.8660290960688144e-05, Learning Rate: 0.000551\n",
      "Epoch 10751/40000, Loss: 7.278856355696917e-05, Learning Rate: 0.000550\n",
      "Epoch 10752/40000, Loss: 2.2494086806545965e-05, Learning Rate: 0.000550\n",
      "Epoch 10753/40000, Loss: 5.761989814345725e-05, Learning Rate: 0.000550\n",
      "Epoch 10754/40000, Loss: 6.298947846516967e-05, Learning Rate: 0.000550\n",
      "Epoch 10755/40000, Loss: 2.8460017347242683e-05, Learning Rate: 0.000550\n",
      "Epoch 10756/40000, Loss: 4.578451989800669e-05, Learning Rate: 0.000550\n",
      "Epoch 10757/40000, Loss: 5.295816299621947e-05, Learning Rate: 0.000550\n",
      "Epoch 10758/40000, Loss: 4.908419577986933e-05, Learning Rate: 0.000550\n",
      "Epoch 10759/40000, Loss: 4.3240783270448446e-05, Learning Rate: 0.000550\n",
      "Epoch 10760/40000, Loss: 8.345597598236054e-05, Learning Rate: 0.000550\n",
      "Epoch 10761/40000, Loss: 2.1732394088758156e-05, Learning Rate: 0.000550\n",
      "Epoch 10762/40000, Loss: 8.420085941907018e-05, Learning Rate: 0.000550\n",
      "Epoch 10763/40000, Loss: 4.9856222176458687e-05, Learning Rate: 0.000550\n",
      "Epoch 10764/40000, Loss: 8.577986591262743e-05, Learning Rate: 0.000550\n",
      "Epoch 10765/40000, Loss: 7.519077189499512e-05, Learning Rate: 0.000550\n",
      "Epoch 10766/40000, Loss: 4.516936678555794e-05, Learning Rate: 0.000549\n",
      "Epoch 10767/40000, Loss: 4.3759882828453556e-05, Learning Rate: 0.000549\n",
      "Epoch 10768/40000, Loss: 2.110581954184454e-05, Learning Rate: 0.000549\n",
      "Epoch 10769/40000, Loss: 5.249588139122352e-05, Learning Rate: 0.000549\n",
      "Epoch 10770/40000, Loss: 4.421634002937935e-05, Learning Rate: 0.000549\n",
      "Epoch 10771/40000, Loss: 8.251884719356894e-05, Learning Rate: 0.000549\n",
      "Epoch 10772/40000, Loss: 5.0043498049490154e-05, Learning Rate: 0.000549\n",
      "Epoch 10773/40000, Loss: 5.0603699492057785e-05, Learning Rate: 0.000549\n",
      "Epoch 10774/40000, Loss: 4.377358709461987e-05, Learning Rate: 0.000549\n",
      "Epoch 10775/40000, Loss: 7.065285899443552e-05, Learning Rate: 0.000549\n",
      "Epoch 10776/40000, Loss: 8.233336848206818e-05, Learning Rate: 0.000549\n",
      "Epoch 10777/40000, Loss: 8.298511238535866e-05, Learning Rate: 0.000549\n",
      "Epoch 10778/40000, Loss: 4.319432991906069e-05, Learning Rate: 0.000549\n",
      "Epoch 10779/40000, Loss: 5.495758887263946e-05, Learning Rate: 0.000549\n",
      "Epoch 10780/40000, Loss: 7.394299609586596e-05, Learning Rate: 0.000549\n",
      "Epoch 10781/40000, Loss: 6.562771159224212e-05, Learning Rate: 0.000548\n",
      "Epoch 10782/40000, Loss: 5.915877045481466e-05, Learning Rate: 0.000548\n",
      "Epoch 10783/40000, Loss: 2.6041427190648392e-05, Learning Rate: 0.000548\n",
      "Epoch 10784/40000, Loss: 4.771012390847318e-05, Learning Rate: 0.000548\n",
      "Epoch 10785/40000, Loss: 5.226982102612965e-05, Learning Rate: 0.000548\n",
      "Epoch 10786/40000, Loss: 2.262201815028675e-05, Learning Rate: 0.000548\n",
      "Epoch 10787/40000, Loss: 7.583684055134654e-05, Learning Rate: 0.000548\n",
      "Epoch 10788/40000, Loss: 4.4768621592083946e-05, Learning Rate: 0.000548\n",
      "Epoch 10789/40000, Loss: 8.408015128225088e-05, Learning Rate: 0.000548\n",
      "Epoch 10790/40000, Loss: 8.247805817518383e-05, Learning Rate: 0.000548\n",
      "Epoch 10791/40000, Loss: 2.103097904182505e-05, Learning Rate: 0.000548\n",
      "Epoch 10792/40000, Loss: 2.0022844182676636e-05, Learning Rate: 0.000548\n",
      "Epoch 10793/40000, Loss: 8.417216304223984e-05, Learning Rate: 0.000548\n",
      "Epoch 10794/40000, Loss: 4.3516960431588814e-05, Learning Rate: 0.000548\n",
      "Epoch 10795/40000, Loss: 4.289515345590189e-05, Learning Rate: 0.000548\n",
      "Epoch 10796/40000, Loss: 8.036705548875034e-05, Learning Rate: 0.000548\n",
      "Epoch 10797/40000, Loss: 2.1436053430079482e-05, Learning Rate: 0.000547\n",
      "Epoch 10798/40000, Loss: 4.353492477093823e-05, Learning Rate: 0.000547\n",
      "Epoch 10799/40000, Loss: 2.0802788640139624e-05, Learning Rate: 0.000547\n",
      "Epoch 10800/40000, Loss: 8.066490408964455e-05, Learning Rate: 0.000547\n",
      "Epoch 10801/40000, Loss: 5.30857068952173e-05, Learning Rate: 0.000547\n",
      "Epoch 10802/40000, Loss: 5.283232530928217e-05, Learning Rate: 0.000547\n",
      "Epoch 10803/40000, Loss: 8.248738595284522e-05, Learning Rate: 0.000547\n",
      "Epoch 10804/40000, Loss: 4.2560637666611e-05, Learning Rate: 0.000547\n",
      "Epoch 10805/40000, Loss: 7.044674566714093e-05, Learning Rate: 0.000547\n",
      "Epoch 10806/40000, Loss: 8.268470264738426e-05, Learning Rate: 0.000547\n",
      "Epoch 10807/40000, Loss: 4.293457095627673e-05, Learning Rate: 0.000547\n",
      "Epoch 10808/40000, Loss: 7.460342749254778e-05, Learning Rate: 0.000547\n",
      "Epoch 10809/40000, Loss: 2.2679749235976487e-05, Learning Rate: 0.000547\n",
      "Epoch 10810/40000, Loss: 8.369253191631287e-05, Learning Rate: 0.000547\n",
      "Epoch 10811/40000, Loss: 5.594324102276005e-05, Learning Rate: 0.000547\n",
      "Epoch 10812/40000, Loss: 5.553130904445425e-05, Learning Rate: 0.000546\n",
      "Epoch 10813/40000, Loss: 5.244603016762994e-05, Learning Rate: 0.000546\n",
      "Epoch 10814/40000, Loss: 9.976280853152275e-05, Learning Rate: 0.000546\n",
      "Epoch 10815/40000, Loss: 8.827760029817e-05, Learning Rate: 0.000546\n",
      "Epoch 10816/40000, Loss: 6.089465750847012e-05, Learning Rate: 0.000546\n",
      "Epoch 10817/40000, Loss: 0.0001206061351695098, Learning Rate: 0.000546\n",
      "Epoch 10818/40000, Loss: 5.786813562735915e-05, Learning Rate: 0.000546\n",
      "Epoch 10819/40000, Loss: 6.393800140358508e-05, Learning Rate: 0.000546\n",
      "Epoch 10820/40000, Loss: 2.5752691726665944e-05, Learning Rate: 0.000546\n",
      "Epoch 10821/40000, Loss: 8.891874313121662e-05, Learning Rate: 0.000546\n",
      "Epoch 10822/40000, Loss: 7.993626786628738e-05, Learning Rate: 0.000546\n",
      "Epoch 10823/40000, Loss: 4.8965368478093296e-05, Learning Rate: 0.000546\n",
      "Epoch 10824/40000, Loss: 9.687650890555233e-05, Learning Rate: 0.000546\n",
      "Epoch 10825/40000, Loss: 5.3674310038331896e-05, Learning Rate: 0.000546\n",
      "Epoch 10826/40000, Loss: 5.826756387250498e-05, Learning Rate: 0.000546\n",
      "Epoch 10827/40000, Loss: 9.161068010143936e-05, Learning Rate: 0.000545\n",
      "Epoch 10828/40000, Loss: 2.8097278118366376e-05, Learning Rate: 0.000545\n",
      "Epoch 10829/40000, Loss: 6.0649570514215156e-05, Learning Rate: 0.000545\n",
      "Epoch 10830/40000, Loss: 4.615262514562346e-05, Learning Rate: 0.000545\n",
      "Epoch 10831/40000, Loss: 8.961746789282188e-05, Learning Rate: 0.000545\n",
      "Epoch 10832/40000, Loss: 5.775465979240835e-05, Learning Rate: 0.000545\n",
      "Epoch 10833/40000, Loss: 9.903944010147825e-05, Learning Rate: 0.000545\n",
      "Epoch 10834/40000, Loss: 9.642191434977576e-05, Learning Rate: 0.000545\n",
      "Epoch 10835/40000, Loss: 3.005463076988235e-05, Learning Rate: 0.000545\n",
      "Epoch 10836/40000, Loss: 5.41072768101003e-05, Learning Rate: 0.000545\n",
      "Epoch 10837/40000, Loss: 5.1263999921502545e-05, Learning Rate: 0.000545\n",
      "Epoch 10838/40000, Loss: 8.267447265097871e-05, Learning Rate: 0.000545\n",
      "Epoch 10839/40000, Loss: 5.602454621111974e-05, Learning Rate: 0.000545\n",
      "Epoch 10840/40000, Loss: 5.0934249884448946e-05, Learning Rate: 0.000545\n",
      "Epoch 10841/40000, Loss: 8.789117418928072e-05, Learning Rate: 0.000545\n",
      "Epoch 10842/40000, Loss: 4.9014342948794365e-05, Learning Rate: 0.000544\n",
      "Epoch 10843/40000, Loss: 2.2619355149799958e-05, Learning Rate: 0.000544\n",
      "Epoch 10844/40000, Loss: 8.235951099777594e-05, Learning Rate: 0.000544\n",
      "Epoch 10845/40000, Loss: 9.784781286725774e-05, Learning Rate: 0.000544\n",
      "Epoch 10846/40000, Loss: 2.211212631664239e-05, Learning Rate: 0.000544\n",
      "Epoch 10847/40000, Loss: 7.196771912276745e-05, Learning Rate: 0.000544\n",
      "Epoch 10848/40000, Loss: 4.925495886709541e-05, Learning Rate: 0.000544\n",
      "Epoch 10849/40000, Loss: 3.127556556137279e-05, Learning Rate: 0.000544\n",
      "Epoch 10850/40000, Loss: 6.638085324084386e-05, Learning Rate: 0.000544\n",
      "Epoch 10851/40000, Loss: 8.247877121903002e-05, Learning Rate: 0.000544\n",
      "Epoch 10852/40000, Loss: 9.07532376004383e-05, Learning Rate: 0.000544\n",
      "Epoch 10853/40000, Loss: 7.123939576558769e-05, Learning Rate: 0.000544\n",
      "Epoch 10854/40000, Loss: 9.956120629794896e-05, Learning Rate: 0.000544\n",
      "Epoch 10855/40000, Loss: 5.748636249336414e-05, Learning Rate: 0.000544\n",
      "Epoch 10856/40000, Loss: 8.641355816507712e-05, Learning Rate: 0.000544\n",
      "Epoch 10857/40000, Loss: 3.051960084121674e-05, Learning Rate: 0.000544\n",
      "Epoch 10858/40000, Loss: 0.0001040929855662398, Learning Rate: 0.000543\n",
      "Epoch 10859/40000, Loss: 8.234018605435267e-05, Learning Rate: 0.000543\n",
      "Epoch 10860/40000, Loss: 6.281126843532547e-05, Learning Rate: 0.000543\n",
      "Epoch 10861/40000, Loss: 4.524948599282652e-05, Learning Rate: 0.000543\n",
      "Epoch 10862/40000, Loss: 5.7091623602900654e-05, Learning Rate: 0.000543\n",
      "Epoch 10863/40000, Loss: 5.179330401006155e-05, Learning Rate: 0.000543\n",
      "Epoch 10864/40000, Loss: 5.497773236129433e-05, Learning Rate: 0.000543\n",
      "Epoch 10865/40000, Loss: 2.5226010620826855e-05, Learning Rate: 0.000543\n",
      "Epoch 10866/40000, Loss: 4.3683918192982674e-05, Learning Rate: 0.000543\n",
      "Epoch 10867/40000, Loss: 5.8512447139946744e-05, Learning Rate: 0.000543\n",
      "Epoch 10868/40000, Loss: 2.2963544324738905e-05, Learning Rate: 0.000543\n",
      "Epoch 10869/40000, Loss: 4.943010208080523e-05, Learning Rate: 0.000543\n",
      "Epoch 10870/40000, Loss: 5.528245310415514e-05, Learning Rate: 0.000543\n",
      "Epoch 10871/40000, Loss: 4.926816109218635e-05, Learning Rate: 0.000543\n",
      "Epoch 10872/40000, Loss: 5.7093322539003566e-05, Learning Rate: 0.000543\n",
      "Epoch 10873/40000, Loss: 3.676120832096785e-05, Learning Rate: 0.000542\n",
      "Epoch 10874/40000, Loss: 6.216322799446061e-05, Learning Rate: 0.000542\n",
      "Epoch 10875/40000, Loss: 8.40998109197244e-05, Learning Rate: 0.000542\n",
      "Epoch 10876/40000, Loss: 9.267601853935048e-05, Learning Rate: 0.000542\n",
      "Epoch 10877/40000, Loss: 6.844995368737727e-05, Learning Rate: 0.000542\n",
      "Epoch 10878/40000, Loss: 5.563955710385926e-05, Learning Rate: 0.000542\n",
      "Epoch 10879/40000, Loss: 5.519109618035145e-05, Learning Rate: 0.000542\n",
      "Epoch 10880/40000, Loss: 7.526933768531308e-05, Learning Rate: 0.000542\n",
      "Epoch 10881/40000, Loss: 8.785645331954584e-05, Learning Rate: 0.000542\n",
      "Epoch 10882/40000, Loss: 2.7876019885297865e-05, Learning Rate: 0.000542\n",
      "Epoch 10883/40000, Loss: 9.783513814909384e-05, Learning Rate: 0.000542\n",
      "Epoch 10884/40000, Loss: 5.826514825457707e-05, Learning Rate: 0.000542\n",
      "Epoch 10885/40000, Loss: 2.460161704220809e-05, Learning Rate: 0.000542\n",
      "Epoch 10886/40000, Loss: 5.417278953245841e-05, Learning Rate: 0.000542\n",
      "Epoch 10887/40000, Loss: 4.6202105295378715e-05, Learning Rate: 0.000542\n",
      "Epoch 10888/40000, Loss: 2.2670303224003874e-05, Learning Rate: 0.000541\n",
      "Epoch 10889/40000, Loss: 7.462456414941698e-05, Learning Rate: 0.000541\n",
      "Epoch 10890/40000, Loss: 8.217707363655791e-05, Learning Rate: 0.000541\n",
      "Epoch 10891/40000, Loss: 4.841458212467842e-05, Learning Rate: 0.000541\n",
      "Epoch 10892/40000, Loss: 2.137728915840853e-05, Learning Rate: 0.000541\n",
      "Epoch 10893/40000, Loss: 2.003771260206122e-05, Learning Rate: 0.000541\n",
      "Epoch 10894/40000, Loss: 5.339146810001694e-05, Learning Rate: 0.000541\n",
      "Epoch 10895/40000, Loss: 6.918726285221055e-05, Learning Rate: 0.000541\n",
      "Epoch 10896/40000, Loss: 4.2593892430886626e-05, Learning Rate: 0.000541\n",
      "Epoch 10897/40000, Loss: 7.137440843507648e-05, Learning Rate: 0.000541\n",
      "Epoch 10898/40000, Loss: 1.9437582523096353e-05, Learning Rate: 0.000541\n",
      "Epoch 10899/40000, Loss: 8.019283995963633e-05, Learning Rate: 0.000541\n",
      "Epoch 10900/40000, Loss: 7.988642028067261e-05, Learning Rate: 0.000541\n",
      "Epoch 10901/40000, Loss: 8.017296204343438e-05, Learning Rate: 0.000541\n",
      "Epoch 10902/40000, Loss: 5.2959359891247004e-05, Learning Rate: 0.000541\n",
      "Epoch 10903/40000, Loss: 1.992917896131985e-05, Learning Rate: 0.000541\n",
      "Epoch 10904/40000, Loss: 4.2100462451344356e-05, Learning Rate: 0.000540\n",
      "Epoch 10905/40000, Loss: 8.039466047193855e-05, Learning Rate: 0.000540\n",
      "Epoch 10906/40000, Loss: 4.2181796743534505e-05, Learning Rate: 0.000540\n",
      "Epoch 10907/40000, Loss: 4.1923911339836195e-05, Learning Rate: 0.000540\n",
      "Epoch 10908/40000, Loss: 4.712504960480146e-05, Learning Rate: 0.000540\n",
      "Epoch 10909/40000, Loss: 5.224551568971947e-05, Learning Rate: 0.000540\n",
      "Epoch 10910/40000, Loss: 5.208789661992341e-05, Learning Rate: 0.000540\n",
      "Epoch 10911/40000, Loss: 5.193824108573608e-05, Learning Rate: 0.000540\n",
      "Epoch 10912/40000, Loss: 1.936344960995484e-05, Learning Rate: 0.000540\n",
      "Epoch 10913/40000, Loss: 5.234700802247971e-05, Learning Rate: 0.000540\n",
      "Epoch 10914/40000, Loss: 1.9654446077765897e-05, Learning Rate: 0.000540\n",
      "Epoch 10915/40000, Loss: 6.826658500358462e-05, Learning Rate: 0.000540\n",
      "Epoch 10916/40000, Loss: 4.251089194440283e-05, Learning Rate: 0.000540\n",
      "Epoch 10917/40000, Loss: 2.1682802980649285e-05, Learning Rate: 0.000540\n",
      "Epoch 10918/40000, Loss: 4.742838063975796e-05, Learning Rate: 0.000540\n",
      "Epoch 10919/40000, Loss: 4.247265314916149e-05, Learning Rate: 0.000539\n",
      "Epoch 10920/40000, Loss: 1.970987068489194e-05, Learning Rate: 0.000539\n",
      "Epoch 10921/40000, Loss: 8.192091627279297e-05, Learning Rate: 0.000539\n",
      "Epoch 10922/40000, Loss: 5.243042323854752e-05, Learning Rate: 0.000539\n",
      "Epoch 10923/40000, Loss: 2.0010220396216027e-05, Learning Rate: 0.000539\n",
      "Epoch 10924/40000, Loss: 8.039609383558854e-05, Learning Rate: 0.000539\n",
      "Epoch 10925/40000, Loss: 6.907115312060341e-05, Learning Rate: 0.000539\n",
      "Epoch 10926/40000, Loss: 6.890354416100308e-05, Learning Rate: 0.000539\n",
      "Epoch 10927/40000, Loss: 4.742979581351392e-05, Learning Rate: 0.000539\n",
      "Epoch 10928/40000, Loss: 4.7061086661415175e-05, Learning Rate: 0.000539\n",
      "Epoch 10929/40000, Loss: 4.7667086619185284e-05, Learning Rate: 0.000539\n",
      "Epoch 10930/40000, Loss: 2.006733848247677e-05, Learning Rate: 0.000539\n",
      "Epoch 10931/40000, Loss: 2.0562103600241244e-05, Learning Rate: 0.000539\n",
      "Epoch 10932/40000, Loss: 5.320643322193064e-05, Learning Rate: 0.000539\n",
      "Epoch 10933/40000, Loss: 2.5009339879034087e-05, Learning Rate: 0.000539\n",
      "Epoch 10934/40000, Loss: 5.7863890106091276e-05, Learning Rate: 0.000539\n",
      "Epoch 10935/40000, Loss: 4.5328277337830514e-05, Learning Rate: 0.000538\n",
      "Epoch 10936/40000, Loss: 8.501938282279298e-05, Learning Rate: 0.000538\n",
      "Epoch 10937/40000, Loss: 5.5120341130532324e-05, Learning Rate: 0.000538\n",
      "Epoch 10938/40000, Loss: 5.466705988510512e-05, Learning Rate: 0.000538\n",
      "Epoch 10939/40000, Loss: 4.869971235166304e-05, Learning Rate: 0.000538\n",
      "Epoch 10940/40000, Loss: 8.177600102499127e-05, Learning Rate: 0.000538\n",
      "Epoch 10941/40000, Loss: 2.3379183403449133e-05, Learning Rate: 0.000538\n",
      "Epoch 10942/40000, Loss: 2.30472614930477e-05, Learning Rate: 0.000538\n",
      "Epoch 10943/40000, Loss: 5.6688742915866897e-05, Learning Rate: 0.000538\n",
      "Epoch 10944/40000, Loss: 4.716531839221716e-05, Learning Rate: 0.000538\n",
      "Epoch 10945/40000, Loss: 7.52821724745445e-05, Learning Rate: 0.000538\n",
      "Epoch 10946/40000, Loss: 5.644169505103491e-05, Learning Rate: 0.000538\n",
      "Epoch 10947/40000, Loss: 5.165965558262542e-05, Learning Rate: 0.000538\n",
      "Epoch 10948/40000, Loss: 9.072183456737548e-05, Learning Rate: 0.000538\n",
      "Epoch 10949/40000, Loss: 6.805294833611697e-05, Learning Rate: 0.000538\n",
      "Epoch 10950/40000, Loss: 5.5914813856361434e-05, Learning Rate: 0.000537\n",
      "Epoch 10951/40000, Loss: 2.682478771021124e-05, Learning Rate: 0.000537\n",
      "Epoch 10952/40000, Loss: 8.846301352605224e-05, Learning Rate: 0.000537\n",
      "Epoch 10953/40000, Loss: 6.613890582229942e-05, Learning Rate: 0.000537\n",
      "Epoch 10954/40000, Loss: 3.384349474799819e-05, Learning Rate: 0.000537\n",
      "Epoch 10955/40000, Loss: 5.5156768212327734e-05, Learning Rate: 0.000537\n",
      "Epoch 10956/40000, Loss: 0.00010263064177706838, Learning Rate: 0.000537\n",
      "Epoch 10957/40000, Loss: 6.755061622243375e-05, Learning Rate: 0.000537\n",
      "Epoch 10958/40000, Loss: 3.7492020055651665e-05, Learning Rate: 0.000537\n",
      "Epoch 10959/40000, Loss: 3.205940083716996e-05, Learning Rate: 0.000537\n",
      "Epoch 10960/40000, Loss: 2.5863642804324627e-05, Learning Rate: 0.000537\n",
      "Epoch 10961/40000, Loss: 9.994623542297632e-05, Learning Rate: 0.000537\n",
      "Epoch 10962/40000, Loss: 7.811991235939786e-05, Learning Rate: 0.000537\n",
      "Epoch 10963/40000, Loss: 5.166846312931739e-05, Learning Rate: 0.000537\n",
      "Epoch 10964/40000, Loss: 6.858383130747825e-05, Learning Rate: 0.000537\n",
      "Epoch 10965/40000, Loss: 9.405657328898087e-05, Learning Rate: 0.000537\n",
      "Epoch 10966/40000, Loss: 6.505324563477188e-05, Learning Rate: 0.000536\n",
      "Epoch 10967/40000, Loss: 7.593462214572355e-05, Learning Rate: 0.000536\n",
      "Epoch 10968/40000, Loss: 3.496954741422087e-05, Learning Rate: 0.000536\n",
      "Epoch 10969/40000, Loss: 8.751464338274673e-05, Learning Rate: 0.000536\n",
      "Epoch 10970/40000, Loss: 7.84222356742248e-05, Learning Rate: 0.000536\n",
      "Epoch 10971/40000, Loss: 7.55405708332546e-05, Learning Rate: 0.000536\n",
      "Epoch 10972/40000, Loss: 4.954191535944119e-05, Learning Rate: 0.000536\n",
      "Epoch 10973/40000, Loss: 2.400465018581599e-05, Learning Rate: 0.000536\n",
      "Epoch 10974/40000, Loss: 8.278963650809601e-05, Learning Rate: 0.000536\n",
      "Epoch 10975/40000, Loss: 8.011414320208132e-05, Learning Rate: 0.000536\n",
      "Epoch 10976/40000, Loss: 5.587548002949916e-05, Learning Rate: 0.000536\n",
      "Epoch 10977/40000, Loss: 0.00010140885569853708, Learning Rate: 0.000536\n",
      "Epoch 10978/40000, Loss: 8.960486593423411e-05, Learning Rate: 0.000536\n",
      "Epoch 10979/40000, Loss: 5.5711578170303255e-05, Learning Rate: 0.000536\n",
      "Epoch 10980/40000, Loss: 8.599767897976562e-05, Learning Rate: 0.000536\n",
      "Epoch 10981/40000, Loss: 2.1682812075596303e-05, Learning Rate: 0.000535\n",
      "Epoch 10982/40000, Loss: 5.193391552893445e-05, Learning Rate: 0.000535\n",
      "Epoch 10983/40000, Loss: 5.7256889704149216e-05, Learning Rate: 0.000535\n",
      "Epoch 10984/40000, Loss: 5.446830255095847e-05, Learning Rate: 0.000535\n",
      "Epoch 10985/40000, Loss: 7.761568849673495e-05, Learning Rate: 0.000535\n",
      "Epoch 10986/40000, Loss: 8.546908793505281e-05, Learning Rate: 0.000535\n",
      "Epoch 10987/40000, Loss: 0.00011436136992415413, Learning Rate: 0.000535\n",
      "Epoch 10988/40000, Loss: 0.00010286617907695472, Learning Rate: 0.000535\n",
      "Epoch 10989/40000, Loss: 0.0003507514193188399, Learning Rate: 0.000535\n",
      "Epoch 10990/40000, Loss: 7.517535414081067e-05, Learning Rate: 0.000535\n",
      "Epoch 10991/40000, Loss: 8.437078213319182e-05, Learning Rate: 0.000535\n",
      "Epoch 10992/40000, Loss: 0.0001511649606982246, Learning Rate: 0.000535\n",
      "Epoch 10993/40000, Loss: 0.00014557158283423632, Learning Rate: 0.000535\n",
      "Epoch 10994/40000, Loss: 0.00027617037994787097, Learning Rate: 0.000535\n",
      "Epoch 10995/40000, Loss: 6.871666118968278e-05, Learning Rate: 0.000535\n",
      "Epoch 10996/40000, Loss: 5.2371247875271365e-05, Learning Rate: 0.000535\n",
      "Epoch 10997/40000, Loss: 4.268536577001214e-05, Learning Rate: 0.000534\n",
      "Epoch 10998/40000, Loss: 9.215991303790361e-05, Learning Rate: 0.000534\n",
      "Epoch 10999/40000, Loss: 0.00010520813520997763, Learning Rate: 0.000534\n",
      "Epoch 11000/40000, Loss: 0.00011022319085896015, Learning Rate: 0.000534\n",
      "Epoch 11001/40000, Loss: 9.270569717045873e-05, Learning Rate: 0.000534\n",
      "Epoch 11002/40000, Loss: 9.424856398254633e-05, Learning Rate: 0.000534\n",
      "Epoch 11003/40000, Loss: 4.951374285155907e-05, Learning Rate: 0.000534\n",
      "Epoch 11004/40000, Loss: 4.386884393170476e-05, Learning Rate: 0.000534\n",
      "Epoch 11005/40000, Loss: 5.636197602143511e-05, Learning Rate: 0.000534\n",
      "Epoch 11006/40000, Loss: 5.593665991909802e-05, Learning Rate: 0.000534\n",
      "Epoch 11007/40000, Loss: 4.973912291461602e-05, Learning Rate: 0.000534\n",
      "Epoch 11008/40000, Loss: 4.983610051567666e-05, Learning Rate: 0.000534\n",
      "Epoch 11009/40000, Loss: 5.352996231522411e-05, Learning Rate: 0.000534\n",
      "Epoch 11010/40000, Loss: 4.27068916906137e-05, Learning Rate: 0.000534\n",
      "Epoch 11011/40000, Loss: 2.1179565010243095e-05, Learning Rate: 0.000534\n",
      "Epoch 11012/40000, Loss: 8.187319326680154e-05, Learning Rate: 0.000533\n",
      "Epoch 11013/40000, Loss: 4.902647197013721e-05, Learning Rate: 0.000533\n",
      "Epoch 11014/40000, Loss: 7.063467637635767e-05, Learning Rate: 0.000533\n",
      "Epoch 11015/40000, Loss: 6.974084681132808e-05, Learning Rate: 0.000533\n",
      "Epoch 11016/40000, Loss: 5.332387809175998e-05, Learning Rate: 0.000533\n",
      "Epoch 11017/40000, Loss: 4.261342837708071e-05, Learning Rate: 0.000533\n",
      "Epoch 11018/40000, Loss: 8.108715701382607e-05, Learning Rate: 0.000533\n",
      "Epoch 11019/40000, Loss: 5.268575478112325e-05, Learning Rate: 0.000533\n",
      "Epoch 11020/40000, Loss: 4.777189315063879e-05, Learning Rate: 0.000533\n",
      "Epoch 11021/40000, Loss: 4.208652535453439e-05, Learning Rate: 0.000533\n",
      "Epoch 11022/40000, Loss: 4.766549318446778e-05, Learning Rate: 0.000533\n",
      "Epoch 11023/40000, Loss: 4.210287079331465e-05, Learning Rate: 0.000533\n",
      "Epoch 11024/40000, Loss: 8.178465941455215e-05, Learning Rate: 0.000533\n",
      "Epoch 11025/40000, Loss: 6.89735243213363e-05, Learning Rate: 0.000533\n",
      "Epoch 11026/40000, Loss: 4.7645320591982454e-05, Learning Rate: 0.000533\n",
      "Epoch 11027/40000, Loss: 2.0976149244233966e-05, Learning Rate: 0.000533\n",
      "Epoch 11028/40000, Loss: 6.827002653153613e-05, Learning Rate: 0.000532\n",
      "Epoch 11029/40000, Loss: 2.0199193386361003e-05, Learning Rate: 0.000532\n",
      "Epoch 11030/40000, Loss: 4.206069206702523e-05, Learning Rate: 0.000532\n",
      "Epoch 11031/40000, Loss: 2.499186302884482e-05, Learning Rate: 0.000532\n",
      "Epoch 11032/40000, Loss: 4.351484676590189e-05, Learning Rate: 0.000532\n",
      "Epoch 11033/40000, Loss: 4.270785939297639e-05, Learning Rate: 0.000532\n",
      "Epoch 11034/40000, Loss: 7.001754420343786e-05, Learning Rate: 0.000532\n",
      "Epoch 11035/40000, Loss: 4.55309527751524e-05, Learning Rate: 0.000532\n",
      "Epoch 11036/40000, Loss: 4.7875197196844965e-05, Learning Rate: 0.000532\n",
      "Epoch 11037/40000, Loss: 5.594973481493071e-05, Learning Rate: 0.000532\n",
      "Epoch 11038/40000, Loss: 2.6317331503378227e-05, Learning Rate: 0.000532\n",
      "Epoch 11039/40000, Loss: 2.0677951397374272e-05, Learning Rate: 0.000532\n",
      "Epoch 11040/40000, Loss: 5.2817667892668396e-05, Learning Rate: 0.000532\n",
      "Epoch 11041/40000, Loss: 5.1949344197055325e-05, Learning Rate: 0.000532\n",
      "Epoch 11042/40000, Loss: 8.010681631276384e-05, Learning Rate: 0.000532\n",
      "Epoch 11043/40000, Loss: 7.989013101905584e-05, Learning Rate: 0.000532\n",
      "Epoch 11044/40000, Loss: 6.890200165798888e-05, Learning Rate: 0.000531\n",
      "Epoch 11045/40000, Loss: 2.1830534024047665e-05, Learning Rate: 0.000531\n",
      "Epoch 11046/40000, Loss: 4.7488705604337156e-05, Learning Rate: 0.000531\n",
      "Epoch 11047/40000, Loss: 6.800963456043974e-05, Learning Rate: 0.000531\n",
      "Epoch 11048/40000, Loss: 7.988026482053101e-05, Learning Rate: 0.000531\n",
      "Epoch 11049/40000, Loss: 5.139874338055961e-05, Learning Rate: 0.000531\n",
      "Epoch 11050/40000, Loss: 8.140519639709964e-05, Learning Rate: 0.000531\n",
      "Epoch 11051/40000, Loss: 1.9373306713532656e-05, Learning Rate: 0.000531\n",
      "Epoch 11052/40000, Loss: 1.9073067960562184e-05, Learning Rate: 0.000531\n",
      "Epoch 11053/40000, Loss: 4.6465745981549844e-05, Learning Rate: 0.000531\n",
      "Epoch 11054/40000, Loss: 1.935018372023478e-05, Learning Rate: 0.000531\n",
      "Epoch 11055/40000, Loss: 6.765504804207012e-05, Learning Rate: 0.000531\n",
      "Epoch 11056/40000, Loss: 7.993215695023537e-05, Learning Rate: 0.000531\n",
      "Epoch 11057/40000, Loss: 4.1796945879468694e-05, Learning Rate: 0.000531\n",
      "Epoch 11058/40000, Loss: 4.6511966502293944e-05, Learning Rate: 0.000531\n",
      "Epoch 11059/40000, Loss: 4.645466469810344e-05, Learning Rate: 0.000530\n",
      "Epoch 11060/40000, Loss: 5.1322866056580096e-05, Learning Rate: 0.000530\n",
      "Epoch 11061/40000, Loss: 1.918130146805197e-05, Learning Rate: 0.000530\n",
      "Epoch 11062/40000, Loss: 6.766226579202339e-05, Learning Rate: 0.000530\n",
      "Epoch 11063/40000, Loss: 4.645662193070166e-05, Learning Rate: 0.000530\n",
      "Epoch 11064/40000, Loss: 8.000047819223255e-05, Learning Rate: 0.000530\n",
      "Epoch 11065/40000, Loss: 4.619856190402061e-05, Learning Rate: 0.000530\n",
      "Epoch 11066/40000, Loss: 6.757817027391866e-05, Learning Rate: 0.000530\n",
      "Epoch 11067/40000, Loss: 4.607194205163978e-05, Learning Rate: 0.000530\n",
      "Epoch 11068/40000, Loss: 6.867191405035555e-05, Learning Rate: 0.000530\n",
      "Epoch 11069/40000, Loss: 4.672192517318763e-05, Learning Rate: 0.000530\n",
      "Epoch 11070/40000, Loss: 6.896240665810183e-05, Learning Rate: 0.000530\n",
      "Epoch 11071/40000, Loss: 4.232624633004889e-05, Learning Rate: 0.000530\n",
      "Epoch 11072/40000, Loss: 8.193446410587057e-05, Learning Rate: 0.000530\n",
      "Epoch 11073/40000, Loss: 8.190221706172451e-05, Learning Rate: 0.000530\n",
      "Epoch 11074/40000, Loss: 8.074465586105362e-05, Learning Rate: 0.000530\n",
      "Epoch 11075/40000, Loss: 4.757849092129618e-05, Learning Rate: 0.000529\n",
      "Epoch 11076/40000, Loss: 8.134518429869786e-05, Learning Rate: 0.000529\n",
      "Epoch 11077/40000, Loss: 4.656899909605272e-05, Learning Rate: 0.000529\n",
      "Epoch 11078/40000, Loss: 8.606166375102475e-05, Learning Rate: 0.000529\n",
      "Epoch 11079/40000, Loss: 5.2973824494984e-05, Learning Rate: 0.000529\n",
      "Epoch 11080/40000, Loss: 4.681011341745034e-05, Learning Rate: 0.000529\n",
      "Epoch 11081/40000, Loss: 5.1777133194264024e-05, Learning Rate: 0.000529\n",
      "Epoch 11082/40000, Loss: 6.84788974467665e-05, Learning Rate: 0.000529\n",
      "Epoch 11083/40000, Loss: 8.383121166843921e-05, Learning Rate: 0.000529\n",
      "Epoch 11084/40000, Loss: 6.852859951322898e-05, Learning Rate: 0.000529\n",
      "Epoch 11085/40000, Loss: 2.02433602680685e-05, Learning Rate: 0.000529\n",
      "Epoch 11086/40000, Loss: 6.842530274298042e-05, Learning Rate: 0.000529\n",
      "Epoch 11087/40000, Loss: 4.8095607780851424e-05, Learning Rate: 0.000529\n",
      "Epoch 11088/40000, Loss: 4.7242665459634736e-05, Learning Rate: 0.000529\n",
      "Epoch 11089/40000, Loss: 5.404980038292706e-05, Learning Rate: 0.000529\n",
      "Epoch 11090/40000, Loss: 1.98703146452317e-05, Learning Rate: 0.000529\n",
      "Epoch 11091/40000, Loss: 4.185464058537036e-05, Learning Rate: 0.000528\n",
      "Epoch 11092/40000, Loss: 1.9079074263572693e-05, Learning Rate: 0.000528\n",
      "Epoch 11093/40000, Loss: 5.371951192501001e-05, Learning Rate: 0.000528\n",
      "Epoch 11094/40000, Loss: 6.749737076461315e-05, Learning Rate: 0.000528\n",
      "Epoch 11095/40000, Loss: 6.795214721933007e-05, Learning Rate: 0.000528\n",
      "Epoch 11096/40000, Loss: 4.786389763467014e-05, Learning Rate: 0.000528\n",
      "Epoch 11097/40000, Loss: 6.984076753724366e-05, Learning Rate: 0.000528\n",
      "Epoch 11098/40000, Loss: 6.822305294917896e-05, Learning Rate: 0.000528\n",
      "Epoch 11099/40000, Loss: 6.832231883890927e-05, Learning Rate: 0.000528\n",
      "Epoch 11100/40000, Loss: 6.792094791308045e-05, Learning Rate: 0.000528\n",
      "Epoch 11101/40000, Loss: 4.717319461633451e-05, Learning Rate: 0.000528\n",
      "Epoch 11102/40000, Loss: 6.77311109029688e-05, Learning Rate: 0.000528\n",
      "Epoch 11103/40000, Loss: 5.0557940994622186e-05, Learning Rate: 0.000528\n",
      "Epoch 11104/40000, Loss: 8.054656791500747e-05, Learning Rate: 0.000528\n",
      "Epoch 11105/40000, Loss: 6.830266647739336e-05, Learning Rate: 0.000528\n",
      "Epoch 11106/40000, Loss: 5.207519279792905e-05, Learning Rate: 0.000528\n",
      "Epoch 11107/40000, Loss: 8.595954568590969e-05, Learning Rate: 0.000527\n",
      "Epoch 11108/40000, Loss: 4.877164610661566e-05, Learning Rate: 0.000527\n",
      "Epoch 11109/40000, Loss: 4.428224201546982e-05, Learning Rate: 0.000527\n",
      "Epoch 11110/40000, Loss: 4.335056655691005e-05, Learning Rate: 0.000527\n",
      "Epoch 11111/40000, Loss: 5.584952305071056e-05, Learning Rate: 0.000527\n",
      "Epoch 11112/40000, Loss: 4.59696093457751e-05, Learning Rate: 0.000527\n",
      "Epoch 11113/40000, Loss: 8.736209565540776e-05, Learning Rate: 0.000527\n",
      "Epoch 11114/40000, Loss: 8.609180804342031e-05, Learning Rate: 0.000527\n",
      "Epoch 11115/40000, Loss: 8.391934534301981e-05, Learning Rate: 0.000527\n",
      "Epoch 11116/40000, Loss: 4.5269935071701184e-05, Learning Rate: 0.000527\n",
      "Epoch 11117/40000, Loss: 4.3385589378885925e-05, Learning Rate: 0.000527\n",
      "Epoch 11118/40000, Loss: 2.223316732852254e-05, Learning Rate: 0.000527\n",
      "Epoch 11119/40000, Loss: 5.5487231293227524e-05, Learning Rate: 0.000527\n",
      "Epoch 11120/40000, Loss: 5.290543049341068e-05, Learning Rate: 0.000527\n",
      "Epoch 11121/40000, Loss: 2.4906192265916616e-05, Learning Rate: 0.000527\n",
      "Epoch 11122/40000, Loss: 7.545288099208847e-05, Learning Rate: 0.000526\n",
      "Epoch 11123/40000, Loss: 5.86258975090459e-05, Learning Rate: 0.000526\n",
      "Epoch 11124/40000, Loss: 7.354567787842825e-05, Learning Rate: 0.000526\n",
      "Epoch 11125/40000, Loss: 8.535701635992154e-05, Learning Rate: 0.000526\n",
      "Epoch 11126/40000, Loss: 4.761467789649032e-05, Learning Rate: 0.000526\n",
      "Epoch 11127/40000, Loss: 4.777069989359006e-05, Learning Rate: 0.000526\n",
      "Epoch 11128/40000, Loss: 5.7824712712317705e-05, Learning Rate: 0.000526\n",
      "Epoch 11129/40000, Loss: 5.914016583119519e-05, Learning Rate: 0.000526\n",
      "Epoch 11130/40000, Loss: 9.690225124359131e-05, Learning Rate: 0.000526\n",
      "Epoch 11131/40000, Loss: 8.22469373815693e-05, Learning Rate: 0.000526\n",
      "Epoch 11132/40000, Loss: 6.538031448144466e-05, Learning Rate: 0.000526\n",
      "Epoch 11133/40000, Loss: 0.00011365635873517022, Learning Rate: 0.000526\n",
      "Epoch 11134/40000, Loss: 9.715172927826643e-05, Learning Rate: 0.000526\n",
      "Epoch 11135/40000, Loss: 5.2797553507843986e-05, Learning Rate: 0.000526\n",
      "Epoch 11136/40000, Loss: 0.0001778559962986037, Learning Rate: 0.000526\n",
      "Epoch 11137/40000, Loss: 0.00012776661606039852, Learning Rate: 0.000526\n",
      "Epoch 11138/40000, Loss: 9.761530964169651e-05, Learning Rate: 0.000525\n",
      "Epoch 11139/40000, Loss: 0.00013231301272753626, Learning Rate: 0.000525\n",
      "Epoch 11140/40000, Loss: 7.748452480882406e-05, Learning Rate: 0.000525\n",
      "Epoch 11141/40000, Loss: 6.642459629802033e-05, Learning Rate: 0.000525\n",
      "Epoch 11142/40000, Loss: 2.763218253676314e-05, Learning Rate: 0.000525\n",
      "Epoch 11143/40000, Loss: 2.3042513930704445e-05, Learning Rate: 0.000525\n",
      "Epoch 11144/40000, Loss: 5.4663210903527215e-05, Learning Rate: 0.000525\n",
      "Epoch 11145/40000, Loss: 6.591221608687192e-05, Learning Rate: 0.000525\n",
      "Epoch 11146/40000, Loss: 7.835270662326366e-05, Learning Rate: 0.000525\n",
      "Epoch 11147/40000, Loss: 4.642651401809417e-05, Learning Rate: 0.000525\n",
      "Epoch 11148/40000, Loss: 3.06100191664882e-05, Learning Rate: 0.000525\n",
      "Epoch 11149/40000, Loss: 8.682614134158939e-05, Learning Rate: 0.000525\n",
      "Epoch 11150/40000, Loss: 3.208099224139005e-05, Learning Rate: 0.000525\n",
      "Epoch 11151/40000, Loss: 2.6019566575996578e-05, Learning Rate: 0.000525\n",
      "Epoch 11152/40000, Loss: 8.521115523763001e-05, Learning Rate: 0.000525\n",
      "Epoch 11153/40000, Loss: 5.942690768279135e-05, Learning Rate: 0.000525\n",
      "Epoch 11154/40000, Loss: 7.490692951250821e-05, Learning Rate: 0.000524\n",
      "Epoch 11155/40000, Loss: 2.4154389393515885e-05, Learning Rate: 0.000524\n",
      "Epoch 11156/40000, Loss: 6.072647011023946e-05, Learning Rate: 0.000524\n",
      "Epoch 11157/40000, Loss: 2.4640805349918082e-05, Learning Rate: 0.000524\n",
      "Epoch 11158/40000, Loss: 8.143411105265841e-05, Learning Rate: 0.000524\n",
      "Epoch 11159/40000, Loss: 5.4741103667765856e-05, Learning Rate: 0.000524\n",
      "Epoch 11160/40000, Loss: 2.0760709958267398e-05, Learning Rate: 0.000524\n",
      "Epoch 11161/40000, Loss: 5.2905783377354965e-05, Learning Rate: 0.000524\n",
      "Epoch 11162/40000, Loss: 6.846718315500766e-05, Learning Rate: 0.000524\n",
      "Epoch 11163/40000, Loss: 4.785875717061572e-05, Learning Rate: 0.000524\n",
      "Epoch 11164/40000, Loss: 1.9970175344496965e-05, Learning Rate: 0.000524\n",
      "Epoch 11165/40000, Loss: 6.734427734045312e-05, Learning Rate: 0.000524\n",
      "Epoch 11166/40000, Loss: 4.652361531043425e-05, Learning Rate: 0.000524\n",
      "Epoch 11167/40000, Loss: 5.1229275413788855e-05, Learning Rate: 0.000524\n",
      "Epoch 11168/40000, Loss: 4.149034066358581e-05, Learning Rate: 0.000524\n",
      "Epoch 11169/40000, Loss: 6.717450742144138e-05, Learning Rate: 0.000524\n",
      "Epoch 11170/40000, Loss: 1.9856963263009675e-05, Learning Rate: 0.000523\n",
      "Epoch 11171/40000, Loss: 1.9051969502470456e-05, Learning Rate: 0.000523\n",
      "Epoch 11172/40000, Loss: 4.771058229380287e-05, Learning Rate: 0.000523\n",
      "Epoch 11173/40000, Loss: 1.935149703058414e-05, Learning Rate: 0.000523\n",
      "Epoch 11174/40000, Loss: 7.952471059979871e-05, Learning Rate: 0.000523\n",
      "Epoch 11175/40000, Loss: 6.804005533922464e-05, Learning Rate: 0.000523\n",
      "Epoch 11176/40000, Loss: 6.784497236367315e-05, Learning Rate: 0.000523\n",
      "Epoch 11177/40000, Loss: 5.136649269843474e-05, Learning Rate: 0.000523\n",
      "Epoch 11178/40000, Loss: 6.798656977480277e-05, Learning Rate: 0.000523\n",
      "Epoch 11179/40000, Loss: 4.651113340514712e-05, Learning Rate: 0.000523\n",
      "Epoch 11180/40000, Loss: 6.781650154152885e-05, Learning Rate: 0.000523\n",
      "Epoch 11181/40000, Loss: 7.999928493518382e-05, Learning Rate: 0.000523\n",
      "Epoch 11182/40000, Loss: 4.1987681470345706e-05, Learning Rate: 0.000523\n",
      "Epoch 11183/40000, Loss: 4.6808854676783085e-05, Learning Rate: 0.000523\n",
      "Epoch 11184/40000, Loss: 1.9791988961515017e-05, Learning Rate: 0.000523\n",
      "Epoch 11185/40000, Loss: 4.205844015814364e-05, Learning Rate: 0.000523\n",
      "Epoch 11186/40000, Loss: 5.193786637391895e-05, Learning Rate: 0.000522\n",
      "Epoch 11187/40000, Loss: 4.298884960007854e-05, Learning Rate: 0.000522\n",
      "Epoch 11188/40000, Loss: 5.307795436237939e-05, Learning Rate: 0.000522\n",
      "Epoch 11189/40000, Loss: 5.1674909627763554e-05, Learning Rate: 0.000522\n",
      "Epoch 11190/40000, Loss: 4.352648466010578e-05, Learning Rate: 0.000522\n",
      "Epoch 11191/40000, Loss: 2.044802749878727e-05, Learning Rate: 0.000522\n",
      "Epoch 11192/40000, Loss: 7.986222044564784e-05, Learning Rate: 0.000522\n",
      "Epoch 11193/40000, Loss: 5.153957681613974e-05, Learning Rate: 0.000522\n",
      "Epoch 11194/40000, Loss: 6.743315316271037e-05, Learning Rate: 0.000522\n",
      "Epoch 11195/40000, Loss: 7.984130206750706e-05, Learning Rate: 0.000522\n",
      "Epoch 11196/40000, Loss: 4.172588887740858e-05, Learning Rate: 0.000522\n",
      "Epoch 11197/40000, Loss: 8.137427357723936e-05, Learning Rate: 0.000522\n",
      "Epoch 11198/40000, Loss: 4.728226122097112e-05, Learning Rate: 0.000522\n",
      "Epoch 11199/40000, Loss: 1.9845709175569937e-05, Learning Rate: 0.000522\n",
      "Epoch 11200/40000, Loss: 6.806878809584305e-05, Learning Rate: 0.000522\n",
      "Epoch 11201/40000, Loss: 5.234270793152973e-05, Learning Rate: 0.000522\n",
      "Epoch 11202/40000, Loss: 6.917347491253167e-05, Learning Rate: 0.000521\n",
      "Epoch 11203/40000, Loss: 6.772619963157922e-05, Learning Rate: 0.000521\n",
      "Epoch 11204/40000, Loss: 5.369600694393739e-05, Learning Rate: 0.000521\n",
      "Epoch 11205/40000, Loss: 6.891771772643551e-05, Learning Rate: 0.000521\n",
      "Epoch 11206/40000, Loss: 8.071009506238624e-05, Learning Rate: 0.000521\n",
      "Epoch 11207/40000, Loss: 4.253928636899218e-05, Learning Rate: 0.000521\n",
      "Epoch 11208/40000, Loss: 4.683464067056775e-05, Learning Rate: 0.000521\n",
      "Epoch 11209/40000, Loss: 4.654797521652654e-05, Learning Rate: 0.000521\n",
      "Epoch 11210/40000, Loss: 4.6592234866693616e-05, Learning Rate: 0.000521\n",
      "Epoch 11211/40000, Loss: 1.9026385416509584e-05, Learning Rate: 0.000521\n",
      "Epoch 11212/40000, Loss: 4.1425264498684555e-05, Learning Rate: 0.000521\n",
      "Epoch 11213/40000, Loss: 4.1616091039031744e-05, Learning Rate: 0.000521\n",
      "Epoch 11214/40000, Loss: 1.9858231098623946e-05, Learning Rate: 0.000521\n",
      "Epoch 11215/40000, Loss: 4.200431430945173e-05, Learning Rate: 0.000521\n",
      "Epoch 11216/40000, Loss: 4.2051251512020826e-05, Learning Rate: 0.000521\n",
      "Epoch 11217/40000, Loss: 6.799726543249562e-05, Learning Rate: 0.000521\n",
      "Epoch 11218/40000, Loss: 6.890053191455081e-05, Learning Rate: 0.000520\n",
      "Epoch 11219/40000, Loss: 5.5926946515683085e-05, Learning Rate: 0.000520\n",
      "Epoch 11220/40000, Loss: 8.519058610545471e-05, Learning Rate: 0.000520\n",
      "Epoch 11221/40000, Loss: 2.8414797270670533e-05, Learning Rate: 0.000520\n",
      "Epoch 11222/40000, Loss: 4.577842992148362e-05, Learning Rate: 0.000520\n",
      "Epoch 11223/40000, Loss: 8.146608161041513e-05, Learning Rate: 0.000520\n",
      "Epoch 11224/40000, Loss: 5.236551078269258e-05, Learning Rate: 0.000520\n",
      "Epoch 11225/40000, Loss: 4.605759386322461e-05, Learning Rate: 0.000520\n",
      "Epoch 11226/40000, Loss: 2.6044444894068874e-05, Learning Rate: 0.000520\n",
      "Epoch 11227/40000, Loss: 7.588501466671005e-05, Learning Rate: 0.000520\n",
      "Epoch 11228/40000, Loss: 2.683438651729375e-05, Learning Rate: 0.000520\n",
      "Epoch 11229/40000, Loss: 5.899343523196876e-05, Learning Rate: 0.000520\n",
      "Epoch 11230/40000, Loss: 5.635666093439795e-05, Learning Rate: 0.000520\n",
      "Epoch 11231/40000, Loss: 8.332383004017174e-05, Learning Rate: 0.000520\n",
      "Epoch 11232/40000, Loss: 5.533936928259209e-05, Learning Rate: 0.000520\n",
      "Epoch 11233/40000, Loss: 9.702827810542658e-05, Learning Rate: 0.000520\n",
      "Epoch 11234/40000, Loss: 8.544896263629198e-05, Learning Rate: 0.000519\n",
      "Epoch 11235/40000, Loss: 2.7901160137844272e-05, Learning Rate: 0.000519\n",
      "Epoch 11236/40000, Loss: 5.4660220484947786e-05, Learning Rate: 0.000519\n",
      "Epoch 11237/40000, Loss: 5.883630728931166e-05, Learning Rate: 0.000519\n",
      "Epoch 11238/40000, Loss: 5.569987843045965e-05, Learning Rate: 0.000519\n",
      "Epoch 11239/40000, Loss: 6.669402500847355e-05, Learning Rate: 0.000519\n",
      "Epoch 11240/40000, Loss: 8.238520240411162e-05, Learning Rate: 0.000519\n",
      "Epoch 11241/40000, Loss: 3.3559113944647834e-05, Learning Rate: 0.000519\n",
      "Epoch 11242/40000, Loss: 4.8821177188074216e-05, Learning Rate: 0.000519\n",
      "Epoch 11243/40000, Loss: 6.058959479560144e-05, Learning Rate: 0.000519\n",
      "Epoch 11244/40000, Loss: 5.701429836335592e-05, Learning Rate: 0.000519\n",
      "Epoch 11245/40000, Loss: 7.593996269861236e-05, Learning Rate: 0.000519\n",
      "Epoch 11246/40000, Loss: 5.462922126753256e-05, Learning Rate: 0.000519\n",
      "Epoch 11247/40000, Loss: 4.993080074200407e-05, Learning Rate: 0.000519\n",
      "Epoch 11248/40000, Loss: 7.335413829423487e-05, Learning Rate: 0.000519\n",
      "Epoch 11249/40000, Loss: 5.333135777618736e-05, Learning Rate: 0.000519\n",
      "Epoch 11250/40000, Loss: 7.981701492099091e-05, Learning Rate: 0.000518\n",
      "Epoch 11251/40000, Loss: 8.058143430389464e-05, Learning Rate: 0.000518\n",
      "Epoch 11252/40000, Loss: 5.5123393394751474e-05, Learning Rate: 0.000518\n",
      "Epoch 11253/40000, Loss: 5.2195646276231855e-05, Learning Rate: 0.000518\n",
      "Epoch 11254/40000, Loss: 4.353253825684078e-05, Learning Rate: 0.000518\n",
      "Epoch 11255/40000, Loss: 8.297983004013076e-05, Learning Rate: 0.000518\n",
      "Epoch 11256/40000, Loss: 6.858232518425211e-05, Learning Rate: 0.000518\n",
      "Epoch 11257/40000, Loss: 4.686430474976078e-05, Learning Rate: 0.000518\n",
      "Epoch 11258/40000, Loss: 5.371601218939759e-05, Learning Rate: 0.000518\n",
      "Epoch 11259/40000, Loss: 5.249383320915513e-05, Learning Rate: 0.000518\n",
      "Epoch 11260/40000, Loss: 8.180591248674318e-05, Learning Rate: 0.000518\n",
      "Epoch 11261/40000, Loss: 4.460450873011723e-05, Learning Rate: 0.000518\n",
      "Epoch 11262/40000, Loss: 4.433439971762709e-05, Learning Rate: 0.000518\n",
      "Epoch 11263/40000, Loss: 8.122773579088971e-05, Learning Rate: 0.000518\n",
      "Epoch 11264/40000, Loss: 3.585398007999174e-05, Learning Rate: 0.000518\n",
      "Epoch 11265/40000, Loss: 0.00013202855188865215, Learning Rate: 0.000518\n",
      "Epoch 11266/40000, Loss: 2.610972660477273e-05, Learning Rate: 0.000517\n",
      "Epoch 11267/40000, Loss: 9.884347673505545e-05, Learning Rate: 0.000517\n",
      "Epoch 11268/40000, Loss: 3.750195901375264e-05, Learning Rate: 0.000517\n",
      "Epoch 11269/40000, Loss: 8.081042324192822e-05, Learning Rate: 0.000517\n",
      "Epoch 11270/40000, Loss: 5.928912287345156e-05, Learning Rate: 0.000517\n",
      "Epoch 11271/40000, Loss: 7.281586294993758e-05, Learning Rate: 0.000517\n",
      "Epoch 11272/40000, Loss: 7.284530875040218e-05, Learning Rate: 0.000517\n",
      "Epoch 11273/40000, Loss: 8.587969205109403e-05, Learning Rate: 0.000517\n",
      "Epoch 11274/40000, Loss: 2.8327578547759913e-05, Learning Rate: 0.000517\n",
      "Epoch 11275/40000, Loss: 0.00010000725160352886, Learning Rate: 0.000517\n",
      "Epoch 11276/40000, Loss: 4.820591493626125e-05, Learning Rate: 0.000517\n",
      "Epoch 11277/40000, Loss: 2.9133425414329395e-05, Learning Rate: 0.000517\n",
      "Epoch 11278/40000, Loss: 0.00012451117800083011, Learning Rate: 0.000517\n",
      "Epoch 11279/40000, Loss: 8.954360237112269e-05, Learning Rate: 0.000517\n",
      "Epoch 11280/40000, Loss: 6.183804362080991e-05, Learning Rate: 0.000517\n",
      "Epoch 11281/40000, Loss: 0.00011262281623203307, Learning Rate: 0.000517\n",
      "Epoch 11282/40000, Loss: 6.368952745106071e-05, Learning Rate: 0.000516\n",
      "Epoch 11283/40000, Loss: 0.00010149209992960095, Learning Rate: 0.000516\n",
      "Epoch 11284/40000, Loss: 0.00010365366324549541, Learning Rate: 0.000516\n",
      "Epoch 11285/40000, Loss: 3.609149280237034e-05, Learning Rate: 0.000516\n",
      "Epoch 11286/40000, Loss: 5.607031562249176e-05, Learning Rate: 0.000516\n",
      "Epoch 11287/40000, Loss: 8.647966024000198e-05, Learning Rate: 0.000516\n",
      "Epoch 11288/40000, Loss: 2.383098762948066e-05, Learning Rate: 0.000516\n",
      "Epoch 11289/40000, Loss: 5.598427014774643e-05, Learning Rate: 0.000516\n",
      "Epoch 11290/40000, Loss: 4.297629857319407e-05, Learning Rate: 0.000516\n",
      "Epoch 11291/40000, Loss: 6.673974712612107e-05, Learning Rate: 0.000516\n",
      "Epoch 11292/40000, Loss: 4.5071614295011386e-05, Learning Rate: 0.000516\n",
      "Epoch 11293/40000, Loss: 2.6172425350523554e-05, Learning Rate: 0.000516\n",
      "Epoch 11294/40000, Loss: 8.173248352250084e-05, Learning Rate: 0.000516\n",
      "Epoch 11295/40000, Loss: 5.7492547057336196e-05, Learning Rate: 0.000516\n",
      "Epoch 11296/40000, Loss: 4.8884059651754797e-05, Learning Rate: 0.000516\n",
      "Epoch 11297/40000, Loss: 2.656479955476243e-05, Learning Rate: 0.000516\n",
      "Epoch 11298/40000, Loss: 4.383004124974832e-05, Learning Rate: 0.000515\n",
      "Epoch 11299/40000, Loss: 4.313201498007402e-05, Learning Rate: 0.000515\n",
      "Epoch 11300/40000, Loss: 8.207143400795758e-05, Learning Rate: 0.000515\n",
      "Epoch 11301/40000, Loss: 8.015673665795475e-05, Learning Rate: 0.000515\n",
      "Epoch 11302/40000, Loss: 7.996876956894994e-05, Learning Rate: 0.000515\n",
      "Epoch 11303/40000, Loss: 4.872700083069503e-05, Learning Rate: 0.000515\n",
      "Epoch 11304/40000, Loss: 6.845520692877471e-05, Learning Rate: 0.000515\n",
      "Epoch 11305/40000, Loss: 8.077463280642405e-05, Learning Rate: 0.000515\n",
      "Epoch 11306/40000, Loss: 7.966317934915423e-05, Learning Rate: 0.000515\n",
      "Epoch 11307/40000, Loss: 4.2312552977818996e-05, Learning Rate: 0.000515\n",
      "Epoch 11308/40000, Loss: 4.761486707138829e-05, Learning Rate: 0.000515\n",
      "Epoch 11309/40000, Loss: 5.147580668563023e-05, Learning Rate: 0.000515\n",
      "Epoch 11310/40000, Loss: 5.145854811416939e-05, Learning Rate: 0.000515\n",
      "Epoch 11311/40000, Loss: 4.2262628994649276e-05, Learning Rate: 0.000515\n",
      "Epoch 11312/40000, Loss: 8.138584234984592e-05, Learning Rate: 0.000515\n",
      "Epoch 11313/40000, Loss: 4.9078866140916944e-05, Learning Rate: 0.000515\n",
      "Epoch 11314/40000, Loss: 4.863561116508208e-05, Learning Rate: 0.000515\n",
      "Epoch 11315/40000, Loss: 8.105501183308661e-05, Learning Rate: 0.000514\n",
      "Epoch 11316/40000, Loss: 6.876565021229908e-05, Learning Rate: 0.000514\n",
      "Epoch 11317/40000, Loss: 5.2230709115974605e-05, Learning Rate: 0.000514\n",
      "Epoch 11318/40000, Loss: 4.723728125100024e-05, Learning Rate: 0.000514\n",
      "Epoch 11319/40000, Loss: 5.083617725176737e-05, Learning Rate: 0.000514\n",
      "Epoch 11320/40000, Loss: 7.953548629302531e-05, Learning Rate: 0.000514\n",
      "Epoch 11321/40000, Loss: 4.639805410988629e-05, Learning Rate: 0.000514\n",
      "Epoch 11322/40000, Loss: 4.6463406761176884e-05, Learning Rate: 0.000514\n",
      "Epoch 11323/40000, Loss: 4.5909495383966714e-05, Learning Rate: 0.000514\n",
      "Epoch 11324/40000, Loss: 4.134291157242842e-05, Learning Rate: 0.000514\n",
      "Epoch 11325/40000, Loss: 7.890702545410022e-05, Learning Rate: 0.000514\n",
      "Epoch 11326/40000, Loss: 7.867072417866439e-05, Learning Rate: 0.000514\n",
      "Epoch 11327/40000, Loss: 4.128227010369301e-05, Learning Rate: 0.000514\n",
      "Epoch 11328/40000, Loss: 4.1586681618355215e-05, Learning Rate: 0.000514\n",
      "Epoch 11329/40000, Loss: 6.661414226982743e-05, Learning Rate: 0.000514\n",
      "Epoch 11330/40000, Loss: 5.245604916126467e-05, Learning Rate: 0.000514\n",
      "Epoch 11331/40000, Loss: 8.185144542949274e-05, Learning Rate: 0.000513\n",
      "Epoch 11332/40000, Loss: 7.261555583681911e-05, Learning Rate: 0.000513\n",
      "Epoch 11333/40000, Loss: 4.83350086142309e-05, Learning Rate: 0.000513\n",
      "Epoch 11334/40000, Loss: 1.9460407202132046e-05, Learning Rate: 0.000513\n",
      "Epoch 11335/40000, Loss: 7.039777119643986e-05, Learning Rate: 0.000513\n",
      "Epoch 11336/40000, Loss: 5.1934857765445486e-05, Learning Rate: 0.000513\n",
      "Epoch 11337/40000, Loss: 4.89418744109571e-05, Learning Rate: 0.000513\n",
      "Epoch 11338/40000, Loss: 4.838363383896649e-05, Learning Rate: 0.000513\n",
      "Epoch 11339/40000, Loss: 8.116300887195393e-05, Learning Rate: 0.000513\n",
      "Epoch 11340/40000, Loss: 7.358430593740195e-05, Learning Rate: 0.000513\n",
      "Epoch 11341/40000, Loss: 4.272357909940183e-05, Learning Rate: 0.000513\n",
      "Epoch 11342/40000, Loss: 2.6985588192474097e-05, Learning Rate: 0.000513\n",
      "Epoch 11343/40000, Loss: 7.363870099652559e-05, Learning Rate: 0.000513\n",
      "Epoch 11344/40000, Loss: 9.08778965822421e-05, Learning Rate: 0.000513\n",
      "Epoch 11345/40000, Loss: 2.318891347385943e-05, Learning Rate: 0.000513\n",
      "Epoch 11346/40000, Loss: 6.675736221950501e-05, Learning Rate: 0.000513\n",
      "Epoch 11347/40000, Loss: 0.0001061370421666652, Learning Rate: 0.000512\n",
      "Epoch 11348/40000, Loss: 0.00010300090070813894, Learning Rate: 0.000512\n",
      "Epoch 11349/40000, Loss: 6.127967935753986e-05, Learning Rate: 0.000512\n",
      "Epoch 11350/40000, Loss: 6.707301508868113e-05, Learning Rate: 0.000512\n",
      "Epoch 11351/40000, Loss: 6.096020297263749e-05, Learning Rate: 0.000512\n",
      "Epoch 11352/40000, Loss: 0.00010902732901740819, Learning Rate: 0.000512\n",
      "Epoch 11353/40000, Loss: 6.97011491865851e-05, Learning Rate: 0.000512\n",
      "Epoch 11354/40000, Loss: 6.36429394944571e-05, Learning Rate: 0.000512\n",
      "Epoch 11355/40000, Loss: 5.808960850117728e-05, Learning Rate: 0.000512\n",
      "Epoch 11356/40000, Loss: 9.404788579558954e-05, Learning Rate: 0.000512\n",
      "Epoch 11357/40000, Loss: 4.6879191359039396e-05, Learning Rate: 0.000512\n",
      "Epoch 11358/40000, Loss: 7.450737757608294e-05, Learning Rate: 0.000512\n",
      "Epoch 11359/40000, Loss: 5.362435331335291e-05, Learning Rate: 0.000512\n",
      "Epoch 11360/40000, Loss: 5.0909857236547396e-05, Learning Rate: 0.000512\n",
      "Epoch 11361/40000, Loss: 5.3839918109588325e-05, Learning Rate: 0.000512\n",
      "Epoch 11362/40000, Loss: 2.269579636049457e-05, Learning Rate: 0.000512\n",
      "Epoch 11363/40000, Loss: 4.9795216909842566e-05, Learning Rate: 0.000511\n",
      "Epoch 11364/40000, Loss: 4.873689977102913e-05, Learning Rate: 0.000511\n",
      "Epoch 11365/40000, Loss: 4.9902846512850374e-05, Learning Rate: 0.000511\n",
      "Epoch 11366/40000, Loss: 5.1071710913674906e-05, Learning Rate: 0.000511\n",
      "Epoch 11367/40000, Loss: 7.944292156025767e-05, Learning Rate: 0.000511\n",
      "Epoch 11368/40000, Loss: 4.385397187434137e-05, Learning Rate: 0.000511\n",
      "Epoch 11369/40000, Loss: 8.510169573128223e-05, Learning Rate: 0.000511\n",
      "Epoch 11370/40000, Loss: 5.080384653410874e-05, Learning Rate: 0.000511\n",
      "Epoch 11371/40000, Loss: 9.408581536263227e-05, Learning Rate: 0.000511\n",
      "Epoch 11372/40000, Loss: 4.967542190570384e-05, Learning Rate: 0.000511\n",
      "Epoch 11373/40000, Loss: 9.092909749597311e-05, Learning Rate: 0.000511\n",
      "Epoch 11374/40000, Loss: 8.350237476406619e-05, Learning Rate: 0.000511\n",
      "Epoch 11375/40000, Loss: 5.272390990285203e-05, Learning Rate: 0.000511\n",
      "Epoch 11376/40000, Loss: 5.2002556913066655e-05, Learning Rate: 0.000511\n",
      "Epoch 11377/40000, Loss: 4.992513277102262e-05, Learning Rate: 0.000511\n",
      "Epoch 11378/40000, Loss: 4.2404732084833086e-05, Learning Rate: 0.000511\n",
      "Epoch 11379/40000, Loss: 6.846871110610664e-05, Learning Rate: 0.000511\n",
      "Epoch 11380/40000, Loss: 5.2674553444376215e-05, Learning Rate: 0.000510\n",
      "Epoch 11381/40000, Loss: 2.0931591279804707e-05, Learning Rate: 0.000510\n",
      "Epoch 11382/40000, Loss: 1.957084532477893e-05, Learning Rate: 0.000510\n",
      "Epoch 11383/40000, Loss: 8.094801160041243e-05, Learning Rate: 0.000510\n",
      "Epoch 11384/40000, Loss: 6.760043470421806e-05, Learning Rate: 0.000510\n",
      "Epoch 11385/40000, Loss: 4.202173658995889e-05, Learning Rate: 0.000510\n",
      "Epoch 11386/40000, Loss: 4.803553019883111e-05, Learning Rate: 0.000510\n",
      "Epoch 11387/40000, Loss: 4.274016828276217e-05, Learning Rate: 0.000510\n",
      "Epoch 11388/40000, Loss: 8.21693756734021e-05, Learning Rate: 0.000510\n",
      "Epoch 11389/40000, Loss: 8.025090210139751e-05, Learning Rate: 0.000510\n",
      "Epoch 11390/40000, Loss: 4.864017682848498e-05, Learning Rate: 0.000510\n",
      "Epoch 11391/40000, Loss: 2.0477193174883723e-05, Learning Rate: 0.000510\n",
      "Epoch 11392/40000, Loss: 4.993765469407663e-05, Learning Rate: 0.000510\n",
      "Epoch 11393/40000, Loss: 6.832414510427043e-05, Learning Rate: 0.000510\n",
      "Epoch 11394/40000, Loss: 5.152028097654693e-05, Learning Rate: 0.000510\n",
      "Epoch 11395/40000, Loss: 4.982043174095452e-05, Learning Rate: 0.000510\n",
      "Epoch 11396/40000, Loss: 2.1356372599257156e-05, Learning Rate: 0.000509\n",
      "Epoch 11397/40000, Loss: 5.418887303676456e-05, Learning Rate: 0.000509\n",
      "Epoch 11398/40000, Loss: 4.924101085634902e-05, Learning Rate: 0.000509\n",
      "Epoch 11399/40000, Loss: 7.98995461082086e-05, Learning Rate: 0.000509\n",
      "Epoch 11400/40000, Loss: 5.280230107018724e-05, Learning Rate: 0.000509\n",
      "Epoch 11401/40000, Loss: 5.1261049520689994e-05, Learning Rate: 0.000509\n",
      "Epoch 11402/40000, Loss: 8.170623914338648e-05, Learning Rate: 0.000509\n",
      "Epoch 11403/40000, Loss: 4.602857734425925e-05, Learning Rate: 0.000509\n",
      "Epoch 11404/40000, Loss: 4.3943789933109656e-05, Learning Rate: 0.000509\n",
      "Epoch 11405/40000, Loss: 4.7976594942156225e-05, Learning Rate: 0.000509\n",
      "Epoch 11406/40000, Loss: 5.759957275586203e-05, Learning Rate: 0.000509\n",
      "Epoch 11407/40000, Loss: 5.986868927720934e-05, Learning Rate: 0.000509\n",
      "Epoch 11408/40000, Loss: 6.522000330733135e-05, Learning Rate: 0.000509\n",
      "Epoch 11409/40000, Loss: 7.88472025305964e-05, Learning Rate: 0.000509\n",
      "Epoch 11410/40000, Loss: 5.2945404604543e-05, Learning Rate: 0.000509\n",
      "Epoch 11411/40000, Loss: 8.816818444756791e-05, Learning Rate: 0.000509\n",
      "Epoch 11412/40000, Loss: 5.6390130339423195e-05, Learning Rate: 0.000508\n",
      "Epoch 11413/40000, Loss: 4.8888690798776224e-05, Learning Rate: 0.000508\n",
      "Epoch 11414/40000, Loss: 4.579799860948697e-05, Learning Rate: 0.000508\n",
      "Epoch 11415/40000, Loss: 8.61088847159408e-05, Learning Rate: 0.000508\n",
      "Epoch 11416/40000, Loss: 8.464512211503461e-05, Learning Rate: 0.000508\n",
      "Epoch 11417/40000, Loss: 4.578369771479629e-05, Learning Rate: 0.000508\n",
      "Epoch 11418/40000, Loss: 0.00010389125964138657, Learning Rate: 0.000508\n",
      "Epoch 11419/40000, Loss: 5.25474279129412e-05, Learning Rate: 0.000508\n",
      "Epoch 11420/40000, Loss: 7.299840945051983e-05, Learning Rate: 0.000508\n",
      "Epoch 11421/40000, Loss: 6.04095148446504e-05, Learning Rate: 0.000508\n",
      "Epoch 11422/40000, Loss: 8.69610084919259e-05, Learning Rate: 0.000508\n",
      "Epoch 11423/40000, Loss: 4.696610267274082e-05, Learning Rate: 0.000508\n",
      "Epoch 11424/40000, Loss: 7.681227725697681e-05, Learning Rate: 0.000508\n",
      "Epoch 11425/40000, Loss: 5.2850380598101765e-05, Learning Rate: 0.000508\n",
      "Epoch 11426/40000, Loss: 7.628511229995638e-05, Learning Rate: 0.000508\n",
      "Epoch 11427/40000, Loss: 7.532310701208189e-05, Learning Rate: 0.000508\n",
      "Epoch 11428/40000, Loss: 2.9451763111865148e-05, Learning Rate: 0.000508\n",
      "Epoch 11429/40000, Loss: 5.482537017087452e-05, Learning Rate: 0.000507\n",
      "Epoch 11430/40000, Loss: 5.9646474255714566e-05, Learning Rate: 0.000507\n",
      "Epoch 11431/40000, Loss: 2.9175345844123513e-05, Learning Rate: 0.000507\n",
      "Epoch 11432/40000, Loss: 2.5539124180795625e-05, Learning Rate: 0.000507\n",
      "Epoch 11433/40000, Loss: 5.418589717010036e-05, Learning Rate: 0.000507\n",
      "Epoch 11434/40000, Loss: 7.614606147399172e-05, Learning Rate: 0.000507\n",
      "Epoch 11435/40000, Loss: 2.8813403332605958e-05, Learning Rate: 0.000507\n",
      "Epoch 11436/40000, Loss: 7.970791921252385e-05, Learning Rate: 0.000507\n",
      "Epoch 11437/40000, Loss: 5.6296201364602894e-05, Learning Rate: 0.000507\n",
      "Epoch 11438/40000, Loss: 5.3398849559016526e-05, Learning Rate: 0.000507\n",
      "Epoch 11439/40000, Loss: 8.688845264259726e-05, Learning Rate: 0.000507\n",
      "Epoch 11440/40000, Loss: 7.291761721717194e-05, Learning Rate: 0.000507\n",
      "Epoch 11441/40000, Loss: 5.72890457988251e-05, Learning Rate: 0.000507\n",
      "Epoch 11442/40000, Loss: 2.6714216801337898e-05, Learning Rate: 0.000507\n",
      "Epoch 11443/40000, Loss: 8.569000783609226e-05, Learning Rate: 0.000507\n",
      "Epoch 11444/40000, Loss: 6.305753777269274e-05, Learning Rate: 0.000507\n",
      "Epoch 11445/40000, Loss: 7.525271939812228e-05, Learning Rate: 0.000506\n",
      "Epoch 11446/40000, Loss: 8.800431533018127e-05, Learning Rate: 0.000506\n",
      "Epoch 11447/40000, Loss: 8.412406896241009e-05, Learning Rate: 0.000506\n",
      "Epoch 11448/40000, Loss: 4.748128048959188e-05, Learning Rate: 0.000506\n",
      "Epoch 11449/40000, Loss: 8.599939610576257e-05, Learning Rate: 0.000506\n",
      "Epoch 11450/40000, Loss: 5.5594467994524166e-05, Learning Rate: 0.000506\n",
      "Epoch 11451/40000, Loss: 0.00010394935088697821, Learning Rate: 0.000506\n",
      "Epoch 11452/40000, Loss: 5.959023837931454e-05, Learning Rate: 0.000506\n",
      "Epoch 11453/40000, Loss: 5.221199171501212e-05, Learning Rate: 0.000506\n",
      "Epoch 11454/40000, Loss: 7.480871136067435e-05, Learning Rate: 0.000506\n",
      "Epoch 11455/40000, Loss: 8.132230141200125e-05, Learning Rate: 0.000506\n",
      "Epoch 11456/40000, Loss: 7.174803613452241e-05, Learning Rate: 0.000506\n",
      "Epoch 11457/40000, Loss: 2.063948704744689e-05, Learning Rate: 0.000506\n",
      "Epoch 11458/40000, Loss: 5.1476705266395584e-05, Learning Rate: 0.000506\n",
      "Epoch 11459/40000, Loss: 8.059522224357352e-05, Learning Rate: 0.000506\n",
      "Epoch 11460/40000, Loss: 6.930731615284458e-05, Learning Rate: 0.000506\n",
      "Epoch 11461/40000, Loss: 4.1904422687366605e-05, Learning Rate: 0.000506\n",
      "Epoch 11462/40000, Loss: 1.9748742488445714e-05, Learning Rate: 0.000505\n",
      "Epoch 11463/40000, Loss: 4.698565317085013e-05, Learning Rate: 0.000505\n",
      "Epoch 11464/40000, Loss: 7.928045670269057e-05, Learning Rate: 0.000505\n",
      "Epoch 11465/40000, Loss: 6.646904512308538e-05, Learning Rate: 0.000505\n",
      "Epoch 11466/40000, Loss: 4.268830161890946e-05, Learning Rate: 0.000505\n",
      "Epoch 11467/40000, Loss: 6.753180787200108e-05, Learning Rate: 0.000505\n",
      "Epoch 11468/40000, Loss: 4.8100584535859525e-05, Learning Rate: 0.000505\n",
      "Epoch 11469/40000, Loss: 6.828043115092441e-05, Learning Rate: 0.000505\n",
      "Epoch 11470/40000, Loss: 5.1205082854721695e-05, Learning Rate: 0.000505\n",
      "Epoch 11471/40000, Loss: 1.952274760697037e-05, Learning Rate: 0.000505\n",
      "Epoch 11472/40000, Loss: 7.923222437966615e-05, Learning Rate: 0.000505\n",
      "Epoch 11473/40000, Loss: 7.889994594734162e-05, Learning Rate: 0.000505\n",
      "Epoch 11474/40000, Loss: 5.1737402827711776e-05, Learning Rate: 0.000505\n",
      "Epoch 11475/40000, Loss: 7.926725811557844e-05, Learning Rate: 0.000505\n",
      "Epoch 11476/40000, Loss: 5.084710574010387e-05, Learning Rate: 0.000505\n",
      "Epoch 11477/40000, Loss: 5.029971362091601e-05, Learning Rate: 0.000505\n",
      "Epoch 11478/40000, Loss: 4.247354809194803e-05, Learning Rate: 0.000504\n",
      "Epoch 11479/40000, Loss: 6.731354369549081e-05, Learning Rate: 0.000504\n",
      "Epoch 11480/40000, Loss: 7.934822497190908e-05, Learning Rate: 0.000504\n",
      "Epoch 11481/40000, Loss: 4.8967245675157756e-05, Learning Rate: 0.000504\n",
      "Epoch 11482/40000, Loss: 2.2281525161815807e-05, Learning Rate: 0.000504\n",
      "Epoch 11483/40000, Loss: 5.468811286846176e-05, Learning Rate: 0.000504\n",
      "Epoch 11484/40000, Loss: 5.2640840294770896e-05, Learning Rate: 0.000504\n",
      "Epoch 11485/40000, Loss: 7.943768287077546e-05, Learning Rate: 0.000504\n",
      "Epoch 11486/40000, Loss: 7.858148455852643e-05, Learning Rate: 0.000504\n",
      "Epoch 11487/40000, Loss: 4.146565333940089e-05, Learning Rate: 0.000504\n",
      "Epoch 11488/40000, Loss: 5.0540893425932154e-05, Learning Rate: 0.000504\n",
      "Epoch 11489/40000, Loss: 4.661106504499912e-05, Learning Rate: 0.000504\n",
      "Epoch 11490/40000, Loss: 7.948777783894911e-05, Learning Rate: 0.000504\n",
      "Epoch 11491/40000, Loss: 1.9121742298011668e-05, Learning Rate: 0.000504\n",
      "Epoch 11492/40000, Loss: 6.656490586465225e-05, Learning Rate: 0.000504\n",
      "Epoch 11493/40000, Loss: 4.127171996515244e-05, Learning Rate: 0.000504\n",
      "Epoch 11494/40000, Loss: 6.662021041847765e-05, Learning Rate: 0.000504\n",
      "Epoch 11495/40000, Loss: 6.675127224298194e-05, Learning Rate: 0.000503\n",
      "Epoch 11496/40000, Loss: 6.705939449602738e-05, Learning Rate: 0.000503\n",
      "Epoch 11497/40000, Loss: 5.0135888159275055e-05, Learning Rate: 0.000503\n",
      "Epoch 11498/40000, Loss: 4.641134000848979e-05, Learning Rate: 0.000503\n",
      "Epoch 11499/40000, Loss: 5.148477794136852e-05, Learning Rate: 0.000503\n",
      "Epoch 11500/40000, Loss: 7.95711821410805e-05, Learning Rate: 0.000503\n",
      "Epoch 11501/40000, Loss: 6.715562631143257e-05, Learning Rate: 0.000503\n",
      "Epoch 11502/40000, Loss: 4.1444352973485366e-05, Learning Rate: 0.000503\n",
      "Epoch 11503/40000, Loss: 1.8968527001561597e-05, Learning Rate: 0.000503\n",
      "Epoch 11504/40000, Loss: 5.0579244998516515e-05, Learning Rate: 0.000503\n",
      "Epoch 11505/40000, Loss: 5.02745715493802e-05, Learning Rate: 0.000503\n",
      "Epoch 11506/40000, Loss: 2.1209769329288974e-05, Learning Rate: 0.000503\n",
      "Epoch 11507/40000, Loss: 1.9723713194252923e-05, Learning Rate: 0.000503\n",
      "Epoch 11508/40000, Loss: 5.230580427451059e-05, Learning Rate: 0.000503\n",
      "Epoch 11509/40000, Loss: 8.25884853838943e-05, Learning Rate: 0.000503\n",
      "Epoch 11510/40000, Loss: 2.1458770788740367e-05, Learning Rate: 0.000503\n",
      "Epoch 11511/40000, Loss: 5.013229019823484e-05, Learning Rate: 0.000502\n",
      "Epoch 11512/40000, Loss: 5.079872425994836e-05, Learning Rate: 0.000502\n",
      "Epoch 11513/40000, Loss: 4.870719203609042e-05, Learning Rate: 0.000502\n",
      "Epoch 11514/40000, Loss: 2.0557461539283395e-05, Learning Rate: 0.000502\n",
      "Epoch 11515/40000, Loss: 5.140862049302086e-05, Learning Rate: 0.000502\n",
      "Epoch 11516/40000, Loss: 5.138828055351041e-05, Learning Rate: 0.000502\n",
      "Epoch 11517/40000, Loss: 7.037486648187041e-05, Learning Rate: 0.000502\n",
      "Epoch 11518/40000, Loss: 5.6818087614374235e-05, Learning Rate: 0.000502\n",
      "Epoch 11519/40000, Loss: 8.46155162435025e-05, Learning Rate: 0.000502\n",
      "Epoch 11520/40000, Loss: 5.312008215696551e-05, Learning Rate: 0.000502\n",
      "Epoch 11521/40000, Loss: 2.6938578230328858e-05, Learning Rate: 0.000502\n",
      "Epoch 11522/40000, Loss: 0.00010294495587004349, Learning Rate: 0.000502\n",
      "Epoch 11523/40000, Loss: 6.82334357406944e-05, Learning Rate: 0.000502\n",
      "Epoch 11524/40000, Loss: 0.00020638482237700373, Learning Rate: 0.000502\n",
      "Epoch 11525/40000, Loss: 6.834185478510335e-05, Learning Rate: 0.000502\n",
      "Epoch 11526/40000, Loss: 5.674994827131741e-05, Learning Rate: 0.000502\n",
      "Epoch 11527/40000, Loss: 7.900311175035313e-05, Learning Rate: 0.000502\n",
      "Epoch 11528/40000, Loss: 6.135358853498474e-05, Learning Rate: 0.000501\n",
      "Epoch 11529/40000, Loss: 6.195881724124774e-05, Learning Rate: 0.000501\n",
      "Epoch 11530/40000, Loss: 0.00017460550589021295, Learning Rate: 0.000501\n",
      "Epoch 11531/40000, Loss: 9.465374023420736e-05, Learning Rate: 0.000501\n",
      "Epoch 11532/40000, Loss: 0.00010607780131977051, Learning Rate: 0.000501\n",
      "Epoch 11533/40000, Loss: 8.710780093679205e-05, Learning Rate: 0.000501\n",
      "Epoch 11534/40000, Loss: 4.638392056222074e-05, Learning Rate: 0.000501\n",
      "Epoch 11535/40000, Loss: 5.4098367400001734e-05, Learning Rate: 0.000501\n",
      "Epoch 11536/40000, Loss: 7.655098306713626e-05, Learning Rate: 0.000501\n",
      "Epoch 11537/40000, Loss: 4.9330807087244466e-05, Learning Rate: 0.000501\n",
      "Epoch 11538/40000, Loss: 2.0825294996029697e-05, Learning Rate: 0.000501\n",
      "Epoch 11539/40000, Loss: 7.000147888902575e-05, Learning Rate: 0.000501\n",
      "Epoch 11540/40000, Loss: 8.02279100753367e-05, Learning Rate: 0.000501\n",
      "Epoch 11541/40000, Loss: 7.915541209513322e-05, Learning Rate: 0.000501\n",
      "Epoch 11542/40000, Loss: 4.219140100758523e-05, Learning Rate: 0.000501\n",
      "Epoch 11543/40000, Loss: 5.09975798195228e-05, Learning Rate: 0.000501\n",
      "Epoch 11544/40000, Loss: 4.692783113569021e-05, Learning Rate: 0.000501\n",
      "Epoch 11545/40000, Loss: 1.909505226649344e-05, Learning Rate: 0.000500\n",
      "Epoch 11546/40000, Loss: 7.833752169972286e-05, Learning Rate: 0.000500\n",
      "Epoch 11547/40000, Loss: 4.6349086915142834e-05, Learning Rate: 0.000500\n",
      "Epoch 11548/40000, Loss: 6.730401946697384e-05, Learning Rate: 0.000500\n",
      "Epoch 11549/40000, Loss: 4.1410552512388676e-05, Learning Rate: 0.000500\n",
      "Epoch 11550/40000, Loss: 5.0831666158046573e-05, Learning Rate: 0.000500\n",
      "Epoch 11551/40000, Loss: 1.9647872250061482e-05, Learning Rate: 0.000500\n",
      "Epoch 11552/40000, Loss: 6.909666990395635e-05, Learning Rate: 0.000500\n",
      "Epoch 11553/40000, Loss: 7.825760985724628e-05, Learning Rate: 0.000500\n",
      "Epoch 11554/40000, Loss: 5.0250520871486515e-05, Learning Rate: 0.000500\n",
      "Epoch 11555/40000, Loss: 4.115841875318438e-05, Learning Rate: 0.000500\n",
      "Epoch 11556/40000, Loss: 6.70122608426027e-05, Learning Rate: 0.000500\n",
      "Epoch 11557/40000, Loss: 4.6257682697614655e-05, Learning Rate: 0.000500\n",
      "Epoch 11558/40000, Loss: 4.589813033817336e-05, Learning Rate: 0.000500\n",
      "Epoch 11559/40000, Loss: 6.675260374322534e-05, Learning Rate: 0.000500\n",
      "Epoch 11560/40000, Loss: 7.831182301742956e-05, Learning Rate: 0.000500\n",
      "Epoch 11561/40000, Loss: 4.996832285542041e-05, Learning Rate: 0.000499\n",
      "Epoch 11562/40000, Loss: 7.815561548341066e-05, Learning Rate: 0.000499\n",
      "Epoch 11563/40000, Loss: 7.262073631864041e-05, Learning Rate: 0.000499\n",
      "Epoch 11564/40000, Loss: 1.9418324882281013e-05, Learning Rate: 0.000499\n",
      "Epoch 11565/40000, Loss: 4.126585918129422e-05, Learning Rate: 0.000499\n",
      "Epoch 11566/40000, Loss: 5.0878734327852726e-05, Learning Rate: 0.000499\n",
      "Epoch 11567/40000, Loss: 4.925701068714261e-05, Learning Rate: 0.000499\n",
      "Epoch 11568/40000, Loss: 2.07271605177084e-05, Learning Rate: 0.000499\n",
      "Epoch 11569/40000, Loss: 7.844694482628256e-05, Learning Rate: 0.000499\n",
      "Epoch 11570/40000, Loss: 6.727065920131281e-05, Learning Rate: 0.000499\n",
      "Epoch 11571/40000, Loss: 4.1418708860874176e-05, Learning Rate: 0.000499\n",
      "Epoch 11572/40000, Loss: 4.738391362479888e-05, Learning Rate: 0.000499\n",
      "Epoch 11573/40000, Loss: 6.719903467455879e-05, Learning Rate: 0.000499\n",
      "Epoch 11574/40000, Loss: 7.886833918746561e-05, Learning Rate: 0.000499\n",
      "Epoch 11575/40000, Loss: 4.432331115822308e-05, Learning Rate: 0.000499\n",
      "Epoch 11576/40000, Loss: 6.762217526556924e-05, Learning Rate: 0.000499\n",
      "Epoch 11577/40000, Loss: 4.7167548473225906e-05, Learning Rate: 0.000499\n",
      "Epoch 11578/40000, Loss: 1.9285676899016835e-05, Learning Rate: 0.000498\n",
      "Epoch 11579/40000, Loss: 4.248758341418579e-05, Learning Rate: 0.000498\n",
      "Epoch 11580/40000, Loss: 2.1190244297031313e-05, Learning Rate: 0.000498\n",
      "Epoch 11581/40000, Loss: 5.065674486104399e-05, Learning Rate: 0.000498\n",
      "Epoch 11582/40000, Loss: 2.0002975361421704e-05, Learning Rate: 0.000498\n",
      "Epoch 11583/40000, Loss: 6.826184107922018e-05, Learning Rate: 0.000498\n",
      "Epoch 11584/40000, Loss: 6.905874033691362e-05, Learning Rate: 0.000498\n",
      "Epoch 11585/40000, Loss: 6.796155503252521e-05, Learning Rate: 0.000498\n",
      "Epoch 11586/40000, Loss: 5.218777369009331e-05, Learning Rate: 0.000498\n",
      "Epoch 11587/40000, Loss: 4.806793367606588e-05, Learning Rate: 0.000498\n",
      "Epoch 11588/40000, Loss: 2.0248862711014226e-05, Learning Rate: 0.000498\n",
      "Epoch 11589/40000, Loss: 4.15053655160591e-05, Learning Rate: 0.000498\n",
      "Epoch 11590/40000, Loss: 2.1007868781453e-05, Learning Rate: 0.000498\n",
      "Epoch 11591/40000, Loss: 5.0542905228212476e-05, Learning Rate: 0.000498\n",
      "Epoch 11592/40000, Loss: 2.3779490220476873e-05, Learning Rate: 0.000498\n",
      "Epoch 11593/40000, Loss: 5.1614038966363296e-05, Learning Rate: 0.000498\n",
      "Epoch 11594/40000, Loss: 6.843630399089307e-05, Learning Rate: 0.000498\n",
      "Epoch 11595/40000, Loss: 2.4133449187502265e-05, Learning Rate: 0.000497\n",
      "Epoch 11596/40000, Loss: 5.5086213251343e-05, Learning Rate: 0.000497\n",
      "Epoch 11597/40000, Loss: 3.4108372346963733e-05, Learning Rate: 0.000497\n",
      "Epoch 11598/40000, Loss: 0.00010984972323058173, Learning Rate: 0.000497\n",
      "Epoch 11599/40000, Loss: 9.311673056799918e-05, Learning Rate: 0.000497\n",
      "Epoch 11600/40000, Loss: 0.00011057002848247066, Learning Rate: 0.000497\n",
      "Epoch 11601/40000, Loss: 8.786132093518972e-05, Learning Rate: 0.000497\n",
      "Epoch 11602/40000, Loss: 6.82920144754462e-05, Learning Rate: 0.000497\n",
      "Epoch 11603/40000, Loss: 0.00010158912482438609, Learning Rate: 0.000497\n",
      "Epoch 11604/40000, Loss: 6.96348724886775e-05, Learning Rate: 0.000497\n",
      "Epoch 11605/40000, Loss: 9.956349094863981e-05, Learning Rate: 0.000497\n",
      "Epoch 11606/40000, Loss: 0.00012104080815333873, Learning Rate: 0.000497\n",
      "Epoch 11607/40000, Loss: 7.936966721899807e-05, Learning Rate: 0.000497\n",
      "Epoch 11608/40000, Loss: 7.343530160142109e-05, Learning Rate: 0.000497\n",
      "Epoch 11609/40000, Loss: 4.463779987418093e-05, Learning Rate: 0.000497\n",
      "Epoch 11610/40000, Loss: 0.00014085143629927188, Learning Rate: 0.000497\n",
      "Epoch 11611/40000, Loss: 7.601615652674809e-05, Learning Rate: 0.000496\n",
      "Epoch 11612/40000, Loss: 3.7678317312384024e-05, Learning Rate: 0.000496\n",
      "Epoch 11613/40000, Loss: 3.318282324471511e-05, Learning Rate: 0.000496\n",
      "Epoch 11614/40000, Loss: 0.00010338348511140794, Learning Rate: 0.000496\n",
      "Epoch 11615/40000, Loss: 0.00010025395749835297, Learning Rate: 0.000496\n",
      "Epoch 11616/40000, Loss: 5.893710476811975e-05, Learning Rate: 0.000496\n",
      "Epoch 11617/40000, Loss: 9.975067223422229e-05, Learning Rate: 0.000496\n",
      "Epoch 11618/40000, Loss: 3.95990427932702e-05, Learning Rate: 0.000496\n",
      "Epoch 11619/40000, Loss: 0.00011371351865818724, Learning Rate: 0.000496\n",
      "Epoch 11620/40000, Loss: 8.179932774510235e-05, Learning Rate: 0.000496\n",
      "Epoch 11621/40000, Loss: 3.617722541093826e-05, Learning Rate: 0.000496\n",
      "Epoch 11622/40000, Loss: 6.887801282573491e-05, Learning Rate: 0.000496\n",
      "Epoch 11623/40000, Loss: 5.675378633895889e-05, Learning Rate: 0.000496\n",
      "Epoch 11624/40000, Loss: 5.4800835641799495e-05, Learning Rate: 0.000496\n",
      "Epoch 11625/40000, Loss: 7.87454191595316e-05, Learning Rate: 0.000496\n",
      "Epoch 11626/40000, Loss: 3.395343810552731e-05, Learning Rate: 0.000496\n",
      "Epoch 11627/40000, Loss: 3.227648630854674e-05, Learning Rate: 0.000496\n",
      "Epoch 11628/40000, Loss: 6.733209738740698e-05, Learning Rate: 0.000495\n",
      "Epoch 11629/40000, Loss: 5.341694850358181e-05, Learning Rate: 0.000495\n",
      "Epoch 11630/40000, Loss: 5.105757009005174e-05, Learning Rate: 0.000495\n",
      "Epoch 11631/40000, Loss: 5.6373930419795215e-05, Learning Rate: 0.000495\n",
      "Epoch 11632/40000, Loss: 5.924495417275466e-05, Learning Rate: 0.000495\n",
      "Epoch 11633/40000, Loss: 2.2962747607380152e-05, Learning Rate: 0.000495\n",
      "Epoch 11634/40000, Loss: 4.379914389573969e-05, Learning Rate: 0.000495\n",
      "Epoch 11635/40000, Loss: 5.043731289333664e-05, Learning Rate: 0.000495\n",
      "Epoch 11636/40000, Loss: 6.690428563160822e-05, Learning Rate: 0.000495\n",
      "Epoch 11637/40000, Loss: 5.029373278375715e-05, Learning Rate: 0.000495\n",
      "Epoch 11638/40000, Loss: 5.0225429731654e-05, Learning Rate: 0.000495\n",
      "Epoch 11639/40000, Loss: 5.108664117869921e-05, Learning Rate: 0.000495\n",
      "Epoch 11640/40000, Loss: 6.73949543852359e-05, Learning Rate: 0.000495\n",
      "Epoch 11641/40000, Loss: 6.683720130240545e-05, Learning Rate: 0.000495\n",
      "Epoch 11642/40000, Loss: 7.900532364146784e-05, Learning Rate: 0.000495\n",
      "Epoch 11643/40000, Loss: 7.844243373256177e-05, Learning Rate: 0.000495\n",
      "Epoch 11644/40000, Loss: 7.840099715394899e-05, Learning Rate: 0.000495\n",
      "Epoch 11645/40000, Loss: 5.012882320443168e-05, Learning Rate: 0.000494\n",
      "Epoch 11646/40000, Loss: 7.840005127945915e-05, Learning Rate: 0.000494\n",
      "Epoch 11647/40000, Loss: 4.965556945535354e-05, Learning Rate: 0.000494\n",
      "Epoch 11648/40000, Loss: 4.6425728214671835e-05, Learning Rate: 0.000494\n",
      "Epoch 11649/40000, Loss: 4.6000935981282964e-05, Learning Rate: 0.000494\n",
      "Epoch 11650/40000, Loss: 4.5931996282888576e-05, Learning Rate: 0.000494\n",
      "Epoch 11651/40000, Loss: 6.552114064106718e-05, Learning Rate: 0.000494\n",
      "Epoch 11652/40000, Loss: 4.153667759965174e-05, Learning Rate: 0.000494\n",
      "Epoch 11653/40000, Loss: 6.67371423332952e-05, Learning Rate: 0.000494\n",
      "Epoch 11654/40000, Loss: 4.429304681252688e-05, Learning Rate: 0.000494\n",
      "Epoch 11655/40000, Loss: 1.9102935766568407e-05, Learning Rate: 0.000494\n",
      "Epoch 11656/40000, Loss: 6.640615174546838e-05, Learning Rate: 0.000494\n",
      "Epoch 11657/40000, Loss: 2.227235927421134e-05, Learning Rate: 0.000494\n",
      "Epoch 11658/40000, Loss: 7.819126039976254e-05, Learning Rate: 0.000494\n",
      "Epoch 11659/40000, Loss: 4.950720176566392e-05, Learning Rate: 0.000494\n",
      "Epoch 11660/40000, Loss: 4.945836917613633e-05, Learning Rate: 0.000494\n",
      "Epoch 11661/40000, Loss: 4.157304283580743e-05, Learning Rate: 0.000494\n",
      "Epoch 11662/40000, Loss: 4.1267987398896366e-05, Learning Rate: 0.000493\n",
      "Epoch 11663/40000, Loss: 7.956856279633939e-05, Learning Rate: 0.000493\n",
      "Epoch 11664/40000, Loss: 7.80787886469625e-05, Learning Rate: 0.000493\n",
      "Epoch 11665/40000, Loss: 4.147580693825148e-05, Learning Rate: 0.000493\n",
      "Epoch 11666/40000, Loss: 6.77376810926944e-05, Learning Rate: 0.000493\n",
      "Epoch 11667/40000, Loss: 6.623818626394495e-05, Learning Rate: 0.000493\n",
      "Epoch 11668/40000, Loss: 4.1538372897775844e-05, Learning Rate: 0.000493\n",
      "Epoch 11669/40000, Loss: 1.8788507077260874e-05, Learning Rate: 0.000493\n",
      "Epoch 11670/40000, Loss: 4.582549081533216e-05, Learning Rate: 0.000493\n",
      "Epoch 11671/40000, Loss: 7.787950016791001e-05, Learning Rate: 0.000493\n",
      "Epoch 11672/40000, Loss: 1.874436202342622e-05, Learning Rate: 0.000493\n",
      "Epoch 11673/40000, Loss: 7.806987559888512e-05, Learning Rate: 0.000493\n",
      "Epoch 11674/40000, Loss: 4.6104258217383176e-05, Learning Rate: 0.000493\n",
      "Epoch 11675/40000, Loss: 4.9220554501516744e-05, Learning Rate: 0.000493\n",
      "Epoch 11676/40000, Loss: 4.5235647121444345e-05, Learning Rate: 0.000493\n",
      "Epoch 11677/40000, Loss: 7.728989294264466e-05, Learning Rate: 0.000493\n",
      "Epoch 11678/40000, Loss: 6.502932228613645e-05, Learning Rate: 0.000493\n",
      "Epoch 11679/40000, Loss: 4.0559105400461704e-05, Learning Rate: 0.000492\n",
      "Epoch 11680/40000, Loss: 4.0652419556863606e-05, Learning Rate: 0.000492\n",
      "Epoch 11681/40000, Loss: 4.9274844059254974e-05, Learning Rate: 0.000492\n",
      "Epoch 11682/40000, Loss: 1.8411143173580058e-05, Learning Rate: 0.000492\n",
      "Epoch 11683/40000, Loss: 1.8339436792302877e-05, Learning Rate: 0.000492\n",
      "Epoch 11684/40000, Loss: 4.058197737322189e-05, Learning Rate: 0.000492\n",
      "Epoch 11685/40000, Loss: 7.813343108864501e-05, Learning Rate: 0.000492\n",
      "Epoch 11686/40000, Loss: 4.906241156277247e-05, Learning Rate: 0.000492\n",
      "Epoch 11687/40000, Loss: 4.5456679799826816e-05, Learning Rate: 0.000492\n",
      "Epoch 11688/40000, Loss: 4.04537859139964e-05, Learning Rate: 0.000492\n",
      "Epoch 11689/40000, Loss: 6.479187140939757e-05, Learning Rate: 0.000492\n",
      "Epoch 11690/40000, Loss: 1.8404723959974945e-05, Learning Rate: 0.000492\n",
      "Epoch 11691/40000, Loss: 4.88758014398627e-05, Learning Rate: 0.000492\n",
      "Epoch 11692/40000, Loss: 7.720659050391987e-05, Learning Rate: 0.000492\n",
      "Epoch 11693/40000, Loss: 6.501388270407915e-05, Learning Rate: 0.000492\n",
      "Epoch 11694/40000, Loss: 6.498125003417954e-05, Learning Rate: 0.000492\n",
      "Epoch 11695/40000, Loss: 7.750960503472015e-05, Learning Rate: 0.000492\n",
      "Epoch 11696/40000, Loss: 4.029210322187282e-05, Learning Rate: 0.000491\n",
      "Epoch 11697/40000, Loss: 4.8930007324088365e-05, Learning Rate: 0.000491\n",
      "Epoch 11698/40000, Loss: 1.8513468603487127e-05, Learning Rate: 0.000491\n",
      "Epoch 11699/40000, Loss: 7.729710341664031e-05, Learning Rate: 0.000491\n",
      "Epoch 11700/40000, Loss: 4.9163238145411015e-05, Learning Rate: 0.000491\n",
      "Epoch 11701/40000, Loss: 7.757628918625414e-05, Learning Rate: 0.000491\n",
      "Epoch 11702/40000, Loss: 6.483540346380323e-05, Learning Rate: 0.000491\n",
      "Epoch 11703/40000, Loss: 4.910874122288078e-05, Learning Rate: 0.000491\n",
      "Epoch 11704/40000, Loss: 4.541223097476177e-05, Learning Rate: 0.000491\n",
      "Epoch 11705/40000, Loss: 4.070841896464117e-05, Learning Rate: 0.000491\n",
      "Epoch 11706/40000, Loss: 4.985172927263193e-05, Learning Rate: 0.000491\n",
      "Epoch 11707/40000, Loss: 7.754699618089944e-05, Learning Rate: 0.000491\n",
      "Epoch 11708/40000, Loss: 4.566547067952342e-05, Learning Rate: 0.000491\n",
      "Epoch 11709/40000, Loss: 1.8414362784824334e-05, Learning Rate: 0.000491\n",
      "Epoch 11710/40000, Loss: 6.540575122926384e-05, Learning Rate: 0.000491\n",
      "Epoch 11711/40000, Loss: 4.636919038603082e-05, Learning Rate: 0.000491\n",
      "Epoch 11712/40000, Loss: 4.978394281351939e-05, Learning Rate: 0.000491\n",
      "Epoch 11713/40000, Loss: 1.893256194307469e-05, Learning Rate: 0.000490\n",
      "Epoch 11714/40000, Loss: 1.9260518456576392e-05, Learning Rate: 0.000490\n",
      "Epoch 11715/40000, Loss: 7.936194742796943e-05, Learning Rate: 0.000490\n",
      "Epoch 11716/40000, Loss: 1.954093750100583e-05, Learning Rate: 0.000490\n",
      "Epoch 11717/40000, Loss: 7.406262011500075e-05, Learning Rate: 0.000490\n",
      "Epoch 11718/40000, Loss: 7.98508626758121e-05, Learning Rate: 0.000490\n",
      "Epoch 11719/40000, Loss: 4.7927122068358585e-05, Learning Rate: 0.000490\n",
      "Epoch 11720/40000, Loss: 8.583930321037769e-05, Learning Rate: 0.000490\n",
      "Epoch 11721/40000, Loss: 2.0514653442660347e-05, Learning Rate: 0.000490\n",
      "Epoch 11722/40000, Loss: 4.2052055505337194e-05, Learning Rate: 0.000490\n",
      "Epoch 11723/40000, Loss: 7.0242713263724e-05, Learning Rate: 0.000490\n",
      "Epoch 11724/40000, Loss: 5.088145553600043e-05, Learning Rate: 0.000490\n",
      "Epoch 11725/40000, Loss: 8.26330651761964e-05, Learning Rate: 0.000490\n",
      "Epoch 11726/40000, Loss: 2.136773764505051e-05, Learning Rate: 0.000490\n",
      "Epoch 11727/40000, Loss: 8.723030623514205e-05, Learning Rate: 0.000490\n",
      "Epoch 11728/40000, Loss: 8.072237687883899e-05, Learning Rate: 0.000490\n",
      "Epoch 11729/40000, Loss: 7.070669380482286e-05, Learning Rate: 0.000490\n",
      "Epoch 11730/40000, Loss: 5.580190918408334e-05, Learning Rate: 0.000489\n",
      "Epoch 11731/40000, Loss: 2.5849436497082934e-05, Learning Rate: 0.000489\n",
      "Epoch 11732/40000, Loss: 0.0001300766016356647, Learning Rate: 0.000489\n",
      "Epoch 11733/40000, Loss: 5.955011511105113e-05, Learning Rate: 0.000489\n",
      "Epoch 11734/40000, Loss: 6.280992238316685e-05, Learning Rate: 0.000489\n",
      "Epoch 11735/40000, Loss: 9.69678076216951e-05, Learning Rate: 0.000489\n",
      "Epoch 11736/40000, Loss: 4.963322135154158e-05, Learning Rate: 0.000489\n",
      "Epoch 11737/40000, Loss: 5.7997072872240096e-05, Learning Rate: 0.000489\n",
      "Epoch 11738/40000, Loss: 5.490156763698906e-05, Learning Rate: 0.000489\n",
      "Epoch 11739/40000, Loss: 2.4570952518843114e-05, Learning Rate: 0.000489\n",
      "Epoch 11740/40000, Loss: 4.4992295443080366e-05, Learning Rate: 0.000489\n",
      "Epoch 11741/40000, Loss: 5.346102261682972e-05, Learning Rate: 0.000489\n",
      "Epoch 11742/40000, Loss: 4.715016984846443e-05, Learning Rate: 0.000489\n",
      "Epoch 11743/40000, Loss: 5.1288880058564246e-05, Learning Rate: 0.000489\n",
      "Epoch 11744/40000, Loss: 5.194281038711779e-05, Learning Rate: 0.000489\n",
      "Epoch 11745/40000, Loss: 7.354300032602623e-05, Learning Rate: 0.000489\n",
      "Epoch 11746/40000, Loss: 6.656770710833371e-05, Learning Rate: 0.000489\n",
      "Epoch 11747/40000, Loss: 3.062078758375719e-05, Learning Rate: 0.000488\n",
      "Epoch 11748/40000, Loss: 2.5974826712626964e-05, Learning Rate: 0.000488\n",
      "Epoch 11749/40000, Loss: 7.124924741219729e-05, Learning Rate: 0.000488\n",
      "Epoch 11750/40000, Loss: 5.6120374210877344e-05, Learning Rate: 0.000488\n",
      "Epoch 11751/40000, Loss: 5.4852047469466925e-05, Learning Rate: 0.000488\n",
      "Epoch 11752/40000, Loss: 3.407394251553342e-05, Learning Rate: 0.000488\n",
      "Epoch 11753/40000, Loss: 3.450041913310997e-05, Learning Rate: 0.000488\n",
      "Epoch 11754/40000, Loss: 4.304787944420241e-05, Learning Rate: 0.000488\n",
      "Epoch 11755/40000, Loss: 3.1173018214758486e-05, Learning Rate: 0.000488\n",
      "Epoch 11756/40000, Loss: 5.335891182767227e-05, Learning Rate: 0.000488\n",
      "Epoch 11757/40000, Loss: 4.645506487577222e-05, Learning Rate: 0.000488\n",
      "Epoch 11758/40000, Loss: 8.276272274088115e-05, Learning Rate: 0.000488\n",
      "Epoch 11759/40000, Loss: 5.873943155165762e-05, Learning Rate: 0.000488\n",
      "Epoch 11760/40000, Loss: 9.360409603687003e-05, Learning Rate: 0.000488\n",
      "Epoch 11761/40000, Loss: 5.797767880721949e-05, Learning Rate: 0.000488\n",
      "Epoch 11762/40000, Loss: 3.225455657229759e-05, Learning Rate: 0.000488\n",
      "Epoch 11763/40000, Loss: 0.00015623729268554598, Learning Rate: 0.000488\n",
      "Epoch 11764/40000, Loss: 9.830078488448635e-05, Learning Rate: 0.000487\n",
      "Epoch 11765/40000, Loss: 9.27395885810256e-05, Learning Rate: 0.000487\n",
      "Epoch 11766/40000, Loss: 0.00010154764459002763, Learning Rate: 0.000487\n",
      "Epoch 11767/40000, Loss: 6.7011387727689e-05, Learning Rate: 0.000487\n",
      "Epoch 11768/40000, Loss: 8.73496537678875e-05, Learning Rate: 0.000487\n",
      "Epoch 11769/40000, Loss: 7.101190567482263e-05, Learning Rate: 0.000487\n",
      "Epoch 11770/40000, Loss: 8.290457481052727e-05, Learning Rate: 0.000487\n",
      "Epoch 11771/40000, Loss: 5.3949970606481656e-05, Learning Rate: 0.000487\n",
      "Epoch 11772/40000, Loss: 4.51174819318112e-05, Learning Rate: 0.000487\n",
      "Epoch 11773/40000, Loss: 7.031333370832726e-05, Learning Rate: 0.000487\n",
      "Epoch 11774/40000, Loss: 8.766312384977937e-05, Learning Rate: 0.000487\n",
      "Epoch 11775/40000, Loss: 7.285510218935087e-05, Learning Rate: 0.000487\n",
      "Epoch 11776/40000, Loss: 0.00012650150165427476, Learning Rate: 0.000487\n",
      "Epoch 11777/40000, Loss: 9.570617112331092e-05, Learning Rate: 0.000487\n",
      "Epoch 11778/40000, Loss: 4.559343142318539e-05, Learning Rate: 0.000487\n",
      "Epoch 11779/40000, Loss: 3.0317529308376834e-05, Learning Rate: 0.000487\n",
      "Epoch 11780/40000, Loss: 8.758662443142384e-05, Learning Rate: 0.000487\n",
      "Epoch 11781/40000, Loss: 4.7280482249334455e-05, Learning Rate: 0.000486\n",
      "Epoch 11782/40000, Loss: 4.417033778736368e-05, Learning Rate: 0.000486\n",
      "Epoch 11783/40000, Loss: 2.251165824418422e-05, Learning Rate: 0.000486\n",
      "Epoch 11784/40000, Loss: 5.3573276090901345e-05, Learning Rate: 0.000486\n",
      "Epoch 11785/40000, Loss: 5.223519110586494e-05, Learning Rate: 0.000486\n",
      "Epoch 11786/40000, Loss: 4.91139326186385e-05, Learning Rate: 0.000486\n",
      "Epoch 11787/40000, Loss: 7.100752554833889e-05, Learning Rate: 0.000486\n",
      "Epoch 11788/40000, Loss: 6.697794742649421e-05, Learning Rate: 0.000486\n",
      "Epoch 11789/40000, Loss: 4.194913344690576e-05, Learning Rate: 0.000486\n",
      "Epoch 11790/40000, Loss: 6.739790114806965e-05, Learning Rate: 0.000486\n",
      "Epoch 11791/40000, Loss: 6.601917993975803e-05, Learning Rate: 0.000486\n",
      "Epoch 11792/40000, Loss: 1.9273977159173228e-05, Learning Rate: 0.000486\n",
      "Epoch 11793/40000, Loss: 4.1451919969404116e-05, Learning Rate: 0.000486\n",
      "Epoch 11794/40000, Loss: 4.079787322552875e-05, Learning Rate: 0.000486\n",
      "Epoch 11795/40000, Loss: 7.794051634846255e-05, Learning Rate: 0.000486\n",
      "Epoch 11796/40000, Loss: 6.582518108189106e-05, Learning Rate: 0.000486\n",
      "Epoch 11797/40000, Loss: 8.215467096306384e-05, Learning Rate: 0.000486\n",
      "Epoch 11798/40000, Loss: 1.8729451767285354e-05, Learning Rate: 0.000485\n",
      "Epoch 11799/40000, Loss: 1.8521008314564824e-05, Learning Rate: 0.000485\n",
      "Epoch 11800/40000, Loss: 7.834962161723524e-05, Learning Rate: 0.000485\n",
      "Epoch 11801/40000, Loss: 4.6259221562650055e-05, Learning Rate: 0.000485\n",
      "Epoch 11802/40000, Loss: 8.378725760849193e-05, Learning Rate: 0.000485\n",
      "Epoch 11803/40000, Loss: 7.962174277054146e-05, Learning Rate: 0.000485\n",
      "Epoch 11804/40000, Loss: 1.8674376406124793e-05, Learning Rate: 0.000485\n",
      "Epoch 11805/40000, Loss: 4.904336674371734e-05, Learning Rate: 0.000485\n",
      "Epoch 11806/40000, Loss: 4.6047607611399144e-05, Learning Rate: 0.000485\n",
      "Epoch 11807/40000, Loss: 1.8713082681642845e-05, Learning Rate: 0.000485\n",
      "Epoch 11808/40000, Loss: 7.83815557952039e-05, Learning Rate: 0.000485\n",
      "Epoch 11809/40000, Loss: 1.9243494534748606e-05, Learning Rate: 0.000485\n",
      "Epoch 11810/40000, Loss: 6.507273792522028e-05, Learning Rate: 0.000485\n",
      "Epoch 11811/40000, Loss: 7.854198338463902e-05, Learning Rate: 0.000485\n",
      "Epoch 11812/40000, Loss: 4.088116475031711e-05, Learning Rate: 0.000485\n",
      "Epoch 11813/40000, Loss: 1.8707423805608414e-05, Learning Rate: 0.000485\n",
      "Epoch 11814/40000, Loss: 4.621960033546202e-05, Learning Rate: 0.000485\n",
      "Epoch 11815/40000, Loss: 4.594823258230463e-05, Learning Rate: 0.000484\n",
      "Epoch 11816/40000, Loss: 6.539230525959283e-05, Learning Rate: 0.000484\n",
      "Epoch 11817/40000, Loss: 4.9000394938047975e-05, Learning Rate: 0.000484\n",
      "Epoch 11818/40000, Loss: 4.9378781113773584e-05, Learning Rate: 0.000484\n",
      "Epoch 11819/40000, Loss: 4.980921221431345e-05, Learning Rate: 0.000484\n",
      "Epoch 11820/40000, Loss: 6.53220631647855e-05, Learning Rate: 0.000484\n",
      "Epoch 11821/40000, Loss: 4.093849929631688e-05, Learning Rate: 0.000484\n",
      "Epoch 11822/40000, Loss: 6.529531674459577e-05, Learning Rate: 0.000484\n",
      "Epoch 11823/40000, Loss: 4.911280848318711e-05, Learning Rate: 0.000484\n",
      "Epoch 11824/40000, Loss: 4.105180414626375e-05, Learning Rate: 0.000484\n",
      "Epoch 11825/40000, Loss: 4.8854752094484866e-05, Learning Rate: 0.000484\n",
      "Epoch 11826/40000, Loss: 1.8487473425921053e-05, Learning Rate: 0.000484\n",
      "Epoch 11827/40000, Loss: 1.8579319657874294e-05, Learning Rate: 0.000484\n",
      "Epoch 11828/40000, Loss: 4.596238795784302e-05, Learning Rate: 0.000484\n",
      "Epoch 11829/40000, Loss: 4.63747710455209e-05, Learning Rate: 0.000484\n",
      "Epoch 11830/40000, Loss: 1.8585891666589305e-05, Learning Rate: 0.000484\n",
      "Epoch 11831/40000, Loss: 6.57279888400808e-05, Learning Rate: 0.000484\n",
      "Epoch 11832/40000, Loss: 4.740569420391694e-05, Learning Rate: 0.000483\n",
      "Epoch 11833/40000, Loss: 2.0208313799230382e-05, Learning Rate: 0.000483\n",
      "Epoch 11834/40000, Loss: 5.221269020694308e-05, Learning Rate: 0.000483\n",
      "Epoch 11835/40000, Loss: 2.1029965864727274e-05, Learning Rate: 0.000483\n",
      "Epoch 11836/40000, Loss: 7.954518514452502e-05, Learning Rate: 0.000483\n",
      "Epoch 11837/40000, Loss: 5.048766252002679e-05, Learning Rate: 0.000483\n",
      "Epoch 11838/40000, Loss: 8.18369589978829e-05, Learning Rate: 0.000483\n",
      "Epoch 11839/40000, Loss: 1.9100343706668355e-05, Learning Rate: 0.000483\n",
      "Epoch 11840/40000, Loss: 4.153375266469084e-05, Learning Rate: 0.000483\n",
      "Epoch 11841/40000, Loss: 5.9750473155872896e-05, Learning Rate: 0.000483\n",
      "Epoch 11842/40000, Loss: 5.799331484013237e-05, Learning Rate: 0.000483\n",
      "Epoch 11843/40000, Loss: 2.315137317054905e-05, Learning Rate: 0.000483\n",
      "Epoch 11844/40000, Loss: 2.2065887606004253e-05, Learning Rate: 0.000483\n",
      "Epoch 11845/40000, Loss: 5.2292140026111156e-05, Learning Rate: 0.000483\n",
      "Epoch 11846/40000, Loss: 5.220040475251153e-05, Learning Rate: 0.000483\n",
      "Epoch 11847/40000, Loss: 9.485098416917026e-05, Learning Rate: 0.000483\n",
      "Epoch 11848/40000, Loss: 0.00011436487693572417, Learning Rate: 0.000483\n",
      "Epoch 11849/40000, Loss: 9.36750293476507e-05, Learning Rate: 0.000483\n",
      "Epoch 11850/40000, Loss: 6.414227391360328e-05, Learning Rate: 0.000482\n",
      "Epoch 11851/40000, Loss: 5.663953925250098e-05, Learning Rate: 0.000482\n",
      "Epoch 11852/40000, Loss: 9.024524479173124e-05, Learning Rate: 0.000482\n",
      "Epoch 11853/40000, Loss: 8.127639739541337e-05, Learning Rate: 0.000482\n",
      "Epoch 11854/40000, Loss: 6.729903543600813e-05, Learning Rate: 0.000482\n",
      "Epoch 11855/40000, Loss: 0.00012481985322665423, Learning Rate: 0.000482\n",
      "Epoch 11856/40000, Loss: 3.221212682547048e-05, Learning Rate: 0.000482\n",
      "Epoch 11857/40000, Loss: 0.00010750671208370477, Learning Rate: 0.000482\n",
      "Epoch 11858/40000, Loss: 0.00010226448648609221, Learning Rate: 0.000482\n",
      "Epoch 11859/40000, Loss: 8.005311246961355e-05, Learning Rate: 0.000482\n",
      "Epoch 11860/40000, Loss: 7.705089956289157e-05, Learning Rate: 0.000482\n",
      "Epoch 11861/40000, Loss: 6.747462612111121e-05, Learning Rate: 0.000482\n",
      "Epoch 11862/40000, Loss: 9.202551154885441e-05, Learning Rate: 0.000482\n",
      "Epoch 11863/40000, Loss: 5.903691635467112e-05, Learning Rate: 0.000482\n",
      "Epoch 11864/40000, Loss: 5.7019846281036735e-05, Learning Rate: 0.000482\n",
      "Epoch 11865/40000, Loss: 5.742383291362785e-05, Learning Rate: 0.000482\n",
      "Epoch 11866/40000, Loss: 6.673557800240815e-05, Learning Rate: 0.000482\n",
      "Epoch 11867/40000, Loss: 0.00011638837167993188, Learning Rate: 0.000481\n",
      "Epoch 11868/40000, Loss: 2.8116452085669152e-05, Learning Rate: 0.000481\n",
      "Epoch 11869/40000, Loss: 5.710814002668485e-05, Learning Rate: 0.000481\n",
      "Epoch 11870/40000, Loss: 4.129113585804589e-05, Learning Rate: 0.000481\n",
      "Epoch 11871/40000, Loss: 0.00010032654245151207, Learning Rate: 0.000481\n",
      "Epoch 11872/40000, Loss: 6.489746738225222e-05, Learning Rate: 0.000481\n",
      "Epoch 11873/40000, Loss: 8.664102642796934e-05, Learning Rate: 0.000481\n",
      "Epoch 11874/40000, Loss: 2.488871177774854e-05, Learning Rate: 0.000481\n",
      "Epoch 11875/40000, Loss: 6.238721834961325e-05, Learning Rate: 0.000481\n",
      "Epoch 11876/40000, Loss: 5.1512568461475894e-05, Learning Rate: 0.000481\n",
      "Epoch 11877/40000, Loss: 6.963490886846557e-05, Learning Rate: 0.000481\n",
      "Epoch 11878/40000, Loss: 2.400363882770762e-05, Learning Rate: 0.000481\n",
      "Epoch 11879/40000, Loss: 8.055425860220566e-05, Learning Rate: 0.000481\n",
      "Epoch 11880/40000, Loss: 4.445797094376758e-05, Learning Rate: 0.000481\n",
      "Epoch 11881/40000, Loss: 6.985871004872024e-05, Learning Rate: 0.000481\n",
      "Epoch 11882/40000, Loss: 4.925957910018042e-05, Learning Rate: 0.000481\n",
      "Epoch 11883/40000, Loss: 2.026615220529493e-05, Learning Rate: 0.000481\n",
      "Epoch 11884/40000, Loss: 4.160894968663342e-05, Learning Rate: 0.000480\n",
      "Epoch 11885/40000, Loss: 8.106158202281222e-05, Learning Rate: 0.000480\n",
      "Epoch 11886/40000, Loss: 4.332643948146142e-05, Learning Rate: 0.000480\n",
      "Epoch 11887/40000, Loss: 1.98390735022258e-05, Learning Rate: 0.000480\n",
      "Epoch 11888/40000, Loss: 1.9172566680936143e-05, Learning Rate: 0.000480\n",
      "Epoch 11889/40000, Loss: 4.077420453540981e-05, Learning Rate: 0.000480\n",
      "Epoch 11890/40000, Loss: 6.577691237907857e-05, Learning Rate: 0.000480\n",
      "Epoch 11891/40000, Loss: 1.8726717826211825e-05, Learning Rate: 0.000480\n",
      "Epoch 11892/40000, Loss: 7.881820783950388e-05, Learning Rate: 0.000480\n",
      "Epoch 11893/40000, Loss: 4.09290732932277e-05, Learning Rate: 0.000480\n",
      "Epoch 11894/40000, Loss: 4.8789581342134625e-05, Learning Rate: 0.000480\n",
      "Epoch 11895/40000, Loss: 7.779047882650048e-05, Learning Rate: 0.000480\n",
      "Epoch 11896/40000, Loss: 4.681272912421264e-05, Learning Rate: 0.000480\n",
      "Epoch 11897/40000, Loss: 4.946041008224711e-05, Learning Rate: 0.000480\n",
      "Epoch 11898/40000, Loss: 6.639680941589177e-05, Learning Rate: 0.000480\n",
      "Epoch 11899/40000, Loss: 4.1652914660517126e-05, Learning Rate: 0.000480\n",
      "Epoch 11900/40000, Loss: 7.803488551871851e-05, Learning Rate: 0.000480\n",
      "Epoch 11901/40000, Loss: 7.749391079414636e-05, Learning Rate: 0.000480\n",
      "Epoch 11902/40000, Loss: 4.601581531460397e-05, Learning Rate: 0.000479\n",
      "Epoch 11903/40000, Loss: 6.520168972201645e-05, Learning Rate: 0.000479\n",
      "Epoch 11904/40000, Loss: 1.9934208467020653e-05, Learning Rate: 0.000479\n",
      "Epoch 11905/40000, Loss: 1.9675964722409844e-05, Learning Rate: 0.000479\n",
      "Epoch 11906/40000, Loss: 4.894478843198158e-05, Learning Rate: 0.000479\n",
      "Epoch 11907/40000, Loss: 7.948157144710422e-05, Learning Rate: 0.000479\n",
      "Epoch 11908/40000, Loss: 5.223756670602597e-05, Learning Rate: 0.000479\n",
      "Epoch 11909/40000, Loss: 4.962002640240826e-05, Learning Rate: 0.000479\n",
      "Epoch 11910/40000, Loss: 4.949406866217032e-05, Learning Rate: 0.000479\n",
      "Epoch 11911/40000, Loss: 7.868457032600418e-05, Learning Rate: 0.000479\n",
      "Epoch 11912/40000, Loss: 4.190853360341862e-05, Learning Rate: 0.000479\n",
      "Epoch 11913/40000, Loss: 5.066738958703354e-05, Learning Rate: 0.000479\n",
      "Epoch 11914/40000, Loss: 5.064490687800571e-05, Learning Rate: 0.000479\n",
      "Epoch 11915/40000, Loss: 4.78178117191419e-05, Learning Rate: 0.000479\n",
      "Epoch 11916/40000, Loss: 4.6527140511898324e-05, Learning Rate: 0.000479\n",
      "Epoch 11917/40000, Loss: 4.151404573349282e-05, Learning Rate: 0.000479\n",
      "Epoch 11918/40000, Loss: 1.894984598038718e-05, Learning Rate: 0.000479\n",
      "Epoch 11919/40000, Loss: 4.419101242092438e-05, Learning Rate: 0.000478\n",
      "Epoch 11920/40000, Loss: 2.0472114556469023e-05, Learning Rate: 0.000478\n",
      "Epoch 11921/40000, Loss: 2.0246672647772357e-05, Learning Rate: 0.000478\n",
      "Epoch 11922/40000, Loss: 4.8451300244778395e-05, Learning Rate: 0.000478\n",
      "Epoch 11923/40000, Loss: 4.678242839872837e-05, Learning Rate: 0.000478\n",
      "Epoch 11924/40000, Loss: 4.690290006692521e-05, Learning Rate: 0.000478\n",
      "Epoch 11925/40000, Loss: 4.879452899331227e-05, Learning Rate: 0.000478\n",
      "Epoch 11926/40000, Loss: 4.756991984322667e-05, Learning Rate: 0.000478\n",
      "Epoch 11927/40000, Loss: 4.156364593654871e-05, Learning Rate: 0.000478\n",
      "Epoch 11928/40000, Loss: 7.76011947891675e-05, Learning Rate: 0.000478\n",
      "Epoch 11929/40000, Loss: 7.733616803307086e-05, Learning Rate: 0.000478\n",
      "Epoch 11930/40000, Loss: 6.651291187154129e-05, Learning Rate: 0.000478\n",
      "Epoch 11931/40000, Loss: 1.9028027963940986e-05, Learning Rate: 0.000478\n",
      "Epoch 11932/40000, Loss: 1.8891132640419528e-05, Learning Rate: 0.000478\n",
      "Epoch 11933/40000, Loss: 7.775775156915188e-05, Learning Rate: 0.000478\n",
      "Epoch 11934/40000, Loss: 6.570418190676719e-05, Learning Rate: 0.000478\n",
      "Epoch 11935/40000, Loss: 1.9061622879235074e-05, Learning Rate: 0.000478\n",
      "Epoch 11936/40000, Loss: 4.7522382374154404e-05, Learning Rate: 0.000478\n",
      "Epoch 11937/40000, Loss: 4.100385922356509e-05, Learning Rate: 0.000477\n",
      "Epoch 11938/40000, Loss: 5.0162183470092714e-05, Learning Rate: 0.000477\n",
      "Epoch 11939/40000, Loss: 2.2019843527232297e-05, Learning Rate: 0.000477\n",
      "Epoch 11940/40000, Loss: 4.47004204033874e-05, Learning Rate: 0.000477\n",
      "Epoch 11941/40000, Loss: 5.754064113716595e-05, Learning Rate: 0.000477\n",
      "Epoch 11942/40000, Loss: 5.1723294745897874e-05, Learning Rate: 0.000477\n",
      "Epoch 11943/40000, Loss: 5.09139645146206e-05, Learning Rate: 0.000477\n",
      "Epoch 11944/40000, Loss: 5.829792644362897e-05, Learning Rate: 0.000477\n",
      "Epoch 11945/40000, Loss: 7.969762373249978e-05, Learning Rate: 0.000477\n",
      "Epoch 11946/40000, Loss: 2.249228418804705e-05, Learning Rate: 0.000477\n",
      "Epoch 11947/40000, Loss: 5.1563605666160583e-05, Learning Rate: 0.000477\n",
      "Epoch 11948/40000, Loss: 5.251979746390134e-05, Learning Rate: 0.000477\n",
      "Epoch 11949/40000, Loss: 6.415174721041694e-05, Learning Rate: 0.000477\n",
      "Epoch 11950/40000, Loss: 5.2404535381356254e-05, Learning Rate: 0.000477\n",
      "Epoch 11951/40000, Loss: 8.002926915651187e-05, Learning Rate: 0.000477\n",
      "Epoch 11952/40000, Loss: 4.8171998059842736e-05, Learning Rate: 0.000477\n",
      "Epoch 11953/40000, Loss: 6.689695874229074e-05, Learning Rate: 0.000477\n",
      "Epoch 11954/40000, Loss: 4.2134011891903356e-05, Learning Rate: 0.000476\n",
      "Epoch 11955/40000, Loss: 4.760444790008478e-05, Learning Rate: 0.000476\n",
      "Epoch 11956/40000, Loss: 7.99707995611243e-05, Learning Rate: 0.000476\n",
      "Epoch 11957/40000, Loss: 7.266319153131917e-05, Learning Rate: 0.000476\n",
      "Epoch 11958/40000, Loss: 4.183669079793617e-05, Learning Rate: 0.000476\n",
      "Epoch 11959/40000, Loss: 4.6924389607738703e-05, Learning Rate: 0.000476\n",
      "Epoch 11960/40000, Loss: 1.9486175006022677e-05, Learning Rate: 0.000476\n",
      "Epoch 11961/40000, Loss: 4.727110353996977e-05, Learning Rate: 0.000476\n",
      "Epoch 11962/40000, Loss: 4.616831574821845e-05, Learning Rate: 0.000476\n",
      "Epoch 11963/40000, Loss: 1.845245060394518e-05, Learning Rate: 0.000476\n",
      "Epoch 11964/40000, Loss: 4.615101352101192e-05, Learning Rate: 0.000476\n",
      "Epoch 11965/40000, Loss: 4.58085305581335e-05, Learning Rate: 0.000476\n",
      "Epoch 11966/40000, Loss: 7.73225910961628e-05, Learning Rate: 0.000476\n",
      "Epoch 11967/40000, Loss: 6.457973358919844e-05, Learning Rate: 0.000476\n",
      "Epoch 11968/40000, Loss: 6.457653944380581e-05, Learning Rate: 0.000476\n",
      "Epoch 11969/40000, Loss: 7.735523104202002e-05, Learning Rate: 0.000476\n",
      "Epoch 11970/40000, Loss: 4.028313560411334e-05, Learning Rate: 0.000476\n",
      "Epoch 11971/40000, Loss: 7.697562250541523e-05, Learning Rate: 0.000476\n",
      "Epoch 11972/40000, Loss: 4.822519622393884e-05, Learning Rate: 0.000475\n",
      "Epoch 11973/40000, Loss: 4.807681398233399e-05, Learning Rate: 0.000475\n",
      "Epoch 11974/40000, Loss: 6.43810344627127e-05, Learning Rate: 0.000475\n",
      "Epoch 11975/40000, Loss: 6.437502452172339e-05, Learning Rate: 0.000475\n",
      "Epoch 11976/40000, Loss: 7.68930694903247e-05, Learning Rate: 0.000475\n",
      "Epoch 11977/40000, Loss: 7.673975051147863e-05, Learning Rate: 0.000475\n",
      "Epoch 11978/40000, Loss: 4.00128701585345e-05, Learning Rate: 0.000475\n",
      "Epoch 11979/40000, Loss: 3.997232852270827e-05, Learning Rate: 0.000475\n",
      "Epoch 11980/40000, Loss: 4.014761725557037e-05, Learning Rate: 0.000475\n",
      "Epoch 11981/40000, Loss: 4.006527524325065e-05, Learning Rate: 0.000475\n",
      "Epoch 11982/40000, Loss: 4.012182034784928e-05, Learning Rate: 0.000475\n",
      "Epoch 11983/40000, Loss: 7.699639536440372e-05, Learning Rate: 0.000475\n",
      "Epoch 11984/40000, Loss: 1.8228758563054726e-05, Learning Rate: 0.000475\n",
      "Epoch 11985/40000, Loss: 4.8142115701921284e-05, Learning Rate: 0.000475\n",
      "Epoch 11986/40000, Loss: 4.515770706348121e-05, Learning Rate: 0.000475\n",
      "Epoch 11987/40000, Loss: 4.824432835448533e-05, Learning Rate: 0.000475\n",
      "Epoch 11988/40000, Loss: 4.000279295723885e-05, Learning Rate: 0.000475\n",
      "Epoch 11989/40000, Loss: 4.4965814595343545e-05, Learning Rate: 0.000474\n",
      "Epoch 11990/40000, Loss: 1.8135116988560185e-05, Learning Rate: 0.000474\n",
      "Epoch 11991/40000, Loss: 7.656305388081819e-05, Learning Rate: 0.000474\n",
      "Epoch 11992/40000, Loss: 7.654150977032259e-05, Learning Rate: 0.000474\n",
      "Epoch 11993/40000, Loss: 1.817779593693558e-05, Learning Rate: 0.000474\n",
      "Epoch 11994/40000, Loss: 6.421544821932912e-05, Learning Rate: 0.000474\n",
      "Epoch 11995/40000, Loss: 1.797582081053406e-05, Learning Rate: 0.000474\n",
      "Epoch 11996/40000, Loss: 1.8293016182724386e-05, Learning Rate: 0.000474\n",
      "Epoch 11997/40000, Loss: 6.485933408839628e-05, Learning Rate: 0.000474\n",
      "Epoch 11998/40000, Loss: 4.561905006994493e-05, Learning Rate: 0.000474\n",
      "Epoch 11999/40000, Loss: 1.8589595129014924e-05, Learning Rate: 0.000474\n",
      "Epoch 12000/40000, Loss: 6.443316669901833e-05, Learning Rate: 0.000474\n",
      "Epoch 12001/40000, Loss: 4.8498557589482516e-05, Learning Rate: 0.000474\n",
      "Epoch 12002/40000, Loss: 4.020282722194679e-05, Learning Rate: 0.000474\n",
      "Epoch 12003/40000, Loss: 1.8600538169266656e-05, Learning Rate: 0.000474\n",
      "Epoch 12004/40000, Loss: 4.8783313104650006e-05, Learning Rate: 0.000474\n",
      "Epoch 12005/40000, Loss: 1.842097299231682e-05, Learning Rate: 0.000474\n",
      "Epoch 12006/40000, Loss: 4.5579483412439004e-05, Learning Rate: 0.000474\n",
      "Epoch 12007/40000, Loss: 7.71646446082741e-05, Learning Rate: 0.000473\n",
      "Epoch 12008/40000, Loss: 6.61037556710653e-05, Learning Rate: 0.000473\n",
      "Epoch 12009/40000, Loss: 1.929527752508875e-05, Learning Rate: 0.000473\n",
      "Epoch 12010/40000, Loss: 1.8537777577876113e-05, Learning Rate: 0.000473\n",
      "Epoch 12011/40000, Loss: 4.031051867059432e-05, Learning Rate: 0.000473\n",
      "Epoch 12012/40000, Loss: 6.616215250687674e-05, Learning Rate: 0.000473\n",
      "Epoch 12013/40000, Loss: 7.79699839768e-05, Learning Rate: 0.000473\n",
      "Epoch 12014/40000, Loss: 2.421474528091494e-05, Learning Rate: 0.000473\n",
      "Epoch 12015/40000, Loss: 8.737645839573815e-05, Learning Rate: 0.000473\n",
      "Epoch 12016/40000, Loss: 8.83459288161248e-05, Learning Rate: 0.000473\n",
      "Epoch 12017/40000, Loss: 3.13809396175202e-05, Learning Rate: 0.000473\n",
      "Epoch 12018/40000, Loss: 4.9762868002289906e-05, Learning Rate: 0.000473\n",
      "Epoch 12019/40000, Loss: 4.803823321708478e-05, Learning Rate: 0.000473\n",
      "Epoch 12020/40000, Loss: 8.045134745771065e-05, Learning Rate: 0.000473\n",
      "Epoch 12021/40000, Loss: 8.151659130817279e-05, Learning Rate: 0.000473\n",
      "Epoch 12022/40000, Loss: 5.7498964451951906e-05, Learning Rate: 0.000473\n",
      "Epoch 12023/40000, Loss: 4.4874999730382115e-05, Learning Rate: 0.000473\n",
      "Epoch 12024/40000, Loss: 2.2760988940717652e-05, Learning Rate: 0.000472\n",
      "Epoch 12025/40000, Loss: 7.020863995421678e-05, Learning Rate: 0.000472\n",
      "Epoch 12026/40000, Loss: 7.227484456961975e-05, Learning Rate: 0.000472\n",
      "Epoch 12027/40000, Loss: 6.26781620667316e-05, Learning Rate: 0.000472\n",
      "Epoch 12028/40000, Loss: 5.855846393387765e-05, Learning Rate: 0.000472\n",
      "Epoch 12029/40000, Loss: 5.2314833737909794e-05, Learning Rate: 0.000472\n",
      "Epoch 12030/40000, Loss: 8.376088226214051e-05, Learning Rate: 0.000472\n",
      "Epoch 12031/40000, Loss: 5.02565671922639e-05, Learning Rate: 0.000472\n",
      "Epoch 12032/40000, Loss: 5.0435177399776876e-05, Learning Rate: 0.000472\n",
      "Epoch 12033/40000, Loss: 4.55940498795826e-05, Learning Rate: 0.000472\n",
      "Epoch 12034/40000, Loss: 5.428462827694602e-05, Learning Rate: 0.000472\n",
      "Epoch 12035/40000, Loss: 5.303720536176115e-05, Learning Rate: 0.000472\n",
      "Epoch 12036/40000, Loss: 8.115643140627071e-05, Learning Rate: 0.000472\n",
      "Epoch 12037/40000, Loss: 5.360586874303408e-05, Learning Rate: 0.000472\n",
      "Epoch 12038/40000, Loss: 5.3802628826815635e-05, Learning Rate: 0.000472\n",
      "Epoch 12039/40000, Loss: 8.948321919888258e-05, Learning Rate: 0.000472\n",
      "Epoch 12040/40000, Loss: 9.61628247750923e-05, Learning Rate: 0.000472\n",
      "Epoch 12041/40000, Loss: 4.642831481760368e-05, Learning Rate: 0.000472\n",
      "Epoch 12042/40000, Loss: 6.016927363816649e-05, Learning Rate: 0.000471\n",
      "Epoch 12043/40000, Loss: 8.666166831972077e-05, Learning Rate: 0.000471\n",
      "Epoch 12044/40000, Loss: 6.835819658590481e-05, Learning Rate: 0.000471\n",
      "Epoch 12045/40000, Loss: 3.8812369894003496e-05, Learning Rate: 0.000471\n",
      "Epoch 12046/40000, Loss: 7.555675983894616e-05, Learning Rate: 0.000471\n",
      "Epoch 12047/40000, Loss: 7.185259164543822e-05, Learning Rate: 0.000471\n",
      "Epoch 12048/40000, Loss: 6.538832531077787e-05, Learning Rate: 0.000471\n",
      "Epoch 12049/40000, Loss: 7.938087219372392e-05, Learning Rate: 0.000471\n",
      "Epoch 12050/40000, Loss: 5.0641850975807756e-05, Learning Rate: 0.000471\n",
      "Epoch 12051/40000, Loss: 0.00010973302414640784, Learning Rate: 0.000471\n",
      "Epoch 12052/40000, Loss: 5.808160130982287e-05, Learning Rate: 0.000471\n",
      "Epoch 12053/40000, Loss: 6.888990174047649e-05, Learning Rate: 0.000471\n",
      "Epoch 12054/40000, Loss: 0.0001333046384388581, Learning Rate: 0.000471\n",
      "Epoch 12055/40000, Loss: 5.307022365741432e-05, Learning Rate: 0.000471\n",
      "Epoch 12056/40000, Loss: 6.331050826702267e-05, Learning Rate: 0.000471\n",
      "Epoch 12057/40000, Loss: 5.6197815865743905e-05, Learning Rate: 0.000471\n",
      "Epoch 12058/40000, Loss: 7.326483319047838e-05, Learning Rate: 0.000471\n",
      "Epoch 12059/40000, Loss: 9.131657861871645e-05, Learning Rate: 0.000471\n",
      "Epoch 12060/40000, Loss: 8.18778935354203e-05, Learning Rate: 0.000470\n",
      "Epoch 12061/40000, Loss: 2.2996606276137754e-05, Learning Rate: 0.000470\n",
      "Epoch 12062/40000, Loss: 7.162303518271074e-05, Learning Rate: 0.000470\n",
      "Epoch 12063/40000, Loss: 4.986783824278973e-05, Learning Rate: 0.000470\n",
      "Epoch 12064/40000, Loss: 4.114690818823874e-05, Learning Rate: 0.000470\n",
      "Epoch 12065/40000, Loss: 7.877374446252361e-05, Learning Rate: 0.000470\n",
      "Epoch 12066/40000, Loss: 7.784931949572638e-05, Learning Rate: 0.000470\n",
      "Epoch 12067/40000, Loss: 7.7020340540912e-05, Learning Rate: 0.000470\n",
      "Epoch 12068/40000, Loss: 4.754135443363339e-05, Learning Rate: 0.000470\n",
      "Epoch 12069/40000, Loss: 6.499859591713175e-05, Learning Rate: 0.000470\n",
      "Epoch 12070/40000, Loss: 6.463518366217613e-05, Learning Rate: 0.000470\n",
      "Epoch 12071/40000, Loss: 7.743790047243237e-05, Learning Rate: 0.000470\n",
      "Epoch 12072/40000, Loss: 6.739950913470238e-05, Learning Rate: 0.000470\n",
      "Epoch 12073/40000, Loss: 6.754115747753531e-05, Learning Rate: 0.000470\n",
      "Epoch 12074/40000, Loss: 4.640956467483193e-05, Learning Rate: 0.000470\n",
      "Epoch 12075/40000, Loss: 6.473337271017954e-05, Learning Rate: 0.000470\n",
      "Epoch 12076/40000, Loss: 7.64574360800907e-05, Learning Rate: 0.000470\n",
      "Epoch 12077/40000, Loss: 4.0283302951138467e-05, Learning Rate: 0.000469\n",
      "Epoch 12078/40000, Loss: 1.8861124772229232e-05, Learning Rate: 0.000469\n",
      "Epoch 12079/40000, Loss: 7.658641698071733e-05, Learning Rate: 0.000469\n",
      "Epoch 12080/40000, Loss: 1.8666420146473683e-05, Learning Rate: 0.000469\n",
      "Epoch 12081/40000, Loss: 4.814206840819679e-05, Learning Rate: 0.000469\n",
      "Epoch 12082/40000, Loss: 4.584417911246419e-05, Learning Rate: 0.000469\n",
      "Epoch 12083/40000, Loss: 4.686237662099302e-05, Learning Rate: 0.000469\n",
      "Epoch 12084/40000, Loss: 4.0646868001203984e-05, Learning Rate: 0.000469\n",
      "Epoch 12085/40000, Loss: 4.059549974044785e-05, Learning Rate: 0.000469\n",
      "Epoch 12086/40000, Loss: 4.751509186462499e-05, Learning Rate: 0.000469\n",
      "Epoch 12087/40000, Loss: 1.8406139133730903e-05, Learning Rate: 0.000469\n",
      "Epoch 12088/40000, Loss: 1.8545120838098228e-05, Learning Rate: 0.000469\n",
      "Epoch 12089/40000, Loss: 6.476320413639769e-05, Learning Rate: 0.000469\n",
      "Epoch 12090/40000, Loss: 4.858337706536986e-05, Learning Rate: 0.000469\n",
      "Epoch 12091/40000, Loss: 4.776506102643907e-05, Learning Rate: 0.000469\n",
      "Epoch 12092/40000, Loss: 4.029272167827003e-05, Learning Rate: 0.000469\n",
      "Epoch 12093/40000, Loss: 6.465436308644712e-05, Learning Rate: 0.000469\n",
      "Epoch 12094/40000, Loss: 4.557410647976212e-05, Learning Rate: 0.000469\n",
      "Epoch 12095/40000, Loss: 7.690340862609446e-05, Learning Rate: 0.000468\n",
      "Epoch 12096/40000, Loss: 1.849979344115127e-05, Learning Rate: 0.000468\n",
      "Epoch 12097/40000, Loss: 6.498613947769627e-05, Learning Rate: 0.000468\n",
      "Epoch 12098/40000, Loss: 4.014196019852534e-05, Learning Rate: 0.000468\n",
      "Epoch 12099/40000, Loss: 4.029479168821126e-05, Learning Rate: 0.000468\n",
      "Epoch 12100/40000, Loss: 4.5615131966769695e-05, Learning Rate: 0.000468\n",
      "Epoch 12101/40000, Loss: 7.706035830779001e-05, Learning Rate: 0.000468\n",
      "Epoch 12102/40000, Loss: 4.544058901956305e-05, Learning Rate: 0.000468\n",
      "Epoch 12103/40000, Loss: 6.440138531615958e-05, Learning Rate: 0.000468\n",
      "Epoch 12104/40000, Loss: 4.630258263205178e-05, Learning Rate: 0.000468\n",
      "Epoch 12105/40000, Loss: 4.064956010552123e-05, Learning Rate: 0.000468\n",
      "Epoch 12106/40000, Loss: 7.667211320949718e-05, Learning Rate: 0.000468\n",
      "Epoch 12107/40000, Loss: 4.823731433134526e-05, Learning Rate: 0.000468\n",
      "Epoch 12108/40000, Loss: 4.784939665114507e-05, Learning Rate: 0.000468\n",
      "Epoch 12109/40000, Loss: 1.8299535440746695e-05, Learning Rate: 0.000468\n",
      "Epoch 12110/40000, Loss: 1.8610859115142375e-05, Learning Rate: 0.000468\n",
      "Epoch 12111/40000, Loss: 4.51593441539444e-05, Learning Rate: 0.000468\n",
      "Epoch 12112/40000, Loss: 4.550374796963297e-05, Learning Rate: 0.000468\n",
      "Epoch 12113/40000, Loss: 4.802745388587937e-05, Learning Rate: 0.000467\n",
      "Epoch 12114/40000, Loss: 4.546561831375584e-05, Learning Rate: 0.000467\n",
      "Epoch 12115/40000, Loss: 4.871817509410903e-05, Learning Rate: 0.000467\n",
      "Epoch 12116/40000, Loss: 4.8194426199188456e-05, Learning Rate: 0.000467\n",
      "Epoch 12117/40000, Loss: 4.778669244842604e-05, Learning Rate: 0.000467\n",
      "Epoch 12118/40000, Loss: 4.045264722662978e-05, Learning Rate: 0.000467\n",
      "Epoch 12119/40000, Loss: 6.550248508574441e-05, Learning Rate: 0.000467\n",
      "Epoch 12120/40000, Loss: 4.627143789548427e-05, Learning Rate: 0.000467\n",
      "Epoch 12121/40000, Loss: 7.457353785866871e-05, Learning Rate: 0.000467\n",
      "Epoch 12122/40000, Loss: 4.856437590206042e-05, Learning Rate: 0.000467\n",
      "Epoch 12123/40000, Loss: 2.233863779110834e-05, Learning Rate: 0.000467\n",
      "Epoch 12124/40000, Loss: 4.224568692734465e-05, Learning Rate: 0.000467\n",
      "Epoch 12125/40000, Loss: 5.096180393593386e-05, Learning Rate: 0.000467\n",
      "Epoch 12126/40000, Loss: 6.888678763061762e-05, Learning Rate: 0.000467\n",
      "Epoch 12127/40000, Loss: 5.073802458355203e-05, Learning Rate: 0.000467\n",
      "Epoch 12128/40000, Loss: 4.310012809582986e-05, Learning Rate: 0.000467\n",
      "Epoch 12129/40000, Loss: 2.139698881364893e-05, Learning Rate: 0.000467\n",
      "Epoch 12130/40000, Loss: 6.895724072819576e-05, Learning Rate: 0.000467\n",
      "Epoch 12131/40000, Loss: 2.1595900761894882e-05, Learning Rate: 0.000466\n",
      "Epoch 12132/40000, Loss: 7.907684630481526e-05, Learning Rate: 0.000466\n",
      "Epoch 12133/40000, Loss: 2.026058064075187e-05, Learning Rate: 0.000466\n",
      "Epoch 12134/40000, Loss: 1.9848190277116373e-05, Learning Rate: 0.000466\n",
      "Epoch 12135/40000, Loss: 4.948302375851199e-05, Learning Rate: 0.000466\n",
      "Epoch 12136/40000, Loss: 4.931450530420989e-05, Learning Rate: 0.000466\n",
      "Epoch 12137/40000, Loss: 8.420694211963564e-05, Learning Rate: 0.000466\n",
      "Epoch 12138/40000, Loss: 4.378030644147657e-05, Learning Rate: 0.000466\n",
      "Epoch 12139/40000, Loss: 0.00012858500122092664, Learning Rate: 0.000466\n",
      "Epoch 12140/40000, Loss: 5.611219967249781e-05, Learning Rate: 0.000466\n",
      "Epoch 12141/40000, Loss: 0.00010519605712033808, Learning Rate: 0.000466\n",
      "Epoch 12142/40000, Loss: 9.947712533175945e-05, Learning Rate: 0.000466\n",
      "Epoch 12143/40000, Loss: 0.00010125729750143364, Learning Rate: 0.000466\n",
      "Epoch 12144/40000, Loss: 0.00014512863708660007, Learning Rate: 0.000466\n",
      "Epoch 12145/40000, Loss: 9.719739318825305e-05, Learning Rate: 0.000466\n",
      "Epoch 12146/40000, Loss: 8.781572250882164e-05, Learning Rate: 0.000466\n",
      "Epoch 12147/40000, Loss: 6.758869130862877e-05, Learning Rate: 0.000466\n",
      "Epoch 12148/40000, Loss: 5.1244511269032955e-05, Learning Rate: 0.000466\n",
      "Epoch 12149/40000, Loss: 8.438172517344356e-05, Learning Rate: 0.000465\n",
      "Epoch 12150/40000, Loss: 0.00010611175821395591, Learning Rate: 0.000465\n",
      "Epoch 12151/40000, Loss: 2.9279231966938823e-05, Learning Rate: 0.000465\n",
      "Epoch 12152/40000, Loss: 5.397001586970873e-05, Learning Rate: 0.000465\n",
      "Epoch 12153/40000, Loss: 3.368618490640074e-05, Learning Rate: 0.000465\n",
      "Epoch 12154/40000, Loss: 4.6215300244512036e-05, Learning Rate: 0.000465\n",
      "Epoch 12155/40000, Loss: 5.916886584600434e-05, Learning Rate: 0.000465\n",
      "Epoch 12156/40000, Loss: 5.850230809301138e-05, Learning Rate: 0.000465\n",
      "Epoch 12157/40000, Loss: 7.11365501047112e-05, Learning Rate: 0.000465\n",
      "Epoch 12158/40000, Loss: 4.482274016481824e-05, Learning Rate: 0.000465\n",
      "Epoch 12159/40000, Loss: 5.0685357564361766e-05, Learning Rate: 0.000465\n",
      "Epoch 12160/40000, Loss: 7.989820733200759e-05, Learning Rate: 0.000465\n",
      "Epoch 12161/40000, Loss: 2.0385301468195394e-05, Learning Rate: 0.000465\n",
      "Epoch 12162/40000, Loss: 8.541653369320557e-05, Learning Rate: 0.000465\n",
      "Epoch 12163/40000, Loss: 4.826448639505543e-05, Learning Rate: 0.000465\n",
      "Epoch 12164/40000, Loss: 8.8600063463673e-05, Learning Rate: 0.000465\n",
      "Epoch 12165/40000, Loss: 6.643210508627817e-05, Learning Rate: 0.000465\n",
      "Epoch 12166/40000, Loss: 4.809894380741753e-05, Learning Rate: 0.000465\n",
      "Epoch 12167/40000, Loss: 6.592521094717085e-05, Learning Rate: 0.000464\n",
      "Epoch 12168/40000, Loss: 7.968101999722421e-05, Learning Rate: 0.000464\n",
      "Epoch 12169/40000, Loss: 4.0597195038571954e-05, Learning Rate: 0.000464\n",
      "Epoch 12170/40000, Loss: 6.448580825235695e-05, Learning Rate: 0.000464\n",
      "Epoch 12171/40000, Loss: 6.378753460012376e-05, Learning Rate: 0.000464\n",
      "Epoch 12172/40000, Loss: 1.8618062313180417e-05, Learning Rate: 0.000464\n",
      "Epoch 12173/40000, Loss: 1.843616337282583e-05, Learning Rate: 0.000464\n",
      "Epoch 12174/40000, Loss: 1.806126965675503e-05, Learning Rate: 0.000464\n",
      "Epoch 12175/40000, Loss: 4.603655179380439e-05, Learning Rate: 0.000464\n",
      "Epoch 12176/40000, Loss: 6.385174492606893e-05, Learning Rate: 0.000464\n",
      "Epoch 12177/40000, Loss: 7.629402534803376e-05, Learning Rate: 0.000464\n",
      "Epoch 12178/40000, Loss: 1.8359642126597464e-05, Learning Rate: 0.000464\n",
      "Epoch 12179/40000, Loss: 1.8321281459066086e-05, Learning Rate: 0.000464\n",
      "Epoch 12180/40000, Loss: 4.781984898727387e-05, Learning Rate: 0.000464\n",
      "Epoch 12181/40000, Loss: 6.386556924553588e-05, Learning Rate: 0.000464\n",
      "Epoch 12182/40000, Loss: 4.5676875743083656e-05, Learning Rate: 0.000464\n",
      "Epoch 12183/40000, Loss: 4.565910785458982e-05, Learning Rate: 0.000464\n",
      "Epoch 12184/40000, Loss: 4.784681004821323e-05, Learning Rate: 0.000464\n",
      "Epoch 12185/40000, Loss: 6.344862777041271e-05, Learning Rate: 0.000463\n",
      "Epoch 12186/40000, Loss: 1.8496180928195827e-05, Learning Rate: 0.000463\n",
      "Epoch 12187/40000, Loss: 7.627438026247546e-05, Learning Rate: 0.000463\n",
      "Epoch 12188/40000, Loss: 7.600319077027962e-05, Learning Rate: 0.000463\n",
      "Epoch 12189/40000, Loss: 1.802182123356033e-05, Learning Rate: 0.000463\n",
      "Epoch 12190/40000, Loss: 7.566260319435969e-05, Learning Rate: 0.000463\n",
      "Epoch 12191/40000, Loss: 4.512309533311054e-05, Learning Rate: 0.000463\n",
      "Epoch 12192/40000, Loss: 4.752404129249044e-05, Learning Rate: 0.000463\n",
      "Epoch 12193/40000, Loss: 3.954794010496698e-05, Learning Rate: 0.000463\n",
      "Epoch 12194/40000, Loss: 3.955788997700438e-05, Learning Rate: 0.000463\n",
      "Epoch 12195/40000, Loss: 1.8123411791748367e-05, Learning Rate: 0.000463\n",
      "Epoch 12196/40000, Loss: 1.7792079233913682e-05, Learning Rate: 0.000463\n",
      "Epoch 12197/40000, Loss: 7.572869799332693e-05, Learning Rate: 0.000463\n",
      "Epoch 12198/40000, Loss: 3.9720624045003206e-05, Learning Rate: 0.000463\n",
      "Epoch 12199/40000, Loss: 4.5140277507016435e-05, Learning Rate: 0.000463\n",
      "Epoch 12200/40000, Loss: 3.986377123510465e-05, Learning Rate: 0.000463\n",
      "Epoch 12201/40000, Loss: 3.959541209042072e-05, Learning Rate: 0.000463\n",
      "Epoch 12202/40000, Loss: 1.801392500055954e-05, Learning Rate: 0.000463\n",
      "Epoch 12203/40000, Loss: 4.7436482418561354e-05, Learning Rate: 0.000462\n",
      "Epoch 12204/40000, Loss: 7.552997703896835e-05, Learning Rate: 0.000462\n",
      "Epoch 12205/40000, Loss: 4.4989905291004106e-05, Learning Rate: 0.000462\n",
      "Epoch 12206/40000, Loss: 6.314523488981649e-05, Learning Rate: 0.000462\n",
      "Epoch 12207/40000, Loss: 4.7324137995019555e-05, Learning Rate: 0.000462\n",
      "Epoch 12208/40000, Loss: 7.568893488496542e-05, Learning Rate: 0.000462\n",
      "Epoch 12209/40000, Loss: 4.737533527077176e-05, Learning Rate: 0.000462\n",
      "Epoch 12210/40000, Loss: 6.317778752418235e-05, Learning Rate: 0.000462\n",
      "Epoch 12211/40000, Loss: 4.5056203816784546e-05, Learning Rate: 0.000462\n",
      "Epoch 12212/40000, Loss: 4.50365332653746e-05, Learning Rate: 0.000462\n",
      "Epoch 12213/40000, Loss: 4.767797508975491e-05, Learning Rate: 0.000462\n",
      "Epoch 12214/40000, Loss: 4.493113374337554e-05, Learning Rate: 0.000462\n",
      "Epoch 12215/40000, Loss: 1.791793511074502e-05, Learning Rate: 0.000462\n",
      "Epoch 12216/40000, Loss: 7.602434197906405e-05, Learning Rate: 0.000462\n",
      "Epoch 12217/40000, Loss: 3.955382271669805e-05, Learning Rate: 0.000462\n",
      "Epoch 12218/40000, Loss: 4.736622213385999e-05, Learning Rate: 0.000462\n",
      "Epoch 12219/40000, Loss: 4.493098094826564e-05, Learning Rate: 0.000462\n",
      "Epoch 12220/40000, Loss: 6.312259938567877e-05, Learning Rate: 0.000462\n",
      "Epoch 12221/40000, Loss: 3.978468885179609e-05, Learning Rate: 0.000461\n",
      "Epoch 12222/40000, Loss: 4.491928848437965e-05, Learning Rate: 0.000461\n",
      "Epoch 12223/40000, Loss: 6.322903209365904e-05, Learning Rate: 0.000461\n",
      "Epoch 12224/40000, Loss: 4.5308072003535926e-05, Learning Rate: 0.000461\n",
      "Epoch 12225/40000, Loss: 1.7918502635438927e-05, Learning Rate: 0.000461\n",
      "Epoch 12226/40000, Loss: 3.9622329495614395e-05, Learning Rate: 0.000461\n",
      "Epoch 12227/40000, Loss: 7.59912800276652e-05, Learning Rate: 0.000461\n",
      "Epoch 12228/40000, Loss: 4.016535967821255e-05, Learning Rate: 0.000461\n",
      "Epoch 12229/40000, Loss: 4.003024150733836e-05, Learning Rate: 0.000461\n",
      "Epoch 12230/40000, Loss: 7.669239130336791e-05, Learning Rate: 0.000461\n",
      "Epoch 12231/40000, Loss: 7.700706191826612e-05, Learning Rate: 0.000461\n",
      "Epoch 12232/40000, Loss: 4.764895493281074e-05, Learning Rate: 0.000461\n",
      "Epoch 12233/40000, Loss: 1.8436418031342328e-05, Learning Rate: 0.000461\n",
      "Epoch 12234/40000, Loss: 4.6786062739556655e-05, Learning Rate: 0.000461\n",
      "Epoch 12235/40000, Loss: 4.6867146011209115e-05, Learning Rate: 0.000461\n",
      "Epoch 12236/40000, Loss: 8.291933772852644e-05, Learning Rate: 0.000461\n",
      "Epoch 12237/40000, Loss: 4.825009455089457e-05, Learning Rate: 0.000461\n",
      "Epoch 12238/40000, Loss: 4.811268445337191e-05, Learning Rate: 0.000461\n",
      "Epoch 12239/40000, Loss: 1.9900917322956957e-05, Learning Rate: 0.000460\n",
      "Epoch 12240/40000, Loss: 0.00011255054414505139, Learning Rate: 0.000460\n",
      "Epoch 12241/40000, Loss: 4.350749804871157e-05, Learning Rate: 0.000460\n",
      "Epoch 12242/40000, Loss: 5.026676444686018e-05, Learning Rate: 0.000460\n",
      "Epoch 12243/40000, Loss: 6.75637784297578e-05, Learning Rate: 0.000460\n",
      "Epoch 12244/40000, Loss: 4.452156281331554e-05, Learning Rate: 0.000460\n",
      "Epoch 12245/40000, Loss: 9.217292245011777e-05, Learning Rate: 0.000460\n",
      "Epoch 12246/40000, Loss: 5.273355418466963e-05, Learning Rate: 0.000460\n",
      "Epoch 12247/40000, Loss: 7.078804628690705e-05, Learning Rate: 0.000460\n",
      "Epoch 12248/40000, Loss: 7.750102668069303e-05, Learning Rate: 0.000460\n",
      "Epoch 12249/40000, Loss: 6.08299596933648e-05, Learning Rate: 0.000460\n",
      "Epoch 12250/40000, Loss: 8.395527402171865e-05, Learning Rate: 0.000460\n",
      "Epoch 12251/40000, Loss: 5.587656778516248e-05, Learning Rate: 0.000460\n",
      "Epoch 12252/40000, Loss: 8.070272451732308e-05, Learning Rate: 0.000460\n",
      "Epoch 12253/40000, Loss: 8.249746315414086e-05, Learning Rate: 0.000460\n",
      "Epoch 12254/40000, Loss: 5.905337820877321e-05, Learning Rate: 0.000460\n",
      "Epoch 12255/40000, Loss: 4.5425746066030115e-05, Learning Rate: 0.000460\n",
      "Epoch 12256/40000, Loss: 7.207166345324367e-05, Learning Rate: 0.000460\n",
      "Epoch 12257/40000, Loss: 5.343266457202844e-05, Learning Rate: 0.000459\n",
      "Epoch 12258/40000, Loss: 2.9250411898829043e-05, Learning Rate: 0.000459\n",
      "Epoch 12259/40000, Loss: 5.792865340481512e-05, Learning Rate: 0.000459\n",
      "Epoch 12260/40000, Loss: 8.115162199828774e-05, Learning Rate: 0.000459\n",
      "Epoch 12261/40000, Loss: 7.651359192095697e-05, Learning Rate: 0.000459\n",
      "Epoch 12262/40000, Loss: 2.4139153538271785e-05, Learning Rate: 0.000459\n",
      "Epoch 12263/40000, Loss: 4.564844130072743e-05, Learning Rate: 0.000459\n",
      "Epoch 12264/40000, Loss: 5.3646643209503964e-05, Learning Rate: 0.000459\n",
      "Epoch 12265/40000, Loss: 5.1617978897411376e-05, Learning Rate: 0.000459\n",
      "Epoch 12266/40000, Loss: 2.4804374334053136e-05, Learning Rate: 0.000459\n",
      "Epoch 12267/40000, Loss: 5.214222619542852e-05, Learning Rate: 0.000459\n",
      "Epoch 12268/40000, Loss: 8.242372132372111e-05, Learning Rate: 0.000459\n",
      "Epoch 12269/40000, Loss: 8.051217446336523e-05, Learning Rate: 0.000459\n",
      "Epoch 12270/40000, Loss: 7.277556869667023e-05, Learning Rate: 0.000459\n",
      "Epoch 12271/40000, Loss: 6.90904853399843e-05, Learning Rate: 0.000459\n",
      "Epoch 12272/40000, Loss: 2.7715166652342305e-05, Learning Rate: 0.000459\n",
      "Epoch 12273/40000, Loss: 8.570776117267087e-05, Learning Rate: 0.000459\n",
      "Epoch 12274/40000, Loss: 6.037763523636386e-05, Learning Rate: 0.000459\n",
      "Epoch 12275/40000, Loss: 8.712269482202828e-05, Learning Rate: 0.000458\n",
      "Epoch 12276/40000, Loss: 7.39373208489269e-05, Learning Rate: 0.000458\n",
      "Epoch 12277/40000, Loss: 4.391497714095749e-05, Learning Rate: 0.000458\n",
      "Epoch 12278/40000, Loss: 4.250812344253063e-05, Learning Rate: 0.000458\n",
      "Epoch 12279/40000, Loss: 8.382432133657858e-05, Learning Rate: 0.000458\n",
      "Epoch 12280/40000, Loss: 4.650485061574727e-05, Learning Rate: 0.000458\n",
      "Epoch 12281/40000, Loss: 4.834894207306206e-05, Learning Rate: 0.000458\n",
      "Epoch 12282/40000, Loss: 5.012941255699843e-05, Learning Rate: 0.000458\n",
      "Epoch 12283/40000, Loss: 5.020669414079748e-05, Learning Rate: 0.000458\n",
      "Epoch 12284/40000, Loss: 4.864714355790056e-05, Learning Rate: 0.000458\n",
      "Epoch 12285/40000, Loss: 4.8247879021801054e-05, Learning Rate: 0.000458\n",
      "Epoch 12286/40000, Loss: 4.098270073882304e-05, Learning Rate: 0.000458\n",
      "Epoch 12287/40000, Loss: 4.985107443644665e-05, Learning Rate: 0.000458\n",
      "Epoch 12288/40000, Loss: 7.855283183744177e-05, Learning Rate: 0.000458\n",
      "Epoch 12289/40000, Loss: 6.860770372441038e-05, Learning Rate: 0.000458\n",
      "Epoch 12290/40000, Loss: 4.984501356375404e-05, Learning Rate: 0.000458\n",
      "Epoch 12291/40000, Loss: 1.914991116791498e-05, Learning Rate: 0.000458\n",
      "Epoch 12292/40000, Loss: 4.169515159446746e-05, Learning Rate: 0.000458\n",
      "Epoch 12293/40000, Loss: 5.0471768190618604e-05, Learning Rate: 0.000457\n",
      "Epoch 12294/40000, Loss: 4.9817710532806814e-05, Learning Rate: 0.000457\n",
      "Epoch 12295/40000, Loss: 2.1568343072431162e-05, Learning Rate: 0.000457\n",
      "Epoch 12296/40000, Loss: 2.309899718966335e-05, Learning Rate: 0.000457\n",
      "Epoch 12297/40000, Loss: 8.655444980831817e-05, Learning Rate: 0.000457\n",
      "Epoch 12298/40000, Loss: 4.903465014649555e-05, Learning Rate: 0.000457\n",
      "Epoch 12299/40000, Loss: 8.330182754434645e-05, Learning Rate: 0.000457\n",
      "Epoch 12300/40000, Loss: 5.159596912562847e-05, Learning Rate: 0.000457\n",
      "Epoch 12301/40000, Loss: 6.933311669854447e-05, Learning Rate: 0.000457\n",
      "Epoch 12302/40000, Loss: 6.685405969619751e-05, Learning Rate: 0.000457\n",
      "Epoch 12303/40000, Loss: 2.4451477656839415e-05, Learning Rate: 0.000457\n",
      "Epoch 12304/40000, Loss: 4.272175647201948e-05, Learning Rate: 0.000457\n",
      "Epoch 12305/40000, Loss: 2.1112851754878648e-05, Learning Rate: 0.000457\n",
      "Epoch 12306/40000, Loss: 4.870825432590209e-05, Learning Rate: 0.000457\n",
      "Epoch 12307/40000, Loss: 6.63993414491415e-05, Learning Rate: 0.000457\n",
      "Epoch 12308/40000, Loss: 8.241285104304552e-05, Learning Rate: 0.000457\n",
      "Epoch 12309/40000, Loss: 8.280047768494114e-05, Learning Rate: 0.000457\n",
      "Epoch 12310/40000, Loss: 8.391389565076679e-05, Learning Rate: 0.000457\n",
      "Epoch 12311/40000, Loss: 4.7679506678832695e-05, Learning Rate: 0.000456\n",
      "Epoch 12312/40000, Loss: 5.097638859297149e-05, Learning Rate: 0.000456\n",
      "Epoch 12313/40000, Loss: 5.727394818677567e-05, Learning Rate: 0.000456\n",
      "Epoch 12314/40000, Loss: 7.951020961627364e-05, Learning Rate: 0.000456\n",
      "Epoch 12315/40000, Loss: 5.7743804063647985e-05, Learning Rate: 0.000456\n",
      "Epoch 12316/40000, Loss: 6.6493681515567e-05, Learning Rate: 0.000456\n",
      "Epoch 12317/40000, Loss: 9.787176531972364e-05, Learning Rate: 0.000456\n",
      "Epoch 12318/40000, Loss: 4.9491085519548506e-05, Learning Rate: 0.000456\n",
      "Epoch 12319/40000, Loss: 5.822470848215744e-05, Learning Rate: 0.000456\n",
      "Epoch 12320/40000, Loss: 7.755996193736792e-05, Learning Rate: 0.000456\n",
      "Epoch 12321/40000, Loss: 4.556008570943959e-05, Learning Rate: 0.000456\n",
      "Epoch 12322/40000, Loss: 5.056445297668688e-05, Learning Rate: 0.000456\n",
      "Epoch 12323/40000, Loss: 4.913073280476965e-05, Learning Rate: 0.000456\n",
      "Epoch 12324/40000, Loss: 4.807210643775761e-05, Learning Rate: 0.000456\n",
      "Epoch 12325/40000, Loss: 4.34607281931676e-05, Learning Rate: 0.000456\n",
      "Epoch 12326/40000, Loss: 4.933226955472492e-05, Learning Rate: 0.000456\n",
      "Epoch 12327/40000, Loss: 4.883925794274546e-05, Learning Rate: 0.000456\n",
      "Epoch 12328/40000, Loss: 4.209189864923246e-05, Learning Rate: 0.000456\n",
      "Epoch 12329/40000, Loss: 2.1530377125600353e-05, Learning Rate: 0.000456\n",
      "Epoch 12330/40000, Loss: 4.678915865952149e-05, Learning Rate: 0.000455\n",
      "Epoch 12331/40000, Loss: 4.955690383212641e-05, Learning Rate: 0.000455\n",
      "Epoch 12332/40000, Loss: 4.144124613958411e-05, Learning Rate: 0.000455\n",
      "Epoch 12333/40000, Loss: 7.618532981723547e-05, Learning Rate: 0.000455\n",
      "Epoch 12334/40000, Loss: 6.599196058232337e-05, Learning Rate: 0.000455\n",
      "Epoch 12335/40000, Loss: 5.1842180255334824e-05, Learning Rate: 0.000455\n",
      "Epoch 12336/40000, Loss: 7.940088835312054e-05, Learning Rate: 0.000455\n",
      "Epoch 12337/40000, Loss: 7.751082739559934e-05, Learning Rate: 0.000455\n",
      "Epoch 12338/40000, Loss: 5.0619724788703024e-05, Learning Rate: 0.000455\n",
      "Epoch 12339/40000, Loss: 7.816499419277534e-05, Learning Rate: 0.000455\n",
      "Epoch 12340/40000, Loss: 7.88295001257211e-05, Learning Rate: 0.000455\n",
      "Epoch 12341/40000, Loss: 7.712326623732224e-05, Learning Rate: 0.000455\n",
      "Epoch 12342/40000, Loss: 5.14104358444456e-05, Learning Rate: 0.000455\n",
      "Epoch 12343/40000, Loss: 5.129969213157892e-05, Learning Rate: 0.000455\n",
      "Epoch 12344/40000, Loss: 4.923919550492428e-05, Learning Rate: 0.000455\n",
      "Epoch 12345/40000, Loss: 4.0375671233050525e-05, Learning Rate: 0.000455\n",
      "Epoch 12346/40000, Loss: 5.4232965339906514e-05, Learning Rate: 0.000455\n",
      "Epoch 12347/40000, Loss: 4.467079270398244e-05, Learning Rate: 0.000455\n",
      "Epoch 12348/40000, Loss: 4.089027061127126e-05, Learning Rate: 0.000454\n",
      "Epoch 12349/40000, Loss: 6.731694884365425e-05, Learning Rate: 0.000454\n",
      "Epoch 12350/40000, Loss: 4.876676030107774e-05, Learning Rate: 0.000454\n",
      "Epoch 12351/40000, Loss: 2.0175350073259324e-05, Learning Rate: 0.000454\n",
      "Epoch 12352/40000, Loss: 4.7597070079064e-05, Learning Rate: 0.000454\n",
      "Epoch 12353/40000, Loss: 7.819442544132471e-05, Learning Rate: 0.000454\n",
      "Epoch 12354/40000, Loss: 1.9409440938034095e-05, Learning Rate: 0.000454\n",
      "Epoch 12355/40000, Loss: 1.8820765035343356e-05, Learning Rate: 0.000454\n",
      "Epoch 12356/40000, Loss: 4.0761813579592854e-05, Learning Rate: 0.000454\n",
      "Epoch 12357/40000, Loss: 4.6829431084915996e-05, Learning Rate: 0.000454\n",
      "Epoch 12358/40000, Loss: 7.646182348253205e-05, Learning Rate: 0.000454\n",
      "Epoch 12359/40000, Loss: 4.03941266995389e-05, Learning Rate: 0.000454\n",
      "Epoch 12360/40000, Loss: 4.709752829512581e-05, Learning Rate: 0.000454\n",
      "Epoch 12361/40000, Loss: 4.77824141853489e-05, Learning Rate: 0.000454\n",
      "Epoch 12362/40000, Loss: 5.1938150136265904e-05, Learning Rate: 0.000454\n",
      "Epoch 12363/40000, Loss: 8.214334957301617e-05, Learning Rate: 0.000454\n",
      "Epoch 12364/40000, Loss: 4.266447285772301e-05, Learning Rate: 0.000454\n",
      "Epoch 12365/40000, Loss: 4.187250669929199e-05, Learning Rate: 0.000454\n",
      "Epoch 12366/40000, Loss: 6.541683978866786e-05, Learning Rate: 0.000453\n",
      "Epoch 12367/40000, Loss: 6.600272899959236e-05, Learning Rate: 0.000453\n",
      "Epoch 12368/40000, Loss: 2.0357019820949063e-05, Learning Rate: 0.000453\n",
      "Epoch 12369/40000, Loss: 4.834649007534608e-05, Learning Rate: 0.000453\n",
      "Epoch 12370/40000, Loss: 6.645326357102022e-05, Learning Rate: 0.000453\n",
      "Epoch 12371/40000, Loss: 8.01591741037555e-05, Learning Rate: 0.000453\n",
      "Epoch 12372/40000, Loss: 4.357187572168186e-05, Learning Rate: 0.000453\n",
      "Epoch 12373/40000, Loss: 6.791829946450889e-05, Learning Rate: 0.000453\n",
      "Epoch 12374/40000, Loss: 5.071263876743615e-05, Learning Rate: 0.000453\n",
      "Epoch 12375/40000, Loss: 8.879868983058259e-05, Learning Rate: 0.000453\n",
      "Epoch 12376/40000, Loss: 2.071890776278451e-05, Learning Rate: 0.000453\n",
      "Epoch 12377/40000, Loss: 7.76089436840266e-05, Learning Rate: 0.000453\n",
      "Epoch 12378/40000, Loss: 5.0704511522781104e-05, Learning Rate: 0.000453\n",
      "Epoch 12379/40000, Loss: 8.56824335642159e-05, Learning Rate: 0.000453\n",
      "Epoch 12380/40000, Loss: 6.595523154828697e-05, Learning Rate: 0.000453\n",
      "Epoch 12381/40000, Loss: 4.523638199316338e-05, Learning Rate: 0.000453\n",
      "Epoch 12382/40000, Loss: 4.135638300795108e-05, Learning Rate: 0.000453\n",
      "Epoch 12383/40000, Loss: 4.802139665116556e-05, Learning Rate: 0.000453\n",
      "Epoch 12384/40000, Loss: 6.540120375575498e-05, Learning Rate: 0.000453\n",
      "Epoch 12385/40000, Loss: 7.855185685912147e-05, Learning Rate: 0.000452\n",
      "Epoch 12386/40000, Loss: 4.981644815416075e-05, Learning Rate: 0.000452\n",
      "Epoch 12387/40000, Loss: 4.130570596316829e-05, Learning Rate: 0.000452\n",
      "Epoch 12388/40000, Loss: 4.043062654091045e-05, Learning Rate: 0.000452\n",
      "Epoch 12389/40000, Loss: 4.045716559630819e-05, Learning Rate: 0.000452\n",
      "Epoch 12390/40000, Loss: 2.1374573407229036e-05, Learning Rate: 0.000452\n",
      "Epoch 12391/40000, Loss: 8.173312380677089e-05, Learning Rate: 0.000452\n",
      "Epoch 12392/40000, Loss: 5.9677997342078015e-05, Learning Rate: 0.000452\n",
      "Epoch 12393/40000, Loss: 2.5688323148642667e-05, Learning Rate: 0.000452\n",
      "Epoch 12394/40000, Loss: 2.049576687568333e-05, Learning Rate: 0.000452\n",
      "Epoch 12395/40000, Loss: 5.2293929911684245e-05, Learning Rate: 0.000452\n",
      "Epoch 12396/40000, Loss: 2.2765956600778736e-05, Learning Rate: 0.000452\n",
      "Epoch 12397/40000, Loss: 5.360131763154641e-05, Learning Rate: 0.000452\n",
      "Epoch 12398/40000, Loss: 6.791571649955586e-05, Learning Rate: 0.000452\n",
      "Epoch 12399/40000, Loss: 4.2802461393876e-05, Learning Rate: 0.000452\n",
      "Epoch 12400/40000, Loss: 8.05281160864979e-05, Learning Rate: 0.000452\n",
      "Epoch 12401/40000, Loss: 2.098079858114943e-05, Learning Rate: 0.000452\n",
      "Epoch 12402/40000, Loss: 5.0123668188462034e-05, Learning Rate: 0.000452\n",
      "Epoch 12403/40000, Loss: 4.632185664377175e-05, Learning Rate: 0.000451\n",
      "Epoch 12404/40000, Loss: 4.89008889417164e-05, Learning Rate: 0.000451\n",
      "Epoch 12405/40000, Loss: 4.1429080738453194e-05, Learning Rate: 0.000451\n",
      "Epoch 12406/40000, Loss: 1.9932936993427575e-05, Learning Rate: 0.000451\n",
      "Epoch 12407/40000, Loss: 4.8404752305941656e-05, Learning Rate: 0.000451\n",
      "Epoch 12408/40000, Loss: 4.259668639861047e-05, Learning Rate: 0.000451\n",
      "Epoch 12409/40000, Loss: 4.177219307166524e-05, Learning Rate: 0.000451\n",
      "Epoch 12410/40000, Loss: 4.693636219599284e-05, Learning Rate: 0.000451\n",
      "Epoch 12411/40000, Loss: 6.585809023818001e-05, Learning Rate: 0.000451\n",
      "Epoch 12412/40000, Loss: 4.81334536743816e-05, Learning Rate: 0.000451\n",
      "Epoch 12413/40000, Loss: 2.0511271941359155e-05, Learning Rate: 0.000451\n",
      "Epoch 12414/40000, Loss: 1.9063854779233225e-05, Learning Rate: 0.000451\n",
      "Epoch 12415/40000, Loss: 5.158328713150695e-05, Learning Rate: 0.000451\n",
      "Epoch 12416/40000, Loss: 4.866608287557028e-05, Learning Rate: 0.000451\n",
      "Epoch 12417/40000, Loss: 4.0463572076987475e-05, Learning Rate: 0.000451\n",
      "Epoch 12418/40000, Loss: 1.940804395417217e-05, Learning Rate: 0.000451\n",
      "Epoch 12419/40000, Loss: 6.65765255689621e-05, Learning Rate: 0.000451\n",
      "Epoch 12420/40000, Loss: 7.833889685571194e-05, Learning Rate: 0.000451\n",
      "Epoch 12421/40000, Loss: 7.736931002000347e-05, Learning Rate: 0.000451\n",
      "Epoch 12422/40000, Loss: 4.2241266783094034e-05, Learning Rate: 0.000450\n",
      "Epoch 12423/40000, Loss: 1.8767572328215465e-05, Learning Rate: 0.000450\n",
      "Epoch 12424/40000, Loss: 7.71544364397414e-05, Learning Rate: 0.000450\n",
      "Epoch 12425/40000, Loss: 4.046354297315702e-05, Learning Rate: 0.000450\n",
      "Epoch 12426/40000, Loss: 4.6148339606588706e-05, Learning Rate: 0.000450\n",
      "Epoch 12427/40000, Loss: 4.57782662124373e-05, Learning Rate: 0.000450\n",
      "Epoch 12428/40000, Loss: 4.7278819693019614e-05, Learning Rate: 0.000450\n",
      "Epoch 12429/40000, Loss: 4.5830398448742926e-05, Learning Rate: 0.000450\n",
      "Epoch 12430/40000, Loss: 4.526244447333738e-05, Learning Rate: 0.000450\n",
      "Epoch 12431/40000, Loss: 6.473621760960668e-05, Learning Rate: 0.000450\n",
      "Epoch 12432/40000, Loss: 4.710999201051891e-05, Learning Rate: 0.000450\n",
      "Epoch 12433/40000, Loss: 4.555166742647998e-05, Learning Rate: 0.000450\n",
      "Epoch 12434/40000, Loss: 3.985220973845571e-05, Learning Rate: 0.000450\n",
      "Epoch 12435/40000, Loss: 3.9581751480000094e-05, Learning Rate: 0.000450\n",
      "Epoch 12436/40000, Loss: 4.6819062845315784e-05, Learning Rate: 0.000450\n",
      "Epoch 12437/40000, Loss: 4.670058478950523e-05, Learning Rate: 0.000450\n",
      "Epoch 12438/40000, Loss: 4.644705404643901e-05, Learning Rate: 0.000450\n",
      "Epoch 12439/40000, Loss: 4.655905649997294e-05, Learning Rate: 0.000450\n",
      "Epoch 12440/40000, Loss: 4.6494555135723203e-05, Learning Rate: 0.000449\n",
      "Epoch 12441/40000, Loss: 3.934635969926603e-05, Learning Rate: 0.000449\n",
      "Epoch 12442/40000, Loss: 6.301670509856194e-05, Learning Rate: 0.000449\n",
      "Epoch 12443/40000, Loss: 6.314579513855278e-05, Learning Rate: 0.000449\n",
      "Epoch 12444/40000, Loss: 1.8419124899082817e-05, Learning Rate: 0.000449\n",
      "Epoch 12445/40000, Loss: 6.400307029252872e-05, Learning Rate: 0.000449\n",
      "Epoch 12446/40000, Loss: 6.455536640714854e-05, Learning Rate: 0.000449\n",
      "Epoch 12447/40000, Loss: 1.8685772374738008e-05, Learning Rate: 0.000449\n",
      "Epoch 12448/40000, Loss: 4.178477684035897e-05, Learning Rate: 0.000449\n",
      "Epoch 12449/40000, Loss: 7.70985207054764e-05, Learning Rate: 0.000449\n",
      "Epoch 12450/40000, Loss: 4.801208706339821e-05, Learning Rate: 0.000449\n",
      "Epoch 12451/40000, Loss: 7.083160744514316e-05, Learning Rate: 0.000449\n",
      "Epoch 12452/40000, Loss: 2.040727122221142e-05, Learning Rate: 0.000449\n",
      "Epoch 12453/40000, Loss: 4.792502295458689e-05, Learning Rate: 0.000449\n",
      "Epoch 12454/40000, Loss: 8.94489130587317e-05, Learning Rate: 0.000449\n",
      "Epoch 12455/40000, Loss: 7.559239747934043e-05, Learning Rate: 0.000449\n",
      "Epoch 12456/40000, Loss: 5.079867332824506e-05, Learning Rate: 0.000449\n",
      "Epoch 12457/40000, Loss: 8.842777606332675e-05, Learning Rate: 0.000449\n",
      "Epoch 12458/40000, Loss: 5.816904013045132e-05, Learning Rate: 0.000449\n",
      "Epoch 12459/40000, Loss: 2.634755765029695e-05, Learning Rate: 0.000448\n",
      "Epoch 12460/40000, Loss: 6.306071009021252e-05, Learning Rate: 0.000448\n",
      "Epoch 12461/40000, Loss: 3.2820993510540575e-05, Learning Rate: 0.000448\n",
      "Epoch 12462/40000, Loss: 6.527957884827629e-05, Learning Rate: 0.000448\n",
      "Epoch 12463/40000, Loss: 5.8744324633153155e-05, Learning Rate: 0.000448\n",
      "Epoch 12464/40000, Loss: 4.657107638195157e-05, Learning Rate: 0.000448\n",
      "Epoch 12465/40000, Loss: 8.36664839880541e-05, Learning Rate: 0.000448\n",
      "Epoch 12466/40000, Loss: 8.613226964371279e-05, Learning Rate: 0.000448\n",
      "Epoch 12467/40000, Loss: 6.931801181053743e-05, Learning Rate: 0.000448\n",
      "Epoch 12468/40000, Loss: 0.00012937419523950666, Learning Rate: 0.000448\n",
      "Epoch 12469/40000, Loss: 8.338676707353443e-05, Learning Rate: 0.000448\n",
      "Epoch 12470/40000, Loss: 6.816191307734698e-05, Learning Rate: 0.000448\n",
      "Epoch 12471/40000, Loss: 6.104165368014947e-05, Learning Rate: 0.000448\n",
      "Epoch 12472/40000, Loss: 3.46154447470326e-05, Learning Rate: 0.000448\n",
      "Epoch 12473/40000, Loss: 5.1842042012140155e-05, Learning Rate: 0.000448\n",
      "Epoch 12474/40000, Loss: 5.452714685816318e-05, Learning Rate: 0.000448\n",
      "Epoch 12475/40000, Loss: 5.657346991938539e-05, Learning Rate: 0.000448\n",
      "Epoch 12476/40000, Loss: 9.476030390942469e-05, Learning Rate: 0.000448\n",
      "Epoch 12477/40000, Loss: 7.25602512829937e-05, Learning Rate: 0.000447\n",
      "Epoch 12478/40000, Loss: 5.8672321756603196e-05, Learning Rate: 0.000447\n",
      "Epoch 12479/40000, Loss: 9.28660883801058e-05, Learning Rate: 0.000447\n",
      "Epoch 12480/40000, Loss: 7.241085404530168e-05, Learning Rate: 0.000447\n",
      "Epoch 12481/40000, Loss: 3.0194116334314458e-05, Learning Rate: 0.000447\n",
      "Epoch 12482/40000, Loss: 2.130305438186042e-05, Learning Rate: 0.000447\n",
      "Epoch 12483/40000, Loss: 6.389105692505836e-05, Learning Rate: 0.000447\n",
      "Epoch 12484/40000, Loss: 8.700977923581377e-05, Learning Rate: 0.000447\n",
      "Epoch 12485/40000, Loss: 4.6644960093544796e-05, Learning Rate: 0.000447\n",
      "Epoch 12486/40000, Loss: 5.20274079462979e-05, Learning Rate: 0.000447\n",
      "Epoch 12487/40000, Loss: 3.200142236892134e-05, Learning Rate: 0.000447\n",
      "Epoch 12488/40000, Loss: 7.786224159644917e-05, Learning Rate: 0.000447\n",
      "Epoch 12489/40000, Loss: 4.3074753193650395e-05, Learning Rate: 0.000447\n",
      "Epoch 12490/40000, Loss: 5.017443254473619e-05, Learning Rate: 0.000447\n",
      "Epoch 12491/40000, Loss: 2.4755619961069897e-05, Learning Rate: 0.000447\n",
      "Epoch 12492/40000, Loss: 8.44882961246185e-05, Learning Rate: 0.000447\n",
      "Epoch 12493/40000, Loss: 8.155026444001123e-05, Learning Rate: 0.000447\n",
      "Epoch 12494/40000, Loss: 5.399669680627994e-05, Learning Rate: 0.000447\n",
      "Epoch 12495/40000, Loss: 2.1481777366716415e-05, Learning Rate: 0.000447\n",
      "Epoch 12496/40000, Loss: 5.7787237892625853e-05, Learning Rate: 0.000446\n",
      "Epoch 12497/40000, Loss: 4.395539508550428e-05, Learning Rate: 0.000446\n",
      "Epoch 12498/40000, Loss: 5.690944453817792e-05, Learning Rate: 0.000446\n",
      "Epoch 12499/40000, Loss: 4.9718073569238186e-05, Learning Rate: 0.000446\n",
      "Epoch 12500/40000, Loss: 8.01247515482828e-05, Learning Rate: 0.000446\n",
      "Epoch 12501/40000, Loss: 6.945020140847191e-05, Learning Rate: 0.000446\n",
      "Epoch 12502/40000, Loss: 4.0251117752632126e-05, Learning Rate: 0.000446\n",
      "Epoch 12503/40000, Loss: 4.7185476432787254e-05, Learning Rate: 0.000446\n",
      "Epoch 12504/40000, Loss: 4.0198356145992875e-05, Learning Rate: 0.000446\n",
      "Epoch 12505/40000, Loss: 4.0250917663797736e-05, Learning Rate: 0.000446\n",
      "Epoch 12506/40000, Loss: 1.81581071956316e-05, Learning Rate: 0.000446\n",
      "Epoch 12507/40000, Loss: 1.916584005812183e-05, Learning Rate: 0.000446\n",
      "Epoch 12508/40000, Loss: 4.6504010242642835e-05, Learning Rate: 0.000446\n",
      "Epoch 12509/40000, Loss: 4.6221332013374195e-05, Learning Rate: 0.000446\n",
      "Epoch 12510/40000, Loss: 4.6240696974564344e-05, Learning Rate: 0.000446\n",
      "Epoch 12511/40000, Loss: 7.501318759750575e-05, Learning Rate: 0.000446\n",
      "Epoch 12512/40000, Loss: 4.508570782491006e-05, Learning Rate: 0.000446\n",
      "Epoch 12513/40000, Loss: 4.6208173444028944e-05, Learning Rate: 0.000446\n",
      "Epoch 12514/40000, Loss: 7.514723984058946e-05, Learning Rate: 0.000446\n",
      "Epoch 12515/40000, Loss: 4.522833478404209e-05, Learning Rate: 0.000445\n",
      "Epoch 12516/40000, Loss: 6.271763413678855e-05, Learning Rate: 0.000445\n",
      "Epoch 12517/40000, Loss: 4.510005601332523e-05, Learning Rate: 0.000445\n",
      "Epoch 12518/40000, Loss: 1.7981456039706245e-05, Learning Rate: 0.000445\n",
      "Epoch 12519/40000, Loss: 4.623459608410485e-05, Learning Rate: 0.000445\n",
      "Epoch 12520/40000, Loss: 4.501922012423165e-05, Learning Rate: 0.000445\n",
      "Epoch 12521/40000, Loss: 1.785654603736475e-05, Learning Rate: 0.000445\n",
      "Epoch 12522/40000, Loss: 1.7782978829927742e-05, Learning Rate: 0.000445\n",
      "Epoch 12523/40000, Loss: 1.785438507795334e-05, Learning Rate: 0.000445\n",
      "Epoch 12524/40000, Loss: 1.7973519788938574e-05, Learning Rate: 0.000445\n",
      "Epoch 12525/40000, Loss: 4.622348569682799e-05, Learning Rate: 0.000445\n",
      "Epoch 12526/40000, Loss: 3.930826278519817e-05, Learning Rate: 0.000445\n",
      "Epoch 12527/40000, Loss: 3.9292241126531735e-05, Learning Rate: 0.000445\n",
      "Epoch 12528/40000, Loss: 6.2733561208006e-05, Learning Rate: 0.000445\n",
      "Epoch 12529/40000, Loss: 6.27461340627633e-05, Learning Rate: 0.000445\n",
      "Epoch 12530/40000, Loss: 4.536724736681208e-05, Learning Rate: 0.000445\n",
      "Epoch 12531/40000, Loss: 4.6307883167173713e-05, Learning Rate: 0.000445\n",
      "Epoch 12532/40000, Loss: 1.815172072383575e-05, Learning Rate: 0.000445\n",
      "Epoch 12533/40000, Loss: 4.651258859666996e-05, Learning Rate: 0.000444\n",
      "Epoch 12534/40000, Loss: 4.6170494897523895e-05, Learning Rate: 0.000444\n",
      "Epoch 12535/40000, Loss: 1.8688053387450054e-05, Learning Rate: 0.000444\n",
      "Epoch 12536/40000, Loss: 6.256451888475567e-05, Learning Rate: 0.000444\n",
      "Epoch 12537/40000, Loss: 1.8084068869939074e-05, Learning Rate: 0.000444\n",
      "Epoch 12538/40000, Loss: 4.6105436922516674e-05, Learning Rate: 0.000444\n",
      "Epoch 12539/40000, Loss: 4.552831524051726e-05, Learning Rate: 0.000444\n",
      "Epoch 12540/40000, Loss: 4.617503873305395e-05, Learning Rate: 0.000444\n",
      "Epoch 12541/40000, Loss: 7.503228698624298e-05, Learning Rate: 0.000444\n",
      "Epoch 12542/40000, Loss: 1.7970847693504766e-05, Learning Rate: 0.000444\n",
      "Epoch 12543/40000, Loss: 3.977758387918584e-05, Learning Rate: 0.000444\n",
      "Epoch 12544/40000, Loss: 3.9359165384666994e-05, Learning Rate: 0.000444\n",
      "Epoch 12545/40000, Loss: 6.224066601134837e-05, Learning Rate: 0.000444\n",
      "Epoch 12546/40000, Loss: 1.7720853065839037e-05, Learning Rate: 0.000444\n",
      "Epoch 12547/40000, Loss: 7.50200342736207e-05, Learning Rate: 0.000444\n",
      "Epoch 12548/40000, Loss: 4.517799607128836e-05, Learning Rate: 0.000444\n",
      "Epoch 12549/40000, Loss: 7.504245877498761e-05, Learning Rate: 0.000444\n",
      "Epoch 12550/40000, Loss: 1.7735066649038345e-05, Learning Rate: 0.000444\n",
      "Epoch 12551/40000, Loss: 3.93099726352375e-05, Learning Rate: 0.000444\n",
      "Epoch 12552/40000, Loss: 3.919527443940751e-05, Learning Rate: 0.000443\n",
      "Epoch 12553/40000, Loss: 1.759901351761073e-05, Learning Rate: 0.000443\n",
      "Epoch 12554/40000, Loss: 1.7687510990072042e-05, Learning Rate: 0.000443\n",
      "Epoch 12555/40000, Loss: 7.583550905110314e-05, Learning Rate: 0.000443\n",
      "Epoch 12556/40000, Loss: 1.75230252352776e-05, Learning Rate: 0.000443\n",
      "Epoch 12557/40000, Loss: 3.910957821062766e-05, Learning Rate: 0.000443\n",
      "Epoch 12558/40000, Loss: 4.492930747801438e-05, Learning Rate: 0.000443\n",
      "Epoch 12559/40000, Loss: 3.9133312384365126e-05, Learning Rate: 0.000443\n",
      "Epoch 12560/40000, Loss: 4.471396096050739e-05, Learning Rate: 0.000443\n",
      "Epoch 12561/40000, Loss: 7.44920180295594e-05, Learning Rate: 0.000443\n",
      "Epoch 12562/40000, Loss: 4.449250263860449e-05, Learning Rate: 0.000443\n",
      "Epoch 12563/40000, Loss: 4.4625194277614355e-05, Learning Rate: 0.000443\n",
      "Epoch 12564/40000, Loss: 4.58188442280516e-05, Learning Rate: 0.000443\n",
      "Epoch 12565/40000, Loss: 4.594819256453775e-05, Learning Rate: 0.000443\n",
      "Epoch 12566/40000, Loss: 6.20989449089393e-05, Learning Rate: 0.000443\n",
      "Epoch 12567/40000, Loss: 3.9018483221298084e-05, Learning Rate: 0.000443\n",
      "Epoch 12568/40000, Loss: 4.4797419832320884e-05, Learning Rate: 0.000443\n",
      "Epoch 12569/40000, Loss: 1.7623326129978523e-05, Learning Rate: 0.000443\n",
      "Epoch 12570/40000, Loss: 7.429813558701426e-05, Learning Rate: 0.000443\n",
      "Epoch 12571/40000, Loss: 6.219831993803382e-05, Learning Rate: 0.000442\n",
      "Epoch 12572/40000, Loss: 6.23069645371288e-05, Learning Rate: 0.000442\n",
      "Epoch 12573/40000, Loss: 6.211509753484279e-05, Learning Rate: 0.000442\n",
      "Epoch 12574/40000, Loss: 4.5215481804916635e-05, Learning Rate: 0.000442\n",
      "Epoch 12575/40000, Loss: 1.8020611605606973e-05, Learning Rate: 0.000442\n",
      "Epoch 12576/40000, Loss: 1.8198625184595585e-05, Learning Rate: 0.000442\n",
      "Epoch 12577/40000, Loss: 1.7899603335536085e-05, Learning Rate: 0.000442\n",
      "Epoch 12578/40000, Loss: 4.5936725655337796e-05, Learning Rate: 0.000442\n",
      "Epoch 12579/40000, Loss: 4.669018744607456e-05, Learning Rate: 0.000442\n",
      "Epoch 12580/40000, Loss: 1.7871405361802317e-05, Learning Rate: 0.000442\n",
      "Epoch 12581/40000, Loss: 7.593778718728572e-05, Learning Rate: 0.000442\n",
      "Epoch 12582/40000, Loss: 4.655045268009417e-05, Learning Rate: 0.000442\n",
      "Epoch 12583/40000, Loss: 6.364264118019491e-05, Learning Rate: 0.000442\n",
      "Epoch 12584/40000, Loss: 1.9756233086809516e-05, Learning Rate: 0.000442\n",
      "Epoch 12585/40000, Loss: 6.458327698055655e-05, Learning Rate: 0.000442\n",
      "Epoch 12586/40000, Loss: 4.0837505366653204e-05, Learning Rate: 0.000442\n",
      "Epoch 12587/40000, Loss: 4.0345123125007376e-05, Learning Rate: 0.000442\n",
      "Epoch 12588/40000, Loss: 2.197347566834651e-05, Learning Rate: 0.000442\n",
      "Epoch 12589/40000, Loss: 4.644583896151744e-05, Learning Rate: 0.000442\n",
      "Epoch 12590/40000, Loss: 1.976719795493409e-05, Learning Rate: 0.000441\n",
      "Epoch 12591/40000, Loss: 1.988223084481433e-05, Learning Rate: 0.000441\n",
      "Epoch 12592/40000, Loss: 4.067906047566794e-05, Learning Rate: 0.000441\n",
      "Epoch 12593/40000, Loss: 4.419366086949594e-05, Learning Rate: 0.000441\n",
      "Epoch 12594/40000, Loss: 4.988460204913281e-05, Learning Rate: 0.000441\n",
      "Epoch 12595/40000, Loss: 4.9132515414385125e-05, Learning Rate: 0.000441\n",
      "Epoch 12596/40000, Loss: 4.334611730882898e-05, Learning Rate: 0.000441\n",
      "Epoch 12597/40000, Loss: 2.27412638196256e-05, Learning Rate: 0.000441\n",
      "Epoch 12598/40000, Loss: 5.2470408263616264e-05, Learning Rate: 0.000441\n",
      "Epoch 12599/40000, Loss: 5.182508175494149e-05, Learning Rate: 0.000441\n",
      "Epoch 12600/40000, Loss: 4.4516455091070384e-05, Learning Rate: 0.000441\n",
      "Epoch 12601/40000, Loss: 2.190526902268175e-05, Learning Rate: 0.000441\n",
      "Epoch 12602/40000, Loss: 7.846893277019262e-05, Learning Rate: 0.000441\n",
      "Epoch 12603/40000, Loss: 5.1617043936857954e-05, Learning Rate: 0.000441\n",
      "Epoch 12604/40000, Loss: 8.113193325698376e-05, Learning Rate: 0.000441\n",
      "Epoch 12605/40000, Loss: 2.148851126548834e-05, Learning Rate: 0.000441\n",
      "Epoch 12606/40000, Loss: 4.9245834816247225e-05, Learning Rate: 0.000441\n",
      "Epoch 12607/40000, Loss: 4.236964741721749e-05, Learning Rate: 0.000441\n",
      "Epoch 12608/40000, Loss: 4.212970452499576e-05, Learning Rate: 0.000441\n",
      "Epoch 12609/40000, Loss: 5.3209063480608165e-05, Learning Rate: 0.000440\n",
      "Epoch 12610/40000, Loss: 5.228176451055333e-05, Learning Rate: 0.000440\n",
      "Epoch 12611/40000, Loss: 2.2506346795125864e-05, Learning Rate: 0.000440\n",
      "Epoch 12612/40000, Loss: 4.4917906052432954e-05, Learning Rate: 0.000440\n",
      "Epoch 12613/40000, Loss: 5.310234337230213e-05, Learning Rate: 0.000440\n",
      "Epoch 12614/40000, Loss: 8.072918717516586e-05, Learning Rate: 0.000440\n",
      "Epoch 12615/40000, Loss: 4.463251752895303e-05, Learning Rate: 0.000440\n",
      "Epoch 12616/40000, Loss: 0.00010908659896813333, Learning Rate: 0.000440\n",
      "Epoch 12617/40000, Loss: 5.4264059144770727e-05, Learning Rate: 0.000440\n",
      "Epoch 12618/40000, Loss: 5.306010280037299e-05, Learning Rate: 0.000440\n",
      "Epoch 12619/40000, Loss: 7.819249731255695e-05, Learning Rate: 0.000440\n",
      "Epoch 12620/40000, Loss: 0.00010651513730408624, Learning Rate: 0.000440\n",
      "Epoch 12621/40000, Loss: 3.1560848583467305e-05, Learning Rate: 0.000440\n",
      "Epoch 12622/40000, Loss: 6.435569957830012e-05, Learning Rate: 0.000440\n",
      "Epoch 12623/40000, Loss: 0.00019747213809750974, Learning Rate: 0.000440\n",
      "Epoch 12624/40000, Loss: 0.00010422058403491974, Learning Rate: 0.000440\n",
      "Epoch 12625/40000, Loss: 7.065798854455352e-05, Learning Rate: 0.000440\n",
      "Epoch 12626/40000, Loss: 5.79638326598797e-05, Learning Rate: 0.000440\n",
      "Epoch 12627/40000, Loss: 9.03827603906393e-05, Learning Rate: 0.000440\n",
      "Epoch 12628/40000, Loss: 8.127161709126085e-05, Learning Rate: 0.000439\n",
      "Epoch 12629/40000, Loss: 7.450775592587888e-05, Learning Rate: 0.000439\n",
      "Epoch 12630/40000, Loss: 4.819489913643338e-05, Learning Rate: 0.000439\n",
      "Epoch 12631/40000, Loss: 8.140091085806489e-05, Learning Rate: 0.000439\n",
      "Epoch 12632/40000, Loss: 4.354287011665292e-05, Learning Rate: 0.000439\n",
      "Epoch 12633/40000, Loss: 9.319515083916485e-05, Learning Rate: 0.000439\n",
      "Epoch 12634/40000, Loss: 5.168638381292112e-05, Learning Rate: 0.000439\n",
      "Epoch 12635/40000, Loss: 5.218735532253049e-05, Learning Rate: 0.000439\n",
      "Epoch 12636/40000, Loss: 5.8998131862608716e-05, Learning Rate: 0.000439\n",
      "Epoch 12637/40000, Loss: 5.4879430535947904e-05, Learning Rate: 0.000439\n",
      "Epoch 12638/40000, Loss: 8.011297177290544e-05, Learning Rate: 0.000439\n",
      "Epoch 12639/40000, Loss: 4.9528338422533125e-05, Learning Rate: 0.000439\n",
      "Epoch 12640/40000, Loss: 5.404253170127049e-05, Learning Rate: 0.000439\n",
      "Epoch 12641/40000, Loss: 8.90355440787971e-05, Learning Rate: 0.000439\n",
      "Epoch 12642/40000, Loss: 7.527721754740924e-05, Learning Rate: 0.000439\n",
      "Epoch 12643/40000, Loss: 0.0001045522149070166, Learning Rate: 0.000439\n",
      "Epoch 12644/40000, Loss: 7.741009176243097e-05, Learning Rate: 0.000439\n",
      "Epoch 12645/40000, Loss: 8.639919542474672e-05, Learning Rate: 0.000439\n",
      "Epoch 12646/40000, Loss: 5.188701834413223e-05, Learning Rate: 0.000439\n",
      "Epoch 12647/40000, Loss: 8.801317017059773e-05, Learning Rate: 0.000438\n",
      "Epoch 12648/40000, Loss: 7.95540981926024e-05, Learning Rate: 0.000438\n",
      "Epoch 12649/40000, Loss: 4.493720916798338e-05, Learning Rate: 0.000438\n",
      "Epoch 12650/40000, Loss: 5.634245098917745e-05, Learning Rate: 0.000438\n",
      "Epoch 12651/40000, Loss: 6.957496952963993e-05, Learning Rate: 0.000438\n",
      "Epoch 12652/40000, Loss: 6.892279634485021e-05, Learning Rate: 0.000438\n",
      "Epoch 12653/40000, Loss: 8.362523658433929e-05, Learning Rate: 0.000438\n",
      "Epoch 12654/40000, Loss: 7.8421006037388e-05, Learning Rate: 0.000438\n",
      "Epoch 12655/40000, Loss: 0.00010751297668321058, Learning Rate: 0.000438\n",
      "Epoch 12656/40000, Loss: 2.1804275093018077e-05, Learning Rate: 0.000438\n",
      "Epoch 12657/40000, Loss: 0.00017451281019020826, Learning Rate: 0.000438\n",
      "Epoch 12658/40000, Loss: 8.456899377051741e-05, Learning Rate: 0.000438\n",
      "Epoch 12659/40000, Loss: 6.623178342124447e-05, Learning Rate: 0.000438\n",
      "Epoch 12660/40000, Loss: 2.584587491583079e-05, Learning Rate: 0.000438\n",
      "Epoch 12661/40000, Loss: 4.522160816122778e-05, Learning Rate: 0.000438\n",
      "Epoch 12662/40000, Loss: 0.00010070636926684529, Learning Rate: 0.000438\n",
      "Epoch 12663/40000, Loss: 6.0145546740386635e-05, Learning Rate: 0.000438\n",
      "Epoch 12664/40000, Loss: 5.436723949969746e-05, Learning Rate: 0.000438\n",
      "Epoch 12665/40000, Loss: 8.519793482264504e-05, Learning Rate: 0.000438\n",
      "Epoch 12666/40000, Loss: 8.084690489340574e-05, Learning Rate: 0.000437\n",
      "Epoch 12667/40000, Loss: 5.339292692951858e-05, Learning Rate: 0.000437\n",
      "Epoch 12668/40000, Loss: 2.892465090553742e-05, Learning Rate: 0.000437\n",
      "Epoch 12669/40000, Loss: 2.1105206542415544e-05, Learning Rate: 0.000437\n",
      "Epoch 12670/40000, Loss: 5.102684008306824e-05, Learning Rate: 0.000437\n",
      "Epoch 12671/40000, Loss: 7.892152643762529e-05, Learning Rate: 0.000437\n",
      "Epoch 12672/40000, Loss: 4.13037087128032e-05, Learning Rate: 0.000437\n",
      "Epoch 12673/40000, Loss: 6.65204570395872e-05, Learning Rate: 0.000437\n",
      "Epoch 12674/40000, Loss: 4.5130396756576374e-05, Learning Rate: 0.000437\n",
      "Epoch 12675/40000, Loss: 4.83677358715795e-05, Learning Rate: 0.000437\n",
      "Epoch 12676/40000, Loss: 4.7984638513298705e-05, Learning Rate: 0.000437\n",
      "Epoch 12677/40000, Loss: 2.015608515648637e-05, Learning Rate: 0.000437\n",
      "Epoch 12678/40000, Loss: 4.670437192544341e-05, Learning Rate: 0.000437\n",
      "Epoch 12679/40000, Loss: 4.650614209822379e-05, Learning Rate: 0.000437\n",
      "Epoch 12680/40000, Loss: 4.6319415559992194e-05, Learning Rate: 0.000437\n",
      "Epoch 12681/40000, Loss: 6.319597741821781e-05, Learning Rate: 0.000437\n",
      "Epoch 12682/40000, Loss: 6.301305984379724e-05, Learning Rate: 0.000437\n",
      "Epoch 12683/40000, Loss: 1.846497252699919e-05, Learning Rate: 0.000437\n",
      "Epoch 12684/40000, Loss: 7.568332512164488e-05, Learning Rate: 0.000437\n",
      "Epoch 12685/40000, Loss: 1.988990516110789e-05, Learning Rate: 0.000436\n",
      "Epoch 12686/40000, Loss: 7.571317109977826e-05, Learning Rate: 0.000436\n",
      "Epoch 12687/40000, Loss: 4.580866152537055e-05, Learning Rate: 0.000436\n",
      "Epoch 12688/40000, Loss: 6.29234709776938e-05, Learning Rate: 0.000436\n",
      "Epoch 12689/40000, Loss: 7.486274262191728e-05, Learning Rate: 0.000436\n",
      "Epoch 12690/40000, Loss: 4.54749351774808e-05, Learning Rate: 0.000436\n",
      "Epoch 12691/40000, Loss: 1.8664508388610557e-05, Learning Rate: 0.000436\n",
      "Epoch 12692/40000, Loss: 1.830853580031544e-05, Learning Rate: 0.000436\n",
      "Epoch 12693/40000, Loss: 1.8272055967827328e-05, Learning Rate: 0.000436\n",
      "Epoch 12694/40000, Loss: 7.494285091524944e-05, Learning Rate: 0.000436\n",
      "Epoch 12695/40000, Loss: 3.9645707147428766e-05, Learning Rate: 0.000436\n",
      "Epoch 12696/40000, Loss: 4.6080956963123754e-05, Learning Rate: 0.000436\n",
      "Epoch 12697/40000, Loss: 7.549184374511242e-05, Learning Rate: 0.000436\n",
      "Epoch 12698/40000, Loss: 4.5672975829802454e-05, Learning Rate: 0.000436\n",
      "Epoch 12699/40000, Loss: 3.962995469919406e-05, Learning Rate: 0.000436\n",
      "Epoch 12700/40000, Loss: 1.7916974684339948e-05, Learning Rate: 0.000436\n",
      "Epoch 12701/40000, Loss: 3.920160816051066e-05, Learning Rate: 0.000436\n",
      "Epoch 12702/40000, Loss: 4.627904127119109e-05, Learning Rate: 0.000436\n",
      "Epoch 12703/40000, Loss: 7.454135629814118e-05, Learning Rate: 0.000436\n",
      "Epoch 12704/40000, Loss: 7.428121898556128e-05, Learning Rate: 0.000435\n",
      "Epoch 12705/40000, Loss: 1.769284972397145e-05, Learning Rate: 0.000435\n",
      "Epoch 12706/40000, Loss: 7.514470053138211e-05, Learning Rate: 0.000435\n",
      "Epoch 12707/40000, Loss: 4.690232526627369e-05, Learning Rate: 0.000435\n",
      "Epoch 12708/40000, Loss: 3.926725185010582e-05, Learning Rate: 0.000435\n",
      "Epoch 12709/40000, Loss: 5.45829207112547e-05, Learning Rate: 0.000435\n",
      "Epoch 12710/40000, Loss: 4.102544698980637e-05, Learning Rate: 0.000435\n",
      "Epoch 12711/40000, Loss: 4.6212298911996186e-05, Learning Rate: 0.000435\n",
      "Epoch 12712/40000, Loss: 7.910884596640244e-05, Learning Rate: 0.000435\n",
      "Epoch 12713/40000, Loss: 4.014433579868637e-05, Learning Rate: 0.000435\n",
      "Epoch 12714/40000, Loss: 3.933197513106279e-05, Learning Rate: 0.000435\n",
      "Epoch 12715/40000, Loss: 4.705552782979794e-05, Learning Rate: 0.000435\n",
      "Epoch 12716/40000, Loss: 7.950524741318077e-05, Learning Rate: 0.000435\n",
      "Epoch 12717/40000, Loss: 6.301466783042997e-05, Learning Rate: 0.000435\n",
      "Epoch 12718/40000, Loss: 1.7991673303185962e-05, Learning Rate: 0.000435\n",
      "Epoch 12719/40000, Loss: 4.5241824409458786e-05, Learning Rate: 0.000435\n",
      "Epoch 12720/40000, Loss: 3.9281872886931524e-05, Learning Rate: 0.000435\n",
      "Epoch 12721/40000, Loss: 6.362673593685031e-05, Learning Rate: 0.000435\n",
      "Epoch 12722/40000, Loss: 1.867587343440391e-05, Learning Rate: 0.000435\n",
      "Epoch 12723/40000, Loss: 3.914803164661862e-05, Learning Rate: 0.000434\n",
      "Epoch 12724/40000, Loss: 4.568357690004632e-05, Learning Rate: 0.000434\n",
      "Epoch 12725/40000, Loss: 6.232913437997922e-05, Learning Rate: 0.000434\n",
      "Epoch 12726/40000, Loss: 6.196212052600458e-05, Learning Rate: 0.000434\n",
      "Epoch 12727/40000, Loss: 4.577998333843425e-05, Learning Rate: 0.000434\n",
      "Epoch 12728/40000, Loss: 7.4405426857993e-05, Learning Rate: 0.000434\n",
      "Epoch 12729/40000, Loss: 3.897339411196299e-05, Learning Rate: 0.000434\n",
      "Epoch 12730/40000, Loss: 3.9019119867589325e-05, Learning Rate: 0.000434\n",
      "Epoch 12731/40000, Loss: 7.426289812428877e-05, Learning Rate: 0.000434\n",
      "Epoch 12732/40000, Loss: 1.7742755517247133e-05, Learning Rate: 0.000434\n",
      "Epoch 12733/40000, Loss: 3.8769165257690474e-05, Learning Rate: 0.000434\n",
      "Epoch 12734/40000, Loss: 1.7851787561085075e-05, Learning Rate: 0.000434\n",
      "Epoch 12735/40000, Loss: 7.427566015394405e-05, Learning Rate: 0.000434\n",
      "Epoch 12736/40000, Loss: 7.431248377542943e-05, Learning Rate: 0.000434\n",
      "Epoch 12737/40000, Loss: 7.437949534505606e-05, Learning Rate: 0.000434\n",
      "Epoch 12738/40000, Loss: 1.7734211724018678e-05, Learning Rate: 0.000434\n",
      "Epoch 12739/40000, Loss: 3.915362322004512e-05, Learning Rate: 0.000434\n",
      "Epoch 12740/40000, Loss: 3.9024074794724584e-05, Learning Rate: 0.000434\n",
      "Epoch 12741/40000, Loss: 1.743568282108754e-05, Learning Rate: 0.000434\n",
      "Epoch 12742/40000, Loss: 1.743274879117962e-05, Learning Rate: 0.000433\n",
      "Epoch 12743/40000, Loss: 3.9025948353810236e-05, Learning Rate: 0.000433\n",
      "Epoch 12744/40000, Loss: 4.528599311015569e-05, Learning Rate: 0.000433\n",
      "Epoch 12745/40000, Loss: 4.5320146455196664e-05, Learning Rate: 0.000433\n",
      "Epoch 12746/40000, Loss: 7.416156586259604e-05, Learning Rate: 0.000433\n",
      "Epoch 12747/40000, Loss: 6.199812196427956e-05, Learning Rate: 0.000433\n",
      "Epoch 12748/40000, Loss: 4.4565527787199244e-05, Learning Rate: 0.000433\n",
      "Epoch 12749/40000, Loss: 1.7600134015083313e-05, Learning Rate: 0.000433\n",
      "Epoch 12750/40000, Loss: 3.895991903846152e-05, Learning Rate: 0.000433\n",
      "Epoch 12751/40000, Loss: 3.895864210790023e-05, Learning Rate: 0.000433\n",
      "Epoch 12752/40000, Loss: 1.7436163034290075e-05, Learning Rate: 0.000433\n",
      "Epoch 12753/40000, Loss: 6.181846401887015e-05, Learning Rate: 0.000433\n",
      "Epoch 12754/40000, Loss: 7.417579035973176e-05, Learning Rate: 0.000433\n",
      "Epoch 12755/40000, Loss: 6.248866702662781e-05, Learning Rate: 0.000433\n",
      "Epoch 12756/40000, Loss: 4.491232903092168e-05, Learning Rate: 0.000433\n",
      "Epoch 12757/40000, Loss: 7.442283094860613e-05, Learning Rate: 0.000433\n",
      "Epoch 12758/40000, Loss: 4.480087955016643e-05, Learning Rate: 0.000433\n",
      "Epoch 12759/40000, Loss: 1.847626845119521e-05, Learning Rate: 0.000433\n",
      "Epoch 12760/40000, Loss: 7.461437053279951e-05, Learning Rate: 0.000433\n",
      "Epoch 12761/40000, Loss: 7.450188422808424e-05, Learning Rate: 0.000432\n",
      "Epoch 12762/40000, Loss: 1.818186865421012e-05, Learning Rate: 0.000432\n",
      "Epoch 12763/40000, Loss: 4.639246253645979e-05, Learning Rate: 0.000432\n",
      "Epoch 12764/40000, Loss: 7.546343840658665e-05, Learning Rate: 0.000432\n",
      "Epoch 12765/40000, Loss: 6.321825640043244e-05, Learning Rate: 0.000432\n",
      "Epoch 12766/40000, Loss: 4.032966535305604e-05, Learning Rate: 0.000432\n",
      "Epoch 12767/40000, Loss: 4.74359912914224e-05, Learning Rate: 0.000432\n",
      "Epoch 12768/40000, Loss: 4.964855543221347e-05, Learning Rate: 0.000432\n",
      "Epoch 12769/40000, Loss: 1.9804183466476388e-05, Learning Rate: 0.000432\n",
      "Epoch 12770/40000, Loss: 1.9830253222608007e-05, Learning Rate: 0.000432\n",
      "Epoch 12771/40000, Loss: 4.7813315177336335e-05, Learning Rate: 0.000432\n",
      "Epoch 12772/40000, Loss: 4.232617357047275e-05, Learning Rate: 0.000432\n",
      "Epoch 12773/40000, Loss: 4.068975613336079e-05, Learning Rate: 0.000432\n",
      "Epoch 12774/40000, Loss: 7.602668483741581e-05, Learning Rate: 0.000432\n",
      "Epoch 12775/40000, Loss: 5.020559183321893e-05, Learning Rate: 0.000432\n",
      "Epoch 12776/40000, Loss: 6.470581865869462e-05, Learning Rate: 0.000432\n",
      "Epoch 12777/40000, Loss: 4.058867125422694e-05, Learning Rate: 0.000432\n",
      "Epoch 12778/40000, Loss: 4.7066503611858934e-05, Learning Rate: 0.000432\n",
      "Epoch 12779/40000, Loss: 4.6259421651484445e-05, Learning Rate: 0.000432\n",
      "Epoch 12780/40000, Loss: 3.97551448259037e-05, Learning Rate: 0.000432\n",
      "Epoch 12781/40000, Loss: 6.411121285054833e-05, Learning Rate: 0.000431\n",
      "Epoch 12782/40000, Loss: 3.9737660699756816e-05, Learning Rate: 0.000431\n",
      "Epoch 12783/40000, Loss: 4.105333209736273e-05, Learning Rate: 0.000431\n",
      "Epoch 12784/40000, Loss: 1.9001228793058544e-05, Learning Rate: 0.000431\n",
      "Epoch 12785/40000, Loss: 1.895545756269712e-05, Learning Rate: 0.000431\n",
      "Epoch 12786/40000, Loss: 4.6305965952342376e-05, Learning Rate: 0.000431\n",
      "Epoch 12787/40000, Loss: 7.701422146055847e-05, Learning Rate: 0.000431\n",
      "Epoch 12788/40000, Loss: 2.0197490812279284e-05, Learning Rate: 0.000431\n",
      "Epoch 12789/40000, Loss: 5.03738883708138e-05, Learning Rate: 0.000431\n",
      "Epoch 12790/40000, Loss: 7.149540761020035e-05, Learning Rate: 0.000431\n",
      "Epoch 12791/40000, Loss: 5.151225923327729e-05, Learning Rate: 0.000431\n",
      "Epoch 12792/40000, Loss: 8.239808084908873e-05, Learning Rate: 0.000431\n",
      "Epoch 12793/40000, Loss: 8.539682312402874e-05, Learning Rate: 0.000431\n",
      "Epoch 12794/40000, Loss: 5.934310320299119e-05, Learning Rate: 0.000431\n",
      "Epoch 12795/40000, Loss: 8.060969412326813e-05, Learning Rate: 0.000431\n",
      "Epoch 12796/40000, Loss: 5.434469130705111e-05, Learning Rate: 0.000431\n",
      "Epoch 12797/40000, Loss: 2.6102183255716227e-05, Learning Rate: 0.000431\n",
      "Epoch 12798/40000, Loss: 2.1311894670361653e-05, Learning Rate: 0.000431\n",
      "Epoch 12799/40000, Loss: 5.123046867083758e-05, Learning Rate: 0.000431\n",
      "Epoch 12800/40000, Loss: 5.062812124378979e-05, Learning Rate: 0.000430\n",
      "Epoch 12801/40000, Loss: 4.899304622085765e-05, Learning Rate: 0.000430\n",
      "Epoch 12802/40000, Loss: 8.092205098364502e-05, Learning Rate: 0.000430\n",
      "Epoch 12803/40000, Loss: 4.3472387915244326e-05, Learning Rate: 0.000430\n",
      "Epoch 12804/40000, Loss: 1.994345620914828e-05, Learning Rate: 0.000430\n",
      "Epoch 12805/40000, Loss: 2.4246564862551168e-05, Learning Rate: 0.000430\n",
      "Epoch 12806/40000, Loss: 7.111384184099734e-05, Learning Rate: 0.000430\n",
      "Epoch 12807/40000, Loss: 4.327124042902142e-05, Learning Rate: 0.000430\n",
      "Epoch 12808/40000, Loss: 4.050832285429351e-05, Learning Rate: 0.000430\n",
      "Epoch 12809/40000, Loss: 2.4486174879712053e-05, Learning Rate: 0.000430\n",
      "Epoch 12810/40000, Loss: 7.285847823368385e-05, Learning Rate: 0.000430\n",
      "Epoch 12811/40000, Loss: 4.3168671254534274e-05, Learning Rate: 0.000430\n",
      "Epoch 12812/40000, Loss: 5.2906481869285926e-05, Learning Rate: 0.000430\n",
      "Epoch 12813/40000, Loss: 5.5595894082216546e-05, Learning Rate: 0.000430\n",
      "Epoch 12814/40000, Loss: 2.5722174541442655e-05, Learning Rate: 0.000430\n",
      "Epoch 12815/40000, Loss: 4.8192086978815496e-05, Learning Rate: 0.000430\n",
      "Epoch 12816/40000, Loss: 5.437325671664439e-05, Learning Rate: 0.000430\n",
      "Epoch 12817/40000, Loss: 8.02821887191385e-05, Learning Rate: 0.000430\n",
      "Epoch 12818/40000, Loss: 2.2717369574820623e-05, Learning Rate: 0.000430\n",
      "Epoch 12819/40000, Loss: 4.8282880015904084e-05, Learning Rate: 0.000429\n",
      "Epoch 12820/40000, Loss: 4.385977081255987e-05, Learning Rate: 0.000429\n",
      "Epoch 12821/40000, Loss: 4.7176643420243636e-05, Learning Rate: 0.000429\n",
      "Epoch 12822/40000, Loss: 7.678383553866297e-05, Learning Rate: 0.000429\n",
      "Epoch 12823/40000, Loss: 4.7177843953249976e-05, Learning Rate: 0.000429\n",
      "Epoch 12824/40000, Loss: 4.681403879658319e-05, Learning Rate: 0.000429\n",
      "Epoch 12825/40000, Loss: 3.996441955678165e-05, Learning Rate: 0.000429\n",
      "Epoch 12826/40000, Loss: 0.0001013617729768157, Learning Rate: 0.000429\n",
      "Epoch 12827/40000, Loss: 4.3106349039589986e-05, Learning Rate: 0.000429\n",
      "Epoch 12828/40000, Loss: 6.8606786953751e-05, Learning Rate: 0.000429\n",
      "Epoch 12829/40000, Loss: 4.9751961341826245e-05, Learning Rate: 0.000429\n",
      "Epoch 12830/40000, Loss: 9.404951561009511e-05, Learning Rate: 0.000429\n",
      "Epoch 12831/40000, Loss: 2.1206917153904215e-05, Learning Rate: 0.000429\n",
      "Epoch 12832/40000, Loss: 8.078210521489382e-05, Learning Rate: 0.000429\n",
      "Epoch 12833/40000, Loss: 0.00010060240310849622, Learning Rate: 0.000429\n",
      "Epoch 12834/40000, Loss: 6.024749018251896e-05, Learning Rate: 0.000429\n",
      "Epoch 12835/40000, Loss: 3.9100079447962344e-05, Learning Rate: 0.000429\n",
      "Epoch 12836/40000, Loss: 7.598424417665228e-05, Learning Rate: 0.000429\n",
      "Epoch 12837/40000, Loss: 6.057817518012598e-05, Learning Rate: 0.000429\n",
      "Epoch 12838/40000, Loss: 0.00013120906078256667, Learning Rate: 0.000429\n",
      "Epoch 12839/40000, Loss: 6.0941558331251144e-05, Learning Rate: 0.000428\n",
      "Epoch 12840/40000, Loss: 5.300808697938919e-05, Learning Rate: 0.000428\n",
      "Epoch 12841/40000, Loss: 0.00010008484241552651, Learning Rate: 0.000428\n",
      "Epoch 12842/40000, Loss: 5.532315117307007e-05, Learning Rate: 0.000428\n",
      "Epoch 12843/40000, Loss: 8.470863394904882e-05, Learning Rate: 0.000428\n",
      "Epoch 12844/40000, Loss: 0.00011522541899466887, Learning Rate: 0.000428\n",
      "Epoch 12845/40000, Loss: 5.895381036680192e-05, Learning Rate: 0.000428\n",
      "Epoch 12846/40000, Loss: 2.9256327252369374e-05, Learning Rate: 0.000428\n",
      "Epoch 12847/40000, Loss: 7.914018351584673e-05, Learning Rate: 0.000428\n",
      "Epoch 12848/40000, Loss: 4.5462409616447985e-05, Learning Rate: 0.000428\n",
      "Epoch 12849/40000, Loss: 6.376334931701422e-05, Learning Rate: 0.000428\n",
      "Epoch 12850/40000, Loss: 4.792992695001885e-05, Learning Rate: 0.000428\n",
      "Epoch 12851/40000, Loss: 5.7964487496064976e-05, Learning Rate: 0.000428\n",
      "Epoch 12852/40000, Loss: 5.752651486545801e-05, Learning Rate: 0.000428\n",
      "Epoch 12853/40000, Loss: 6.822275463491678e-05, Learning Rate: 0.000428\n",
      "Epoch 12854/40000, Loss: 5.4073385399533436e-05, Learning Rate: 0.000428\n",
      "Epoch 12855/40000, Loss: 5.0189515604870394e-05, Learning Rate: 0.000428\n",
      "Epoch 12856/40000, Loss: 1.9256816813140176e-05, Learning Rate: 0.000428\n",
      "Epoch 12857/40000, Loss: 5.137347397976555e-05, Learning Rate: 0.000428\n",
      "Epoch 12858/40000, Loss: 4.8234924179269e-05, Learning Rate: 0.000427\n",
      "Epoch 12859/40000, Loss: 4.638914469978772e-05, Learning Rate: 0.000427\n",
      "Epoch 12860/40000, Loss: 4.697615804616362e-05, Learning Rate: 0.000427\n",
      "Epoch 12861/40000, Loss: 4.7294841351686046e-05, Learning Rate: 0.000427\n",
      "Epoch 12862/40000, Loss: 7.491883297916502e-05, Learning Rate: 0.000427\n",
      "Epoch 12863/40000, Loss: 4.579472079058178e-05, Learning Rate: 0.000427\n",
      "Epoch 12864/40000, Loss: 1.8170605471823364e-05, Learning Rate: 0.000427\n",
      "Epoch 12865/40000, Loss: 4.064772656420246e-05, Learning Rate: 0.000427\n",
      "Epoch 12866/40000, Loss: 6.224795652087778e-05, Learning Rate: 0.000427\n",
      "Epoch 12867/40000, Loss: 7.451880810549483e-05, Learning Rate: 0.000427\n",
      "Epoch 12868/40000, Loss: 4.531591184786521e-05, Learning Rate: 0.000427\n",
      "Epoch 12869/40000, Loss: 3.89783745049499e-05, Learning Rate: 0.000427\n",
      "Epoch 12870/40000, Loss: 4.488542253966443e-05, Learning Rate: 0.000427\n",
      "Epoch 12871/40000, Loss: 6.191457941895351e-05, Learning Rate: 0.000427\n",
      "Epoch 12872/40000, Loss: 3.9310005377046764e-05, Learning Rate: 0.000427\n",
      "Epoch 12873/40000, Loss: 6.255048356251791e-05, Learning Rate: 0.000427\n",
      "Epoch 12874/40000, Loss: 3.933843254344538e-05, Learning Rate: 0.000427\n",
      "Epoch 12875/40000, Loss: 4.5007080188952386e-05, Learning Rate: 0.000427\n",
      "Epoch 12876/40000, Loss: 4.052084841532633e-05, Learning Rate: 0.000427\n",
      "Epoch 12877/40000, Loss: 4.5444197894539684e-05, Learning Rate: 0.000427\n",
      "Epoch 12878/40000, Loss: 1.7548234609421343e-05, Learning Rate: 0.000426\n",
      "Epoch 12879/40000, Loss: 7.404747884720564e-05, Learning Rate: 0.000426\n",
      "Epoch 12880/40000, Loss: 1.7817133993958123e-05, Learning Rate: 0.000426\n",
      "Epoch 12881/40000, Loss: 6.198423216119409e-05, Learning Rate: 0.000426\n",
      "Epoch 12882/40000, Loss: 4.474974412005395e-05, Learning Rate: 0.000426\n",
      "Epoch 12883/40000, Loss: 1.7559943444211967e-05, Learning Rate: 0.000426\n",
      "Epoch 12884/40000, Loss: 1.7645956177148037e-05, Learning Rate: 0.000426\n",
      "Epoch 12885/40000, Loss: 4.501271541812457e-05, Learning Rate: 0.000426\n",
      "Epoch 12886/40000, Loss: 6.187575490912423e-05, Learning Rate: 0.000426\n",
      "Epoch 12887/40000, Loss: 4.4735348637914285e-05, Learning Rate: 0.000426\n",
      "Epoch 12888/40000, Loss: 1.747632813930977e-05, Learning Rate: 0.000426\n",
      "Epoch 12889/40000, Loss: 3.859749995172024e-05, Learning Rate: 0.000426\n",
      "Epoch 12890/40000, Loss: 7.392026600427926e-05, Learning Rate: 0.000426\n",
      "Epoch 12891/40000, Loss: 3.873296736855991e-05, Learning Rate: 0.000426\n",
      "Epoch 12892/40000, Loss: 4.44776305812411e-05, Learning Rate: 0.000426\n",
      "Epoch 12893/40000, Loss: 6.231984298210591e-05, Learning Rate: 0.000426\n",
      "Epoch 12894/40000, Loss: 4.4464679376687855e-05, Learning Rate: 0.000426\n",
      "Epoch 12895/40000, Loss: 4.464335142984055e-05, Learning Rate: 0.000426\n",
      "Epoch 12896/40000, Loss: 4.5146658521844074e-05, Learning Rate: 0.000426\n",
      "Epoch 12897/40000, Loss: 4.486581383389421e-05, Learning Rate: 0.000425\n",
      "Epoch 12898/40000, Loss: 3.8913880416657776e-05, Learning Rate: 0.000425\n",
      "Epoch 12899/40000, Loss: 4.5187400246504694e-05, Learning Rate: 0.000425\n",
      "Epoch 12900/40000, Loss: 1.787853216228541e-05, Learning Rate: 0.000425\n",
      "Epoch 12901/40000, Loss: 3.870712316711433e-05, Learning Rate: 0.000425\n",
      "Epoch 12902/40000, Loss: 3.8728983781766146e-05, Learning Rate: 0.000425\n",
      "Epoch 12903/40000, Loss: 3.873512105201371e-05, Learning Rate: 0.000425\n",
      "Epoch 12904/40000, Loss: 7.363996701315045e-05, Learning Rate: 0.000425\n",
      "Epoch 12905/40000, Loss: 7.351413660217077e-05, Learning Rate: 0.000425\n",
      "Epoch 12906/40000, Loss: 1.7711388863972388e-05, Learning Rate: 0.000425\n",
      "Epoch 12907/40000, Loss: 3.86009705835022e-05, Learning Rate: 0.000425\n",
      "Epoch 12908/40000, Loss: 6.150440458441153e-05, Learning Rate: 0.000425\n",
      "Epoch 12909/40000, Loss: 7.385059143416584e-05, Learning Rate: 0.000425\n",
      "Epoch 12910/40000, Loss: 3.85393577744253e-05, Learning Rate: 0.000425\n",
      "Epoch 12911/40000, Loss: 3.846741310553625e-05, Learning Rate: 0.000425\n",
      "Epoch 12912/40000, Loss: 4.505051037995145e-05, Learning Rate: 0.000425\n",
      "Epoch 12913/40000, Loss: 4.438845644472167e-05, Learning Rate: 0.000425\n",
      "Epoch 12914/40000, Loss: 7.382572948699817e-05, Learning Rate: 0.000425\n",
      "Epoch 12915/40000, Loss: 4.502633601077832e-05, Learning Rate: 0.000425\n",
      "Epoch 12916/40000, Loss: 7.356053538387641e-05, Learning Rate: 0.000425\n",
      "Epoch 12917/40000, Loss: 3.8599828258156776e-05, Learning Rate: 0.000424\n",
      "Epoch 12918/40000, Loss: 1.736639387672767e-05, Learning Rate: 0.000424\n",
      "Epoch 12919/40000, Loss: 7.33572815079242e-05, Learning Rate: 0.000424\n",
      "Epoch 12920/40000, Loss: 3.9396840293193236e-05, Learning Rate: 0.000424\n",
      "Epoch 12921/40000, Loss: 4.5195065467851236e-05, Learning Rate: 0.000424\n",
      "Epoch 12922/40000, Loss: 6.166045932332054e-05, Learning Rate: 0.000424\n",
      "Epoch 12923/40000, Loss: 4.4608481402974576e-05, Learning Rate: 0.000424\n",
      "Epoch 12924/40000, Loss: 4.502506635617465e-05, Learning Rate: 0.000424\n",
      "Epoch 12925/40000, Loss: 4.5135409891372547e-05, Learning Rate: 0.000424\n",
      "Epoch 12926/40000, Loss: 1.764187982189469e-05, Learning Rate: 0.000424\n",
      "Epoch 12927/40000, Loss: 7.519283099099994e-05, Learning Rate: 0.000424\n",
      "Epoch 12928/40000, Loss: 1.775788769009523e-05, Learning Rate: 0.000424\n",
      "Epoch 12929/40000, Loss: 3.8969006709521636e-05, Learning Rate: 0.000424\n",
      "Epoch 12930/40000, Loss: 7.497381011489779e-05, Learning Rate: 0.000424\n",
      "Epoch 12931/40000, Loss: 1.7610180293559097e-05, Learning Rate: 0.000424\n",
      "Epoch 12932/40000, Loss: 1.7619726349948905e-05, Learning Rate: 0.000424\n",
      "Epoch 12933/40000, Loss: 7.510248542530462e-05, Learning Rate: 0.000424\n",
      "Epoch 12934/40000, Loss: 4.715502291219309e-05, Learning Rate: 0.000424\n",
      "Epoch 12935/40000, Loss: 3.8635782402707264e-05, Learning Rate: 0.000424\n",
      "Epoch 12936/40000, Loss: 7.440653280355036e-05, Learning Rate: 0.000424\n",
      "Epoch 12937/40000, Loss: 4.676247408497147e-05, Learning Rate: 0.000423\n",
      "Epoch 12938/40000, Loss: 6.277343345573172e-05, Learning Rate: 0.000423\n",
      "Epoch 12939/40000, Loss: 1.7580819985596463e-05, Learning Rate: 0.000423\n",
      "Epoch 12940/40000, Loss: 1.7411761291441508e-05, Learning Rate: 0.000423\n",
      "Epoch 12941/40000, Loss: 4.5898457756266e-05, Learning Rate: 0.000423\n",
      "Epoch 12942/40000, Loss: 7.455878949258476e-05, Learning Rate: 0.000423\n",
      "Epoch 12943/40000, Loss: 1.769714435795322e-05, Learning Rate: 0.000423\n",
      "Epoch 12944/40000, Loss: 3.905342236976139e-05, Learning Rate: 0.000423\n",
      "Epoch 12945/40000, Loss: 4.492843072512187e-05, Learning Rate: 0.000423\n",
      "Epoch 12946/40000, Loss: 7.403722702292725e-05, Learning Rate: 0.000423\n",
      "Epoch 12947/40000, Loss: 4.5767304982291535e-05, Learning Rate: 0.000423\n",
      "Epoch 12948/40000, Loss: 1.8301125237485394e-05, Learning Rate: 0.000423\n",
      "Epoch 12949/40000, Loss: 4.760280353366397e-05, Learning Rate: 0.000423\n",
      "Epoch 12950/40000, Loss: 6.522901821881533e-05, Learning Rate: 0.000423\n",
      "Epoch 12951/40000, Loss: 7.534682663390413e-05, Learning Rate: 0.000423\n",
      "Epoch 12952/40000, Loss: 4.9543836212251335e-05, Learning Rate: 0.000423\n",
      "Epoch 12953/40000, Loss: 5.0632406782824546e-05, Learning Rate: 0.000423\n",
      "Epoch 12954/40000, Loss: 2.4671038772794418e-05, Learning Rate: 0.000423\n",
      "Epoch 12955/40000, Loss: 5.0650822231546044e-05, Learning Rate: 0.000423\n",
      "Epoch 12956/40000, Loss: 7.72424609749578e-05, Learning Rate: 0.000422\n",
      "Epoch 12957/40000, Loss: 1.989515840250533e-05, Learning Rate: 0.000422\n",
      "Epoch 12958/40000, Loss: 4.6964705688878894e-05, Learning Rate: 0.000422\n",
      "Epoch 12959/40000, Loss: 4.0675226046005264e-05, Learning Rate: 0.000422\n",
      "Epoch 12960/40000, Loss: 1.842216261138674e-05, Learning Rate: 0.000422\n",
      "Epoch 12961/40000, Loss: 5.102874638396315e-05, Learning Rate: 0.000422\n",
      "Epoch 12962/40000, Loss: 4.636825906345621e-05, Learning Rate: 0.000422\n",
      "Epoch 12963/40000, Loss: 1.8713424651650712e-05, Learning Rate: 0.000422\n",
      "Epoch 12964/40000, Loss: 4.127535430598073e-05, Learning Rate: 0.000422\n",
      "Epoch 12965/40000, Loss: 4.796721987077035e-05, Learning Rate: 0.000422\n",
      "Epoch 12966/40000, Loss: 7.71212944528088e-05, Learning Rate: 0.000422\n",
      "Epoch 12967/40000, Loss: 7.623367855558172e-05, Learning Rate: 0.000422\n",
      "Epoch 12968/40000, Loss: 4.174964487901889e-05, Learning Rate: 0.000422\n",
      "Epoch 12969/40000, Loss: 4.6201017539715394e-05, Learning Rate: 0.000422\n",
      "Epoch 12970/40000, Loss: 4.4720389269059524e-05, Learning Rate: 0.000422\n",
      "Epoch 12971/40000, Loss: 6.500192830571905e-05, Learning Rate: 0.000422\n",
      "Epoch 12972/40000, Loss: 5.442714973469265e-05, Learning Rate: 0.000422\n",
      "Epoch 12973/40000, Loss: 5.529338523047045e-05, Learning Rate: 0.000422\n",
      "Epoch 12974/40000, Loss: 3.060963354073465e-05, Learning Rate: 0.000422\n",
      "Epoch 12975/40000, Loss: 2.1953856048639864e-05, Learning Rate: 0.000422\n",
      "Epoch 12976/40000, Loss: 7.560487574664876e-05, Learning Rate: 0.000421\n",
      "Epoch 12977/40000, Loss: 6.810500053688884e-05, Learning Rate: 0.000421\n",
      "Epoch 12978/40000, Loss: 7.55418004700914e-05, Learning Rate: 0.000421\n",
      "Epoch 12979/40000, Loss: 4.630668627214618e-05, Learning Rate: 0.000421\n",
      "Epoch 12980/40000, Loss: 1.816686926758848e-05, Learning Rate: 0.000421\n",
      "Epoch 12981/40000, Loss: 4.708276173914783e-05, Learning Rate: 0.000421\n",
      "Epoch 12982/40000, Loss: 6.673168536508456e-05, Learning Rate: 0.000421\n",
      "Epoch 12983/40000, Loss: 6.535067223012447e-05, Learning Rate: 0.000421\n",
      "Epoch 12984/40000, Loss: 1.94141121028224e-05, Learning Rate: 0.000421\n",
      "Epoch 12985/40000, Loss: 5.018145020585507e-05, Learning Rate: 0.000421\n",
      "Epoch 12986/40000, Loss: 4.0590050048194826e-05, Learning Rate: 0.000421\n",
      "Epoch 12987/40000, Loss: 7.627069135196507e-05, Learning Rate: 0.000421\n",
      "Epoch 12988/40000, Loss: 4.650631672120653e-05, Learning Rate: 0.000421\n",
      "Epoch 12989/40000, Loss: 1.8648723198566586e-05, Learning Rate: 0.000421\n",
      "Epoch 12990/40000, Loss: 7.485198875656351e-05, Learning Rate: 0.000421\n",
      "Epoch 12991/40000, Loss: 1.8959432054543868e-05, Learning Rate: 0.000421\n",
      "Epoch 12992/40000, Loss: 6.210654828464612e-05, Learning Rate: 0.000421\n",
      "Epoch 12993/40000, Loss: 4.540134614217095e-05, Learning Rate: 0.000421\n",
      "Epoch 12994/40000, Loss: 4.485789395403117e-05, Learning Rate: 0.000421\n",
      "Epoch 12995/40000, Loss: 7.39283932489343e-05, Learning Rate: 0.000421\n",
      "Epoch 12996/40000, Loss: 6.180640048114583e-05, Learning Rate: 0.000420\n",
      "Epoch 12997/40000, Loss: 3.889733852702193e-05, Learning Rate: 0.000420\n",
      "Epoch 12998/40000, Loss: 3.8817575841676444e-05, Learning Rate: 0.000420\n",
      "Epoch 12999/40000, Loss: 1.797331788111478e-05, Learning Rate: 0.000420\n",
      "Epoch 13000/40000, Loss: 6.275527266552672e-05, Learning Rate: 0.000420\n",
      "Epoch 13001/40000, Loss: 4.5384033001028e-05, Learning Rate: 0.000420\n",
      "Epoch 13002/40000, Loss: 4.520661605056375e-05, Learning Rate: 0.000420\n",
      "Epoch 13003/40000, Loss: 4.506694313022308e-05, Learning Rate: 0.000420\n",
      "Epoch 13004/40000, Loss: 4.583602276397869e-05, Learning Rate: 0.000420\n",
      "Epoch 13005/40000, Loss: 4.5380464143818244e-05, Learning Rate: 0.000420\n",
      "Epoch 13006/40000, Loss: 4.8304231313522905e-05, Learning Rate: 0.000420\n",
      "Epoch 13007/40000, Loss: 7.747476047370583e-05, Learning Rate: 0.000420\n",
      "Epoch 13008/40000, Loss: 3.989828110206872e-05, Learning Rate: 0.000420\n",
      "Epoch 13009/40000, Loss: 3.9585753256687894e-05, Learning Rate: 0.000420\n",
      "Epoch 13010/40000, Loss: 3.9373138861265033e-05, Learning Rate: 0.000420\n",
      "Epoch 13011/40000, Loss: 4.000957414973527e-05, Learning Rate: 0.000420\n",
      "Epoch 13012/40000, Loss: 6.777299131499603e-05, Learning Rate: 0.000420\n",
      "Epoch 13013/40000, Loss: 4.8452562623424456e-05, Learning Rate: 0.000420\n",
      "Epoch 13014/40000, Loss: 4.903085937257856e-05, Learning Rate: 0.000420\n",
      "Epoch 13015/40000, Loss: 2.5321574867120944e-05, Learning Rate: 0.000420\n",
      "Epoch 13016/40000, Loss: 2.0642319213948213e-05, Learning Rate: 0.000419\n",
      "Epoch 13017/40000, Loss: 4.7584424464730546e-05, Learning Rate: 0.000419\n",
      "Epoch 13018/40000, Loss: 7.66689408919774e-05, Learning Rate: 0.000419\n",
      "Epoch 13019/40000, Loss: 4.780679591931403e-05, Learning Rate: 0.000419\n",
      "Epoch 13020/40000, Loss: 3.412350270082243e-05, Learning Rate: 0.000419\n",
      "Epoch 13021/40000, Loss: 9.352937922812998e-05, Learning Rate: 0.000419\n",
      "Epoch 13022/40000, Loss: 6.958504673093557e-05, Learning Rate: 0.000419\n",
      "Epoch 13023/40000, Loss: 0.00010298248525941744, Learning Rate: 0.000419\n",
      "Epoch 13024/40000, Loss: 0.00010708200716180727, Learning Rate: 0.000419\n",
      "Epoch 13025/40000, Loss: 0.0001266216131625697, Learning Rate: 0.000419\n",
      "Epoch 13026/40000, Loss: 6.78252981742844e-05, Learning Rate: 0.000419\n",
      "Epoch 13027/40000, Loss: 9.425095049664378e-05, Learning Rate: 0.000419\n",
      "Epoch 13028/40000, Loss: 2.453848901495803e-05, Learning Rate: 0.000419\n",
      "Epoch 13029/40000, Loss: 7.842807099223137e-05, Learning Rate: 0.000419\n",
      "Epoch 13030/40000, Loss: 4.604315836331807e-05, Learning Rate: 0.000419\n",
      "Epoch 13031/40000, Loss: 4.3042404286097735e-05, Learning Rate: 0.000419\n",
      "Epoch 13032/40000, Loss: 8.756434544920921e-05, Learning Rate: 0.000419\n",
      "Epoch 13033/40000, Loss: 5.143690941622481e-05, Learning Rate: 0.000419\n",
      "Epoch 13034/40000, Loss: 2.8571148504852317e-05, Learning Rate: 0.000419\n",
      "Epoch 13035/40000, Loss: 4.8907044401858e-05, Learning Rate: 0.000419\n",
      "Epoch 13036/40000, Loss: 6.93708352628164e-05, Learning Rate: 0.000418\n",
      "Epoch 13037/40000, Loss: 6.891208613524213e-05, Learning Rate: 0.000418\n",
      "Epoch 13038/40000, Loss: 5.341578435036354e-05, Learning Rate: 0.000418\n",
      "Epoch 13039/40000, Loss: 4.110085865249857e-05, Learning Rate: 0.000418\n",
      "Epoch 13040/40000, Loss: 4.810834434465505e-05, Learning Rate: 0.000418\n",
      "Epoch 13041/40000, Loss: 4.145921047893353e-05, Learning Rate: 0.000418\n",
      "Epoch 13042/40000, Loss: 2.1156451111892238e-05, Learning Rate: 0.000418\n",
      "Epoch 13043/40000, Loss: 4.7627105232095346e-05, Learning Rate: 0.000418\n",
      "Epoch 13044/40000, Loss: 7.756365812383592e-05, Learning Rate: 0.000418\n",
      "Epoch 13045/40000, Loss: 7.660761184524745e-05, Learning Rate: 0.000418\n",
      "Epoch 13046/40000, Loss: 6.34626267128624e-05, Learning Rate: 0.000418\n",
      "Epoch 13047/40000, Loss: 1.8378048480371945e-05, Learning Rate: 0.000418\n",
      "Epoch 13048/40000, Loss: 4.0253820770885795e-05, Learning Rate: 0.000418\n",
      "Epoch 13049/40000, Loss: 4.7522393288090825e-05, Learning Rate: 0.000418\n",
      "Epoch 13050/40000, Loss: 4.578490916173905e-05, Learning Rate: 0.000418\n",
      "Epoch 13051/40000, Loss: 4.562827598419972e-05, Learning Rate: 0.000418\n",
      "Epoch 13052/40000, Loss: 1.831707777455449e-05, Learning Rate: 0.000418\n",
      "Epoch 13053/40000, Loss: 6.292692705756053e-05, Learning Rate: 0.000418\n",
      "Epoch 13054/40000, Loss: 7.423652277793735e-05, Learning Rate: 0.000418\n",
      "Epoch 13055/40000, Loss: 1.8071812519337982e-05, Learning Rate: 0.000418\n",
      "Epoch 13056/40000, Loss: 4.646257002605125e-05, Learning Rate: 0.000417\n",
      "Epoch 13057/40000, Loss: 4.532498132903129e-05, Learning Rate: 0.000417\n",
      "Epoch 13058/40000, Loss: 6.177384784677997e-05, Learning Rate: 0.000417\n",
      "Epoch 13059/40000, Loss: 4.488880585995503e-05, Learning Rate: 0.000417\n",
      "Epoch 13060/40000, Loss: 6.147511885501444e-05, Learning Rate: 0.000417\n",
      "Epoch 13061/40000, Loss: 4.476626418181695e-05, Learning Rate: 0.000417\n",
      "Epoch 13062/40000, Loss: 3.847253174171783e-05, Learning Rate: 0.000417\n",
      "Epoch 13063/40000, Loss: 6.131122790975496e-05, Learning Rate: 0.000417\n",
      "Epoch 13064/40000, Loss: 7.393723353743553e-05, Learning Rate: 0.000417\n",
      "Epoch 13065/40000, Loss: 4.4740441808244213e-05, Learning Rate: 0.000417\n",
      "Epoch 13066/40000, Loss: 4.4551641622092575e-05, Learning Rate: 0.000417\n",
      "Epoch 13067/40000, Loss: 4.4381144107319415e-05, Learning Rate: 0.000417\n",
      "Epoch 13068/40000, Loss: 4.423129576025531e-05, Learning Rate: 0.000417\n",
      "Epoch 13069/40000, Loss: 6.121448677731678e-05, Learning Rate: 0.000417\n",
      "Epoch 13070/40000, Loss: 3.826690590358339e-05, Learning Rate: 0.000417\n",
      "Epoch 13071/40000, Loss: 6.095840217312798e-05, Learning Rate: 0.000417\n",
      "Epoch 13072/40000, Loss: 7.324673788389191e-05, Learning Rate: 0.000417\n",
      "Epoch 13073/40000, Loss: 4.429416003404185e-05, Learning Rate: 0.000417\n",
      "Epoch 13074/40000, Loss: 3.854288297588937e-05, Learning Rate: 0.000417\n",
      "Epoch 13075/40000, Loss: 4.43256831204053e-05, Learning Rate: 0.000417\n",
      "Epoch 13076/40000, Loss: 1.747778514982201e-05, Learning Rate: 0.000416\n",
      "Epoch 13077/40000, Loss: 3.909796942025423e-05, Learning Rate: 0.000416\n",
      "Epoch 13078/40000, Loss: 7.310985529329628e-05, Learning Rate: 0.000416\n",
      "Epoch 13079/40000, Loss: 3.840789577225223e-05, Learning Rate: 0.000416\n",
      "Epoch 13080/40000, Loss: 7.3285773396492e-05, Learning Rate: 0.000416\n",
      "Epoch 13081/40000, Loss: 1.7531589037389494e-05, Learning Rate: 0.000416\n",
      "Epoch 13082/40000, Loss: 4.450675623957068e-05, Learning Rate: 0.000416\n",
      "Epoch 13083/40000, Loss: 1.7680134988040663e-05, Learning Rate: 0.000416\n",
      "Epoch 13084/40000, Loss: 4.515588079812005e-05, Learning Rate: 0.000416\n",
      "Epoch 13085/40000, Loss: 3.8774654967710376e-05, Learning Rate: 0.000416\n",
      "Epoch 13086/40000, Loss: 7.365590136032552e-05, Learning Rate: 0.000416\n",
      "Epoch 13087/40000, Loss: 4.489260754780844e-05, Learning Rate: 0.000416\n",
      "Epoch 13088/40000, Loss: 4.486269244807772e-05, Learning Rate: 0.000416\n",
      "Epoch 13089/40000, Loss: 3.8734375266358256e-05, Learning Rate: 0.000416\n",
      "Epoch 13090/40000, Loss: 6.176521128509194e-05, Learning Rate: 0.000416\n",
      "Epoch 13091/40000, Loss: 6.191019201651216e-05, Learning Rate: 0.000416\n",
      "Epoch 13092/40000, Loss: 6.149682303657755e-05, Learning Rate: 0.000416\n",
      "Epoch 13093/40000, Loss: 6.196234608069062e-05, Learning Rate: 0.000416\n",
      "Epoch 13094/40000, Loss: 4.556406202027574e-05, Learning Rate: 0.000416\n",
      "Epoch 13095/40000, Loss: 1.8094933693646453e-05, Learning Rate: 0.000416\n",
      "Epoch 13096/40000, Loss: 3.915587512892671e-05, Learning Rate: 0.000415\n",
      "Epoch 13097/40000, Loss: 4.627775342669338e-05, Learning Rate: 0.000415\n",
      "Epoch 13098/40000, Loss: 6.43595922156237e-05, Learning Rate: 0.000415\n",
      "Epoch 13099/40000, Loss: 5.254595816950314e-05, Learning Rate: 0.000415\n",
      "Epoch 13100/40000, Loss: 4.253832594258711e-05, Learning Rate: 0.000415\n",
      "Epoch 13101/40000, Loss: 6.826534809079021e-05, Learning Rate: 0.000415\n",
      "Epoch 13102/40000, Loss: 5.184091060073115e-05, Learning Rate: 0.000415\n",
      "Epoch 13103/40000, Loss: 2.0655626940424554e-05, Learning Rate: 0.000415\n",
      "Epoch 13104/40000, Loss: 6.77673815516755e-05, Learning Rate: 0.000415\n",
      "Epoch 13105/40000, Loss: 6.547135853907093e-05, Learning Rate: 0.000415\n",
      "Epoch 13106/40000, Loss: 5.2457984565990046e-05, Learning Rate: 0.000415\n",
      "Epoch 13107/40000, Loss: 7.647353049833328e-05, Learning Rate: 0.000415\n",
      "Epoch 13108/40000, Loss: 5.231745672062971e-05, Learning Rate: 0.000415\n",
      "Epoch 13109/40000, Loss: 7.830296090105549e-05, Learning Rate: 0.000415\n",
      "Epoch 13110/40000, Loss: 7.503390224883333e-05, Learning Rate: 0.000415\n",
      "Epoch 13111/40000, Loss: 7.611772889504209e-05, Learning Rate: 0.000415\n",
      "Epoch 13112/40000, Loss: 4.2942509026033804e-05, Learning Rate: 0.000415\n",
      "Epoch 13113/40000, Loss: 5.301797864376567e-05, Learning Rate: 0.000415\n",
      "Epoch 13114/40000, Loss: 5.495933146448806e-05, Learning Rate: 0.000415\n",
      "Epoch 13115/40000, Loss: 6.432321242755279e-05, Learning Rate: 0.000415\n",
      "Epoch 13116/40000, Loss: 1.941222399182152e-05, Learning Rate: 0.000414\n",
      "Epoch 13117/40000, Loss: 4.9080605094786733e-05, Learning Rate: 0.000414\n",
      "Epoch 13118/40000, Loss: 6.325043796095997e-05, Learning Rate: 0.000414\n",
      "Epoch 13119/40000, Loss: 7.79486435931176e-05, Learning Rate: 0.000414\n",
      "Epoch 13120/40000, Loss: 7.014521543169394e-05, Learning Rate: 0.000414\n",
      "Epoch 13121/40000, Loss: 7.699192792642862e-05, Learning Rate: 0.000414\n",
      "Epoch 13122/40000, Loss: 4.2363153625046834e-05, Learning Rate: 0.000414\n",
      "Epoch 13123/40000, Loss: 4.0220911614596844e-05, Learning Rate: 0.000414\n",
      "Epoch 13124/40000, Loss: 7.817109144525602e-05, Learning Rate: 0.000414\n",
      "Epoch 13125/40000, Loss: 2.5379138605785556e-05, Learning Rate: 0.000414\n",
      "Epoch 13126/40000, Loss: 2.3223306925501674e-05, Learning Rate: 0.000414\n",
      "Epoch 13127/40000, Loss: 4.044502566102892e-05, Learning Rate: 0.000414\n",
      "Epoch 13128/40000, Loss: 3.396685133338906e-05, Learning Rate: 0.000414\n",
      "Epoch 13129/40000, Loss: 8.735309529583901e-05, Learning Rate: 0.000414\n",
      "Epoch 13130/40000, Loss: 5.3598294471157715e-05, Learning Rate: 0.000414\n",
      "Epoch 13131/40000, Loss: 4.7790599637664855e-05, Learning Rate: 0.000414\n",
      "Epoch 13132/40000, Loss: 7.951815496198833e-05, Learning Rate: 0.000414\n",
      "Epoch 13133/40000, Loss: 5.2659732318716124e-05, Learning Rate: 0.000414\n",
      "Epoch 13134/40000, Loss: 5.777199112344533e-05, Learning Rate: 0.000414\n",
      "Epoch 13135/40000, Loss: 0.00013321766164153814, Learning Rate: 0.000414\n",
      "Epoch 13136/40000, Loss: 0.00010023039067164063, Learning Rate: 0.000413\n",
      "Epoch 13137/40000, Loss: 2.579664214863442e-05, Learning Rate: 0.000413\n",
      "Epoch 13138/40000, Loss: 5.8336805523140356e-05, Learning Rate: 0.000413\n",
      "Epoch 13139/40000, Loss: 7.690231723245233e-05, Learning Rate: 0.000413\n",
      "Epoch 13140/40000, Loss: 4.339577571954578e-05, Learning Rate: 0.000413\n",
      "Epoch 13141/40000, Loss: 3.378245673957281e-05, Learning Rate: 0.000413\n",
      "Epoch 13142/40000, Loss: 7.756566628813744e-05, Learning Rate: 0.000413\n",
      "Epoch 13143/40000, Loss: 4.52088461315725e-05, Learning Rate: 0.000413\n",
      "Epoch 13144/40000, Loss: 5.5746102589182556e-05, Learning Rate: 0.000413\n",
      "Epoch 13145/40000, Loss: 4.20065043726936e-05, Learning Rate: 0.000413\n",
      "Epoch 13146/40000, Loss: 4.552568861981854e-05, Learning Rate: 0.000413\n",
      "Epoch 13147/40000, Loss: 8.544193406123668e-05, Learning Rate: 0.000413\n",
      "Epoch 13148/40000, Loss: 5.232123294263147e-05, Learning Rate: 0.000413\n",
      "Epoch 13149/40000, Loss: 5.4702639317838475e-05, Learning Rate: 0.000413\n",
      "Epoch 13150/40000, Loss: 2.0892732209176756e-05, Learning Rate: 0.000413\n",
      "Epoch 13151/40000, Loss: 2.3196696929517202e-05, Learning Rate: 0.000413\n",
      "Epoch 13152/40000, Loss: 7.896666647866368e-05, Learning Rate: 0.000413\n",
      "Epoch 13153/40000, Loss: 7.62298732297495e-05, Learning Rate: 0.000413\n",
      "Epoch 13154/40000, Loss: 2.276135637657717e-05, Learning Rate: 0.000413\n",
      "Epoch 13155/40000, Loss: 2.5522063879179768e-05, Learning Rate: 0.000413\n",
      "Epoch 13156/40000, Loss: 5.2313280320959166e-05, Learning Rate: 0.000412\n",
      "Epoch 13157/40000, Loss: 5.8065936173079535e-05, Learning Rate: 0.000412\n",
      "Epoch 13158/40000, Loss: 5.0147547881351784e-05, Learning Rate: 0.000412\n",
      "Epoch 13159/40000, Loss: 7.747970812488347e-05, Learning Rate: 0.000412\n",
      "Epoch 13160/40000, Loss: 2.2986350813880563e-05, Learning Rate: 0.000412\n",
      "Epoch 13161/40000, Loss: 8.161494770320132e-05, Learning Rate: 0.000412\n",
      "Epoch 13162/40000, Loss: 6.892318197060376e-05, Learning Rate: 0.000412\n",
      "Epoch 13163/40000, Loss: 7.667784666409716e-05, Learning Rate: 0.000412\n",
      "Epoch 13164/40000, Loss: 4.657816680264659e-05, Learning Rate: 0.000412\n",
      "Epoch 13165/40000, Loss: 2.2253714632824995e-05, Learning Rate: 0.000412\n",
      "Epoch 13166/40000, Loss: 4.6355966333067045e-05, Learning Rate: 0.000412\n",
      "Epoch 13167/40000, Loss: 7.364610064541921e-05, Learning Rate: 0.000412\n",
      "Epoch 13168/40000, Loss: 2.325328932784032e-05, Learning Rate: 0.000412\n",
      "Epoch 13169/40000, Loss: 5.11936450493522e-05, Learning Rate: 0.000412\n",
      "Epoch 13170/40000, Loss: 7.95430678408593e-05, Learning Rate: 0.000412\n",
      "Epoch 13171/40000, Loss: 7.721720612607896e-05, Learning Rate: 0.000412\n",
      "Epoch 13172/40000, Loss: 7.503838423872367e-05, Learning Rate: 0.000412\n",
      "Epoch 13173/40000, Loss: 4.2363491957075894e-05, Learning Rate: 0.000412\n",
      "Epoch 13174/40000, Loss: 5.046397200203501e-05, Learning Rate: 0.000412\n",
      "Epoch 13175/40000, Loss: 4.522844756138511e-05, Learning Rate: 0.000412\n",
      "Epoch 13176/40000, Loss: 5.021638935431838e-05, Learning Rate: 0.000411\n",
      "Epoch 13177/40000, Loss: 4.6534081775462255e-05, Learning Rate: 0.000411\n",
      "Epoch 13178/40000, Loss: 2.32075199164683e-05, Learning Rate: 0.000411\n",
      "Epoch 13179/40000, Loss: 7.319027645280585e-05, Learning Rate: 0.000411\n",
      "Epoch 13180/40000, Loss: 4.87273937324062e-05, Learning Rate: 0.000411\n",
      "Epoch 13181/40000, Loss: 1.973517646547407e-05, Learning Rate: 0.000411\n",
      "Epoch 13182/40000, Loss: 7.863180508138612e-05, Learning Rate: 0.000411\n",
      "Epoch 13183/40000, Loss: 4.974267358193174e-05, Learning Rate: 0.000411\n",
      "Epoch 13184/40000, Loss: 7.541675586253405e-05, Learning Rate: 0.000411\n",
      "Epoch 13185/40000, Loss: 7.44284552638419e-05, Learning Rate: 0.000411\n",
      "Epoch 13186/40000, Loss: 6.54587711323984e-05, Learning Rate: 0.000411\n",
      "Epoch 13187/40000, Loss: 4.675552554544993e-05, Learning Rate: 0.000411\n",
      "Epoch 13188/40000, Loss: 3.995338192908093e-05, Learning Rate: 0.000411\n",
      "Epoch 13189/40000, Loss: 3.9096401451388374e-05, Learning Rate: 0.000411\n",
      "Epoch 13190/40000, Loss: 4.7715508117107674e-05, Learning Rate: 0.000411\n",
      "Epoch 13191/40000, Loss: 7.624932914040983e-05, Learning Rate: 0.000411\n",
      "Epoch 13192/40000, Loss: 1.9822517060674727e-05, Learning Rate: 0.000411\n",
      "Epoch 13193/40000, Loss: 4.655840166378766e-05, Learning Rate: 0.000411\n",
      "Epoch 13194/40000, Loss: 7.666922465432435e-05, Learning Rate: 0.000411\n",
      "Epoch 13195/40000, Loss: 7.49938262742944e-05, Learning Rate: 0.000411\n",
      "Epoch 13196/40000, Loss: 7.448070391546935e-05, Learning Rate: 0.000410\n",
      "Epoch 13197/40000, Loss: 1.9453662389423698e-05, Learning Rate: 0.000410\n",
      "Epoch 13198/40000, Loss: 4.833951606997289e-05, Learning Rate: 0.000410\n",
      "Epoch 13199/40000, Loss: 4.5689099351875484e-05, Learning Rate: 0.000410\n",
      "Epoch 13200/40000, Loss: 4.9015456170309335e-05, Learning Rate: 0.000410\n",
      "Epoch 13201/40000, Loss: 4.777089270646684e-05, Learning Rate: 0.000410\n",
      "Epoch 13202/40000, Loss: 5.3926003602100536e-05, Learning Rate: 0.000410\n",
      "Epoch 13203/40000, Loss: 1.9957613403676078e-05, Learning Rate: 0.000410\n",
      "Epoch 13204/40000, Loss: 5.331792635843158e-05, Learning Rate: 0.000410\n",
      "Epoch 13205/40000, Loss: 4.192117557977326e-05, Learning Rate: 0.000410\n",
      "Epoch 13206/40000, Loss: 4.782715404871851e-05, Learning Rate: 0.000410\n",
      "Epoch 13207/40000, Loss: 7.082356751197949e-05, Learning Rate: 0.000410\n",
      "Epoch 13208/40000, Loss: 8.27342810225673e-05, Learning Rate: 0.000410\n",
      "Epoch 13209/40000, Loss: 4.207729216432199e-05, Learning Rate: 0.000410\n",
      "Epoch 13210/40000, Loss: 8.448061271337792e-05, Learning Rate: 0.000410\n",
      "Epoch 13211/40000, Loss: 6.257680070120841e-05, Learning Rate: 0.000410\n",
      "Epoch 13212/40000, Loss: 6.275668420130387e-05, Learning Rate: 0.000410\n",
      "Epoch 13213/40000, Loss: 6.249853322515264e-05, Learning Rate: 0.000410\n",
      "Epoch 13214/40000, Loss: 7.441359775839373e-05, Learning Rate: 0.000410\n",
      "Epoch 13215/40000, Loss: 1.835285911511164e-05, Learning Rate: 0.000410\n",
      "Epoch 13216/40000, Loss: 1.7454645785619505e-05, Learning Rate: 0.000410\n",
      "Epoch 13217/40000, Loss: 7.392519182758406e-05, Learning Rate: 0.000409\n",
      "Epoch 13218/40000, Loss: 4.686389729613438e-05, Learning Rate: 0.000409\n",
      "Epoch 13219/40000, Loss: 3.936507346224971e-05, Learning Rate: 0.000409\n",
      "Epoch 13220/40000, Loss: 6.165963714011014e-05, Learning Rate: 0.000409\n",
      "Epoch 13221/40000, Loss: 6.258632492972538e-05, Learning Rate: 0.000409\n",
      "Epoch 13222/40000, Loss: 6.722088437527418e-05, Learning Rate: 0.000409\n",
      "Epoch 13223/40000, Loss: 1.7820002540247515e-05, Learning Rate: 0.000409\n",
      "Epoch 13224/40000, Loss: 3.9649487007409334e-05, Learning Rate: 0.000409\n",
      "Epoch 13225/40000, Loss: 4.555686246021651e-05, Learning Rate: 0.000409\n",
      "Epoch 13226/40000, Loss: 4.5331398723647e-05, Learning Rate: 0.000409\n",
      "Epoch 13227/40000, Loss: 4.490982973948121e-05, Learning Rate: 0.000409\n",
      "Epoch 13228/40000, Loss: 4.481643554754555e-05, Learning Rate: 0.000409\n",
      "Epoch 13229/40000, Loss: 3.9240265323314816e-05, Learning Rate: 0.000409\n",
      "Epoch 13230/40000, Loss: 7.353674300247803e-05, Learning Rate: 0.000409\n",
      "Epoch 13231/40000, Loss: 1.805996726034209e-05, Learning Rate: 0.000409\n",
      "Epoch 13232/40000, Loss: 7.384634227491915e-05, Learning Rate: 0.000409\n",
      "Epoch 13233/40000, Loss: 7.334936526603997e-05, Learning Rate: 0.000409\n",
      "Epoch 13234/40000, Loss: 4.550551966531202e-05, Learning Rate: 0.000409\n",
      "Epoch 13235/40000, Loss: 3.836557880276814e-05, Learning Rate: 0.000409\n",
      "Epoch 13236/40000, Loss: 6.136589945526794e-05, Learning Rate: 0.000409\n",
      "Epoch 13237/40000, Loss: 7.245988672366366e-05, Learning Rate: 0.000408\n",
      "Epoch 13238/40000, Loss: 6.147395470179617e-05, Learning Rate: 0.000408\n",
      "Epoch 13239/40000, Loss: 3.825770181720145e-05, Learning Rate: 0.000408\n",
      "Epoch 13240/40000, Loss: 4.4383570639183745e-05, Learning Rate: 0.000408\n",
      "Epoch 13241/40000, Loss: 4.4549065933097154e-05, Learning Rate: 0.000408\n",
      "Epoch 13242/40000, Loss: 1.7580636267666705e-05, Learning Rate: 0.000408\n",
      "Epoch 13243/40000, Loss: 7.368695514742285e-05, Learning Rate: 0.000408\n",
      "Epoch 13244/40000, Loss: 6.10692732152529e-05, Learning Rate: 0.000408\n",
      "Epoch 13245/40000, Loss: 7.345763151533902e-05, Learning Rate: 0.000408\n",
      "Epoch 13246/40000, Loss: 7.315888069570065e-05, Learning Rate: 0.000408\n",
      "Epoch 13247/40000, Loss: 7.315760740311816e-05, Learning Rate: 0.000408\n",
      "Epoch 13248/40000, Loss: 4.439469921635464e-05, Learning Rate: 0.000408\n",
      "Epoch 13249/40000, Loss: 6.0873284382978454e-05, Learning Rate: 0.000408\n",
      "Epoch 13250/40000, Loss: 6.07167930866126e-05, Learning Rate: 0.000408\n",
      "Epoch 13251/40000, Loss: 4.42954660684336e-05, Learning Rate: 0.000408\n",
      "Epoch 13252/40000, Loss: 4.431391425896436e-05, Learning Rate: 0.000408\n",
      "Epoch 13253/40000, Loss: 6.084222695790231e-05, Learning Rate: 0.000408\n",
      "Epoch 13254/40000, Loss: 3.792677307501435e-05, Learning Rate: 0.000408\n",
      "Epoch 13255/40000, Loss: 6.080259845475666e-05, Learning Rate: 0.000408\n",
      "Epoch 13256/40000, Loss: 6.063640466891229e-05, Learning Rate: 0.000408\n",
      "Epoch 13257/40000, Loss: 3.8107289583422244e-05, Learning Rate: 0.000408\n",
      "Epoch 13258/40000, Loss: 4.416258161654696e-05, Learning Rate: 0.000407\n",
      "Epoch 13259/40000, Loss: 4.441497003426775e-05, Learning Rate: 0.000407\n",
      "Epoch 13260/40000, Loss: 4.450034975889139e-05, Learning Rate: 0.000407\n",
      "Epoch 13261/40000, Loss: 3.8084235711721703e-05, Learning Rate: 0.000407\n",
      "Epoch 13262/40000, Loss: 6.244648102438077e-05, Learning Rate: 0.000407\n",
      "Epoch 13263/40000, Loss: 4.415113289724104e-05, Learning Rate: 0.000407\n",
      "Epoch 13264/40000, Loss: 1.735910336719826e-05, Learning Rate: 0.000407\n",
      "Epoch 13265/40000, Loss: 7.344092591665685e-05, Learning Rate: 0.000407\n",
      "Epoch 13266/40000, Loss: 3.831540379906073e-05, Learning Rate: 0.000407\n",
      "Epoch 13267/40000, Loss: 6.0923368437215686e-05, Learning Rate: 0.000407\n",
      "Epoch 13268/40000, Loss: 1.7188584024552256e-05, Learning Rate: 0.000407\n",
      "Epoch 13269/40000, Loss: 3.844550155918114e-05, Learning Rate: 0.000407\n",
      "Epoch 13270/40000, Loss: 7.38507296773605e-05, Learning Rate: 0.000407\n",
      "Epoch 13271/40000, Loss: 4.494048698688857e-05, Learning Rate: 0.000407\n",
      "Epoch 13272/40000, Loss: 1.7426144040655345e-05, Learning Rate: 0.000407\n",
      "Epoch 13273/40000, Loss: 6.185673555592075e-05, Learning Rate: 0.000407\n",
      "Epoch 13274/40000, Loss: 7.606244616908953e-05, Learning Rate: 0.000407\n",
      "Epoch 13275/40000, Loss: 6.206276157172397e-05, Learning Rate: 0.000407\n",
      "Epoch 13276/40000, Loss: 1.8716222257353365e-05, Learning Rate: 0.000407\n",
      "Epoch 13277/40000, Loss: 6.72248424962163e-05, Learning Rate: 0.000407\n",
      "Epoch 13278/40000, Loss: 7.391811959678307e-05, Learning Rate: 0.000406\n",
      "Epoch 13279/40000, Loss: 3.9900605770526454e-05, Learning Rate: 0.000406\n",
      "Epoch 13280/40000, Loss: 1.869439256552141e-05, Learning Rate: 0.000406\n",
      "Epoch 13281/40000, Loss: 7.554683543276042e-05, Learning Rate: 0.000406\n",
      "Epoch 13282/40000, Loss: 4.616035948856734e-05, Learning Rate: 0.000406\n",
      "Epoch 13283/40000, Loss: 4.658635953092016e-05, Learning Rate: 0.000406\n",
      "Epoch 13284/40000, Loss: 1.8298509530723095e-05, Learning Rate: 0.000406\n",
      "Epoch 13285/40000, Loss: 4.780291783390567e-05, Learning Rate: 0.000406\n",
      "Epoch 13286/40000, Loss: 7.956603803904727e-05, Learning Rate: 0.000406\n",
      "Epoch 13287/40000, Loss: 2.5209707018802874e-05, Learning Rate: 0.000406\n",
      "Epoch 13288/40000, Loss: 2.7749245418817736e-05, Learning Rate: 0.000406\n",
      "Epoch 13289/40000, Loss: 5.687253360520117e-05, Learning Rate: 0.000406\n",
      "Epoch 13290/40000, Loss: 4.999691736884415e-05, Learning Rate: 0.000406\n",
      "Epoch 13291/40000, Loss: 8.504289871780202e-05, Learning Rate: 0.000406\n",
      "Epoch 13292/40000, Loss: 5.613734538201243e-05, Learning Rate: 0.000406\n",
      "Epoch 13293/40000, Loss: 7.631260814378038e-05, Learning Rate: 0.000406\n",
      "Epoch 13294/40000, Loss: 4.1601928387535736e-05, Learning Rate: 0.000406\n",
      "Epoch 13295/40000, Loss: 7.916294998722151e-05, Learning Rate: 0.000406\n",
      "Epoch 13296/40000, Loss: 6.70956724206917e-05, Learning Rate: 0.000406\n",
      "Epoch 13297/40000, Loss: 4.982360769645311e-05, Learning Rate: 0.000406\n",
      "Epoch 13298/40000, Loss: 4.70208797196392e-05, Learning Rate: 0.000406\n",
      "Epoch 13299/40000, Loss: 2.0397665139171295e-05, Learning Rate: 0.000405\n",
      "Epoch 13300/40000, Loss: 7.77581735746935e-05, Learning Rate: 0.000405\n",
      "Epoch 13301/40000, Loss: 4.736498885904439e-05, Learning Rate: 0.000405\n",
      "Epoch 13302/40000, Loss: 4.723008896689862e-05, Learning Rate: 0.000405\n",
      "Epoch 13303/40000, Loss: 8.271162369055673e-05, Learning Rate: 0.000405\n",
      "Epoch 13304/40000, Loss: 7.853275019442663e-05, Learning Rate: 0.000405\n",
      "Epoch 13305/40000, Loss: 2.1329393348423764e-05, Learning Rate: 0.000405\n",
      "Epoch 13306/40000, Loss: 4.9797781684901565e-05, Learning Rate: 0.000405\n",
      "Epoch 13307/40000, Loss: 7.722337613813579e-05, Learning Rate: 0.000405\n",
      "Epoch 13308/40000, Loss: 4.5774286263622344e-05, Learning Rate: 0.000405\n",
      "Epoch 13309/40000, Loss: 4.6640721848234534e-05, Learning Rate: 0.000405\n",
      "Epoch 13310/40000, Loss: 6.254856998566538e-05, Learning Rate: 0.000405\n",
      "Epoch 13311/40000, Loss: 7.523157546529546e-05, Learning Rate: 0.000405\n",
      "Epoch 13312/40000, Loss: 4.539504516287707e-05, Learning Rate: 0.000405\n",
      "Epoch 13313/40000, Loss: 1.811197944334708e-05, Learning Rate: 0.000405\n",
      "Epoch 13314/40000, Loss: 1.7795669918996282e-05, Learning Rate: 0.000405\n",
      "Epoch 13315/40000, Loss: 7.655365334358066e-05, Learning Rate: 0.000405\n",
      "Epoch 13316/40000, Loss: 6.232214218471199e-05, Learning Rate: 0.000405\n",
      "Epoch 13317/40000, Loss: 7.561685924883932e-05, Learning Rate: 0.000405\n",
      "Epoch 13318/40000, Loss: 3.873742025461979e-05, Learning Rate: 0.000405\n",
      "Epoch 13319/40000, Loss: 8.722418715478852e-05, Learning Rate: 0.000404\n",
      "Epoch 13320/40000, Loss: 1.803467603167519e-05, Learning Rate: 0.000404\n",
      "Epoch 13321/40000, Loss: 1.772018185874913e-05, Learning Rate: 0.000404\n",
      "Epoch 13322/40000, Loss: 4.500988143263385e-05, Learning Rate: 0.000404\n",
      "Epoch 13323/40000, Loss: 3.825294697890058e-05, Learning Rate: 0.000404\n",
      "Epoch 13324/40000, Loss: 1.7840009604697116e-05, Learning Rate: 0.000404\n",
      "Epoch 13325/40000, Loss: 1.7582628061063588e-05, Learning Rate: 0.000404\n",
      "Epoch 13326/40000, Loss: 4.493448795983568e-05, Learning Rate: 0.000404\n",
      "Epoch 13327/40000, Loss: 4.488221020437777e-05, Learning Rate: 0.000404\n",
      "Epoch 13328/40000, Loss: 6.149491673568264e-05, Learning Rate: 0.000404\n",
      "Epoch 13329/40000, Loss: 3.829383786069229e-05, Learning Rate: 0.000404\n",
      "Epoch 13330/40000, Loss: 3.8085483538452536e-05, Learning Rate: 0.000404\n",
      "Epoch 13331/40000, Loss: 4.4701173465000466e-05, Learning Rate: 0.000404\n",
      "Epoch 13332/40000, Loss: 4.4661595893558115e-05, Learning Rate: 0.000404\n",
      "Epoch 13333/40000, Loss: 4.502681258600205e-05, Learning Rate: 0.000404\n",
      "Epoch 13334/40000, Loss: 3.8239933928707615e-05, Learning Rate: 0.000404\n",
      "Epoch 13335/40000, Loss: 6.110367394285277e-05, Learning Rate: 0.000404\n",
      "Epoch 13336/40000, Loss: 4.5156775740906596e-05, Learning Rate: 0.000404\n",
      "Epoch 13337/40000, Loss: 4.51690393674653e-05, Learning Rate: 0.000404\n",
      "Epoch 13338/40000, Loss: 4.463111326913349e-05, Learning Rate: 0.000404\n",
      "Epoch 13339/40000, Loss: 1.7532267520437017e-05, Learning Rate: 0.000404\n",
      "Epoch 13340/40000, Loss: 1.7386648323736154e-05, Learning Rate: 0.000403\n",
      "Epoch 13341/40000, Loss: 6.134324939921498e-05, Learning Rate: 0.000403\n",
      "Epoch 13342/40000, Loss: 3.89526248909533e-05, Learning Rate: 0.000403\n",
      "Epoch 13343/40000, Loss: 1.7496593500254676e-05, Learning Rate: 0.000403\n",
      "Epoch 13344/40000, Loss: 6.144145299913362e-05, Learning Rate: 0.000403\n",
      "Epoch 13345/40000, Loss: 4.495171015150845e-05, Learning Rate: 0.000403\n",
      "Epoch 13346/40000, Loss: 3.848218693747185e-05, Learning Rate: 0.000403\n",
      "Epoch 13347/40000, Loss: 4.532334060058929e-05, Learning Rate: 0.000403\n",
      "Epoch 13348/40000, Loss: 1.880900344986003e-05, Learning Rate: 0.000403\n",
      "Epoch 13349/40000, Loss: 7.369974628090858e-05, Learning Rate: 0.000403\n",
      "Epoch 13350/40000, Loss: 7.325213664444163e-05, Learning Rate: 0.000403\n",
      "Epoch 13351/40000, Loss: 1.9586121197789907e-05, Learning Rate: 0.000403\n",
      "Epoch 13352/40000, Loss: 5.5036256526364014e-05, Learning Rate: 0.000403\n",
      "Epoch 13353/40000, Loss: 7.533707685070112e-05, Learning Rate: 0.000403\n",
      "Epoch 13354/40000, Loss: 7.500372885260731e-05, Learning Rate: 0.000403\n",
      "Epoch 13355/40000, Loss: 4.752680251840502e-05, Learning Rate: 0.000403\n",
      "Epoch 13356/40000, Loss: 5.028981468058191e-05, Learning Rate: 0.000403\n",
      "Epoch 13357/40000, Loss: 4.753927350975573e-05, Learning Rate: 0.000403\n",
      "Epoch 13358/40000, Loss: 4.6679786464665085e-05, Learning Rate: 0.000403\n",
      "Epoch 13359/40000, Loss: 4.242383147357032e-05, Learning Rate: 0.000403\n",
      "Epoch 13360/40000, Loss: 4.2592266254359856e-05, Learning Rate: 0.000402\n",
      "Epoch 13361/40000, Loss: 4.095330223208293e-05, Learning Rate: 0.000402\n",
      "Epoch 13362/40000, Loss: 4.6638608182547614e-05, Learning Rate: 0.000402\n",
      "Epoch 13363/40000, Loss: 4.7594381612725556e-05, Learning Rate: 0.000402\n",
      "Epoch 13364/40000, Loss: 5.026192593504675e-05, Learning Rate: 0.000402\n",
      "Epoch 13365/40000, Loss: 4.695405004895292e-05, Learning Rate: 0.000402\n",
      "Epoch 13366/40000, Loss: 5.260117904981598e-05, Learning Rate: 0.000402\n",
      "Epoch 13367/40000, Loss: 6.825673335697502e-05, Learning Rate: 0.000402\n",
      "Epoch 13368/40000, Loss: 7.699125853832811e-05, Learning Rate: 0.000402\n",
      "Epoch 13369/40000, Loss: 4.313589306548238e-05, Learning Rate: 0.000402\n",
      "Epoch 13370/40000, Loss: 5.025669452152215e-05, Learning Rate: 0.000402\n",
      "Epoch 13371/40000, Loss: 4.923609958495945e-05, Learning Rate: 0.000402\n",
      "Epoch 13372/40000, Loss: 7.572949834866449e-05, Learning Rate: 0.000402\n",
      "Epoch 13373/40000, Loss: 4.5706197852268815e-05, Learning Rate: 0.000402\n",
      "Epoch 13374/40000, Loss: 9.974514250643551e-05, Learning Rate: 0.000402\n",
      "Epoch 13375/40000, Loss: 5.181372398510575e-05, Learning Rate: 0.000402\n",
      "Epoch 13376/40000, Loss: 7.001501944614574e-05, Learning Rate: 0.000402\n",
      "Epoch 13377/40000, Loss: 2.0340625269454904e-05, Learning Rate: 0.000402\n",
      "Epoch 13378/40000, Loss: 4.789753438672051e-05, Learning Rate: 0.000402\n",
      "Epoch 13379/40000, Loss: 6.359622784657404e-05, Learning Rate: 0.000402\n",
      "Epoch 13380/40000, Loss: 7.587477739434689e-05, Learning Rate: 0.000402\n",
      "Epoch 13381/40000, Loss: 6.471245433203876e-05, Learning Rate: 0.000401\n",
      "Epoch 13382/40000, Loss: 4.808685480384156e-05, Learning Rate: 0.000401\n",
      "Epoch 13383/40000, Loss: 6.375499651767313e-05, Learning Rate: 0.000401\n",
      "Epoch 13384/40000, Loss: 3.884596299030818e-05, Learning Rate: 0.000401\n",
      "Epoch 13385/40000, Loss: 1.7766984456102364e-05, Learning Rate: 0.000401\n",
      "Epoch 13386/40000, Loss: 4.5434488129103556e-05, Learning Rate: 0.000401\n",
      "Epoch 13387/40000, Loss: 6.358383689075708e-05, Learning Rate: 0.000401\n",
      "Epoch 13388/40000, Loss: 4.609807001543231e-05, Learning Rate: 0.000401\n",
      "Epoch 13389/40000, Loss: 5.05069583596196e-05, Learning Rate: 0.000401\n",
      "Epoch 13390/40000, Loss: 7.091327279340476e-05, Learning Rate: 0.000401\n",
      "Epoch 13391/40000, Loss: 2.505690645193681e-05, Learning Rate: 0.000401\n",
      "Epoch 13392/40000, Loss: 0.0001059170244843699, Learning Rate: 0.000401\n",
      "Epoch 13393/40000, Loss: 8.041632827371359e-05, Learning Rate: 0.000401\n",
      "Epoch 13394/40000, Loss: 7.732378435321152e-05, Learning Rate: 0.000401\n",
      "Epoch 13395/40000, Loss: 7.146329153329134e-05, Learning Rate: 0.000401\n",
      "Epoch 13396/40000, Loss: 2.6769059331854805e-05, Learning Rate: 0.000401\n",
      "Epoch 13397/40000, Loss: 8.449531742371619e-05, Learning Rate: 0.000401\n",
      "Epoch 13398/40000, Loss: 5.195374978939071e-05, Learning Rate: 0.000401\n",
      "Epoch 13399/40000, Loss: 3.1134390155784786e-05, Learning Rate: 0.000401\n",
      "Epoch 13400/40000, Loss: 4.6425302571151406e-05, Learning Rate: 0.000401\n",
      "Epoch 13401/40000, Loss: 5.259355020825751e-05, Learning Rate: 0.000401\n",
      "Epoch 13402/40000, Loss: 6.95892667863518e-05, Learning Rate: 0.000400\n",
      "Epoch 13403/40000, Loss: 5.071545820101164e-05, Learning Rate: 0.000400\n",
      "Epoch 13404/40000, Loss: 4.821656330022961e-05, Learning Rate: 0.000400\n",
      "Epoch 13405/40000, Loss: 2.1922000087215565e-05, Learning Rate: 0.000400\n",
      "Epoch 13406/40000, Loss: 2.004805537580978e-05, Learning Rate: 0.000400\n",
      "Epoch 13407/40000, Loss: 6.702891550958157e-05, Learning Rate: 0.000400\n",
      "Epoch 13408/40000, Loss: 6.476070848293602e-05, Learning Rate: 0.000400\n",
      "Epoch 13409/40000, Loss: 5.0699694838840514e-05, Learning Rate: 0.000400\n",
      "Epoch 13410/40000, Loss: 3.997786916443147e-05, Learning Rate: 0.000400\n",
      "Epoch 13411/40000, Loss: 6.605911767110229e-05, Learning Rate: 0.000400\n",
      "Epoch 13412/40000, Loss: 4.9234226025873795e-05, Learning Rate: 0.000400\n",
      "Epoch 13413/40000, Loss: 2.0410734578035772e-05, Learning Rate: 0.000400\n",
      "Epoch 13414/40000, Loss: 7.713321974733844e-05, Learning Rate: 0.000400\n",
      "Epoch 13415/40000, Loss: 6.632282747887075e-05, Learning Rate: 0.000400\n",
      "Epoch 13416/40000, Loss: 6.550538091687486e-05, Learning Rate: 0.000400\n",
      "Epoch 13417/40000, Loss: 4.722164885606617e-05, Learning Rate: 0.000400\n",
      "Epoch 13418/40000, Loss: 4.0921564504969865e-05, Learning Rate: 0.000400\n",
      "Epoch 13419/40000, Loss: 8.90012743184343e-05, Learning Rate: 0.000400\n",
      "Epoch 13420/40000, Loss: 7.03327968949452e-05, Learning Rate: 0.000400\n",
      "Epoch 13421/40000, Loss: 6.27983536105603e-05, Learning Rate: 0.000400\n",
      "Epoch 13422/40000, Loss: 5.7258719607489184e-05, Learning Rate: 0.000400\n",
      "Epoch 13423/40000, Loss: 2.870737444027327e-05, Learning Rate: 0.000399\n",
      "Epoch 13424/40000, Loss: 7.457692117895931e-05, Learning Rate: 0.000399\n",
      "Epoch 13425/40000, Loss: 2.6853642339119688e-05, Learning Rate: 0.000399\n",
      "Epoch 13426/40000, Loss: 5.6941596994875e-05, Learning Rate: 0.000399\n",
      "Epoch 13427/40000, Loss: 5.0563547119963914e-05, Learning Rate: 0.000399\n",
      "Epoch 13428/40000, Loss: 7.855897274566814e-05, Learning Rate: 0.000399\n",
      "Epoch 13429/40000, Loss: 7.520725921494886e-05, Learning Rate: 0.000399\n",
      "Epoch 13430/40000, Loss: 5.6834025599528104e-05, Learning Rate: 0.000399\n",
      "Epoch 13431/40000, Loss: 2.5218150767614134e-05, Learning Rate: 0.000399\n",
      "Epoch 13432/40000, Loss: 4.4621578126680106e-05, Learning Rate: 0.000399\n",
      "Epoch 13433/40000, Loss: 7.977343921083957e-05, Learning Rate: 0.000399\n",
      "Epoch 13434/40000, Loss: 6.771798507543281e-05, Learning Rate: 0.000399\n",
      "Epoch 13435/40000, Loss: 4.4617863750318065e-05, Learning Rate: 0.000399\n",
      "Epoch 13436/40000, Loss: 9.8073658591602e-05, Learning Rate: 0.000399\n",
      "Epoch 13437/40000, Loss: 4.1597890231059864e-05, Learning Rate: 0.000399\n",
      "Epoch 13438/40000, Loss: 9.428922203369439e-05, Learning Rate: 0.000399\n",
      "Epoch 13439/40000, Loss: 5.279678589431569e-05, Learning Rate: 0.000399\n",
      "Epoch 13440/40000, Loss: 6.598061736440286e-05, Learning Rate: 0.000399\n",
      "Epoch 13441/40000, Loss: 2.1090807422297075e-05, Learning Rate: 0.000399\n",
      "Epoch 13442/40000, Loss: 6.415003008442e-05, Learning Rate: 0.000399\n",
      "Epoch 13443/40000, Loss: 4.237306711729616e-05, Learning Rate: 0.000399\n",
      "Epoch 13444/40000, Loss: 4.6954308345448226e-05, Learning Rate: 0.000398\n",
      "Epoch 13445/40000, Loss: 4.6908051444916055e-05, Learning Rate: 0.000398\n",
      "Epoch 13446/40000, Loss: 4.487095065996982e-05, Learning Rate: 0.000398\n",
      "Epoch 13447/40000, Loss: 6.468594074249268e-05, Learning Rate: 0.000398\n",
      "Epoch 13448/40000, Loss: 4.494667882681824e-05, Learning Rate: 0.000398\n",
      "Epoch 13449/40000, Loss: 6.508974911412224e-05, Learning Rate: 0.000398\n",
      "Epoch 13450/40000, Loss: 3.878238203469664e-05, Learning Rate: 0.000398\n",
      "Epoch 13451/40000, Loss: 1.9130236978526227e-05, Learning Rate: 0.000398\n",
      "Epoch 13452/40000, Loss: 6.116944859968498e-05, Learning Rate: 0.000398\n",
      "Epoch 13453/40000, Loss: 4.546495620161295e-05, Learning Rate: 0.000398\n",
      "Epoch 13454/40000, Loss: 7.324049511225894e-05, Learning Rate: 0.000398\n",
      "Epoch 13455/40000, Loss: 6.358444079523906e-05, Learning Rate: 0.000398\n",
      "Epoch 13456/40000, Loss: 7.375412678811699e-05, Learning Rate: 0.000398\n",
      "Epoch 13457/40000, Loss: 1.898869959404692e-05, Learning Rate: 0.000398\n",
      "Epoch 13458/40000, Loss: 6.509642844321206e-05, Learning Rate: 0.000398\n",
      "Epoch 13459/40000, Loss: 7.337330316659063e-05, Learning Rate: 0.000398\n",
      "Epoch 13460/40000, Loss: 4.518893183558248e-05, Learning Rate: 0.000398\n",
      "Epoch 13461/40000, Loss: 5.383464667829685e-05, Learning Rate: 0.000398\n",
      "Epoch 13462/40000, Loss: 4.7028199332999066e-05, Learning Rate: 0.000398\n",
      "Epoch 13463/40000, Loss: 4.664957305067219e-05, Learning Rate: 0.000398\n",
      "Epoch 13464/40000, Loss: 4.0096969314618036e-05, Learning Rate: 0.000398\n",
      "Epoch 13465/40000, Loss: 5.4564090532949194e-05, Learning Rate: 0.000397\n",
      "Epoch 13466/40000, Loss: 6.861147994641215e-05, Learning Rate: 0.000397\n",
      "Epoch 13467/40000, Loss: 2.2932556021260098e-05, Learning Rate: 0.000397\n",
      "Epoch 13468/40000, Loss: 6.92773173796013e-05, Learning Rate: 0.000397\n",
      "Epoch 13469/40000, Loss: 6.335003854474053e-05, Learning Rate: 0.000397\n",
      "Epoch 13470/40000, Loss: 2.1443282093969174e-05, Learning Rate: 0.000397\n",
      "Epoch 13471/40000, Loss: 1.9233932107454166e-05, Learning Rate: 0.000397\n",
      "Epoch 13472/40000, Loss: 6.33938325336203e-05, Learning Rate: 0.000397\n",
      "Epoch 13473/40000, Loss: 7.592852489324287e-05, Learning Rate: 0.000397\n",
      "Epoch 13474/40000, Loss: 6.362268322845921e-05, Learning Rate: 0.000397\n",
      "Epoch 13475/40000, Loss: 2.0317760572652332e-05, Learning Rate: 0.000397\n",
      "Epoch 13476/40000, Loss: 7.447147072525695e-05, Learning Rate: 0.000397\n",
      "Epoch 13477/40000, Loss: 2.2010450265952386e-05, Learning Rate: 0.000397\n",
      "Epoch 13478/40000, Loss: 8.537991379853338e-05, Learning Rate: 0.000397\n",
      "Epoch 13479/40000, Loss: 5.0003167416434735e-05, Learning Rate: 0.000397\n",
      "Epoch 13480/40000, Loss: 4.733183959615417e-05, Learning Rate: 0.000397\n",
      "Epoch 13481/40000, Loss: 2.3337750462815166e-05, Learning Rate: 0.000397\n",
      "Epoch 13482/40000, Loss: 7.910840213298798e-05, Learning Rate: 0.000397\n",
      "Epoch 13483/40000, Loss: 6.566425872733817e-05, Learning Rate: 0.000397\n",
      "Epoch 13484/40000, Loss: 2.0478457372519188e-05, Learning Rate: 0.000397\n",
      "Epoch 13485/40000, Loss: 5.9444617363624275e-05, Learning Rate: 0.000397\n",
      "Epoch 13486/40000, Loss: 3.999954424216412e-05, Learning Rate: 0.000396\n",
      "Epoch 13487/40000, Loss: 6.934686825843528e-05, Learning Rate: 0.000396\n",
      "Epoch 13488/40000, Loss: 4.695905226981267e-05, Learning Rate: 0.000396\n",
      "Epoch 13489/40000, Loss: 8.181583689292893e-05, Learning Rate: 0.000396\n",
      "Epoch 13490/40000, Loss: 2.1550029487116262e-05, Learning Rate: 0.000396\n",
      "Epoch 13491/40000, Loss: 7.756300328765064e-05, Learning Rate: 0.000396\n",
      "Epoch 13492/40000, Loss: 7.365117926383391e-05, Learning Rate: 0.000396\n",
      "Epoch 13493/40000, Loss: 4.777041613124311e-05, Learning Rate: 0.000396\n",
      "Epoch 13494/40000, Loss: 4.664854350266978e-05, Learning Rate: 0.000396\n",
      "Epoch 13495/40000, Loss: 4.3404852476669475e-05, Learning Rate: 0.000396\n",
      "Epoch 13496/40000, Loss: 4.810478276340291e-05, Learning Rate: 0.000396\n",
      "Epoch 13497/40000, Loss: 1.905473618535325e-05, Learning Rate: 0.000396\n",
      "Epoch 13498/40000, Loss: 4.017478568130173e-05, Learning Rate: 0.000396\n",
      "Epoch 13499/40000, Loss: 8.027430158108473e-05, Learning Rate: 0.000396\n",
      "Epoch 13500/40000, Loss: 2.3425107428920455e-05, Learning Rate: 0.000396\n",
      "Epoch 13501/40000, Loss: 4.8019337555160746e-05, Learning Rate: 0.000396\n",
      "Epoch 13502/40000, Loss: 3.4613385651027784e-05, Learning Rate: 0.000396\n",
      "Epoch 13503/40000, Loss: 6.49249559501186e-05, Learning Rate: 0.000396\n",
      "Epoch 13504/40000, Loss: 4.1561539546819404e-05, Learning Rate: 0.000396\n",
      "Epoch 13505/40000, Loss: 2.8544503948069178e-05, Learning Rate: 0.000396\n",
      "Epoch 13506/40000, Loss: 6.863796443212777e-05, Learning Rate: 0.000396\n",
      "Epoch 13507/40000, Loss: 7.592383190058172e-05, Learning Rate: 0.000395\n",
      "Epoch 13508/40000, Loss: 4.8623573093209416e-05, Learning Rate: 0.000395\n",
      "Epoch 13509/40000, Loss: 7.447927055181935e-05, Learning Rate: 0.000395\n",
      "Epoch 13510/40000, Loss: 6.520074384752661e-05, Learning Rate: 0.000395\n",
      "Epoch 13511/40000, Loss: 7.91260099504143e-05, Learning Rate: 0.000395\n",
      "Epoch 13512/40000, Loss: 4.8398662329418585e-05, Learning Rate: 0.000395\n",
      "Epoch 13513/40000, Loss: 7.934889436000958e-05, Learning Rate: 0.000395\n",
      "Epoch 13514/40000, Loss: 4.019953848910518e-05, Learning Rate: 0.000395\n",
      "Epoch 13515/40000, Loss: 4.979248842573725e-05, Learning Rate: 0.000395\n",
      "Epoch 13516/40000, Loss: 1.9637433069874533e-05, Learning Rate: 0.000395\n",
      "Epoch 13517/40000, Loss: 5.086767123430036e-05, Learning Rate: 0.000395\n",
      "Epoch 13518/40000, Loss: 7.478541374439374e-05, Learning Rate: 0.000395\n",
      "Epoch 13519/40000, Loss: 4.66451674583368e-05, Learning Rate: 0.000395\n",
      "Epoch 13520/40000, Loss: 4.5587894419441e-05, Learning Rate: 0.000395\n",
      "Epoch 13521/40000, Loss: 7.309659849852324e-05, Learning Rate: 0.000395\n",
      "Epoch 13522/40000, Loss: 4.473885564948432e-05, Learning Rate: 0.000395\n",
      "Epoch 13523/40000, Loss: 6.078988371882588e-05, Learning Rate: 0.000395\n",
      "Epoch 13524/40000, Loss: 7.287191692739725e-05, Learning Rate: 0.000395\n",
      "Epoch 13525/40000, Loss: 1.7331878552795388e-05, Learning Rate: 0.000395\n",
      "Epoch 13526/40000, Loss: 6.0817605117335916e-05, Learning Rate: 0.000395\n",
      "Epoch 13527/40000, Loss: 7.36155197955668e-05, Learning Rate: 0.000395\n",
      "Epoch 13528/40000, Loss: 6.245464464882389e-05, Learning Rate: 0.000394\n",
      "Epoch 13529/40000, Loss: 6.114374264143407e-05, Learning Rate: 0.000394\n",
      "Epoch 13530/40000, Loss: 4.4241729483474046e-05, Learning Rate: 0.000394\n",
      "Epoch 13531/40000, Loss: 6.262176611926407e-05, Learning Rate: 0.000394\n",
      "Epoch 13532/40000, Loss: 4.495909161050804e-05, Learning Rate: 0.000394\n",
      "Epoch 13533/40000, Loss: 7.388042286038399e-05, Learning Rate: 0.000394\n",
      "Epoch 13534/40000, Loss: 3.9084690797608346e-05, Learning Rate: 0.000394\n",
      "Epoch 13535/40000, Loss: 3.829315392067656e-05, Learning Rate: 0.000394\n",
      "Epoch 13536/40000, Loss: 4.521077062236145e-05, Learning Rate: 0.000394\n",
      "Epoch 13537/40000, Loss: 7.415120489895344e-05, Learning Rate: 0.000394\n",
      "Epoch 13538/40000, Loss: 7.37651571398601e-05, Learning Rate: 0.000394\n",
      "Epoch 13539/40000, Loss: 6.343489076243713e-05, Learning Rate: 0.000394\n",
      "Epoch 13540/40000, Loss: 7.389982783934101e-05, Learning Rate: 0.000394\n",
      "Epoch 13541/40000, Loss: 4.6239656512625515e-05, Learning Rate: 0.000394\n",
      "Epoch 13542/40000, Loss: 1.9380635421839543e-05, Learning Rate: 0.000394\n",
      "Epoch 13543/40000, Loss: 4.615879515768029e-05, Learning Rate: 0.000394\n",
      "Epoch 13544/40000, Loss: 3.955257125198841e-05, Learning Rate: 0.000394\n",
      "Epoch 13545/40000, Loss: 6.356221274472773e-05, Learning Rate: 0.000394\n",
      "Epoch 13546/40000, Loss: 4.789315062225796e-05, Learning Rate: 0.000394\n",
      "Epoch 13547/40000, Loss: 3.991811900050379e-05, Learning Rate: 0.000394\n",
      "Epoch 13548/40000, Loss: 7.405550422845408e-05, Learning Rate: 0.000394\n",
      "Epoch 13549/40000, Loss: 4.928856287733652e-05, Learning Rate: 0.000393\n",
      "Epoch 13550/40000, Loss: 6.365631270455196e-05, Learning Rate: 0.000393\n",
      "Epoch 13551/40000, Loss: 4.161046308581717e-05, Learning Rate: 0.000393\n",
      "Epoch 13552/40000, Loss: 4.684606392402202e-05, Learning Rate: 0.000393\n",
      "Epoch 13553/40000, Loss: 1.841979792516213e-05, Learning Rate: 0.000393\n",
      "Epoch 13554/40000, Loss: 6.94353730068542e-05, Learning Rate: 0.000393\n",
      "Epoch 13555/40000, Loss: 6.28977213636972e-05, Learning Rate: 0.000393\n",
      "Epoch 13556/40000, Loss: 6.300624227151275e-05, Learning Rate: 0.000393\n",
      "Epoch 13557/40000, Loss: 4.709222412202507e-05, Learning Rate: 0.000393\n",
      "Epoch 13558/40000, Loss: 1.9163970137014985e-05, Learning Rate: 0.000393\n",
      "Epoch 13559/40000, Loss: 6.424644379876554e-05, Learning Rate: 0.000393\n",
      "Epoch 13560/40000, Loss: 8.263193012680858e-05, Learning Rate: 0.000393\n",
      "Epoch 13561/40000, Loss: 4.6245550038293004e-05, Learning Rate: 0.000393\n",
      "Epoch 13562/40000, Loss: 0.00010371539246989414, Learning Rate: 0.000393\n",
      "Epoch 13563/40000, Loss: 4.7069592255866155e-05, Learning Rate: 0.000393\n",
      "Epoch 13564/40000, Loss: 6.406476313713938e-05, Learning Rate: 0.000393\n",
      "Epoch 13565/40000, Loss: 1.856230119301472e-05, Learning Rate: 0.000393\n",
      "Epoch 13566/40000, Loss: 1.811776019167155e-05, Learning Rate: 0.000393\n",
      "Epoch 13567/40000, Loss: 7.681239367229864e-05, Learning Rate: 0.000393\n",
      "Epoch 13568/40000, Loss: 1.781213541107718e-05, Learning Rate: 0.000393\n",
      "Epoch 13569/40000, Loss: 8.553009683964774e-05, Learning Rate: 0.000393\n",
      "Epoch 13570/40000, Loss: 4.565374183584936e-05, Learning Rate: 0.000392\n",
      "Epoch 13571/40000, Loss: 4.1787563532125205e-05, Learning Rate: 0.000392\n",
      "Epoch 13572/40000, Loss: 1.8834896764019504e-05, Learning Rate: 0.000392\n",
      "Epoch 13573/40000, Loss: 4.091793016414158e-05, Learning Rate: 0.000392\n",
      "Epoch 13574/40000, Loss: 1.847146086220164e-05, Learning Rate: 0.000392\n",
      "Epoch 13575/40000, Loss: 4.4588694436242804e-05, Learning Rate: 0.000392\n",
      "Epoch 13576/40000, Loss: 4.5753193262498826e-05, Learning Rate: 0.000392\n",
      "Epoch 13577/40000, Loss: 6.138533353805542e-05, Learning Rate: 0.000392\n",
      "Epoch 13578/40000, Loss: 4.4514290493680164e-05, Learning Rate: 0.000392\n",
      "Epoch 13579/40000, Loss: 7.354420813499019e-05, Learning Rate: 0.000392\n",
      "Epoch 13580/40000, Loss: 4.5440483518177643e-05, Learning Rate: 0.000392\n",
      "Epoch 13581/40000, Loss: 1.796280594135169e-05, Learning Rate: 0.000392\n",
      "Epoch 13582/40000, Loss: 4.556695785140619e-05, Learning Rate: 0.000392\n",
      "Epoch 13583/40000, Loss: 4.456004171515815e-05, Learning Rate: 0.000392\n",
      "Epoch 13584/40000, Loss: 4.486266334424727e-05, Learning Rate: 0.000392\n",
      "Epoch 13585/40000, Loss: 7.307355554075912e-05, Learning Rate: 0.000392\n",
      "Epoch 13586/40000, Loss: 6.320359534583986e-05, Learning Rate: 0.000392\n",
      "Epoch 13587/40000, Loss: 4.779627488460392e-05, Learning Rate: 0.000392\n",
      "Epoch 13588/40000, Loss: 7.560431549791247e-05, Learning Rate: 0.000392\n",
      "Epoch 13589/40000, Loss: 2.068441972369328e-05, Learning Rate: 0.000392\n",
      "Epoch 13590/40000, Loss: 4.696792893810198e-05, Learning Rate: 0.000392\n",
      "Epoch 13591/40000, Loss: 3.9124621252994984e-05, Learning Rate: 0.000391\n",
      "Epoch 13592/40000, Loss: 6.447359919548035e-05, Learning Rate: 0.000391\n",
      "Epoch 13593/40000, Loss: 7.535688928328454e-05, Learning Rate: 0.000391\n",
      "Epoch 13594/40000, Loss: 7.499639468733221e-05, Learning Rate: 0.000391\n",
      "Epoch 13595/40000, Loss: 1.9190429156878963e-05, Learning Rate: 0.000391\n",
      "Epoch 13596/40000, Loss: 7.507655391236767e-05, Learning Rate: 0.000391\n",
      "Epoch 13597/40000, Loss: 4.015748709207401e-05, Learning Rate: 0.000391\n",
      "Epoch 13598/40000, Loss: 4.567749783745967e-05, Learning Rate: 0.000391\n",
      "Epoch 13599/40000, Loss: 4.0537506720284e-05, Learning Rate: 0.000391\n",
      "Epoch 13600/40000, Loss: 4.492301377467811e-05, Learning Rate: 0.000391\n",
      "Epoch 13601/40000, Loss: 7.520456711063161e-05, Learning Rate: 0.000391\n",
      "Epoch 13602/40000, Loss: 7.649099279660732e-05, Learning Rate: 0.000391\n",
      "Epoch 13603/40000, Loss: 1.839502692746464e-05, Learning Rate: 0.000391\n",
      "Epoch 13604/40000, Loss: 7.397171430056915e-05, Learning Rate: 0.000391\n",
      "Epoch 13605/40000, Loss: 7.33536944608204e-05, Learning Rate: 0.000391\n",
      "Epoch 13606/40000, Loss: 7.330383232329041e-05, Learning Rate: 0.000391\n",
      "Epoch 13607/40000, Loss: 6.229874998098239e-05, Learning Rate: 0.000391\n",
      "Epoch 13608/40000, Loss: 6.281022797338665e-05, Learning Rate: 0.000391\n",
      "Epoch 13609/40000, Loss: 4.476108733797446e-05, Learning Rate: 0.000391\n",
      "Epoch 13610/40000, Loss: 3.917032154276967e-05, Learning Rate: 0.000391\n",
      "Epoch 13611/40000, Loss: 6.657367339357734e-05, Learning Rate: 0.000391\n",
      "Epoch 13612/40000, Loss: 4.6094752178760245e-05, Learning Rate: 0.000391\n",
      "Epoch 13613/40000, Loss: 4.367491783341393e-05, Learning Rate: 0.000390\n",
      "Epoch 13614/40000, Loss: 7.936160545796156e-05, Learning Rate: 0.000390\n",
      "Epoch 13615/40000, Loss: 7.361819734796882e-05, Learning Rate: 0.000390\n",
      "Epoch 13616/40000, Loss: 5.450921889860183e-05, Learning Rate: 0.000390\n",
      "Epoch 13617/40000, Loss: 2.0653995306929573e-05, Learning Rate: 0.000390\n",
      "Epoch 13618/40000, Loss: 4.797313522431068e-05, Learning Rate: 0.000390\n",
      "Epoch 13619/40000, Loss: 4.183298005955294e-05, Learning Rate: 0.000390\n",
      "Epoch 13620/40000, Loss: 2.0290284737711772e-05, Learning Rate: 0.000390\n",
      "Epoch 13621/40000, Loss: 1.9260261979070492e-05, Learning Rate: 0.000390\n",
      "Epoch 13622/40000, Loss: 4.683073711930774e-05, Learning Rate: 0.000390\n",
      "Epoch 13623/40000, Loss: 2.0204653992550448e-05, Learning Rate: 0.000390\n",
      "Epoch 13624/40000, Loss: 6.764275894965976e-05, Learning Rate: 0.000390\n",
      "Epoch 13625/40000, Loss: 1.9038991013076156e-05, Learning Rate: 0.000390\n",
      "Epoch 13626/40000, Loss: 4.8407106078229845e-05, Learning Rate: 0.000390\n",
      "Epoch 13627/40000, Loss: 6.487482460215688e-05, Learning Rate: 0.000390\n",
      "Epoch 13628/40000, Loss: 4.804078707820736e-05, Learning Rate: 0.000390\n",
      "Epoch 13629/40000, Loss: 4.6419572754530236e-05, Learning Rate: 0.000390\n",
      "Epoch 13630/40000, Loss: 4.649842230719514e-05, Learning Rate: 0.000390\n",
      "Epoch 13631/40000, Loss: 2.0179417333565652e-05, Learning Rate: 0.000390\n",
      "Epoch 13632/40000, Loss: 5.076584784546867e-05, Learning Rate: 0.000390\n",
      "Epoch 13633/40000, Loss: 5.054518624092452e-05, Learning Rate: 0.000390\n",
      "Epoch 13634/40000, Loss: 4.946319677401334e-05, Learning Rate: 0.000389\n",
      "Epoch 13635/40000, Loss: 6.673323514405638e-05, Learning Rate: 0.000389\n",
      "Epoch 13636/40000, Loss: 5.967233664705418e-05, Learning Rate: 0.000389\n",
      "Epoch 13637/40000, Loss: 4.3206120608374476e-05, Learning Rate: 0.000389\n",
      "Epoch 13638/40000, Loss: 7.337082934100181e-05, Learning Rate: 0.000389\n",
      "Epoch 13639/40000, Loss: 5.552508810069412e-05, Learning Rate: 0.000389\n",
      "Epoch 13640/40000, Loss: 8.625314512755722e-05, Learning Rate: 0.000389\n",
      "Epoch 13641/40000, Loss: 4.832624108530581e-05, Learning Rate: 0.000389\n",
      "Epoch 13642/40000, Loss: 7.731675577815622e-05, Learning Rate: 0.000389\n",
      "Epoch 13643/40000, Loss: 4.4050466385670006e-05, Learning Rate: 0.000389\n",
      "Epoch 13644/40000, Loss: 4.8734127631178126e-05, Learning Rate: 0.000389\n",
      "Epoch 13645/40000, Loss: 7.486619870178401e-05, Learning Rate: 0.000389\n",
      "Epoch 13646/40000, Loss: 1.8457814803696238e-05, Learning Rate: 0.000389\n",
      "Epoch 13647/40000, Loss: 7.361730968113989e-05, Learning Rate: 0.000389\n",
      "Epoch 13648/40000, Loss: 6.233877502381802e-05, Learning Rate: 0.000389\n",
      "Epoch 13649/40000, Loss: 7.332392851822078e-05, Learning Rate: 0.000389\n",
      "Epoch 13650/40000, Loss: 4.615812576957978e-05, Learning Rate: 0.000389\n",
      "Epoch 13651/40000, Loss: 7.48289967305027e-05, Learning Rate: 0.000389\n",
      "Epoch 13652/40000, Loss: 1.936330590979196e-05, Learning Rate: 0.000389\n",
      "Epoch 13653/40000, Loss: 8.162454469129443e-05, Learning Rate: 0.000389\n",
      "Epoch 13654/40000, Loss: 5.034468267695047e-05, Learning Rate: 0.000389\n",
      "Epoch 13655/40000, Loss: 4.936551704304293e-05, Learning Rate: 0.000388\n",
      "Epoch 13656/40000, Loss: 4.452879511518404e-05, Learning Rate: 0.000388\n",
      "Epoch 13657/40000, Loss: 5.4548127081943676e-05, Learning Rate: 0.000388\n",
      "Epoch 13658/40000, Loss: 2.3706619685981423e-05, Learning Rate: 0.000388\n",
      "Epoch 13659/40000, Loss: 4.0040140447672457e-05, Learning Rate: 0.000388\n",
      "Epoch 13660/40000, Loss: 6.996098818490282e-05, Learning Rate: 0.000388\n",
      "Epoch 13661/40000, Loss: 6.623859371757135e-05, Learning Rate: 0.000388\n",
      "Epoch 13662/40000, Loss: 1.9157074348186143e-05, Learning Rate: 0.000388\n",
      "Epoch 13663/40000, Loss: 3.92607762478292e-05, Learning Rate: 0.000388\n",
      "Epoch 13664/40000, Loss: 4.8589499783702195e-05, Learning Rate: 0.000388\n",
      "Epoch 13665/40000, Loss: 6.490688247140497e-05, Learning Rate: 0.000388\n",
      "Epoch 13666/40000, Loss: 6.328360177576542e-05, Learning Rate: 0.000388\n",
      "Epoch 13667/40000, Loss: 4.670013368013315e-05, Learning Rate: 0.000388\n",
      "Epoch 13668/40000, Loss: 7.350857049459592e-05, Learning Rate: 0.000388\n",
      "Epoch 13669/40000, Loss: 3.931492756237276e-05, Learning Rate: 0.000388\n",
      "Epoch 13670/40000, Loss: 7.22278346074745e-05, Learning Rate: 0.000388\n",
      "Epoch 13671/40000, Loss: 4.5055840018903837e-05, Learning Rate: 0.000388\n",
      "Epoch 13672/40000, Loss: 4.463017830858007e-05, Learning Rate: 0.000388\n",
      "Epoch 13673/40000, Loss: 4.459529372979887e-05, Learning Rate: 0.000388\n",
      "Epoch 13674/40000, Loss: 6.03812477493193e-05, Learning Rate: 0.000388\n",
      "Epoch 13675/40000, Loss: 4.3991611164528877e-05, Learning Rate: 0.000388\n",
      "Epoch 13676/40000, Loss: 7.194495265139267e-05, Learning Rate: 0.000388\n",
      "Epoch 13677/40000, Loss: 4.44391553173773e-05, Learning Rate: 0.000387\n",
      "Epoch 13678/40000, Loss: 3.778546306421049e-05, Learning Rate: 0.000387\n",
      "Epoch 13679/40000, Loss: 4.4643194996751845e-05, Learning Rate: 0.000387\n",
      "Epoch 13680/40000, Loss: 6.122779450379312e-05, Learning Rate: 0.000387\n",
      "Epoch 13681/40000, Loss: 4.490841820370406e-05, Learning Rate: 0.000387\n",
      "Epoch 13682/40000, Loss: 3.809484405792318e-05, Learning Rate: 0.000387\n",
      "Epoch 13683/40000, Loss: 7.303377788048238e-05, Learning Rate: 0.000387\n",
      "Epoch 13684/40000, Loss: 3.886856939061545e-05, Learning Rate: 0.000387\n",
      "Epoch 13685/40000, Loss: 4.5863311243010685e-05, Learning Rate: 0.000387\n",
      "Epoch 13686/40000, Loss: 7.740366709185764e-05, Learning Rate: 0.000387\n",
      "Epoch 13687/40000, Loss: 6.177716568345204e-05, Learning Rate: 0.000387\n",
      "Epoch 13688/40000, Loss: 1.7988350009545684e-05, Learning Rate: 0.000387\n",
      "Epoch 13689/40000, Loss: 4.6078617742750794e-05, Learning Rate: 0.000387\n",
      "Epoch 13690/40000, Loss: 7.357830327237025e-05, Learning Rate: 0.000387\n",
      "Epoch 13691/40000, Loss: 1.7710510292090476e-05, Learning Rate: 0.000387\n",
      "Epoch 13692/40000, Loss: 3.801061029662378e-05, Learning Rate: 0.000387\n",
      "Epoch 13693/40000, Loss: 3.77301184926182e-05, Learning Rate: 0.000387\n",
      "Epoch 13694/40000, Loss: 4.4673543015960604e-05, Learning Rate: 0.000387\n",
      "Epoch 13695/40000, Loss: 4.392469782033004e-05, Learning Rate: 0.000387\n",
      "Epoch 13696/40000, Loss: 4.5103726733941585e-05, Learning Rate: 0.000387\n",
      "Epoch 13697/40000, Loss: 3.9378926885547116e-05, Learning Rate: 0.000387\n",
      "Epoch 13698/40000, Loss: 6.064445187803358e-05, Learning Rate: 0.000386\n",
      "Epoch 13699/40000, Loss: 4.472868386073969e-05, Learning Rate: 0.000386\n",
      "Epoch 13700/40000, Loss: 6.541856419062242e-05, Learning Rate: 0.000386\n",
      "Epoch 13701/40000, Loss: 1.7381960788043216e-05, Learning Rate: 0.000386\n",
      "Epoch 13702/40000, Loss: 4.4153253838885576e-05, Learning Rate: 0.000386\n",
      "Epoch 13703/40000, Loss: 7.28670711396262e-05, Learning Rate: 0.000386\n",
      "Epoch 13704/40000, Loss: 7.320145232370123e-05, Learning Rate: 0.000386\n",
      "Epoch 13705/40000, Loss: 4.62523712485563e-05, Learning Rate: 0.000386\n",
      "Epoch 13706/40000, Loss: 1.834804606914986e-05, Learning Rate: 0.000386\n",
      "Epoch 13707/40000, Loss: 6.157172902021557e-05, Learning Rate: 0.000386\n",
      "Epoch 13708/40000, Loss: 2.0932626284775324e-05, Learning Rate: 0.000386\n",
      "Epoch 13709/40000, Loss: 4.724583777715452e-05, Learning Rate: 0.000386\n",
      "Epoch 13710/40000, Loss: 7.390652172034606e-05, Learning Rate: 0.000386\n",
      "Epoch 13711/40000, Loss: 4.548431752482429e-05, Learning Rate: 0.000386\n",
      "Epoch 13712/40000, Loss: 7.431942503899336e-05, Learning Rate: 0.000386\n",
      "Epoch 13713/40000, Loss: 4.791070750798099e-05, Learning Rate: 0.000386\n",
      "Epoch 13714/40000, Loss: 6.008879790897481e-05, Learning Rate: 0.000386\n",
      "Epoch 13715/40000, Loss: 6.696221680613235e-05, Learning Rate: 0.000386\n",
      "Epoch 13716/40000, Loss: 2.42809510382358e-05, Learning Rate: 0.000386\n",
      "Epoch 13717/40000, Loss: 4.01042852899991e-05, Learning Rate: 0.000386\n",
      "Epoch 13718/40000, Loss: 2.2402924514608458e-05, Learning Rate: 0.000386\n",
      "Epoch 13719/40000, Loss: 1.935448744916357e-05, Learning Rate: 0.000386\n",
      "Epoch 13720/40000, Loss: 1.817516385926865e-05, Learning Rate: 0.000385\n",
      "Epoch 13721/40000, Loss: 7.360077142948285e-05, Learning Rate: 0.000385\n",
      "Epoch 13722/40000, Loss: 4.790593447978608e-05, Learning Rate: 0.000385\n",
      "Epoch 13723/40000, Loss: 7.79768597567454e-05, Learning Rate: 0.000385\n",
      "Epoch 13724/40000, Loss: 3.8894228055141866e-05, Learning Rate: 0.000385\n",
      "Epoch 13725/40000, Loss: 1.846353552537039e-05, Learning Rate: 0.000385\n",
      "Epoch 13726/40000, Loss: 4.012392309959978e-05, Learning Rate: 0.000385\n",
      "Epoch 13727/40000, Loss: 7.581274985568598e-05, Learning Rate: 0.000385\n",
      "Epoch 13728/40000, Loss: 4.617097511072643e-05, Learning Rate: 0.000385\n",
      "Epoch 13729/40000, Loss: 4.55080225947313e-05, Learning Rate: 0.000385\n",
      "Epoch 13730/40000, Loss: 6.516537541756406e-05, Learning Rate: 0.000385\n",
      "Epoch 13731/40000, Loss: 4.560535307973623e-05, Learning Rate: 0.000385\n",
      "Epoch 13732/40000, Loss: 1.925289507198613e-05, Learning Rate: 0.000385\n",
      "Epoch 13733/40000, Loss: 6.40564103377983e-05, Learning Rate: 0.000385\n",
      "Epoch 13734/40000, Loss: 4.5302771468413994e-05, Learning Rate: 0.000385\n",
      "Epoch 13735/40000, Loss: 4.477434777072631e-05, Learning Rate: 0.000385\n",
      "Epoch 13736/40000, Loss: 2.0233435861882754e-05, Learning Rate: 0.000385\n",
      "Epoch 13737/40000, Loss: 2.0779079932253808e-05, Learning Rate: 0.000385\n",
      "Epoch 13738/40000, Loss: 4.725609323941171e-05, Learning Rate: 0.000385\n",
      "Epoch 13739/40000, Loss: 3.9747545088175684e-05, Learning Rate: 0.000385\n",
      "Epoch 13740/40000, Loss: 3.9244543586391956e-05, Learning Rate: 0.000385\n",
      "Epoch 13741/40000, Loss: 4.689284469350241e-05, Learning Rate: 0.000385\n",
      "Epoch 13742/40000, Loss: 4.734907633974217e-05, Learning Rate: 0.000384\n",
      "Epoch 13743/40000, Loss: 4.526536213234067e-05, Learning Rate: 0.000384\n",
      "Epoch 13744/40000, Loss: 3.921258030459285e-05, Learning Rate: 0.000384\n",
      "Epoch 13745/40000, Loss: 4.689476190833375e-05, Learning Rate: 0.000384\n",
      "Epoch 13746/40000, Loss: 6.379029946401715e-05, Learning Rate: 0.000384\n",
      "Epoch 13747/40000, Loss: 3.821245627477765e-05, Learning Rate: 0.000384\n",
      "Epoch 13748/40000, Loss: 4.683934457716532e-05, Learning Rate: 0.000384\n",
      "Epoch 13749/40000, Loss: 6.343921268125996e-05, Learning Rate: 0.000384\n",
      "Epoch 13750/40000, Loss: 4.6473189286189154e-05, Learning Rate: 0.000384\n",
      "Epoch 13751/40000, Loss: 4.471618740353733e-05, Learning Rate: 0.000384\n",
      "Epoch 13752/40000, Loss: 4.885281668975949e-05, Learning Rate: 0.000384\n",
      "Epoch 13753/40000, Loss: 2.0361079805297777e-05, Learning Rate: 0.000384\n",
      "Epoch 13754/40000, Loss: 4.441661440068856e-05, Learning Rate: 0.000384\n",
      "Epoch 13755/40000, Loss: 7.705654570600018e-05, Learning Rate: 0.000384\n",
      "Epoch 13756/40000, Loss: 7.359017035923898e-05, Learning Rate: 0.000384\n",
      "Epoch 13757/40000, Loss: 4.598272425937466e-05, Learning Rate: 0.000384\n",
      "Epoch 13758/40000, Loss: 6.233410385902971e-05, Learning Rate: 0.000384\n",
      "Epoch 13759/40000, Loss: 7.786604692228138e-05, Learning Rate: 0.000384\n",
      "Epoch 13760/40000, Loss: 4.633572098100558e-05, Learning Rate: 0.000384\n",
      "Epoch 13761/40000, Loss: 4.6296736400108784e-05, Learning Rate: 0.000384\n",
      "Epoch 13762/40000, Loss: 6.384596781572327e-05, Learning Rate: 0.000384\n",
      "Epoch 13763/40000, Loss: 1.8957780412165448e-05, Learning Rate: 0.000383\n",
      "Epoch 13764/40000, Loss: 2.034004864981398e-05, Learning Rate: 0.000383\n",
      "Epoch 13765/40000, Loss: 5.0880400522146374e-05, Learning Rate: 0.000383\n",
      "Epoch 13766/40000, Loss: 7.976537744980305e-05, Learning Rate: 0.000383\n",
      "Epoch 13767/40000, Loss: 4.095240728929639e-05, Learning Rate: 0.000383\n",
      "Epoch 13768/40000, Loss: 9.747117292135954e-05, Learning Rate: 0.000383\n",
      "Epoch 13769/40000, Loss: 4.740927761304192e-05, Learning Rate: 0.000383\n",
      "Epoch 13770/40000, Loss: 0.00024180767650250345, Learning Rate: 0.000383\n",
      "Epoch 13771/40000, Loss: 7.574290066258982e-05, Learning Rate: 0.000383\n",
      "Epoch 13772/40000, Loss: 7.07882791175507e-05, Learning Rate: 0.000383\n",
      "Epoch 13773/40000, Loss: 5.716207670047879e-05, Learning Rate: 0.000383\n",
      "Epoch 13774/40000, Loss: 4.957327473675832e-05, Learning Rate: 0.000383\n",
      "Epoch 13775/40000, Loss: 0.00021608680253848433, Learning Rate: 0.000383\n",
      "Epoch 13776/40000, Loss: 9.88702304312028e-05, Learning Rate: 0.000383\n",
      "Epoch 13777/40000, Loss: 3.9068228943506256e-05, Learning Rate: 0.000383\n",
      "Epoch 13778/40000, Loss: 6.148660031612962e-05, Learning Rate: 0.000383\n",
      "Epoch 13779/40000, Loss: 5.518892430700362e-05, Learning Rate: 0.000383\n",
      "Epoch 13780/40000, Loss: 9.089402738027275e-05, Learning Rate: 0.000383\n",
      "Epoch 13781/40000, Loss: 5.3466748795472085e-05, Learning Rate: 0.000383\n",
      "Epoch 13782/40000, Loss: 3.666551492642611e-05, Learning Rate: 0.000383\n",
      "Epoch 13783/40000, Loss: 3.597655813791789e-05, Learning Rate: 0.000383\n",
      "Epoch 13784/40000, Loss: 2.9852883017156273e-05, Learning Rate: 0.000383\n",
      "Epoch 13785/40000, Loss: 4.282655208953656e-05, Learning Rate: 0.000382\n",
      "Epoch 13786/40000, Loss: 5.418405999080278e-05, Learning Rate: 0.000382\n",
      "Epoch 13787/40000, Loss: 8.747030369704589e-05, Learning Rate: 0.000382\n",
      "Epoch 13788/40000, Loss: 2.646861321409233e-05, Learning Rate: 0.000382\n",
      "Epoch 13789/40000, Loss: 8.624028123449534e-05, Learning Rate: 0.000382\n",
      "Epoch 13790/40000, Loss: 4.811495455214754e-05, Learning Rate: 0.000382\n",
      "Epoch 13791/40000, Loss: 2.487779056536965e-05, Learning Rate: 0.000382\n",
      "Epoch 13792/40000, Loss: 7.518546044593677e-05, Learning Rate: 0.000382\n",
      "Epoch 13793/40000, Loss: 4.226063538226299e-05, Learning Rate: 0.000382\n",
      "Epoch 13794/40000, Loss: 4.07044353778474e-05, Learning Rate: 0.000382\n",
      "Epoch 13795/40000, Loss: 4.784198245033622e-05, Learning Rate: 0.000382\n",
      "Epoch 13796/40000, Loss: 4.565171911963262e-05, Learning Rate: 0.000382\n",
      "Epoch 13797/40000, Loss: 4.595036443788558e-05, Learning Rate: 0.000382\n",
      "Epoch 13798/40000, Loss: 3.962302434956655e-05, Learning Rate: 0.000382\n",
      "Epoch 13799/40000, Loss: 7.405372889479622e-05, Learning Rate: 0.000382\n",
      "Epoch 13800/40000, Loss: 7.50846229493618e-05, Learning Rate: 0.000382\n",
      "Epoch 13801/40000, Loss: 1.8968130461871624e-05, Learning Rate: 0.000382\n",
      "Epoch 13802/40000, Loss: 6.427001062547788e-05, Learning Rate: 0.000382\n",
      "Epoch 13803/40000, Loss: 4.704789171228185e-05, Learning Rate: 0.000382\n",
      "Epoch 13804/40000, Loss: 7.24552883184515e-05, Learning Rate: 0.000382\n",
      "Epoch 13805/40000, Loss: 3.762648702831939e-05, Learning Rate: 0.000382\n",
      "Epoch 13806/40000, Loss: 6.215541361598298e-05, Learning Rate: 0.000382\n",
      "Epoch 13807/40000, Loss: 1.7701864635455422e-05, Learning Rate: 0.000381\n",
      "Epoch 13808/40000, Loss: 4.4436885218601674e-05, Learning Rate: 0.000381\n",
      "Epoch 13809/40000, Loss: 1.7197047782246955e-05, Learning Rate: 0.000381\n",
      "Epoch 13810/40000, Loss: 4.360762613941915e-05, Learning Rate: 0.000381\n",
      "Epoch 13811/40000, Loss: 4.35276233474724e-05, Learning Rate: 0.000381\n",
      "Epoch 13812/40000, Loss: 4.478980554267764e-05, Learning Rate: 0.000381\n",
      "Epoch 13813/40000, Loss: 4.4340289605315775e-05, Learning Rate: 0.000381\n",
      "Epoch 13814/40000, Loss: 3.718370135175064e-05, Learning Rate: 0.000381\n",
      "Epoch 13815/40000, Loss: 1.7345886590192094e-05, Learning Rate: 0.000381\n",
      "Epoch 13816/40000, Loss: 6.0441536334110424e-05, Learning Rate: 0.000381\n",
      "Epoch 13817/40000, Loss: 3.738874875125475e-05, Learning Rate: 0.000381\n",
      "Epoch 13818/40000, Loss: 7.110590377124026e-05, Learning Rate: 0.000381\n",
      "Epoch 13819/40000, Loss: 4.413578426465392e-05, Learning Rate: 0.000381\n",
      "Epoch 13820/40000, Loss: 5.978365879855119e-05, Learning Rate: 0.000381\n",
      "Epoch 13821/40000, Loss: 4.3658608774421737e-05, Learning Rate: 0.000381\n",
      "Epoch 13822/40000, Loss: 4.4535041524795815e-05, Learning Rate: 0.000381\n",
      "Epoch 13823/40000, Loss: 4.355960481916554e-05, Learning Rate: 0.000381\n",
      "Epoch 13824/40000, Loss: 4.474731758818962e-05, Learning Rate: 0.000381\n",
      "Epoch 13825/40000, Loss: 5.990178760839626e-05, Learning Rate: 0.000381\n",
      "Epoch 13826/40000, Loss: 5.9973372117383406e-05, Learning Rate: 0.000381\n",
      "Epoch 13827/40000, Loss: 1.8250628272653557e-05, Learning Rate: 0.000381\n",
      "Epoch 13828/40000, Loss: 4.567527139442973e-05, Learning Rate: 0.000381\n",
      "Epoch 13829/40000, Loss: 3.7230031011858955e-05, Learning Rate: 0.000380\n",
      "Epoch 13830/40000, Loss: 7.10786334821023e-05, Learning Rate: 0.000380\n",
      "Epoch 13831/40000, Loss: 4.3685104174073786e-05, Learning Rate: 0.000380\n",
      "Epoch 13832/40000, Loss: 7.143535185605288e-05, Learning Rate: 0.000380\n",
      "Epoch 13833/40000, Loss: 4.3650943553075194e-05, Learning Rate: 0.000380\n",
      "Epoch 13834/40000, Loss: 4.482087751966901e-05, Learning Rate: 0.000380\n",
      "Epoch 13835/40000, Loss: 3.752776683541015e-05, Learning Rate: 0.000380\n",
      "Epoch 13836/40000, Loss: 6.055110861780122e-05, Learning Rate: 0.000380\n",
      "Epoch 13837/40000, Loss: 4.427072417456657e-05, Learning Rate: 0.000380\n",
      "Epoch 13838/40000, Loss: 3.729206946445629e-05, Learning Rate: 0.000380\n",
      "Epoch 13839/40000, Loss: 7.093300519045442e-05, Learning Rate: 0.000380\n",
      "Epoch 13840/40000, Loss: 4.441794953891076e-05, Learning Rate: 0.000380\n",
      "Epoch 13841/40000, Loss: 6.002351801726036e-05, Learning Rate: 0.000380\n",
      "Epoch 13842/40000, Loss: 6.0086880694143474e-05, Learning Rate: 0.000380\n",
      "Epoch 13843/40000, Loss: 5.9943002270301804e-05, Learning Rate: 0.000380\n",
      "Epoch 13844/40000, Loss: 6.002215013722889e-05, Learning Rate: 0.000380\n",
      "Epoch 13845/40000, Loss: 2.1959636796964332e-05, Learning Rate: 0.000380\n",
      "Epoch 13846/40000, Loss: 1.865436388470698e-05, Learning Rate: 0.000380\n",
      "Epoch 13847/40000, Loss: 6.0684811614919454e-05, Learning Rate: 0.000380\n",
      "Epoch 13848/40000, Loss: 6.018269414198585e-05, Learning Rate: 0.000380\n",
      "Epoch 13849/40000, Loss: 4.393393101054244e-05, Learning Rate: 0.000380\n",
      "Epoch 13850/40000, Loss: 7.167146395659074e-05, Learning Rate: 0.000380\n",
      "Epoch 13851/40000, Loss: 3.811904025496915e-05, Learning Rate: 0.000379\n",
      "Epoch 13852/40000, Loss: 1.7549406038597226e-05, Learning Rate: 0.000379\n",
      "Epoch 13853/40000, Loss: 1.763501495588571e-05, Learning Rate: 0.000379\n",
      "Epoch 13854/40000, Loss: 7.11205939296633e-05, Learning Rate: 0.000379\n",
      "Epoch 13855/40000, Loss: 4.360936873126775e-05, Learning Rate: 0.000379\n",
      "Epoch 13856/40000, Loss: 7.107033889042214e-05, Learning Rate: 0.000379\n",
      "Epoch 13857/40000, Loss: 7.081996591296047e-05, Learning Rate: 0.000379\n",
      "Epoch 13858/40000, Loss: 4.4376163714332506e-05, Learning Rate: 0.000379\n",
      "Epoch 13859/40000, Loss: 7.137907959986478e-05, Learning Rate: 0.000379\n",
      "Epoch 13860/40000, Loss: 6.0280657635303214e-05, Learning Rate: 0.000379\n",
      "Epoch 13861/40000, Loss: 6.002085137879476e-05, Learning Rate: 0.000379\n",
      "Epoch 13862/40000, Loss: 5.9923950175289065e-05, Learning Rate: 0.000379\n",
      "Epoch 13863/40000, Loss: 7.07587314536795e-05, Learning Rate: 0.000379\n",
      "Epoch 13864/40000, Loss: 3.768927854252979e-05, Learning Rate: 0.000379\n",
      "Epoch 13865/40000, Loss: 3.7498801248148084e-05, Learning Rate: 0.000379\n",
      "Epoch 13866/40000, Loss: 4.321461892686784e-05, Learning Rate: 0.000379\n",
      "Epoch 13867/40000, Loss: 3.814335650531575e-05, Learning Rate: 0.000379\n",
      "Epoch 13868/40000, Loss: 4.3508240196388215e-05, Learning Rate: 0.000379\n",
      "Epoch 13869/40000, Loss: 4.3452462705317885e-05, Learning Rate: 0.000379\n",
      "Epoch 13870/40000, Loss: 3.735745121957734e-05, Learning Rate: 0.000379\n",
      "Epoch 13871/40000, Loss: 4.3944142817053944e-05, Learning Rate: 0.000379\n",
      "Epoch 13872/40000, Loss: 4.450293999980204e-05, Learning Rate: 0.000379\n",
      "Epoch 13873/40000, Loss: 7.123358227545395e-05, Learning Rate: 0.000378\n",
      "Epoch 13874/40000, Loss: 3.7587964470731094e-05, Learning Rate: 0.000378\n",
      "Epoch 13875/40000, Loss: 1.7403634046786465e-05, Learning Rate: 0.000378\n",
      "Epoch 13876/40000, Loss: 7.143113907659426e-05, Learning Rate: 0.000378\n",
      "Epoch 13877/40000, Loss: 7.085987454047427e-05, Learning Rate: 0.000378\n",
      "Epoch 13878/40000, Loss: 3.740768806892447e-05, Learning Rate: 0.000378\n",
      "Epoch 13879/40000, Loss: 1.723958303045947e-05, Learning Rate: 0.000378\n",
      "Epoch 13880/40000, Loss: 3.704314076458104e-05, Learning Rate: 0.000378\n",
      "Epoch 13881/40000, Loss: 6.085136556066573e-05, Learning Rate: 0.000378\n",
      "Epoch 13882/40000, Loss: 3.7172951124375686e-05, Learning Rate: 0.000378\n",
      "Epoch 13883/40000, Loss: 4.3382049625506625e-05, Learning Rate: 0.000378\n",
      "Epoch 13884/40000, Loss: 6.122310878708959e-05, Learning Rate: 0.000378\n",
      "Epoch 13885/40000, Loss: 4.370428359834477e-05, Learning Rate: 0.000378\n",
      "Epoch 13886/40000, Loss: 4.34668327216059e-05, Learning Rate: 0.000378\n",
      "Epoch 13887/40000, Loss: 6.057083373889327e-05, Learning Rate: 0.000378\n",
      "Epoch 13888/40000, Loss: 1.7148791812360287e-05, Learning Rate: 0.000378\n",
      "Epoch 13889/40000, Loss: 3.708430813276209e-05, Learning Rate: 0.000378\n",
      "Epoch 13890/40000, Loss: 7.08144434611313e-05, Learning Rate: 0.000378\n",
      "Epoch 13891/40000, Loss: 3.7467001675395295e-05, Learning Rate: 0.000378\n",
      "Epoch 13892/40000, Loss: 1.705294562270865e-05, Learning Rate: 0.000378\n",
      "Epoch 13893/40000, Loss: 4.390464528114535e-05, Learning Rate: 0.000378\n",
      "Epoch 13894/40000, Loss: 7.137581269489601e-05, Learning Rate: 0.000378\n",
      "Epoch 13895/40000, Loss: 7.122655370039865e-05, Learning Rate: 0.000377\n",
      "Epoch 13896/40000, Loss: 1.721502121654339e-05, Learning Rate: 0.000377\n",
      "Epoch 13897/40000, Loss: 7.146405550884083e-05, Learning Rate: 0.000377\n",
      "Epoch 13898/40000, Loss: 4.362679828773253e-05, Learning Rate: 0.000377\n",
      "Epoch 13899/40000, Loss: 3.7321195122785866e-05, Learning Rate: 0.000377\n",
      "Epoch 13900/40000, Loss: 1.9328030248288997e-05, Learning Rate: 0.000377\n",
      "Epoch 13901/40000, Loss: 7.124298281269148e-05, Learning Rate: 0.000377\n",
      "Epoch 13902/40000, Loss: 4.473164153750986e-05, Learning Rate: 0.000377\n",
      "Epoch 13903/40000, Loss: 4.45033292635344e-05, Learning Rate: 0.000377\n",
      "Epoch 13904/40000, Loss: 7.130130688892677e-05, Learning Rate: 0.000377\n",
      "Epoch 13905/40000, Loss: 3.7720448744948953e-05, Learning Rate: 0.000377\n",
      "Epoch 13906/40000, Loss: 6.174446025397629e-05, Learning Rate: 0.000377\n",
      "Epoch 13907/40000, Loss: 4.638296741177328e-05, Learning Rate: 0.000377\n",
      "Epoch 13908/40000, Loss: 4.595362042891793e-05, Learning Rate: 0.000377\n",
      "Epoch 13909/40000, Loss: 3.877052222378552e-05, Learning Rate: 0.000377\n",
      "Epoch 13910/40000, Loss: 7.283256854861975e-05, Learning Rate: 0.000377\n",
      "Epoch 13911/40000, Loss: 4.606045695254579e-05, Learning Rate: 0.000377\n",
      "Epoch 13912/40000, Loss: 3.905300036421977e-05, Learning Rate: 0.000377\n",
      "Epoch 13913/40000, Loss: 3.7899189919698983e-05, Learning Rate: 0.000377\n",
      "Epoch 13914/40000, Loss: 7.288780034286901e-05, Learning Rate: 0.000377\n",
      "Epoch 13915/40000, Loss: 5.281322228256613e-05, Learning Rate: 0.000377\n",
      "Epoch 13916/40000, Loss: 2.233281520602759e-05, Learning Rate: 0.000377\n",
      "Epoch 13917/40000, Loss: 5.796237746835686e-05, Learning Rate: 0.000376\n",
      "Epoch 13918/40000, Loss: 4.99191737617366e-05, Learning Rate: 0.000376\n",
      "Epoch 13919/40000, Loss: 2.2848045773571357e-05, Learning Rate: 0.000376\n",
      "Epoch 13920/40000, Loss: 7.316545088542625e-05, Learning Rate: 0.000376\n",
      "Epoch 13921/40000, Loss: 7.214959623524919e-05, Learning Rate: 0.000376\n",
      "Epoch 13922/40000, Loss: 7.197163358796388e-05, Learning Rate: 0.000376\n",
      "Epoch 13923/40000, Loss: 5.032002809457481e-05, Learning Rate: 0.000376\n",
      "Epoch 13924/40000, Loss: 2.0167284674244e-05, Learning Rate: 0.000376\n",
      "Epoch 13925/40000, Loss: 7.219598774099723e-05, Learning Rate: 0.000376\n",
      "Epoch 13926/40000, Loss: 7.171161996666342e-05, Learning Rate: 0.000376\n",
      "Epoch 13927/40000, Loss: 7.15327332727611e-05, Learning Rate: 0.000376\n",
      "Epoch 13928/40000, Loss: 6.105015927460045e-05, Learning Rate: 0.000376\n",
      "Epoch 13929/40000, Loss: 7.221606210805476e-05, Learning Rate: 0.000376\n",
      "Epoch 13930/40000, Loss: 6.178455805638805e-05, Learning Rate: 0.000376\n",
      "Epoch 13931/40000, Loss: 4.633952266885899e-05, Learning Rate: 0.000376\n",
      "Epoch 13932/40000, Loss: 3.8467020203825086e-05, Learning Rate: 0.000376\n",
      "Epoch 13933/40000, Loss: 6.165018567116931e-05, Learning Rate: 0.000376\n",
      "Epoch 13934/40000, Loss: 6.130665133241564e-05, Learning Rate: 0.000376\n",
      "Epoch 13935/40000, Loss: 7.257688412209973e-05, Learning Rate: 0.000376\n",
      "Epoch 13936/40000, Loss: 4.47736783826258e-05, Learning Rate: 0.000376\n",
      "Epoch 13937/40000, Loss: 4.4928336137672886e-05, Learning Rate: 0.000376\n",
      "Epoch 13938/40000, Loss: 3.8019323255866766e-05, Learning Rate: 0.000376\n",
      "Epoch 13939/40000, Loss: 4.7095312766032293e-05, Learning Rate: 0.000375\n",
      "Epoch 13940/40000, Loss: 6.27537738182582e-05, Learning Rate: 0.000375\n",
      "Epoch 13941/40000, Loss: 4.809579695574939e-05, Learning Rate: 0.000375\n",
      "Epoch 13942/40000, Loss: 7.137327338568866e-05, Learning Rate: 0.000375\n",
      "Epoch 13943/40000, Loss: 6.495141133200377e-05, Learning Rate: 0.000375\n",
      "Epoch 13944/40000, Loss: 1.9643588530016132e-05, Learning Rate: 0.000375\n",
      "Epoch 13945/40000, Loss: 4.4345419155433774e-05, Learning Rate: 0.000375\n",
      "Epoch 13946/40000, Loss: 7.256779645103961e-05, Learning Rate: 0.000375\n",
      "Epoch 13947/40000, Loss: 3.9308528357651085e-05, Learning Rate: 0.000375\n",
      "Epoch 13948/40000, Loss: 2.1025642126915045e-05, Learning Rate: 0.000375\n",
      "Epoch 13949/40000, Loss: 6.26125474809669e-05, Learning Rate: 0.000375\n",
      "Epoch 13950/40000, Loss: 3.8525049603777006e-05, Learning Rate: 0.000375\n",
      "Epoch 13951/40000, Loss: 4.950576840201393e-05, Learning Rate: 0.000375\n",
      "Epoch 13952/40000, Loss: 6.73270842526108e-05, Learning Rate: 0.000375\n",
      "Epoch 13953/40000, Loss: 7.771886157570407e-05, Learning Rate: 0.000375\n",
      "Epoch 13954/40000, Loss: 5.1259896281408146e-05, Learning Rate: 0.000375\n",
      "Epoch 13955/40000, Loss: 5.42100788152311e-05, Learning Rate: 0.000375\n",
      "Epoch 13956/40000, Loss: 6.482024036813527e-05, Learning Rate: 0.000375\n",
      "Epoch 13957/40000, Loss: 2.0209890863043256e-05, Learning Rate: 0.000375\n",
      "Epoch 13958/40000, Loss: 3.87613799830433e-05, Learning Rate: 0.000375\n",
      "Epoch 13959/40000, Loss: 4.7181274567265064e-05, Learning Rate: 0.000375\n",
      "Epoch 13960/40000, Loss: 4.619084211299196e-05, Learning Rate: 0.000375\n",
      "Epoch 13961/40000, Loss: 2.312057767994702e-05, Learning Rate: 0.000374\n",
      "Epoch 13962/40000, Loss: 1.916736800922081e-05, Learning Rate: 0.000374\n",
      "Epoch 13963/40000, Loss: 4.689018169301562e-05, Learning Rate: 0.000374\n",
      "Epoch 13964/40000, Loss: 1.9863653506035917e-05, Learning Rate: 0.000374\n",
      "Epoch 13965/40000, Loss: 4.887507020612247e-05, Learning Rate: 0.000374\n",
      "Epoch 13966/40000, Loss: 7.337157148867846e-05, Learning Rate: 0.000374\n",
      "Epoch 13967/40000, Loss: 4.915342651656829e-05, Learning Rate: 0.000374\n",
      "Epoch 13968/40000, Loss: 7.665201701456681e-05, Learning Rate: 0.000374\n",
      "Epoch 13969/40000, Loss: 4.098948193131946e-05, Learning Rate: 0.000374\n",
      "Epoch 13970/40000, Loss: 5.495427831192501e-05, Learning Rate: 0.000374\n",
      "Epoch 13971/40000, Loss: 7.695120439166203e-05, Learning Rate: 0.000374\n",
      "Epoch 13972/40000, Loss: 5.398939538281411e-05, Learning Rate: 0.000374\n",
      "Epoch 13973/40000, Loss: 9.466135816182941e-05, Learning Rate: 0.000374\n",
      "Epoch 13974/40000, Loss: 8.128539775498211e-05, Learning Rate: 0.000374\n",
      "Epoch 13975/40000, Loss: 5.6059492635540664e-05, Learning Rate: 0.000374\n",
      "Epoch 13976/40000, Loss: 6.908826617291197e-05, Learning Rate: 0.000374\n",
      "Epoch 13977/40000, Loss: 5.109994890517555e-05, Learning Rate: 0.000374\n",
      "Epoch 13978/40000, Loss: 2.0259312805137597e-05, Learning Rate: 0.000374\n",
      "Epoch 13979/40000, Loss: 1.8484728570911102e-05, Learning Rate: 0.000374\n",
      "Epoch 13980/40000, Loss: 4.703351078205742e-05, Learning Rate: 0.000374\n",
      "Epoch 13981/40000, Loss: 3.898308932548389e-05, Learning Rate: 0.000374\n",
      "Epoch 13982/40000, Loss: 4.8437566874781623e-05, Learning Rate: 0.000374\n",
      "Epoch 13983/40000, Loss: 7.355708657996729e-05, Learning Rate: 0.000374\n",
      "Epoch 13984/40000, Loss: 4.914287637802772e-05, Learning Rate: 0.000373\n",
      "Epoch 13985/40000, Loss: 2.9478020223905332e-05, Learning Rate: 0.000373\n",
      "Epoch 13986/40000, Loss: 5.533768853638321e-05, Learning Rate: 0.000373\n",
      "Epoch 13987/40000, Loss: 4.894351150142029e-05, Learning Rate: 0.000373\n",
      "Epoch 13988/40000, Loss: 4.601201362675056e-05, Learning Rate: 0.000373\n",
      "Epoch 13989/40000, Loss: 2.21984682866605e-05, Learning Rate: 0.000373\n",
      "Epoch 13990/40000, Loss: 1.8287235434399918e-05, Learning Rate: 0.000373\n",
      "Epoch 13991/40000, Loss: 3.901847230736166e-05, Learning Rate: 0.000373\n",
      "Epoch 13992/40000, Loss: 2.1156378352316096e-05, Learning Rate: 0.000373\n",
      "Epoch 13993/40000, Loss: 2.0443438188522123e-05, Learning Rate: 0.000373\n",
      "Epoch 13994/40000, Loss: 4.587012517731637e-05, Learning Rate: 0.000373\n",
      "Epoch 13995/40000, Loss: 3.902828029822558e-05, Learning Rate: 0.000373\n",
      "Epoch 13996/40000, Loss: 1.918317320814822e-05, Learning Rate: 0.000373\n",
      "Epoch 13997/40000, Loss: 5.03054543514736e-05, Learning Rate: 0.000373\n",
      "Epoch 13998/40000, Loss: 6.360373663483188e-05, Learning Rate: 0.000373\n",
      "Epoch 13999/40000, Loss: 1.9397883079363964e-05, Learning Rate: 0.000373\n",
      "Epoch 14000/40000, Loss: 4.854012877331115e-05, Learning Rate: 0.000373\n",
      "Epoch 14001/40000, Loss: 7.700352580286562e-05, Learning Rate: 0.000373\n",
      "Epoch 14002/40000, Loss: 1.876880560303107e-05, Learning Rate: 0.000373\n",
      "Epoch 14003/40000, Loss: 3.953761188313365e-05, Learning Rate: 0.000373\n",
      "Epoch 14004/40000, Loss: 1.9774273823713884e-05, Learning Rate: 0.000373\n",
      "Epoch 14005/40000, Loss: 4.509345308179036e-05, Learning Rate: 0.000373\n",
      "Epoch 14006/40000, Loss: 2.3222728486871347e-05, Learning Rate: 0.000372\n",
      "Epoch 14007/40000, Loss: 5.272852649795823e-05, Learning Rate: 0.000372\n",
      "Epoch 14008/40000, Loss: 4.9882830353453755e-05, Learning Rate: 0.000372\n",
      "Epoch 14009/40000, Loss: 6.576811574632302e-05, Learning Rate: 0.000372\n",
      "Epoch 14010/40000, Loss: 5.302421050146222e-05, Learning Rate: 0.000372\n",
      "Epoch 14011/40000, Loss: 6.682416278636083e-05, Learning Rate: 0.000372\n",
      "Epoch 14012/40000, Loss: 6.844225572422147e-05, Learning Rate: 0.000372\n",
      "Epoch 14013/40000, Loss: 4.1137995140161365e-05, Learning Rate: 0.000372\n",
      "Epoch 14014/40000, Loss: 2.2454036297858693e-05, Learning Rate: 0.000372\n",
      "Epoch 14015/40000, Loss: 6.476860289694741e-05, Learning Rate: 0.000372\n",
      "Epoch 14016/40000, Loss: 3.858473428408615e-05, Learning Rate: 0.000372\n",
      "Epoch 14017/40000, Loss: 7.318259304156527e-05, Learning Rate: 0.000372\n",
      "Epoch 14018/40000, Loss: 4.3724758143071085e-05, Learning Rate: 0.000372\n",
      "Epoch 14019/40000, Loss: 3.811800343100913e-05, Learning Rate: 0.000372\n",
      "Epoch 14020/40000, Loss: 4.514085594564676e-05, Learning Rate: 0.000372\n",
      "Epoch 14021/40000, Loss: 0.00011290841939626262, Learning Rate: 0.000372\n",
      "Epoch 14022/40000, Loss: 7.663976430194452e-05, Learning Rate: 0.000372\n",
      "Epoch 14023/40000, Loss: 5.101839269627817e-05, Learning Rate: 0.000372\n",
      "Epoch 14024/40000, Loss: 5.384420001064427e-05, Learning Rate: 0.000372\n",
      "Epoch 14025/40000, Loss: 8.039383101277053e-05, Learning Rate: 0.000372\n",
      "Epoch 14026/40000, Loss: 2.0476700228755362e-05, Learning Rate: 0.000372\n",
      "Epoch 14027/40000, Loss: 5.141456131241284e-05, Learning Rate: 0.000372\n",
      "Epoch 14028/40000, Loss: 8.278267341665924e-05, Learning Rate: 0.000371\n",
      "Epoch 14029/40000, Loss: 4.766545680467971e-05, Learning Rate: 0.000371\n",
      "Epoch 14030/40000, Loss: 4.739193900604732e-05, Learning Rate: 0.000371\n",
      "Epoch 14031/40000, Loss: 4.9201295041712e-05, Learning Rate: 0.000371\n",
      "Epoch 14032/40000, Loss: 3.8748286897316575e-05, Learning Rate: 0.000371\n",
      "Epoch 14033/40000, Loss: 6.498100265162066e-05, Learning Rate: 0.000371\n",
      "Epoch 14034/40000, Loss: 7.241691491799429e-05, Learning Rate: 0.000371\n",
      "Epoch 14035/40000, Loss: 4.5359498471952975e-05, Learning Rate: 0.000371\n",
      "Epoch 14036/40000, Loss: 3.906795245711692e-05, Learning Rate: 0.000371\n",
      "Epoch 14037/40000, Loss: 4.536220876616426e-05, Learning Rate: 0.000371\n",
      "Epoch 14038/40000, Loss: 2.0890232917736284e-05, Learning Rate: 0.000371\n",
      "Epoch 14039/40000, Loss: 6.897985440446064e-05, Learning Rate: 0.000371\n",
      "Epoch 14040/40000, Loss: 4.9162586947204545e-05, Learning Rate: 0.000371\n",
      "Epoch 14041/40000, Loss: 3.8128517189761624e-05, Learning Rate: 0.000371\n",
      "Epoch 14042/40000, Loss: 7.366241334239021e-05, Learning Rate: 0.000371\n",
      "Epoch 14043/40000, Loss: 7.208205352071673e-05, Learning Rate: 0.000371\n",
      "Epoch 14044/40000, Loss: 3.8777146983193234e-05, Learning Rate: 0.000371\n",
      "Epoch 14045/40000, Loss: 4.8833124310476705e-05, Learning Rate: 0.000371\n",
      "Epoch 14046/40000, Loss: 4.7793237172299996e-05, Learning Rate: 0.000371\n",
      "Epoch 14047/40000, Loss: 3.933156403945759e-05, Learning Rate: 0.000371\n",
      "Epoch 14048/40000, Loss: 4.483837983570993e-05, Learning Rate: 0.000371\n",
      "Epoch 14049/40000, Loss: 4.0421429730486125e-05, Learning Rate: 0.000371\n",
      "Epoch 14050/40000, Loss: 6.226398545550182e-05, Learning Rate: 0.000371\n",
      "Epoch 14051/40000, Loss: 6.689094880130142e-05, Learning Rate: 0.000370\n",
      "Epoch 14052/40000, Loss: 3.805444066529162e-05, Learning Rate: 0.000370\n",
      "Epoch 14053/40000, Loss: 4.778888978762552e-05, Learning Rate: 0.000370\n",
      "Epoch 14054/40000, Loss: 4.509436621447094e-05, Learning Rate: 0.000370\n",
      "Epoch 14055/40000, Loss: 5.545456224353984e-05, Learning Rate: 0.000370\n",
      "Epoch 14056/40000, Loss: 5.433445767266676e-05, Learning Rate: 0.000370\n",
      "Epoch 14057/40000, Loss: 6.350984040182084e-05, Learning Rate: 0.000370\n",
      "Epoch 14058/40000, Loss: 9.163230424746871e-05, Learning Rate: 0.000370\n",
      "Epoch 14059/40000, Loss: 0.00010984347318299115, Learning Rate: 0.000370\n",
      "Epoch 14060/40000, Loss: 4.189697574474849e-05, Learning Rate: 0.000370\n",
      "Epoch 14061/40000, Loss: 8.877556683728471e-05, Learning Rate: 0.000370\n",
      "Epoch 14062/40000, Loss: 5.7315006415592507e-05, Learning Rate: 0.000370\n",
      "Epoch 14063/40000, Loss: 9.601340570952743e-05, Learning Rate: 0.000370\n",
      "Epoch 14064/40000, Loss: 2.6422625523991883e-05, Learning Rate: 0.000370\n",
      "Epoch 14065/40000, Loss: 5.632018292089924e-05, Learning Rate: 0.000370\n",
      "Epoch 14066/40000, Loss: 7.403112977044657e-05, Learning Rate: 0.000370\n",
      "Epoch 14067/40000, Loss: 2.9135309887351468e-05, Learning Rate: 0.000370\n",
      "Epoch 14068/40000, Loss: 2.5124931198661216e-05, Learning Rate: 0.000370\n",
      "Epoch 14069/40000, Loss: 5.539555786526762e-05, Learning Rate: 0.000370\n",
      "Epoch 14070/40000, Loss: 5.2361316193128005e-05, Learning Rate: 0.000370\n",
      "Epoch 14071/40000, Loss: 2.7505411708261818e-05, Learning Rate: 0.000370\n",
      "Epoch 14072/40000, Loss: 7.87559401942417e-05, Learning Rate: 0.000370\n",
      "Epoch 14073/40000, Loss: 4.247882679919712e-05, Learning Rate: 0.000369\n",
      "Epoch 14074/40000, Loss: 4.063202868564986e-05, Learning Rate: 0.000369\n",
      "Epoch 14075/40000, Loss: 4.690305286203511e-05, Learning Rate: 0.000369\n",
      "Epoch 14076/40000, Loss: 6.765286525478587e-05, Learning Rate: 0.000369\n",
      "Epoch 14077/40000, Loss: 2.1149580788915046e-05, Learning Rate: 0.000369\n",
      "Epoch 14078/40000, Loss: 4.656496093957685e-05, Learning Rate: 0.000369\n",
      "Epoch 14079/40000, Loss: 6.886494520585984e-05, Learning Rate: 0.000369\n",
      "Epoch 14080/40000, Loss: 2.209708145528566e-05, Learning Rate: 0.000369\n",
      "Epoch 14081/40000, Loss: 7.309202192118391e-05, Learning Rate: 0.000369\n",
      "Epoch 14082/40000, Loss: 6.407524779206142e-05, Learning Rate: 0.000369\n",
      "Epoch 14083/40000, Loss: 4.5227367081679404e-05, Learning Rate: 0.000369\n",
      "Epoch 14084/40000, Loss: 4.5606564526678994e-05, Learning Rate: 0.000369\n",
      "Epoch 14085/40000, Loss: 3.836711766780354e-05, Learning Rate: 0.000369\n",
      "Epoch 14086/40000, Loss: 7.361109601333737e-05, Learning Rate: 0.000369\n",
      "Epoch 14087/40000, Loss: 4.4138294470030814e-05, Learning Rate: 0.000369\n",
      "Epoch 14088/40000, Loss: 6.0516853409353644e-05, Learning Rate: 0.000369\n",
      "Epoch 14089/40000, Loss: 4.4963489926885813e-05, Learning Rate: 0.000369\n",
      "Epoch 14090/40000, Loss: 4.473912122193724e-05, Learning Rate: 0.000369\n",
      "Epoch 14091/40000, Loss: 7.086464029271156e-05, Learning Rate: 0.000369\n",
      "Epoch 14092/40000, Loss: 4.478650589589961e-05, Learning Rate: 0.000369\n",
      "Epoch 14093/40000, Loss: 5.988415432511829e-05, Learning Rate: 0.000369\n",
      "Epoch 14094/40000, Loss: 4.3417210690677166e-05, Learning Rate: 0.000369\n",
      "Epoch 14095/40000, Loss: 7.19334784662351e-05, Learning Rate: 0.000369\n",
      "Epoch 14096/40000, Loss: 7.130447920644656e-05, Learning Rate: 0.000368\n",
      "Epoch 14097/40000, Loss: 1.7219797882717103e-05, Learning Rate: 0.000368\n",
      "Epoch 14098/40000, Loss: 7.297821139218286e-05, Learning Rate: 0.000368\n",
      "Epoch 14099/40000, Loss: 4.381760300020687e-05, Learning Rate: 0.000368\n",
      "Epoch 14100/40000, Loss: 4.5061056880513206e-05, Learning Rate: 0.000368\n",
      "Epoch 14101/40000, Loss: 6.086838766350411e-05, Learning Rate: 0.000368\n",
      "Epoch 14102/40000, Loss: 4.515451655606739e-05, Learning Rate: 0.000368\n",
      "Epoch 14103/40000, Loss: 1.7954351278604008e-05, Learning Rate: 0.000368\n",
      "Epoch 14104/40000, Loss: 4.789058584719896e-05, Learning Rate: 0.000368\n",
      "Epoch 14105/40000, Loss: 4.448876643436961e-05, Learning Rate: 0.000368\n",
      "Epoch 14106/40000, Loss: 3.769267277675681e-05, Learning Rate: 0.000368\n",
      "Epoch 14107/40000, Loss: 4.574323247652501e-05, Learning Rate: 0.000368\n",
      "Epoch 14108/40000, Loss: 4.44922516180668e-05, Learning Rate: 0.000368\n",
      "Epoch 14109/40000, Loss: 1.7278325685765594e-05, Learning Rate: 0.000368\n",
      "Epoch 14110/40000, Loss: 4.5434262574417517e-05, Learning Rate: 0.000368\n",
      "Epoch 14111/40000, Loss: 4.469247505767271e-05, Learning Rate: 0.000368\n",
      "Epoch 14112/40000, Loss: 6.248700083233416e-05, Learning Rate: 0.000368\n",
      "Epoch 14113/40000, Loss: 6.0365517128957435e-05, Learning Rate: 0.000368\n",
      "Epoch 14114/40000, Loss: 1.7379063137923367e-05, Learning Rate: 0.000368\n",
      "Epoch 14115/40000, Loss: 4.2961932194884866e-05, Learning Rate: 0.000368\n",
      "Epoch 14116/40000, Loss: 4.2843847040785477e-05, Learning Rate: 0.000368\n",
      "Epoch 14117/40000, Loss: 5.9530368162086233e-05, Learning Rate: 0.000368\n",
      "Epoch 14118/40000, Loss: 3.6915404052706435e-05, Learning Rate: 0.000368\n",
      "Epoch 14119/40000, Loss: 4.276650361134671e-05, Learning Rate: 0.000367\n",
      "Epoch 14120/40000, Loss: 3.70809793821536e-05, Learning Rate: 0.000367\n",
      "Epoch 14121/40000, Loss: 4.275664468877949e-05, Learning Rate: 0.000367\n",
      "Epoch 14122/40000, Loss: 7.024574733804911e-05, Learning Rate: 0.000367\n",
      "Epoch 14123/40000, Loss: 4.301217632018961e-05, Learning Rate: 0.000367\n",
      "Epoch 14124/40000, Loss: 3.67501525033731e-05, Learning Rate: 0.000367\n",
      "Epoch 14125/40000, Loss: 6.03120970481541e-05, Learning Rate: 0.000367\n",
      "Epoch 14126/40000, Loss: 4.2796102206921205e-05, Learning Rate: 0.000367\n",
      "Epoch 14127/40000, Loss: 6.99611337040551e-05, Learning Rate: 0.000367\n",
      "Epoch 14128/40000, Loss: 4.279605855117552e-05, Learning Rate: 0.000367\n",
      "Epoch 14129/40000, Loss: 4.27855848101899e-05, Learning Rate: 0.000367\n",
      "Epoch 14130/40000, Loss: 5.929365215706639e-05, Learning Rate: 0.000367\n",
      "Epoch 14131/40000, Loss: 1.6806827261461876e-05, Learning Rate: 0.000367\n",
      "Epoch 14132/40000, Loss: 3.6533841921482235e-05, Learning Rate: 0.000367\n",
      "Epoch 14133/40000, Loss: 3.6621142498916015e-05, Learning Rate: 0.000367\n",
      "Epoch 14134/40000, Loss: 4.3685191485565156e-05, Learning Rate: 0.000367\n",
      "Epoch 14135/40000, Loss: 6.976576696615666e-05, Learning Rate: 0.000367\n",
      "Epoch 14136/40000, Loss: 4.275252285879105e-05, Learning Rate: 0.000367\n",
      "Epoch 14137/40000, Loss: 6.999399920459837e-05, Learning Rate: 0.000367\n",
      "Epoch 14138/40000, Loss: 5.916337249800563e-05, Learning Rate: 0.000367\n",
      "Epoch 14139/40000, Loss: 1.6894353393581696e-05, Learning Rate: 0.000367\n",
      "Epoch 14140/40000, Loss: 3.674412437248975e-05, Learning Rate: 0.000367\n",
      "Epoch 14141/40000, Loss: 5.906124715693295e-05, Learning Rate: 0.000366\n",
      "Epoch 14142/40000, Loss: 4.2908344767056406e-05, Learning Rate: 0.000366\n",
      "Epoch 14143/40000, Loss: 5.9059162595076486e-05, Learning Rate: 0.000366\n",
      "Epoch 14144/40000, Loss: 1.6858157323440537e-05, Learning Rate: 0.000366\n",
      "Epoch 14145/40000, Loss: 4.29497922596056e-05, Learning Rate: 0.000366\n",
      "Epoch 14146/40000, Loss: 5.926302765146829e-05, Learning Rate: 0.000366\n",
      "Epoch 14147/40000, Loss: 1.6745889297453687e-05, Learning Rate: 0.000366\n",
      "Epoch 14148/40000, Loss: 1.676168540143408e-05, Learning Rate: 0.000366\n",
      "Epoch 14149/40000, Loss: 1.6846879589138553e-05, Learning Rate: 0.000366\n",
      "Epoch 14150/40000, Loss: 3.6590456147678196e-05, Learning Rate: 0.000366\n",
      "Epoch 14151/40000, Loss: 4.40978110418655e-05, Learning Rate: 0.000366\n",
      "Epoch 14152/40000, Loss: 5.928102109464817e-05, Learning Rate: 0.000366\n",
      "Epoch 14153/40000, Loss: 7.014709990471601e-05, Learning Rate: 0.000366\n",
      "Epoch 14154/40000, Loss: 5.917013913858682e-05, Learning Rate: 0.000366\n",
      "Epoch 14155/40000, Loss: 1.6860773030202836e-05, Learning Rate: 0.000366\n",
      "Epoch 14156/40000, Loss: 3.733691846719012e-05, Learning Rate: 0.000366\n",
      "Epoch 14157/40000, Loss: 4.290995275368914e-05, Learning Rate: 0.000366\n",
      "Epoch 14158/40000, Loss: 1.6744575987104326e-05, Learning Rate: 0.000366\n",
      "Epoch 14159/40000, Loss: 1.6798285287222825e-05, Learning Rate: 0.000366\n",
      "Epoch 14160/40000, Loss: 3.722654946614057e-05, Learning Rate: 0.000366\n",
      "Epoch 14161/40000, Loss: 5.935298395343125e-05, Learning Rate: 0.000366\n",
      "Epoch 14162/40000, Loss: 4.4278895074967295e-05, Learning Rate: 0.000366\n",
      "Epoch 14163/40000, Loss: 1.7193666280945763e-05, Learning Rate: 0.000366\n",
      "Epoch 14164/40000, Loss: 5.992720980430022e-05, Learning Rate: 0.000365\n",
      "Epoch 14165/40000, Loss: 7.079314673319459e-05, Learning Rate: 0.000365\n",
      "Epoch 14166/40000, Loss: 3.778827885980718e-05, Learning Rate: 0.000365\n",
      "Epoch 14167/40000, Loss: 3.703651964315213e-05, Learning Rate: 0.000365\n",
      "Epoch 14168/40000, Loss: 3.7201672967057675e-05, Learning Rate: 0.000365\n",
      "Epoch 14169/40000, Loss: 5.999489076202735e-05, Learning Rate: 0.000365\n",
      "Epoch 14170/40000, Loss: 5.9685677115339786e-05, Learning Rate: 0.000365\n",
      "Epoch 14171/40000, Loss: 6.01835417910479e-05, Learning Rate: 0.000365\n",
      "Epoch 14172/40000, Loss: 3.734956408152357e-05, Learning Rate: 0.000365\n",
      "Epoch 14173/40000, Loss: 6.291474710451439e-05, Learning Rate: 0.000365\n",
      "Epoch 14174/40000, Loss: 4.727439591079019e-05, Learning Rate: 0.000365\n",
      "Epoch 14175/40000, Loss: 7.17289003659971e-05, Learning Rate: 0.000365\n",
      "Epoch 14176/40000, Loss: 7.279076089616865e-05, Learning Rate: 0.000365\n",
      "Epoch 14177/40000, Loss: 4.862878267886117e-05, Learning Rate: 0.000365\n",
      "Epoch 14178/40000, Loss: 1.9440327378106304e-05, Learning Rate: 0.000365\n",
      "Epoch 14179/40000, Loss: 6.228088750503957e-05, Learning Rate: 0.000365\n",
      "Epoch 14180/40000, Loss: 6.109665991971269e-05, Learning Rate: 0.000365\n",
      "Epoch 14181/40000, Loss: 4.470039493753575e-05, Learning Rate: 0.000365\n",
      "Epoch 14182/40000, Loss: 1.7750538972904906e-05, Learning Rate: 0.000365\n",
      "Epoch 14183/40000, Loss: 1.810227331588976e-05, Learning Rate: 0.000365\n",
      "Epoch 14184/40000, Loss: 1.820354736992158e-05, Learning Rate: 0.000365\n",
      "Epoch 14185/40000, Loss: 6.061777821741998e-05, Learning Rate: 0.000365\n",
      "Epoch 14186/40000, Loss: 7.207614544313401e-05, Learning Rate: 0.000365\n",
      "Epoch 14187/40000, Loss: 4.4277305278228596e-05, Learning Rate: 0.000364\n",
      "Epoch 14188/40000, Loss: 7.17743459972553e-05, Learning Rate: 0.000364\n",
      "Epoch 14189/40000, Loss: 4.489288403419778e-05, Learning Rate: 0.000364\n",
      "Epoch 14190/40000, Loss: 4.5807370042894036e-05, Learning Rate: 0.000364\n",
      "Epoch 14191/40000, Loss: 4.458549665287137e-05, Learning Rate: 0.000364\n",
      "Epoch 14192/40000, Loss: 4.622884080163203e-05, Learning Rate: 0.000364\n",
      "Epoch 14193/40000, Loss: 7.260405254783109e-05, Learning Rate: 0.000364\n",
      "Epoch 14194/40000, Loss: 4.417875607032329e-05, Learning Rate: 0.000364\n",
      "Epoch 14195/40000, Loss: 3.76309544662945e-05, Learning Rate: 0.000364\n",
      "Epoch 14196/40000, Loss: 6.183149525895715e-05, Learning Rate: 0.000364\n",
      "Epoch 14197/40000, Loss: 7.339189323829487e-05, Learning Rate: 0.000364\n",
      "Epoch 14198/40000, Loss: 1.9656103177112527e-05, Learning Rate: 0.000364\n",
      "Epoch 14199/40000, Loss: 4.111981252208352e-05, Learning Rate: 0.000364\n",
      "Epoch 14200/40000, Loss: 2.1840260160388425e-05, Learning Rate: 0.000364\n",
      "Epoch 14201/40000, Loss: 6.442888115998358e-05, Learning Rate: 0.000364\n",
      "Epoch 14202/40000, Loss: 4.2483738070586696e-05, Learning Rate: 0.000364\n",
      "Epoch 14203/40000, Loss: 2.7769994630943984e-05, Learning Rate: 0.000364\n",
      "Epoch 14204/40000, Loss: 7.310972432605922e-05, Learning Rate: 0.000364\n",
      "Epoch 14205/40000, Loss: 7.593883492518216e-05, Learning Rate: 0.000364\n",
      "Epoch 14206/40000, Loss: 5.260679608909413e-05, Learning Rate: 0.000364\n",
      "Epoch 14207/40000, Loss: 2.360922735533677e-05, Learning Rate: 0.000364\n",
      "Epoch 14208/40000, Loss: 5.258622331894003e-05, Learning Rate: 0.000364\n",
      "Epoch 14209/40000, Loss: 4.6207722334656864e-05, Learning Rate: 0.000364\n",
      "Epoch 14210/40000, Loss: 8.377938502235338e-05, Learning Rate: 0.000363\n",
      "Epoch 14211/40000, Loss: 7.665521843591705e-05, Learning Rate: 0.000363\n",
      "Epoch 14212/40000, Loss: 6.22809020569548e-05, Learning Rate: 0.000363\n",
      "Epoch 14213/40000, Loss: 2.0119574401178397e-05, Learning Rate: 0.000363\n",
      "Epoch 14214/40000, Loss: 1.8168861060985364e-05, Learning Rate: 0.000363\n",
      "Epoch 14215/40000, Loss: 6.734089402016252e-05, Learning Rate: 0.000363\n",
      "Epoch 14216/40000, Loss: 1.8489494323148392e-05, Learning Rate: 0.000363\n",
      "Epoch 14217/40000, Loss: 3.8536323700100183e-05, Learning Rate: 0.000363\n",
      "Epoch 14218/40000, Loss: 7.377994188573211e-05, Learning Rate: 0.000363\n",
      "Epoch 14219/40000, Loss: 4.0406019252259284e-05, Learning Rate: 0.000363\n",
      "Epoch 14220/40000, Loss: 4.713255475508049e-05, Learning Rate: 0.000363\n",
      "Epoch 14221/40000, Loss: 6.29219866823405e-05, Learning Rate: 0.000363\n",
      "Epoch 14222/40000, Loss: 4.97944129165262e-05, Learning Rate: 0.000363\n",
      "Epoch 14223/40000, Loss: 6.858261622255668e-05, Learning Rate: 0.000363\n",
      "Epoch 14224/40000, Loss: 6.291579484241083e-05, Learning Rate: 0.000363\n",
      "Epoch 14225/40000, Loss: 7.526738045271486e-05, Learning Rate: 0.000363\n",
      "Epoch 14226/40000, Loss: 4.703304512077011e-05, Learning Rate: 0.000363\n",
      "Epoch 14227/40000, Loss: 6.353492062771693e-05, Learning Rate: 0.000363\n",
      "Epoch 14228/40000, Loss: 4.8143880121642724e-05, Learning Rate: 0.000363\n",
      "Epoch 14229/40000, Loss: 1.8877823094953783e-05, Learning Rate: 0.000363\n",
      "Epoch 14230/40000, Loss: 4.453399378689937e-05, Learning Rate: 0.000363\n",
      "Epoch 14231/40000, Loss: 2.1360825485317037e-05, Learning Rate: 0.000363\n",
      "Epoch 14232/40000, Loss: 7.174538040999323e-05, Learning Rate: 0.000363\n",
      "Epoch 14233/40000, Loss: 1.8316099158255383e-05, Learning Rate: 0.000362\n",
      "Epoch 14234/40000, Loss: 3.7966194213368e-05, Learning Rate: 0.000362\n",
      "Epoch 14235/40000, Loss: 4.3785952584585175e-05, Learning Rate: 0.000362\n",
      "Epoch 14236/40000, Loss: 4.1446204704698175e-05, Learning Rate: 0.000362\n",
      "Epoch 14237/40000, Loss: 4.657053796108812e-05, Learning Rate: 0.000362\n",
      "Epoch 14238/40000, Loss: 4.554112456389703e-05, Learning Rate: 0.000362\n",
      "Epoch 14239/40000, Loss: 3.8580179534619674e-05, Learning Rate: 0.000362\n",
      "Epoch 14240/40000, Loss: 3.7486777728190646e-05, Learning Rate: 0.000362\n",
      "Epoch 14241/40000, Loss: 3.7109195545781404e-05, Learning Rate: 0.000362\n",
      "Epoch 14242/40000, Loss: 1.743234315654263e-05, Learning Rate: 0.000362\n",
      "Epoch 14243/40000, Loss: 1.702920599200297e-05, Learning Rate: 0.000362\n",
      "Epoch 14244/40000, Loss: 7.034928421489894e-05, Learning Rate: 0.000362\n",
      "Epoch 14245/40000, Loss: 4.313572571845725e-05, Learning Rate: 0.000362\n",
      "Epoch 14246/40000, Loss: 5.9430458350107074e-05, Learning Rate: 0.000362\n",
      "Epoch 14247/40000, Loss: 7.15165733709e-05, Learning Rate: 0.000362\n",
      "Epoch 14248/40000, Loss: 6.0889869928359985e-05, Learning Rate: 0.000362\n",
      "Epoch 14249/40000, Loss: 1.8812797861755826e-05, Learning Rate: 0.000362\n",
      "Epoch 14250/40000, Loss: 1.8155065845348872e-05, Learning Rate: 0.000362\n",
      "Epoch 14251/40000, Loss: 6.14439049968496e-05, Learning Rate: 0.000362\n",
      "Epoch 14252/40000, Loss: 6.08976224611979e-05, Learning Rate: 0.000362\n",
      "Epoch 14253/40000, Loss: 6.086025314289145e-05, Learning Rate: 0.000362\n",
      "Epoch 14254/40000, Loss: 4.463842560653575e-05, Learning Rate: 0.000362\n",
      "Epoch 14255/40000, Loss: 7.026086677797139e-05, Learning Rate: 0.000362\n",
      "Epoch 14256/40000, Loss: 4.460793934413232e-05, Learning Rate: 0.000361\n",
      "Epoch 14257/40000, Loss: 6.131007830845192e-05, Learning Rate: 0.000361\n",
      "Epoch 14258/40000, Loss: 1.702316149021499e-05, Learning Rate: 0.000361\n",
      "Epoch 14259/40000, Loss: 6.582462810911238e-05, Learning Rate: 0.000361\n",
      "Epoch 14260/40000, Loss: 6.201010546647012e-05, Learning Rate: 0.000361\n",
      "Epoch 14261/40000, Loss: 6.26844193902798e-05, Learning Rate: 0.000361\n",
      "Epoch 14262/40000, Loss: 3.8442340155597776e-05, Learning Rate: 0.000361\n",
      "Epoch 14263/40000, Loss: 4.606746733770706e-05, Learning Rate: 0.000361\n",
      "Epoch 14264/40000, Loss: 2.0585966922226362e-05, Learning Rate: 0.000361\n",
      "Epoch 14265/40000, Loss: 1.7972846762859263e-05, Learning Rate: 0.000361\n",
      "Epoch 14266/40000, Loss: 5.228599547990598e-05, Learning Rate: 0.000361\n",
      "Epoch 14267/40000, Loss: 6.621347711188719e-05, Learning Rate: 0.000361\n",
      "Epoch 14268/40000, Loss: 3.8991274777799845e-05, Learning Rate: 0.000361\n",
      "Epoch 14269/40000, Loss: 4.553985854727216e-05, Learning Rate: 0.000361\n",
      "Epoch 14270/40000, Loss: 5.629987208521925e-05, Learning Rate: 0.000361\n",
      "Epoch 14271/40000, Loss: 4.5604563638335094e-05, Learning Rate: 0.000361\n",
      "Epoch 14272/40000, Loss: 6.768255843780935e-05, Learning Rate: 0.000361\n",
      "Epoch 14273/40000, Loss: 6.542227492900565e-05, Learning Rate: 0.000361\n",
      "Epoch 14274/40000, Loss: 2.0413352103787474e-05, Learning Rate: 0.000361\n",
      "Epoch 14275/40000, Loss: 7.694162195548415e-05, Learning Rate: 0.000361\n",
      "Epoch 14276/40000, Loss: 7.412635022774339e-05, Learning Rate: 0.000361\n",
      "Epoch 14277/40000, Loss: 3.889054278261028e-05, Learning Rate: 0.000361\n",
      "Epoch 14278/40000, Loss: 6.322150875348598e-05, Learning Rate: 0.000361\n",
      "Epoch 14279/40000, Loss: 1.8019500203081407e-05, Learning Rate: 0.000360\n",
      "Epoch 14280/40000, Loss: 4.599375824909657e-05, Learning Rate: 0.000360\n",
      "Epoch 14281/40000, Loss: 4.653583528124727e-05, Learning Rate: 0.000360\n",
      "Epoch 14282/40000, Loss: 1.8268920030095614e-05, Learning Rate: 0.000360\n",
      "Epoch 14283/40000, Loss: 4.697018812294118e-05, Learning Rate: 0.000360\n",
      "Epoch 14284/40000, Loss: 7.364767952822149e-05, Learning Rate: 0.000360\n",
      "Epoch 14285/40000, Loss: 4.358366641099565e-05, Learning Rate: 0.000360\n",
      "Epoch 14286/40000, Loss: 7.018420728854835e-05, Learning Rate: 0.000360\n",
      "Epoch 14287/40000, Loss: 6.120435864431784e-05, Learning Rate: 0.000360\n",
      "Epoch 14288/40000, Loss: 1.7665013729128987e-05, Learning Rate: 0.000360\n",
      "Epoch 14289/40000, Loss: 7.2060342063196e-05, Learning Rate: 0.000360\n",
      "Epoch 14290/40000, Loss: 4.6345459850272164e-05, Learning Rate: 0.000360\n",
      "Epoch 14291/40000, Loss: 3.745806679944508e-05, Learning Rate: 0.000360\n",
      "Epoch 14292/40000, Loss: 3.7211593735264614e-05, Learning Rate: 0.000360\n",
      "Epoch 14293/40000, Loss: 6.031610973877832e-05, Learning Rate: 0.000360\n",
      "Epoch 14294/40000, Loss: 3.713241676450707e-05, Learning Rate: 0.000360\n",
      "Epoch 14295/40000, Loss: 1.71301762748044e-05, Learning Rate: 0.000360\n",
      "Epoch 14296/40000, Loss: 3.676446794997901e-05, Learning Rate: 0.000360\n",
      "Epoch 14297/40000, Loss: 4.3055013520643115e-05, Learning Rate: 0.000360\n",
      "Epoch 14298/40000, Loss: 4.270852878107689e-05, Learning Rate: 0.000360\n",
      "Epoch 14299/40000, Loss: 5.965909440419637e-05, Learning Rate: 0.000360\n",
      "Epoch 14300/40000, Loss: 3.674961772048846e-05, Learning Rate: 0.000360\n",
      "Epoch 14301/40000, Loss: 1.690381031949073e-05, Learning Rate: 0.000360\n",
      "Epoch 14302/40000, Loss: 7.082133379299194e-05, Learning Rate: 0.000359\n",
      "Epoch 14303/40000, Loss: 6.121041951701045e-05, Learning Rate: 0.000359\n",
      "Epoch 14304/40000, Loss: 7.156967330956832e-05, Learning Rate: 0.000359\n",
      "Epoch 14305/40000, Loss: 1.7993213987210765e-05, Learning Rate: 0.000359\n",
      "Epoch 14306/40000, Loss: 3.714016929734498e-05, Learning Rate: 0.000359\n",
      "Epoch 14307/40000, Loss: 6.278170621953905e-05, Learning Rate: 0.000359\n",
      "Epoch 14308/40000, Loss: 4.424135840963572e-05, Learning Rate: 0.000359\n",
      "Epoch 14309/40000, Loss: 3.814368028542958e-05, Learning Rate: 0.000359\n",
      "Epoch 14310/40000, Loss: 5.046825754106976e-05, Learning Rate: 0.000359\n",
      "Epoch 14311/40000, Loss: 4.350198287284002e-05, Learning Rate: 0.000359\n",
      "Epoch 14312/40000, Loss: 4.9263158871326596e-05, Learning Rate: 0.000359\n",
      "Epoch 14313/40000, Loss: 2.0640774891944602e-05, Learning Rate: 0.000359\n",
      "Epoch 14314/40000, Loss: 4.2234572902088985e-05, Learning Rate: 0.000359\n",
      "Epoch 14315/40000, Loss: 4.751056985696778e-05, Learning Rate: 0.000359\n",
      "Epoch 14316/40000, Loss: 2.23344613914378e-05, Learning Rate: 0.000359\n",
      "Epoch 14317/40000, Loss: 7.973689207574353e-05, Learning Rate: 0.000359\n",
      "Epoch 14318/40000, Loss: 4.861867637373507e-05, Learning Rate: 0.000359\n",
      "Epoch 14319/40000, Loss: 6.433888484025374e-05, Learning Rate: 0.000359\n",
      "Epoch 14320/40000, Loss: 6.249449506867677e-05, Learning Rate: 0.000359\n",
      "Epoch 14321/40000, Loss: 6.162867066450417e-05, Learning Rate: 0.000359\n",
      "Epoch 14322/40000, Loss: 1.8909971913672052e-05, Learning Rate: 0.000359\n",
      "Epoch 14323/40000, Loss: 4.674435331253335e-05, Learning Rate: 0.000359\n",
      "Epoch 14324/40000, Loss: 3.900895899278112e-05, Learning Rate: 0.000359\n",
      "Epoch 14325/40000, Loss: 3.7561381759587675e-05, Learning Rate: 0.000358\n",
      "Epoch 14326/40000, Loss: 1.832694761105813e-05, Learning Rate: 0.000358\n",
      "Epoch 14327/40000, Loss: 6.558744644280523e-05, Learning Rate: 0.000358\n",
      "Epoch 14328/40000, Loss: 4.57649803138338e-05, Learning Rate: 0.000358\n",
      "Epoch 14329/40000, Loss: 1.745104782457929e-05, Learning Rate: 0.000358\n",
      "Epoch 14330/40000, Loss: 4.8666966904420406e-05, Learning Rate: 0.000358\n",
      "Epoch 14331/40000, Loss: 4.569819793687202e-05, Learning Rate: 0.000358\n",
      "Epoch 14332/40000, Loss: 4.733723108074628e-05, Learning Rate: 0.000358\n",
      "Epoch 14333/40000, Loss: 7.22235708963126e-05, Learning Rate: 0.000358\n",
      "Epoch 14334/40000, Loss: 4.374430136522278e-05, Learning Rate: 0.000358\n",
      "Epoch 14335/40000, Loss: 7.10523163434118e-05, Learning Rate: 0.000358\n",
      "Epoch 14336/40000, Loss: 4.574951162794605e-05, Learning Rate: 0.000358\n",
      "Epoch 14337/40000, Loss: 4.3572897993726656e-05, Learning Rate: 0.000358\n",
      "Epoch 14338/40000, Loss: 7.122352690203115e-05, Learning Rate: 0.000358\n",
      "Epoch 14339/40000, Loss: 4.450181222637184e-05, Learning Rate: 0.000358\n",
      "Epoch 14340/40000, Loss: 1.7912187104229815e-05, Learning Rate: 0.000358\n",
      "Epoch 14341/40000, Loss: 1.7830165234045126e-05, Learning Rate: 0.000358\n",
      "Epoch 14342/40000, Loss: 7.228290633065626e-05, Learning Rate: 0.000358\n",
      "Epoch 14343/40000, Loss: 6.0675716667901725e-05, Learning Rate: 0.000358\n",
      "Epoch 14344/40000, Loss: 4.69044825877063e-05, Learning Rate: 0.000358\n",
      "Epoch 14345/40000, Loss: 1.7696480426820926e-05, Learning Rate: 0.000358\n",
      "Epoch 14346/40000, Loss: 4.574767081066966e-05, Learning Rate: 0.000358\n",
      "Epoch 14347/40000, Loss: 3.6975270631955937e-05, Learning Rate: 0.000358\n",
      "Epoch 14348/40000, Loss: 7.067101978464052e-05, Learning Rate: 0.000357\n",
      "Epoch 14349/40000, Loss: 5.9835409047082067e-05, Learning Rate: 0.000357\n",
      "Epoch 14350/40000, Loss: 3.710495002451353e-05, Learning Rate: 0.000357\n",
      "Epoch 14351/40000, Loss: 3.705349809024483e-05, Learning Rate: 0.000357\n",
      "Epoch 14352/40000, Loss: 4.391858965391293e-05, Learning Rate: 0.000357\n",
      "Epoch 14353/40000, Loss: 1.8684175302041695e-05, Learning Rate: 0.000357\n",
      "Epoch 14354/40000, Loss: 7.065179670462385e-05, Learning Rate: 0.000357\n",
      "Epoch 14355/40000, Loss: 4.316267586546019e-05, Learning Rate: 0.000357\n",
      "Epoch 14356/40000, Loss: 1.786137727322057e-05, Learning Rate: 0.000357\n",
      "Epoch 14357/40000, Loss: 7.129985169740394e-05, Learning Rate: 0.000357\n",
      "Epoch 14358/40000, Loss: 6.23171217739582e-05, Learning Rate: 0.000357\n",
      "Epoch 14359/40000, Loss: 4.514931788435206e-05, Learning Rate: 0.000357\n",
      "Epoch 14360/40000, Loss: 4.9038520955946296e-05, Learning Rate: 0.000357\n",
      "Epoch 14361/40000, Loss: 1.901508767332416e-05, Learning Rate: 0.000357\n",
      "Epoch 14362/40000, Loss: 3.778868631343357e-05, Learning Rate: 0.000357\n",
      "Epoch 14363/40000, Loss: 4.5669621613342315e-05, Learning Rate: 0.000357\n",
      "Epoch 14364/40000, Loss: 3.7508405512198806e-05, Learning Rate: 0.000357\n",
      "Epoch 14365/40000, Loss: 3.739516978384927e-05, Learning Rate: 0.000357\n",
      "Epoch 14366/40000, Loss: 7.075051689753309e-05, Learning Rate: 0.000357\n",
      "Epoch 14367/40000, Loss: 7.046341488603503e-05, Learning Rate: 0.000357\n",
      "Epoch 14368/40000, Loss: 3.729093441506848e-05, Learning Rate: 0.000357\n",
      "Epoch 14369/40000, Loss: 1.7344475054414943e-05, Learning Rate: 0.000357\n",
      "Epoch 14370/40000, Loss: 1.78142490767641e-05, Learning Rate: 0.000357\n",
      "Epoch 14371/40000, Loss: 4.798923328053206e-05, Learning Rate: 0.000357\n",
      "Epoch 14372/40000, Loss: 4.7446501412196085e-05, Learning Rate: 0.000356\n",
      "Epoch 14373/40000, Loss: 4.5221422624308616e-05, Learning Rate: 0.000356\n",
      "Epoch 14374/40000, Loss: 3.84396334993653e-05, Learning Rate: 0.000356\n",
      "Epoch 14375/40000, Loss: 4.4847591198049486e-05, Learning Rate: 0.000356\n",
      "Epoch 14376/40000, Loss: 4.0830112993717194e-05, Learning Rate: 0.000356\n",
      "Epoch 14377/40000, Loss: 2.1681926227756776e-05, Learning Rate: 0.000356\n",
      "Epoch 14378/40000, Loss: 8.064719440881163e-05, Learning Rate: 0.000356\n",
      "Epoch 14379/40000, Loss: 4.888220792054199e-05, Learning Rate: 0.000356\n",
      "Epoch 14380/40000, Loss: 8.726759551791474e-05, Learning Rate: 0.000356\n",
      "Epoch 14381/40000, Loss: 7.187190203694627e-05, Learning Rate: 0.000356\n",
      "Epoch 14382/40000, Loss: 8.627588249510154e-05, Learning Rate: 0.000356\n",
      "Epoch 14383/40000, Loss: 0.00014425371773540974, Learning Rate: 0.000356\n",
      "Epoch 14384/40000, Loss: 6.348621536744758e-05, Learning Rate: 0.000356\n",
      "Epoch 14385/40000, Loss: 6.252971797948703e-05, Learning Rate: 0.000356\n",
      "Epoch 14386/40000, Loss: 4.856562372879125e-05, Learning Rate: 0.000356\n",
      "Epoch 14387/40000, Loss: 9.261209925170988e-05, Learning Rate: 0.000356\n",
      "Epoch 14388/40000, Loss: 5.878275624127127e-05, Learning Rate: 0.000356\n",
      "Epoch 14389/40000, Loss: 2.3942904590512626e-05, Learning Rate: 0.000356\n",
      "Epoch 14390/40000, Loss: 5.77258579141926e-05, Learning Rate: 0.000356\n",
      "Epoch 14391/40000, Loss: 8.27079638838768e-05, Learning Rate: 0.000356\n",
      "Epoch 14392/40000, Loss: 7.325949991354719e-05, Learning Rate: 0.000356\n",
      "Epoch 14393/40000, Loss: 5.527307803276926e-05, Learning Rate: 0.000356\n",
      "Epoch 14394/40000, Loss: 2.183158721891232e-05, Learning Rate: 0.000356\n",
      "Epoch 14395/40000, Loss: 4.623129279934801e-05, Learning Rate: 0.000355\n",
      "Epoch 14396/40000, Loss: 9.568805398885161e-05, Learning Rate: 0.000355\n",
      "Epoch 14397/40000, Loss: 1.9439292373135686e-05, Learning Rate: 0.000355\n",
      "Epoch 14398/40000, Loss: 1.8721602828009054e-05, Learning Rate: 0.000355\n",
      "Epoch 14399/40000, Loss: 7.561129314126447e-05, Learning Rate: 0.000355\n",
      "Epoch 14400/40000, Loss: 6.622273212997243e-05, Learning Rate: 0.000355\n",
      "Epoch 14401/40000, Loss: 1.8295000700163655e-05, Learning Rate: 0.000355\n",
      "Epoch 14402/40000, Loss: 4.637874008039944e-05, Learning Rate: 0.000355\n",
      "Epoch 14403/40000, Loss: 7.190908218035474e-05, Learning Rate: 0.000355\n",
      "Epoch 14404/40000, Loss: 4.723227903014049e-05, Learning Rate: 0.000355\n",
      "Epoch 14405/40000, Loss: 3.834615199593827e-05, Learning Rate: 0.000355\n",
      "Epoch 14406/40000, Loss: 4.7928362619131804e-05, Learning Rate: 0.000355\n",
      "Epoch 14407/40000, Loss: 4.568226358969696e-05, Learning Rate: 0.000355\n",
      "Epoch 14408/40000, Loss: 4.3217682105023414e-05, Learning Rate: 0.000355\n",
      "Epoch 14409/40000, Loss: 1.7591752111911774e-05, Learning Rate: 0.000355\n",
      "Epoch 14410/40000, Loss: 7.055539754219353e-05, Learning Rate: 0.000355\n",
      "Epoch 14411/40000, Loss: 3.692420068546198e-05, Learning Rate: 0.000355\n",
      "Epoch 14412/40000, Loss: 4.279346831026487e-05, Learning Rate: 0.000355\n",
      "Epoch 14413/40000, Loss: 5.966663229628466e-05, Learning Rate: 0.000355\n",
      "Epoch 14414/40000, Loss: 4.2635387217160314e-05, Learning Rate: 0.000355\n",
      "Epoch 14415/40000, Loss: 1.6870755644049495e-05, Learning Rate: 0.000355\n",
      "Epoch 14416/40000, Loss: 5.909663741476834e-05, Learning Rate: 0.000355\n",
      "Epoch 14417/40000, Loss: 5.894867717870511e-05, Learning Rate: 0.000355\n",
      "Epoch 14418/40000, Loss: 5.886138023925014e-05, Learning Rate: 0.000355\n",
      "Epoch 14419/40000, Loss: 4.249883204465732e-05, Learning Rate: 0.000354\n",
      "Epoch 14420/40000, Loss: 5.91259595239535e-05, Learning Rate: 0.000354\n",
      "Epoch 14421/40000, Loss: 3.6263798392610624e-05, Learning Rate: 0.000354\n",
      "Epoch 14422/40000, Loss: 6.0019232478225604e-05, Learning Rate: 0.000354\n",
      "Epoch 14423/40000, Loss: 3.6488647310761735e-05, Learning Rate: 0.000354\n",
      "Epoch 14424/40000, Loss: 4.3586758692981675e-05, Learning Rate: 0.000354\n",
      "Epoch 14425/40000, Loss: 4.3488489609444514e-05, Learning Rate: 0.000354\n",
      "Epoch 14426/40000, Loss: 1.6439449609606527e-05, Learning Rate: 0.000354\n",
      "Epoch 14427/40000, Loss: 5.9096477343700826e-05, Learning Rate: 0.000354\n",
      "Epoch 14428/40000, Loss: 6.928038783371449e-05, Learning Rate: 0.000354\n",
      "Epoch 14429/40000, Loss: 4.3591626308625564e-05, Learning Rate: 0.000354\n",
      "Epoch 14430/40000, Loss: 7.001097401371226e-05, Learning Rate: 0.000354\n",
      "Epoch 14431/40000, Loss: 3.627449041232467e-05, Learning Rate: 0.000354\n",
      "Epoch 14432/40000, Loss: 1.6653011698508635e-05, Learning Rate: 0.000354\n",
      "Epoch 14433/40000, Loss: 3.6340064980322495e-05, Learning Rate: 0.000354\n",
      "Epoch 14434/40000, Loss: 5.867507206858136e-05, Learning Rate: 0.000354\n",
      "Epoch 14435/40000, Loss: 4.3819378333864734e-05, Learning Rate: 0.000354\n",
      "Epoch 14436/40000, Loss: 1.6656777006573975e-05, Learning Rate: 0.000354\n",
      "Epoch 14437/40000, Loss: 4.234207881381735e-05, Learning Rate: 0.000354\n",
      "Epoch 14438/40000, Loss: 1.683159644016996e-05, Learning Rate: 0.000354\n",
      "Epoch 14439/40000, Loss: 3.6289202398620546e-05, Learning Rate: 0.000354\n",
      "Epoch 14440/40000, Loss: 6.981522165006027e-05, Learning Rate: 0.000354\n",
      "Epoch 14441/40000, Loss: 6.941881292732432e-05, Learning Rate: 0.000354\n",
      "Epoch 14442/40000, Loss: 4.235417873132974e-05, Learning Rate: 0.000353\n",
      "Epoch 14443/40000, Loss: 4.247867036610842e-05, Learning Rate: 0.000353\n",
      "Epoch 14444/40000, Loss: 6.943627522559837e-05, Learning Rate: 0.000353\n",
      "Epoch 14445/40000, Loss: 6.927770300535485e-05, Learning Rate: 0.000353\n",
      "Epoch 14446/40000, Loss: 6.931373354746029e-05, Learning Rate: 0.000353\n",
      "Epoch 14447/40000, Loss: 1.6715257515897974e-05, Learning Rate: 0.000353\n",
      "Epoch 14448/40000, Loss: 5.884671554667875e-05, Learning Rate: 0.000353\n",
      "Epoch 14449/40000, Loss: 4.423822247190401e-05, Learning Rate: 0.000353\n",
      "Epoch 14450/40000, Loss: 4.4403073843568563e-05, Learning Rate: 0.000353\n",
      "Epoch 14451/40000, Loss: 7.089916471159086e-05, Learning Rate: 0.000353\n",
      "Epoch 14452/40000, Loss: 4.465228266781196e-05, Learning Rate: 0.000353\n",
      "Epoch 14453/40000, Loss: 7.505964458687231e-05, Learning Rate: 0.000353\n",
      "Epoch 14454/40000, Loss: 5.9553232858888805e-05, Learning Rate: 0.000353\n",
      "Epoch 14455/40000, Loss: 4.468086262932047e-05, Learning Rate: 0.000353\n",
      "Epoch 14456/40000, Loss: 4.51656524091959e-05, Learning Rate: 0.000353\n",
      "Epoch 14457/40000, Loss: 7.074319728417322e-05, Learning Rate: 0.000353\n",
      "Epoch 14458/40000, Loss: 7.031100540189072e-05, Learning Rate: 0.000353\n",
      "Epoch 14459/40000, Loss: 4.313630051910877e-05, Learning Rate: 0.000353\n",
      "Epoch 14460/40000, Loss: 1.945302756212186e-05, Learning Rate: 0.000353\n",
      "Epoch 14461/40000, Loss: 7.352268585236743e-05, Learning Rate: 0.000353\n",
      "Epoch 14462/40000, Loss: 4.747145430883393e-05, Learning Rate: 0.000353\n",
      "Epoch 14463/40000, Loss: 6.168086838442832e-05, Learning Rate: 0.000353\n",
      "Epoch 14464/40000, Loss: 2.0789100744877942e-05, Learning Rate: 0.000353\n",
      "Epoch 14465/40000, Loss: 3.99486962123774e-05, Learning Rate: 0.000353\n",
      "Epoch 14466/40000, Loss: 4.633305798051879e-05, Learning Rate: 0.000352\n",
      "Epoch 14467/40000, Loss: 4.538193388725631e-05, Learning Rate: 0.000352\n",
      "Epoch 14468/40000, Loss: 7.551257294835523e-05, Learning Rate: 0.000352\n",
      "Epoch 14469/40000, Loss: 5.094167863717303e-05, Learning Rate: 0.000352\n",
      "Epoch 14470/40000, Loss: 4.837836968363263e-05, Learning Rate: 0.000352\n",
      "Epoch 14471/40000, Loss: 2.090083398798015e-05, Learning Rate: 0.000352\n",
      "Epoch 14472/40000, Loss: 6.23718515271321e-05, Learning Rate: 0.000352\n",
      "Epoch 14473/40000, Loss: 7.390369137283415e-05, Learning Rate: 0.000352\n",
      "Epoch 14474/40000, Loss: 7.181358523666859e-05, Learning Rate: 0.000352\n",
      "Epoch 14475/40000, Loss: 3.736175494850613e-05, Learning Rate: 0.000352\n",
      "Epoch 14476/40000, Loss: 7.180590182542801e-05, Learning Rate: 0.000352\n",
      "Epoch 14477/40000, Loss: 3.747516529983841e-05, Learning Rate: 0.000352\n",
      "Epoch 14478/40000, Loss: 4.391132097225636e-05, Learning Rate: 0.000352\n",
      "Epoch 14479/40000, Loss: 6.221899820957333e-05, Learning Rate: 0.000352\n",
      "Epoch 14480/40000, Loss: 7.270521018654108e-05, Learning Rate: 0.000352\n",
      "Epoch 14481/40000, Loss: 2.1902240405324847e-05, Learning Rate: 0.000352\n",
      "Epoch 14482/40000, Loss: 5.2122617489658296e-05, Learning Rate: 0.000352\n",
      "Epoch 14483/40000, Loss: 5.02428702020552e-05, Learning Rate: 0.000352\n",
      "Epoch 14484/40000, Loss: 7.436276791850105e-05, Learning Rate: 0.000352\n",
      "Epoch 14485/40000, Loss: 4.104666004423052e-05, Learning Rate: 0.000352\n",
      "Epoch 14486/40000, Loss: 4.801422619493678e-05, Learning Rate: 0.000352\n",
      "Epoch 14487/40000, Loss: 7.901922799646854e-05, Learning Rate: 0.000352\n",
      "Epoch 14488/40000, Loss: 4.731498847831972e-05, Learning Rate: 0.000352\n",
      "Epoch 14489/40000, Loss: 4.4027612602803856e-05, Learning Rate: 0.000351\n",
      "Epoch 14490/40000, Loss: 2.8399546863511205e-05, Learning Rate: 0.000351\n",
      "Epoch 14491/40000, Loss: 0.00019036080630030483, Learning Rate: 0.000351\n",
      "Epoch 14492/40000, Loss: 8.090199844446033e-05, Learning Rate: 0.000351\n",
      "Epoch 14493/40000, Loss: 4.9525409849593416e-05, Learning Rate: 0.000351\n",
      "Epoch 14494/40000, Loss: 4.630264811567031e-05, Learning Rate: 0.000351\n",
      "Epoch 14495/40000, Loss: 4.2086139728780836e-05, Learning Rate: 0.000351\n",
      "Epoch 14496/40000, Loss: 4.435621667653322e-05, Learning Rate: 0.000351\n",
      "Epoch 14497/40000, Loss: 6.888085772516206e-05, Learning Rate: 0.000351\n",
      "Epoch 14498/40000, Loss: 8.23134250822477e-05, Learning Rate: 0.000351\n",
      "Epoch 14499/40000, Loss: 4.8445272113895044e-05, Learning Rate: 0.000351\n",
      "Epoch 14500/40000, Loss: 5.520479680853896e-05, Learning Rate: 0.000351\n",
      "Epoch 14501/40000, Loss: 7.928200648166239e-05, Learning Rate: 0.000351\n",
      "Epoch 14502/40000, Loss: 3.8881596992723644e-05, Learning Rate: 0.000351\n",
      "Epoch 14503/40000, Loss: 5.714796861866489e-05, Learning Rate: 0.000351\n",
      "Epoch 14504/40000, Loss: 7.29196472093463e-05, Learning Rate: 0.000351\n",
      "Epoch 14505/40000, Loss: 3.9790011214790866e-05, Learning Rate: 0.000351\n",
      "Epoch 14506/40000, Loss: 5.671174221788533e-05, Learning Rate: 0.000351\n",
      "Epoch 14507/40000, Loss: 2.2864094717078842e-05, Learning Rate: 0.000351\n",
      "Epoch 14508/40000, Loss: 6.435891555156559e-05, Learning Rate: 0.000351\n",
      "Epoch 14509/40000, Loss: 2.2474305296782404e-05, Learning Rate: 0.000351\n",
      "Epoch 14510/40000, Loss: 7.492301665479317e-05, Learning Rate: 0.000351\n",
      "Epoch 14511/40000, Loss: 6.362466228893027e-05, Learning Rate: 0.000351\n",
      "Epoch 14512/40000, Loss: 2.13553175854031e-05, Learning Rate: 0.000351\n",
      "Epoch 14513/40000, Loss: 4.4951906602364033e-05, Learning Rate: 0.000350\n",
      "Epoch 14514/40000, Loss: 4.98075969517231e-05, Learning Rate: 0.000350\n",
      "Epoch 14515/40000, Loss: 7.226060552056879e-05, Learning Rate: 0.000350\n",
      "Epoch 14516/40000, Loss: 3.975582148996182e-05, Learning Rate: 0.000350\n",
      "Epoch 14517/40000, Loss: 7.815475692041218e-05, Learning Rate: 0.000350\n",
      "Epoch 14518/40000, Loss: 4.4887696276418865e-05, Learning Rate: 0.000350\n",
      "Epoch 14519/40000, Loss: 7.863572682254016e-05, Learning Rate: 0.000350\n",
      "Epoch 14520/40000, Loss: 5.490101102623157e-05, Learning Rate: 0.000350\n",
      "Epoch 14521/40000, Loss: 4.456153328646906e-05, Learning Rate: 0.000350\n",
      "Epoch 14522/40000, Loss: 3.929626473109238e-05, Learning Rate: 0.000350\n",
      "Epoch 14523/40000, Loss: 5.033004708820954e-05, Learning Rate: 0.000350\n",
      "Epoch 14524/40000, Loss: 6.50966030661948e-05, Learning Rate: 0.000350\n",
      "Epoch 14525/40000, Loss: 5.776063335360959e-05, Learning Rate: 0.000350\n",
      "Epoch 14526/40000, Loss: 6.682160892523825e-05, Learning Rate: 0.000350\n",
      "Epoch 14527/40000, Loss: 4.455686575965956e-05, Learning Rate: 0.000350\n",
      "Epoch 14528/40000, Loss: 1.8608523532748222e-05, Learning Rate: 0.000350\n",
      "Epoch 14529/40000, Loss: 2.4159755412256345e-05, Learning Rate: 0.000350\n",
      "Epoch 14530/40000, Loss: 6.521801697090268e-05, Learning Rate: 0.000350\n",
      "Epoch 14531/40000, Loss: 3.961445327149704e-05, Learning Rate: 0.000350\n",
      "Epoch 14532/40000, Loss: 7.242779975058511e-05, Learning Rate: 0.000350\n",
      "Epoch 14533/40000, Loss: 6.0267986555118114e-05, Learning Rate: 0.000350\n",
      "Epoch 14534/40000, Loss: 7.474965241272002e-05, Learning Rate: 0.000350\n",
      "Epoch 14535/40000, Loss: 3.8112240872578695e-05, Learning Rate: 0.000350\n",
      "Epoch 14536/40000, Loss: 4.4969277951167896e-05, Learning Rate: 0.000350\n",
      "Epoch 14537/40000, Loss: 6.491688691312447e-05, Learning Rate: 0.000349\n",
      "Epoch 14538/40000, Loss: 6.027809649822302e-05, Learning Rate: 0.000349\n",
      "Epoch 14539/40000, Loss: 4.477819675230421e-05, Learning Rate: 0.000349\n",
      "Epoch 14540/40000, Loss: 4.2724062950583175e-05, Learning Rate: 0.000349\n",
      "Epoch 14541/40000, Loss: 4.2564173782011494e-05, Learning Rate: 0.000349\n",
      "Epoch 14542/40000, Loss: 1.7128000763477758e-05, Learning Rate: 0.000349\n",
      "Epoch 14543/40000, Loss: 7.025899685686454e-05, Learning Rate: 0.000349\n",
      "Epoch 14544/40000, Loss: 4.2662242776714265e-05, Learning Rate: 0.000349\n",
      "Epoch 14545/40000, Loss: 4.2526018660282716e-05, Learning Rate: 0.000349\n",
      "Epoch 14546/40000, Loss: 5.889499152544886e-05, Learning Rate: 0.000349\n",
      "Epoch 14547/40000, Loss: 4.27669619966764e-05, Learning Rate: 0.000349\n",
      "Epoch 14548/40000, Loss: 3.666069096652791e-05, Learning Rate: 0.000349\n",
      "Epoch 14549/40000, Loss: 5.908910679863766e-05, Learning Rate: 0.000349\n",
      "Epoch 14550/40000, Loss: 7.061885844450444e-05, Learning Rate: 0.000349\n",
      "Epoch 14551/40000, Loss: 1.6882986528798938e-05, Learning Rate: 0.000349\n",
      "Epoch 14552/40000, Loss: 4.402558988658711e-05, Learning Rate: 0.000349\n",
      "Epoch 14553/40000, Loss: 3.631362778833136e-05, Learning Rate: 0.000349\n",
      "Epoch 14554/40000, Loss: 6.983975617913529e-05, Learning Rate: 0.000349\n",
      "Epoch 14555/40000, Loss: 1.6620015230728313e-05, Learning Rate: 0.000349\n",
      "Epoch 14556/40000, Loss: 1.6871072148205712e-05, Learning Rate: 0.000349\n",
      "Epoch 14557/40000, Loss: 4.29965257353615e-05, Learning Rate: 0.000349\n",
      "Epoch 14558/40000, Loss: 3.635414032032713e-05, Learning Rate: 0.000349\n",
      "Epoch 14559/40000, Loss: 1.7014888726407662e-05, Learning Rate: 0.000349\n",
      "Epoch 14560/40000, Loss: 5.844472980243154e-05, Learning Rate: 0.000349\n",
      "Epoch 14561/40000, Loss: 1.734286706778221e-05, Learning Rate: 0.000348\n",
      "Epoch 14562/40000, Loss: 3.645939796115272e-05, Learning Rate: 0.000348\n",
      "Epoch 14563/40000, Loss: 4.395256837597117e-05, Learning Rate: 0.000348\n",
      "Epoch 14564/40000, Loss: 4.375070420792326e-05, Learning Rate: 0.000348\n",
      "Epoch 14565/40000, Loss: 1.7122603821917437e-05, Learning Rate: 0.000348\n",
      "Epoch 14566/40000, Loss: 6.882782327011228e-05, Learning Rate: 0.000348\n",
      "Epoch 14567/40000, Loss: 1.7451238818466663e-05, Learning Rate: 0.000348\n",
      "Epoch 14568/40000, Loss: 3.7535613955697045e-05, Learning Rate: 0.000348\n",
      "Epoch 14569/40000, Loss: 1.8355594875174575e-05, Learning Rate: 0.000348\n",
      "Epoch 14570/40000, Loss: 4.503338277572766e-05, Learning Rate: 0.000348\n",
      "Epoch 14571/40000, Loss: 2.5085162633331493e-05, Learning Rate: 0.000348\n",
      "Epoch 14572/40000, Loss: 1.9165931007592008e-05, Learning Rate: 0.000348\n",
      "Epoch 14573/40000, Loss: 4.5011809561401606e-05, Learning Rate: 0.000348\n",
      "Epoch 14574/40000, Loss: 4.2609379306668416e-05, Learning Rate: 0.000348\n",
      "Epoch 14575/40000, Loss: 4.4702836021315306e-05, Learning Rate: 0.000348\n",
      "Epoch 14576/40000, Loss: 3.718031075550243e-05, Learning Rate: 0.000348\n",
      "Epoch 14577/40000, Loss: 4.432341665960848e-05, Learning Rate: 0.000348\n",
      "Epoch 14578/40000, Loss: 6.935028795851395e-05, Learning Rate: 0.000348\n",
      "Epoch 14579/40000, Loss: 5.904755016672425e-05, Learning Rate: 0.000348\n",
      "Epoch 14580/40000, Loss: 5.864834020030685e-05, Learning Rate: 0.000348\n",
      "Epoch 14581/40000, Loss: 5.86028145335149e-05, Learning Rate: 0.000348\n",
      "Epoch 14582/40000, Loss: 6.899152504047379e-05, Learning Rate: 0.000348\n",
      "Epoch 14583/40000, Loss: 6.872805533930659e-05, Learning Rate: 0.000348\n",
      "Epoch 14584/40000, Loss: 3.638265479821712e-05, Learning Rate: 0.000348\n",
      "Epoch 14585/40000, Loss: 5.840127414558083e-05, Learning Rate: 0.000347\n",
      "Epoch 14586/40000, Loss: 6.904340261826292e-05, Learning Rate: 0.000347\n",
      "Epoch 14587/40000, Loss: 5.859407610842027e-05, Learning Rate: 0.000347\n",
      "Epoch 14588/40000, Loss: 4.2206014768453315e-05, Learning Rate: 0.000347\n",
      "Epoch 14589/40000, Loss: 4.341438034316525e-05, Learning Rate: 0.000347\n",
      "Epoch 14590/40000, Loss: 3.602253127610311e-05, Learning Rate: 0.000347\n",
      "Epoch 14591/40000, Loss: 1.6516023606527597e-05, Learning Rate: 0.000347\n",
      "Epoch 14592/40000, Loss: 4.342941247159615e-05, Learning Rate: 0.000347\n",
      "Epoch 14593/40000, Loss: 4.2124567698920146e-05, Learning Rate: 0.000347\n",
      "Epoch 14594/40000, Loss: 4.210584665997885e-05, Learning Rate: 0.000347\n",
      "Epoch 14595/40000, Loss: 4.205728691886179e-05, Learning Rate: 0.000347\n",
      "Epoch 14596/40000, Loss: 4.218549656798132e-05, Learning Rate: 0.000347\n",
      "Epoch 14597/40000, Loss: 6.877988926135004e-05, Learning Rate: 0.000347\n",
      "Epoch 14598/40000, Loss: 1.6386855349992402e-05, Learning Rate: 0.000347\n",
      "Epoch 14599/40000, Loss: 6.883370951982215e-05, Learning Rate: 0.000347\n",
      "Epoch 14600/40000, Loss: 1.6391803001170047e-05, Learning Rate: 0.000347\n",
      "Epoch 14601/40000, Loss: 3.612396176322363e-05, Learning Rate: 0.000347\n",
      "Epoch 14602/40000, Loss: 3.618107803049497e-05, Learning Rate: 0.000347\n",
      "Epoch 14603/40000, Loss: 1.6334379324689507e-05, Learning Rate: 0.000347\n",
      "Epoch 14604/40000, Loss: 6.925902562215924e-05, Learning Rate: 0.000347\n",
      "Epoch 14605/40000, Loss: 3.621662108344026e-05, Learning Rate: 0.000347\n",
      "Epoch 14606/40000, Loss: 4.246924436301924e-05, Learning Rate: 0.000347\n",
      "Epoch 14607/40000, Loss: 1.6725609384593554e-05, Learning Rate: 0.000347\n",
      "Epoch 14608/40000, Loss: 1.656045969866682e-05, Learning Rate: 0.000347\n",
      "Epoch 14609/40000, Loss: 1.6744317690609023e-05, Learning Rate: 0.000346\n",
      "Epoch 14610/40000, Loss: 5.921345655224286e-05, Learning Rate: 0.000346\n",
      "Epoch 14611/40000, Loss: 1.671670179348439e-05, Learning Rate: 0.000346\n",
      "Epoch 14612/40000, Loss: 3.6950455978512764e-05, Learning Rate: 0.000346\n",
      "Epoch 14613/40000, Loss: 7.036265742499381e-05, Learning Rate: 0.000346\n",
      "Epoch 14614/40000, Loss: 4.292364610591903e-05, Learning Rate: 0.000346\n",
      "Epoch 14615/40000, Loss: 4.574263948597945e-05, Learning Rate: 0.000346\n",
      "Epoch 14616/40000, Loss: 3.678086432046257e-05, Learning Rate: 0.000346\n",
      "Epoch 14617/40000, Loss: 4.2502339056227356e-05, Learning Rate: 0.000346\n",
      "Epoch 14618/40000, Loss: 4.223852010909468e-05, Learning Rate: 0.000346\n",
      "Epoch 14619/40000, Loss: 4.4119304220657796e-05, Learning Rate: 0.000346\n",
      "Epoch 14620/40000, Loss: 3.6531502701109275e-05, Learning Rate: 0.000346\n",
      "Epoch 14621/40000, Loss: 1.6736337784095667e-05, Learning Rate: 0.000346\n",
      "Epoch 14622/40000, Loss: 4.221332346787676e-05, Learning Rate: 0.000346\n",
      "Epoch 14623/40000, Loss: 3.6335412005428225e-05, Learning Rate: 0.000346\n",
      "Epoch 14624/40000, Loss: 5.8916499256156385e-05, Learning Rate: 0.000346\n",
      "Epoch 14625/40000, Loss: 7.114143227227032e-05, Learning Rate: 0.000346\n",
      "Epoch 14626/40000, Loss: 6.254123582039028e-05, Learning Rate: 0.000346\n",
      "Epoch 14627/40000, Loss: 4.2555864638416097e-05, Learning Rate: 0.000346\n",
      "Epoch 14628/40000, Loss: 1.664819137658924e-05, Learning Rate: 0.000346\n",
      "Epoch 14629/40000, Loss: 3.646178083727136e-05, Learning Rate: 0.000346\n",
      "Epoch 14630/40000, Loss: 3.628983904491179e-05, Learning Rate: 0.000346\n",
      "Epoch 14631/40000, Loss: 7.08106454112567e-05, Learning Rate: 0.000346\n",
      "Epoch 14632/40000, Loss: 4.407052983879112e-05, Learning Rate: 0.000346\n",
      "Epoch 14633/40000, Loss: 4.325729241827503e-05, Learning Rate: 0.000345\n",
      "Epoch 14634/40000, Loss: 4.2888132156804204e-05, Learning Rate: 0.000345\n",
      "Epoch 14635/40000, Loss: 7.025073136901483e-05, Learning Rate: 0.000345\n",
      "Epoch 14636/40000, Loss: 6.937399302842095e-05, Learning Rate: 0.000345\n",
      "Epoch 14637/40000, Loss: 1.734832039801404e-05, Learning Rate: 0.000345\n",
      "Epoch 14638/40000, Loss: 4.333538527134806e-05, Learning Rate: 0.000345\n",
      "Epoch 14639/40000, Loss: 4.3504460336407647e-05, Learning Rate: 0.000345\n",
      "Epoch 14640/40000, Loss: 6.145754014141858e-05, Learning Rate: 0.000345\n",
      "Epoch 14641/40000, Loss: 4.336800702731125e-05, Learning Rate: 0.000345\n",
      "Epoch 14642/40000, Loss: 7.275373354787007e-05, Learning Rate: 0.000345\n",
      "Epoch 14643/40000, Loss: 7.180973625509068e-05, Learning Rate: 0.000345\n",
      "Epoch 14644/40000, Loss: 3.866484985337593e-05, Learning Rate: 0.000345\n",
      "Epoch 14645/40000, Loss: 2.0324558136053383e-05, Learning Rate: 0.000345\n",
      "Epoch 14646/40000, Loss: 7.390954851871356e-05, Learning Rate: 0.000345\n",
      "Epoch 14647/40000, Loss: 4.611643453245051e-05, Learning Rate: 0.000345\n",
      "Epoch 14648/40000, Loss: 3.888630453730002e-05, Learning Rate: 0.000345\n",
      "Epoch 14649/40000, Loss: 1.900031929835677e-05, Learning Rate: 0.000345\n",
      "Epoch 14650/40000, Loss: 1.8194643416791223e-05, Learning Rate: 0.000345\n",
      "Epoch 14651/40000, Loss: 7.378226291621104e-05, Learning Rate: 0.000345\n",
      "Epoch 14652/40000, Loss: 4.6550016122637317e-05, Learning Rate: 0.000345\n",
      "Epoch 14653/40000, Loss: 4.652527786674909e-05, Learning Rate: 0.000345\n",
      "Epoch 14654/40000, Loss: 0.00010021217894973233, Learning Rate: 0.000345\n",
      "Epoch 14655/40000, Loss: 6.0725687944795936e-05, Learning Rate: 0.000345\n",
      "Epoch 14656/40000, Loss: 4.4317122956272215e-05, Learning Rate: 0.000345\n",
      "Epoch 14657/40000, Loss: 0.00012649186828639358, Learning Rate: 0.000344\n",
      "Epoch 14658/40000, Loss: 5.673559644492343e-05, Learning Rate: 0.000344\n",
      "Epoch 14659/40000, Loss: 5.54118087165989e-05, Learning Rate: 0.000344\n",
      "Epoch 14660/40000, Loss: 6.020926230121404e-05, Learning Rate: 0.000344\n",
      "Epoch 14661/40000, Loss: 7.948847633088008e-05, Learning Rate: 0.000344\n",
      "Epoch 14662/40000, Loss: 6.0582187870750204e-05, Learning Rate: 0.000344\n",
      "Epoch 14663/40000, Loss: 6.273783219512552e-05, Learning Rate: 0.000344\n",
      "Epoch 14664/40000, Loss: 8.051760232774541e-05, Learning Rate: 0.000344\n",
      "Epoch 14665/40000, Loss: 6.584532820852473e-05, Learning Rate: 0.000344\n",
      "Epoch 14666/40000, Loss: 5.0895770982606336e-05, Learning Rate: 0.000344\n",
      "Epoch 14667/40000, Loss: 4.5982684241607785e-05, Learning Rate: 0.000344\n",
      "Epoch 14668/40000, Loss: 4.3260624806862324e-05, Learning Rate: 0.000344\n",
      "Epoch 14669/40000, Loss: 4.826932854484767e-05, Learning Rate: 0.000344\n",
      "Epoch 14670/40000, Loss: 4.6266537538031116e-05, Learning Rate: 0.000344\n",
      "Epoch 14671/40000, Loss: 4.356016142992303e-05, Learning Rate: 0.000344\n",
      "Epoch 14672/40000, Loss: 3.68627006537281e-05, Learning Rate: 0.000344\n",
      "Epoch 14673/40000, Loss: 3.626563920988701e-05, Learning Rate: 0.000344\n",
      "Epoch 14674/40000, Loss: 3.626064062700607e-05, Learning Rate: 0.000344\n",
      "Epoch 14675/40000, Loss: 1.6950129065662622e-05, Learning Rate: 0.000344\n",
      "Epoch 14676/40000, Loss: 4.3935862777289e-05, Learning Rate: 0.000344\n",
      "Epoch 14677/40000, Loss: 6.949233647901565e-05, Learning Rate: 0.000344\n",
      "Epoch 14678/40000, Loss: 4.255174644640647e-05, Learning Rate: 0.000344\n",
      "Epoch 14679/40000, Loss: 1.671012068982236e-05, Learning Rate: 0.000344\n",
      "Epoch 14680/40000, Loss: 6.930946256034076e-05, Learning Rate: 0.000344\n",
      "Epoch 14681/40000, Loss: 4.2301770008634776e-05, Learning Rate: 0.000343\n",
      "Epoch 14682/40000, Loss: 4.3875428673345596e-05, Learning Rate: 0.000343\n",
      "Epoch 14683/40000, Loss: 4.3978601752314717e-05, Learning Rate: 0.000343\n",
      "Epoch 14684/40000, Loss: 4.350001836428419e-05, Learning Rate: 0.000343\n",
      "Epoch 14685/40000, Loss: 4.373133560875431e-05, Learning Rate: 0.000343\n",
      "Epoch 14686/40000, Loss: 4.2590618249960244e-05, Learning Rate: 0.000343\n",
      "Epoch 14687/40000, Loss: 4.3364390876377e-05, Learning Rate: 0.000343\n",
      "Epoch 14688/40000, Loss: 5.878458250663243e-05, Learning Rate: 0.000343\n",
      "Epoch 14689/40000, Loss: 6.861400470370427e-05, Learning Rate: 0.000343\n",
      "Epoch 14690/40000, Loss: 5.811296068714e-05, Learning Rate: 0.000343\n",
      "Epoch 14691/40000, Loss: 3.587290120776743e-05, Learning Rate: 0.000343\n",
      "Epoch 14692/40000, Loss: 1.641353992454242e-05, Learning Rate: 0.000343\n",
      "Epoch 14693/40000, Loss: 4.212579733575694e-05, Learning Rate: 0.000343\n",
      "Epoch 14694/40000, Loss: 3.599322008085437e-05, Learning Rate: 0.000343\n",
      "Epoch 14695/40000, Loss: 6.86174098518677e-05, Learning Rate: 0.000343\n",
      "Epoch 14696/40000, Loss: 6.881504668854177e-05, Learning Rate: 0.000343\n",
      "Epoch 14697/40000, Loss: 1.64728317031404e-05, Learning Rate: 0.000343\n",
      "Epoch 14698/40000, Loss: 1.6336445696651936e-05, Learning Rate: 0.000343\n",
      "Epoch 14699/40000, Loss: 4.3672607716871426e-05, Learning Rate: 0.000343\n",
      "Epoch 14700/40000, Loss: 5.8389818150317296e-05, Learning Rate: 0.000343\n",
      "Epoch 14701/40000, Loss: 6.851257785456255e-05, Learning Rate: 0.000343\n",
      "Epoch 14702/40000, Loss: 5.8186673413729295e-05, Learning Rate: 0.000343\n",
      "Epoch 14703/40000, Loss: 1.6411879187216982e-05, Learning Rate: 0.000343\n",
      "Epoch 14704/40000, Loss: 3.588041727198288e-05, Learning Rate: 0.000343\n",
      "Epoch 14705/40000, Loss: 3.590996129787527e-05, Learning Rate: 0.000343\n",
      "Epoch 14706/40000, Loss: 4.19536154367961e-05, Learning Rate: 0.000342\n",
      "Epoch 14707/40000, Loss: 6.85169143253006e-05, Learning Rate: 0.000342\n",
      "Epoch 14708/40000, Loss: 6.837761611677706e-05, Learning Rate: 0.000342\n",
      "Epoch 14709/40000, Loss: 6.845077587058768e-05, Learning Rate: 0.000342\n",
      "Epoch 14710/40000, Loss: 4.335894846008159e-05, Learning Rate: 0.000342\n",
      "Epoch 14711/40000, Loss: 5.847762804478407e-05, Learning Rate: 0.000342\n",
      "Epoch 14712/40000, Loss: 6.834755913587287e-05, Learning Rate: 0.000342\n",
      "Epoch 14713/40000, Loss: 6.853233935544267e-05, Learning Rate: 0.000342\n",
      "Epoch 14714/40000, Loss: 3.584843943826854e-05, Learning Rate: 0.000342\n",
      "Epoch 14715/40000, Loss: 5.8309626183472574e-05, Learning Rate: 0.000342\n",
      "Epoch 14716/40000, Loss: 6.850334466435015e-05, Learning Rate: 0.000342\n",
      "Epoch 14717/40000, Loss: 5.813270399812609e-05, Learning Rate: 0.000342\n",
      "Epoch 14718/40000, Loss: 4.33076020271983e-05, Learning Rate: 0.000342\n",
      "Epoch 14719/40000, Loss: 4.177787559456192e-05, Learning Rate: 0.000342\n",
      "Epoch 14720/40000, Loss: 4.190077379462309e-05, Learning Rate: 0.000342\n",
      "Epoch 14721/40000, Loss: 5.801200677524321e-05, Learning Rate: 0.000342\n",
      "Epoch 14722/40000, Loss: 4.198795068077743e-05, Learning Rate: 0.000342\n",
      "Epoch 14723/40000, Loss: 4.199141403660178e-05, Learning Rate: 0.000342\n",
      "Epoch 14724/40000, Loss: 4.343339242041111e-05, Learning Rate: 0.000342\n",
      "Epoch 14725/40000, Loss: 4.204924698569812e-05, Learning Rate: 0.000342\n",
      "Epoch 14726/40000, Loss: 5.818453064421192e-05, Learning Rate: 0.000342\n",
      "Epoch 14727/40000, Loss: 3.600346462917514e-05, Learning Rate: 0.000342\n",
      "Epoch 14728/40000, Loss: 1.6426258298452012e-05, Learning Rate: 0.000342\n",
      "Epoch 14729/40000, Loss: 4.259323395672254e-05, Learning Rate: 0.000342\n",
      "Epoch 14730/40000, Loss: 4.353416079538874e-05, Learning Rate: 0.000341\n",
      "Epoch 14731/40000, Loss: 5.832669558003545e-05, Learning Rate: 0.000341\n",
      "Epoch 14732/40000, Loss: 4.270761564839631e-05, Learning Rate: 0.000341\n",
      "Epoch 14733/40000, Loss: 4.206938319839537e-05, Learning Rate: 0.000341\n",
      "Epoch 14734/40000, Loss: 1.646751661610324e-05, Learning Rate: 0.000341\n",
      "Epoch 14735/40000, Loss: 4.3476804421516135e-05, Learning Rate: 0.000341\n",
      "Epoch 14736/40000, Loss: 4.212955172988586e-05, Learning Rate: 0.000341\n",
      "Epoch 14737/40000, Loss: 5.8480647567193955e-05, Learning Rate: 0.000341\n",
      "Epoch 14738/40000, Loss: 5.843943654326722e-05, Learning Rate: 0.000341\n",
      "Epoch 14739/40000, Loss: 4.375685966806486e-05, Learning Rate: 0.000341\n",
      "Epoch 14740/40000, Loss: 1.6835458154673688e-05, Learning Rate: 0.000341\n",
      "Epoch 14741/40000, Loss: 5.828257417306304e-05, Learning Rate: 0.000341\n",
      "Epoch 14742/40000, Loss: 4.4366282963892445e-05, Learning Rate: 0.000341\n",
      "Epoch 14743/40000, Loss: 6.879945431137457e-05, Learning Rate: 0.000341\n",
      "Epoch 14744/40000, Loss: 6.898488209117204e-05, Learning Rate: 0.000341\n",
      "Epoch 14745/40000, Loss: 6.869174831081182e-05, Learning Rate: 0.000341\n",
      "Epoch 14746/40000, Loss: 6.872647645650432e-05, Learning Rate: 0.000341\n",
      "Epoch 14747/40000, Loss: 5.9995501942466944e-05, Learning Rate: 0.000341\n",
      "Epoch 14748/40000, Loss: 4.580458698910661e-05, Learning Rate: 0.000341\n",
      "Epoch 14749/40000, Loss: 4.546930722426623e-05, Learning Rate: 0.000341\n",
      "Epoch 14750/40000, Loss: 7.282934529939666e-05, Learning Rate: 0.000341\n",
      "Epoch 14751/40000, Loss: 4.349199298303574e-05, Learning Rate: 0.000341\n",
      "Epoch 14752/40000, Loss: 3.947309232898988e-05, Learning Rate: 0.000341\n",
      "Epoch 14753/40000, Loss: 4.81266833958216e-05, Learning Rate: 0.000341\n",
      "Epoch 14754/40000, Loss: 7.033899601083249e-05, Learning Rate: 0.000340\n",
      "Epoch 14755/40000, Loss: 5.0634371291380376e-05, Learning Rate: 0.000340\n",
      "Epoch 14756/40000, Loss: 2.3025790142128244e-05, Learning Rate: 0.000340\n",
      "Epoch 14757/40000, Loss: 4.054891905980185e-05, Learning Rate: 0.000340\n",
      "Epoch 14758/40000, Loss: 4.712293593911454e-05, Learning Rate: 0.000340\n",
      "Epoch 14759/40000, Loss: 7.420944893965498e-05, Learning Rate: 0.000340\n",
      "Epoch 14760/40000, Loss: 2.181623494834639e-05, Learning Rate: 0.000340\n",
      "Epoch 14761/40000, Loss: 2.0550596673274413e-05, Learning Rate: 0.000340\n",
      "Epoch 14762/40000, Loss: 7.299237768165767e-05, Learning Rate: 0.000340\n",
      "Epoch 14763/40000, Loss: 7.167069270508364e-05, Learning Rate: 0.000340\n",
      "Epoch 14764/40000, Loss: 6.389660120476037e-05, Learning Rate: 0.000340\n",
      "Epoch 14765/40000, Loss: 3.9810525777284056e-05, Learning Rate: 0.000340\n",
      "Epoch 14766/40000, Loss: 5.276742376736365e-05, Learning Rate: 0.000340\n",
      "Epoch 14767/40000, Loss: 2.2042311684344895e-05, Learning Rate: 0.000340\n",
      "Epoch 14768/40000, Loss: 4.7277819248847663e-05, Learning Rate: 0.000340\n",
      "Epoch 14769/40000, Loss: 4.486528996494599e-05, Learning Rate: 0.000340\n",
      "Epoch 14770/40000, Loss: 5.0508981075836346e-05, Learning Rate: 0.000340\n",
      "Epoch 14771/40000, Loss: 6.93442634656094e-05, Learning Rate: 0.000340\n",
      "Epoch 14772/40000, Loss: 4.529842044576071e-05, Learning Rate: 0.000340\n",
      "Epoch 14773/40000, Loss: 8.39278509374708e-05, Learning Rate: 0.000340\n",
      "Epoch 14774/40000, Loss: 4.984572296962142e-05, Learning Rate: 0.000340\n",
      "Epoch 14775/40000, Loss: 8.299684850499034e-05, Learning Rate: 0.000340\n",
      "Epoch 14776/40000, Loss: 3.973978164140135e-05, Learning Rate: 0.000340\n",
      "Epoch 14777/40000, Loss: 7.305218605324626e-05, Learning Rate: 0.000340\n",
      "Epoch 14778/40000, Loss: 6.086485882406123e-05, Learning Rate: 0.000340\n",
      "Epoch 14779/40000, Loss: 3.5633907828014344e-05, Learning Rate: 0.000339\n",
      "Epoch 14780/40000, Loss: 7.918231131043285e-05, Learning Rate: 0.000339\n",
      "Epoch 14781/40000, Loss: 6.421170837711543e-05, Learning Rate: 0.000339\n",
      "Epoch 14782/40000, Loss: 5.135062747285701e-05, Learning Rate: 0.000339\n",
      "Epoch 14783/40000, Loss: 2.4182620109058917e-05, Learning Rate: 0.000339\n",
      "Epoch 14784/40000, Loss: 7.954867760417983e-05, Learning Rate: 0.000339\n",
      "Epoch 14785/40000, Loss: 5.118688932270743e-05, Learning Rate: 0.000339\n",
      "Epoch 14786/40000, Loss: 8.267742668977007e-05, Learning Rate: 0.000339\n",
      "Epoch 14787/40000, Loss: 6.610686978092417e-05, Learning Rate: 0.000339\n",
      "Epoch 14788/40000, Loss: 7.320426084334031e-05, Learning Rate: 0.000339\n",
      "Epoch 14789/40000, Loss: 5.100923226564191e-05, Learning Rate: 0.000339\n",
      "Epoch 14790/40000, Loss: 2.185143603128381e-05, Learning Rate: 0.000339\n",
      "Epoch 14791/40000, Loss: 5.027740553487092e-05, Learning Rate: 0.000339\n",
      "Epoch 14792/40000, Loss: 2.8670139727182686e-05, Learning Rate: 0.000339\n",
      "Epoch 14793/40000, Loss: 7.083914533723146e-05, Learning Rate: 0.000339\n",
      "Epoch 14794/40000, Loss: 3.718894367921166e-05, Learning Rate: 0.000339\n",
      "Epoch 14795/40000, Loss: 4.67673598905094e-05, Learning Rate: 0.000339\n",
      "Epoch 14796/40000, Loss: 1.9172659449395724e-05, Learning Rate: 0.000339\n",
      "Epoch 14797/40000, Loss: 5.269741086522117e-05, Learning Rate: 0.000339\n",
      "Epoch 14798/40000, Loss: 6.057594873709604e-05, Learning Rate: 0.000339\n",
      "Epoch 14799/40000, Loss: 3.707289579324424e-05, Learning Rate: 0.000339\n",
      "Epoch 14800/40000, Loss: 7.034515147097409e-05, Learning Rate: 0.000339\n",
      "Epoch 14801/40000, Loss: 4.0556788007961586e-05, Learning Rate: 0.000339\n",
      "Epoch 14802/40000, Loss: 4.353292752057314e-05, Learning Rate: 0.000339\n",
      "Epoch 14803/40000, Loss: 6.0033129557268694e-05, Learning Rate: 0.000339\n",
      "Epoch 14804/40000, Loss: 1.8434591765981168e-05, Learning Rate: 0.000338\n",
      "Epoch 14805/40000, Loss: 3.649845166364685e-05, Learning Rate: 0.000338\n",
      "Epoch 14806/40000, Loss: 3.590757114579901e-05, Learning Rate: 0.000338\n",
      "Epoch 14807/40000, Loss: 6.894976831972599e-05, Learning Rate: 0.000338\n",
      "Epoch 14808/40000, Loss: 5.9248650359222665e-05, Learning Rate: 0.000338\n",
      "Epoch 14809/40000, Loss: 7.017126335995272e-05, Learning Rate: 0.000338\n",
      "Epoch 14810/40000, Loss: 5.958223846391775e-05, Learning Rate: 0.000338\n",
      "Epoch 14811/40000, Loss: 4.372523835627362e-05, Learning Rate: 0.000338\n",
      "Epoch 14812/40000, Loss: 1.6482919818372466e-05, Learning Rate: 0.000338\n",
      "Epoch 14813/40000, Loss: 5.8366684243083e-05, Learning Rate: 0.000338\n",
      "Epoch 14814/40000, Loss: 4.34492394560948e-05, Learning Rate: 0.000338\n",
      "Epoch 14815/40000, Loss: 1.650090780458413e-05, Learning Rate: 0.000338\n",
      "Epoch 14816/40000, Loss: 4.2058756662299857e-05, Learning Rate: 0.000338\n",
      "Epoch 14817/40000, Loss: 4.364700726000592e-05, Learning Rate: 0.000338\n",
      "Epoch 14818/40000, Loss: 6.867561751278117e-05, Learning Rate: 0.000338\n",
      "Epoch 14819/40000, Loss: 1.620615876163356e-05, Learning Rate: 0.000338\n",
      "Epoch 14820/40000, Loss: 6.825709715485573e-05, Learning Rate: 0.000338\n",
      "Epoch 14821/40000, Loss: 1.6398291336372495e-05, Learning Rate: 0.000338\n",
      "Epoch 14822/40000, Loss: 3.583512079785578e-05, Learning Rate: 0.000338\n",
      "Epoch 14823/40000, Loss: 3.5772882256424055e-05, Learning Rate: 0.000338\n",
      "Epoch 14824/40000, Loss: 4.352247196948156e-05, Learning Rate: 0.000338\n",
      "Epoch 14825/40000, Loss: 5.7813329476630315e-05, Learning Rate: 0.000338\n",
      "Epoch 14826/40000, Loss: 4.177584560238756e-05, Learning Rate: 0.000338\n",
      "Epoch 14827/40000, Loss: 3.567494059097953e-05, Learning Rate: 0.000338\n",
      "Epoch 14828/40000, Loss: 5.788387352367863e-05, Learning Rate: 0.000337\n",
      "Epoch 14829/40000, Loss: 6.813126674387604e-05, Learning Rate: 0.000337\n",
      "Epoch 14830/40000, Loss: 1.6231466361205094e-05, Learning Rate: 0.000337\n",
      "Epoch 14831/40000, Loss: 6.83934340486303e-05, Learning Rate: 0.000337\n",
      "Epoch 14832/40000, Loss: 4.355693090474233e-05, Learning Rate: 0.000337\n",
      "Epoch 14833/40000, Loss: 6.830289930803701e-05, Learning Rate: 0.000337\n",
      "Epoch 14834/40000, Loss: 6.793133798055351e-05, Learning Rate: 0.000337\n",
      "Epoch 14835/40000, Loss: 3.582397403079085e-05, Learning Rate: 0.000337\n",
      "Epoch 14836/40000, Loss: 4.380677637527697e-05, Learning Rate: 0.000337\n",
      "Epoch 14837/40000, Loss: 6.859639688627794e-05, Learning Rate: 0.000337\n",
      "Epoch 14838/40000, Loss: 5.8356370573164895e-05, Learning Rate: 0.000337\n",
      "Epoch 14839/40000, Loss: 4.37368216807954e-05, Learning Rate: 0.000337\n",
      "Epoch 14840/40000, Loss: 4.376432116259821e-05, Learning Rate: 0.000337\n",
      "Epoch 14841/40000, Loss: 3.705811468535103e-05, Learning Rate: 0.000337\n",
      "Epoch 14842/40000, Loss: 5.881943798158318e-05, Learning Rate: 0.000337\n",
      "Epoch 14843/40000, Loss: 5.8785928558791056e-05, Learning Rate: 0.000337\n",
      "Epoch 14844/40000, Loss: 4.4786349462810904e-05, Learning Rate: 0.000337\n",
      "Epoch 14845/40000, Loss: 4.434318179846741e-05, Learning Rate: 0.000337\n",
      "Epoch 14846/40000, Loss: 7.061894575599581e-05, Learning Rate: 0.000337\n",
      "Epoch 14847/40000, Loss: 4.231612183502875e-05, Learning Rate: 0.000337\n",
      "Epoch 14848/40000, Loss: 6.970634422032163e-05, Learning Rate: 0.000337\n",
      "Epoch 14849/40000, Loss: 3.6116653063800186e-05, Learning Rate: 0.000337\n",
      "Epoch 14850/40000, Loss: 4.3284453568048775e-05, Learning Rate: 0.000337\n",
      "Epoch 14851/40000, Loss: 3.80718702217564e-05, Learning Rate: 0.000337\n",
      "Epoch 14852/40000, Loss: 1.6508467524545267e-05, Learning Rate: 0.000337\n",
      "Epoch 14853/40000, Loss: 7.20042735338211e-05, Learning Rate: 0.000336\n",
      "Epoch 14854/40000, Loss: 3.6855773942079395e-05, Learning Rate: 0.000336\n",
      "Epoch 14855/40000, Loss: 1.760130180628039e-05, Learning Rate: 0.000336\n",
      "Epoch 14856/40000, Loss: 4.4828877435065806e-05, Learning Rate: 0.000336\n",
      "Epoch 14857/40000, Loss: 3.653442763607018e-05, Learning Rate: 0.000336\n",
      "Epoch 14858/40000, Loss: 1.7102518540923484e-05, Learning Rate: 0.000336\n",
      "Epoch 14859/40000, Loss: 5.832271199324168e-05, Learning Rate: 0.000336\n",
      "Epoch 14860/40000, Loss: 7.067911064950749e-05, Learning Rate: 0.000336\n",
      "Epoch 14861/40000, Loss: 6.194281013449654e-05, Learning Rate: 0.000336\n",
      "Epoch 14862/40000, Loss: 1.738367063808255e-05, Learning Rate: 0.000336\n",
      "Epoch 14863/40000, Loss: 7.101557275746018e-05, Learning Rate: 0.000336\n",
      "Epoch 14864/40000, Loss: 1.879356750578154e-05, Learning Rate: 0.000336\n",
      "Epoch 14865/40000, Loss: 6.0721162299159914e-05, Learning Rate: 0.000336\n",
      "Epoch 14866/40000, Loss: 6.367579044308513e-05, Learning Rate: 0.000336\n",
      "Epoch 14867/40000, Loss: 7.071710570016876e-05, Learning Rate: 0.000336\n",
      "Epoch 14868/40000, Loss: 4.370702299638651e-05, Learning Rate: 0.000336\n",
      "Epoch 14869/40000, Loss: 3.6804362025577575e-05, Learning Rate: 0.000336\n",
      "Epoch 14870/40000, Loss: 7.721218571532518e-05, Learning Rate: 0.000336\n",
      "Epoch 14871/40000, Loss: 1.7965936422115192e-05, Learning Rate: 0.000336\n",
      "Epoch 14872/40000, Loss: 3.7206940760370344e-05, Learning Rate: 0.000336\n",
      "Epoch 14873/40000, Loss: 3.704440678120591e-05, Learning Rate: 0.000336\n",
      "Epoch 14874/40000, Loss: 4.34749381383881e-05, Learning Rate: 0.000336\n",
      "Epoch 14875/40000, Loss: 3.6943776649422944e-05, Learning Rate: 0.000336\n",
      "Epoch 14876/40000, Loss: 7.076379552017897e-05, Learning Rate: 0.000336\n",
      "Epoch 14877/40000, Loss: 6.0342175856931135e-05, Learning Rate: 0.000336\n",
      "Epoch 14878/40000, Loss: 4.763650576933287e-05, Learning Rate: 0.000335\n",
      "Epoch 14879/40000, Loss: 4.5881286496296525e-05, Learning Rate: 0.000335\n",
      "Epoch 14880/40000, Loss: 6.464870239142329e-05, Learning Rate: 0.000335\n",
      "Epoch 14881/40000, Loss: 7.295967952813953e-05, Learning Rate: 0.000335\n",
      "Epoch 14882/40000, Loss: 6.288813892751932e-05, Learning Rate: 0.000335\n",
      "Epoch 14883/40000, Loss: 6.118650344433263e-05, Learning Rate: 0.000335\n",
      "Epoch 14884/40000, Loss: 4.162363620707765e-05, Learning Rate: 0.000335\n",
      "Epoch 14885/40000, Loss: 5.384856558521278e-05, Learning Rate: 0.000335\n",
      "Epoch 14886/40000, Loss: 1.824302671593614e-05, Learning Rate: 0.000335\n",
      "Epoch 14887/40000, Loss: 6.24536769464612e-05, Learning Rate: 0.000335\n",
      "Epoch 14888/40000, Loss: 4.357625948614441e-05, Learning Rate: 0.000335\n",
      "Epoch 14889/40000, Loss: 6.105599459260702e-05, Learning Rate: 0.000335\n",
      "Epoch 14890/40000, Loss: 5.98178812651895e-05, Learning Rate: 0.000335\n",
      "Epoch 14891/40000, Loss: 4.696459654951468e-05, Learning Rate: 0.000335\n",
      "Epoch 14892/40000, Loss: 6.0797603509854525e-05, Learning Rate: 0.000335\n",
      "Epoch 14893/40000, Loss: 4.447537867235951e-05, Learning Rate: 0.000335\n",
      "Epoch 14894/40000, Loss: 3.6564226320479065e-05, Learning Rate: 0.000335\n",
      "Epoch 14895/40000, Loss: 7.010972331045195e-05, Learning Rate: 0.000335\n",
      "Epoch 14896/40000, Loss: 4.703562444774434e-05, Learning Rate: 0.000335\n",
      "Epoch 14897/40000, Loss: 4.590905155055225e-05, Learning Rate: 0.000335\n",
      "Epoch 14898/40000, Loss: 4.301967783248983e-05, Learning Rate: 0.000335\n",
      "Epoch 14899/40000, Loss: 6.933023541932926e-05, Learning Rate: 0.000335\n",
      "Epoch 14900/40000, Loss: 6.0870905144838616e-05, Learning Rate: 0.000335\n",
      "Epoch 14901/40000, Loss: 4.36465852544643e-05, Learning Rate: 0.000335\n",
      "Epoch 14902/40000, Loss: 3.932060644729063e-05, Learning Rate: 0.000335\n",
      "Epoch 14903/40000, Loss: 1.7697555449558422e-05, Learning Rate: 0.000334\n",
      "Epoch 14904/40000, Loss: 6.28943380434066e-05, Learning Rate: 0.000334\n",
      "Epoch 14905/40000, Loss: 4.514094325713813e-05, Learning Rate: 0.000334\n",
      "Epoch 14906/40000, Loss: 3.7234683986753225e-05, Learning Rate: 0.000334\n",
      "Epoch 14907/40000, Loss: 3.66211861546617e-05, Learning Rate: 0.000334\n",
      "Epoch 14908/40000, Loss: 7.296388503164053e-05, Learning Rate: 0.000334\n",
      "Epoch 14909/40000, Loss: 7.088818529155105e-05, Learning Rate: 0.000334\n",
      "Epoch 14910/40000, Loss: 6.95300695952028e-05, Learning Rate: 0.000334\n",
      "Epoch 14911/40000, Loss: 1.7255904822377488e-05, Learning Rate: 0.000334\n",
      "Epoch 14912/40000, Loss: 3.755220313905738e-05, Learning Rate: 0.000334\n",
      "Epoch 14913/40000, Loss: 4.264191011316143e-05, Learning Rate: 0.000334\n",
      "Epoch 14914/40000, Loss: 7.278542761923745e-05, Learning Rate: 0.000334\n",
      "Epoch 14915/40000, Loss: 7.06189384800382e-05, Learning Rate: 0.000334\n",
      "Epoch 14916/40000, Loss: 4.2653839045669883e-05, Learning Rate: 0.000334\n",
      "Epoch 14917/40000, Loss: 4.261310095898807e-05, Learning Rate: 0.000334\n",
      "Epoch 14918/40000, Loss: 3.727481816895306e-05, Learning Rate: 0.000334\n",
      "Epoch 14919/40000, Loss: 1.7182519513880834e-05, Learning Rate: 0.000334\n",
      "Epoch 14920/40000, Loss: 4.3600681237876415e-05, Learning Rate: 0.000334\n",
      "Epoch 14921/40000, Loss: 3.678989378386177e-05, Learning Rate: 0.000334\n",
      "Epoch 14922/40000, Loss: 7.00341333867982e-05, Learning Rate: 0.000334\n",
      "Epoch 14923/40000, Loss: 3.784009822993539e-05, Learning Rate: 0.000334\n",
      "Epoch 14924/40000, Loss: 6.276036583585665e-05, Learning Rate: 0.000334\n",
      "Epoch 14925/40000, Loss: 7.30907340766862e-05, Learning Rate: 0.000334\n",
      "Epoch 14926/40000, Loss: 2.062634848698508e-05, Learning Rate: 0.000334\n",
      "Epoch 14927/40000, Loss: 4.758134309668094e-05, Learning Rate: 0.000334\n",
      "Epoch 14928/40000, Loss: 4.826030635740608e-05, Learning Rate: 0.000333\n",
      "Epoch 14929/40000, Loss: 6.409981870092452e-05, Learning Rate: 0.000333\n",
      "Epoch 14930/40000, Loss: 2.2169477233546786e-05, Learning Rate: 0.000333\n",
      "Epoch 14931/40000, Loss: 3.791385825024918e-05, Learning Rate: 0.000333\n",
      "Epoch 14932/40000, Loss: 3.5884826502297074e-05, Learning Rate: 0.000333\n",
      "Epoch 14933/40000, Loss: 7.907295366749167e-05, Learning Rate: 0.000333\n",
      "Epoch 14934/40000, Loss: 4.8281413910444826e-05, Learning Rate: 0.000333\n",
      "Epoch 14935/40000, Loss: 4.003763024229556e-05, Learning Rate: 0.000333\n",
      "Epoch 14936/40000, Loss: 3.812067734543234e-05, Learning Rate: 0.000333\n",
      "Epoch 14937/40000, Loss: 7.157922664191574e-05, Learning Rate: 0.000333\n",
      "Epoch 14938/40000, Loss: 4.1089122532866895e-05, Learning Rate: 0.000333\n",
      "Epoch 14939/40000, Loss: 6.488126382464543e-05, Learning Rate: 0.000333\n",
      "Epoch 14940/40000, Loss: 4.611031909007579e-05, Learning Rate: 0.000333\n",
      "Epoch 14941/40000, Loss: 4.4151165639050305e-05, Learning Rate: 0.000333\n",
      "Epoch 14942/40000, Loss: 6.300899258349091e-05, Learning Rate: 0.000333\n",
      "Epoch 14943/40000, Loss: 3.846991967293434e-05, Learning Rate: 0.000333\n",
      "Epoch 14944/40000, Loss: 7.20164825906977e-05, Learning Rate: 0.000333\n",
      "Epoch 14945/40000, Loss: 4.555734631139785e-05, Learning Rate: 0.000333\n",
      "Epoch 14946/40000, Loss: 6.252606544876471e-05, Learning Rate: 0.000333\n",
      "Epoch 14947/40000, Loss: 7.382873445749283e-05, Learning Rate: 0.000333\n",
      "Epoch 14948/40000, Loss: 7.356654532486573e-05, Learning Rate: 0.000333\n",
      "Epoch 14949/40000, Loss: 7.68019090173766e-05, Learning Rate: 0.000333\n",
      "Epoch 14950/40000, Loss: 4.00776116293855e-05, Learning Rate: 0.000333\n",
      "Epoch 14951/40000, Loss: 4.56156485597603e-05, Learning Rate: 0.000333\n",
      "Epoch 14952/40000, Loss: 2.0462202883209102e-05, Learning Rate: 0.000333\n",
      "Epoch 14953/40000, Loss: 4.911494397674687e-05, Learning Rate: 0.000332\n",
      "Epoch 14954/40000, Loss: 4.505626202444546e-05, Learning Rate: 0.000332\n",
      "Epoch 14955/40000, Loss: 7.008959073573351e-05, Learning Rate: 0.000332\n",
      "Epoch 14956/40000, Loss: 6.57404525554739e-05, Learning Rate: 0.000332\n",
      "Epoch 14957/40000, Loss: 7.58112728362903e-05, Learning Rate: 0.000332\n",
      "Epoch 14958/40000, Loss: 4.999892917112447e-05, Learning Rate: 0.000332\n",
      "Epoch 14959/40000, Loss: 1.9302930013509467e-05, Learning Rate: 0.000332\n",
      "Epoch 14960/40000, Loss: 5.7999277487397194e-05, Learning Rate: 0.000332\n",
      "Epoch 14961/40000, Loss: 6.98828516760841e-05, Learning Rate: 0.000332\n",
      "Epoch 14962/40000, Loss: 7.93831204646267e-05, Learning Rate: 0.000332\n",
      "Epoch 14963/40000, Loss: 6.3267070800066e-05, Learning Rate: 0.000332\n",
      "Epoch 14964/40000, Loss: 0.00010988026042468846, Learning Rate: 0.000332\n",
      "Epoch 14965/40000, Loss: 2.3917516955407336e-05, Learning Rate: 0.000332\n",
      "Epoch 14966/40000, Loss: 0.00011695239663822576, Learning Rate: 0.000332\n",
      "Epoch 14967/40000, Loss: 3.6375240597408265e-05, Learning Rate: 0.000332\n",
      "Epoch 14968/40000, Loss: 2.455106186971534e-05, Learning Rate: 0.000332\n",
      "Epoch 14969/40000, Loss: 5.029292151448317e-05, Learning Rate: 0.000332\n",
      "Epoch 14970/40000, Loss: 8.283767965622246e-05, Learning Rate: 0.000332\n",
      "Epoch 14971/40000, Loss: 5.6113643950084224e-05, Learning Rate: 0.000332\n",
      "Epoch 14972/40000, Loss: 4.8375619371654466e-05, Learning Rate: 0.000332\n",
      "Epoch 14973/40000, Loss: 6.862000736873597e-05, Learning Rate: 0.000332\n",
      "Epoch 14974/40000, Loss: 2.0983843569410965e-05, Learning Rate: 0.000332\n",
      "Epoch 14975/40000, Loss: 7.661119161639363e-05, Learning Rate: 0.000332\n",
      "Epoch 14976/40000, Loss: 5.406072159530595e-05, Learning Rate: 0.000332\n",
      "Epoch 14977/40000, Loss: 9.298681106884032e-05, Learning Rate: 0.000332\n",
      "Epoch 14978/40000, Loss: 4.934511161991395e-05, Learning Rate: 0.000331\n",
      "Epoch 14979/40000, Loss: 4.028463445138186e-05, Learning Rate: 0.000331\n",
      "Epoch 14980/40000, Loss: 3.818382174358703e-05, Learning Rate: 0.000331\n",
      "Epoch 14981/40000, Loss: 4.823591007152572e-05, Learning Rate: 0.000331\n",
      "Epoch 14982/40000, Loss: 7.146807183744386e-05, Learning Rate: 0.000331\n",
      "Epoch 14983/40000, Loss: 4.556135900202207e-05, Learning Rate: 0.000331\n",
      "Epoch 14984/40000, Loss: 4.668294423026964e-05, Learning Rate: 0.000331\n",
      "Epoch 14985/40000, Loss: 4.251566497259773e-05, Learning Rate: 0.000331\n",
      "Epoch 14986/40000, Loss: 1.7484231648268178e-05, Learning Rate: 0.000331\n",
      "Epoch 14987/40000, Loss: 4.574852209771052e-05, Learning Rate: 0.000331\n",
      "Epoch 14988/40000, Loss: 1.7965148799703456e-05, Learning Rate: 0.000331\n",
      "Epoch 14989/40000, Loss: 6.896704144310206e-05, Learning Rate: 0.000331\n",
      "Epoch 14990/40000, Loss: 1.7104724975069985e-05, Learning Rate: 0.000331\n",
      "Epoch 14991/40000, Loss: 3.660569927887991e-05, Learning Rate: 0.000331\n",
      "Epoch 14992/40000, Loss: 4.311437442083843e-05, Learning Rate: 0.000331\n",
      "Epoch 14993/40000, Loss: 4.2058556573465466e-05, Learning Rate: 0.000331\n",
      "Epoch 14994/40000, Loss: 3.655178443295881e-05, Learning Rate: 0.000331\n",
      "Epoch 14995/40000, Loss: 1.6517729818588123e-05, Learning Rate: 0.000331\n",
      "Epoch 14996/40000, Loss: 6.826422759331763e-05, Learning Rate: 0.000331\n",
      "Epoch 14997/40000, Loss: 3.6236640880815685e-05, Learning Rate: 0.000331\n",
      "Epoch 14998/40000, Loss: 3.5915571061195806e-05, Learning Rate: 0.000331\n",
      "Epoch 14999/40000, Loss: 1.6207146472879685e-05, Learning Rate: 0.000331\n",
      "Epoch 15000/40000, Loss: 4.329895455157384e-05, Learning Rate: 0.000331\n",
      "Epoch 15001/40000, Loss: 1.607369995326735e-05, Learning Rate: 0.000331\n",
      "Epoch 15002/40000, Loss: 5.78183498873841e-05, Learning Rate: 0.000331\n",
      "Epoch 15003/40000, Loss: 6.767716695321724e-05, Learning Rate: 0.000330\n",
      "Epoch 15004/40000, Loss: 5.765465903095901e-05, Learning Rate: 0.000330\n",
      "Epoch 15005/40000, Loss: 3.565954466466792e-05, Learning Rate: 0.000330\n",
      "Epoch 15006/40000, Loss: 4.3322117562638596e-05, Learning Rate: 0.000330\n",
      "Epoch 15007/40000, Loss: 5.7791752624325454e-05, Learning Rate: 0.000330\n",
      "Epoch 15008/40000, Loss: 4.148696461925283e-05, Learning Rate: 0.000330\n",
      "Epoch 15009/40000, Loss: 1.6154712284333073e-05, Learning Rate: 0.000330\n",
      "Epoch 15010/40000, Loss: 3.5610952181741595e-05, Learning Rate: 0.000330\n",
      "Epoch 15011/40000, Loss: 4.3147469114046544e-05, Learning Rate: 0.000330\n",
      "Epoch 15012/40000, Loss: 5.7725879742065445e-05, Learning Rate: 0.000330\n",
      "Epoch 15013/40000, Loss: 6.782208220101893e-05, Learning Rate: 0.000330\n",
      "Epoch 15014/40000, Loss: 4.3131305574206635e-05, Learning Rate: 0.000330\n",
      "Epoch 15015/40000, Loss: 4.1558534576324746e-05, Learning Rate: 0.000330\n",
      "Epoch 15016/40000, Loss: 4.159560558036901e-05, Learning Rate: 0.000330\n",
      "Epoch 15017/40000, Loss: 3.569407272152603e-05, Learning Rate: 0.000330\n",
      "Epoch 15018/40000, Loss: 3.559034666977823e-05, Learning Rate: 0.000330\n",
      "Epoch 15019/40000, Loss: 3.552165435394272e-05, Learning Rate: 0.000330\n",
      "Epoch 15020/40000, Loss: 5.7638590078568086e-05, Learning Rate: 0.000330\n",
      "Epoch 15021/40000, Loss: 1.600511313881725e-05, Learning Rate: 0.000330\n",
      "Epoch 15022/40000, Loss: 3.5517899959813803e-05, Learning Rate: 0.000330\n",
      "Epoch 15023/40000, Loss: 1.5919138604658656e-05, Learning Rate: 0.000330\n",
      "Epoch 15024/40000, Loss: 4.142248144489713e-05, Learning Rate: 0.000330\n",
      "Epoch 15025/40000, Loss: 5.750001946580596e-05, Learning Rate: 0.000330\n",
      "Epoch 15026/40000, Loss: 5.745297312387265e-05, Learning Rate: 0.000330\n",
      "Epoch 15027/40000, Loss: 4.299312422517687e-05, Learning Rate: 0.000330\n",
      "Epoch 15028/40000, Loss: 4.143718251725659e-05, Learning Rate: 0.000329\n",
      "Epoch 15029/40000, Loss: 6.762504199286923e-05, Learning Rate: 0.000329\n",
      "Epoch 15030/40000, Loss: 4.297032501199283e-05, Learning Rate: 0.000329\n",
      "Epoch 15031/40000, Loss: 5.752891593147069e-05, Learning Rate: 0.000329\n",
      "Epoch 15032/40000, Loss: 1.608483216841705e-05, Learning Rate: 0.000329\n",
      "Epoch 15033/40000, Loss: 4.1412182326894253e-05, Learning Rate: 0.000329\n",
      "Epoch 15034/40000, Loss: 5.8165132941212505e-05, Learning Rate: 0.000329\n",
      "Epoch 15035/40000, Loss: 1.599744791747071e-05, Learning Rate: 0.000329\n",
      "Epoch 15036/40000, Loss: 6.791578198317438e-05, Learning Rate: 0.000329\n",
      "Epoch 15037/40000, Loss: 1.617354791960679e-05, Learning Rate: 0.000329\n",
      "Epoch 15038/40000, Loss: 4.327678107074462e-05, Learning Rate: 0.000329\n",
      "Epoch 15039/40000, Loss: 3.5647015465656295e-05, Learning Rate: 0.000329\n",
      "Epoch 15040/40000, Loss: 3.564887447282672e-05, Learning Rate: 0.000329\n",
      "Epoch 15041/40000, Loss: 5.764265370089561e-05, Learning Rate: 0.000329\n",
      "Epoch 15042/40000, Loss: 4.307341805542819e-05, Learning Rate: 0.000329\n",
      "Epoch 15043/40000, Loss: 1.5992511180229485e-05, Learning Rate: 0.000329\n",
      "Epoch 15044/40000, Loss: 5.7546738389646634e-05, Learning Rate: 0.000329\n",
      "Epoch 15045/40000, Loss: 5.7921046391129494e-05, Learning Rate: 0.000329\n",
      "Epoch 15046/40000, Loss: 4.2515323002589867e-05, Learning Rate: 0.000329\n",
      "Epoch 15047/40000, Loss: 3.584853402571753e-05, Learning Rate: 0.000329\n",
      "Epoch 15048/40000, Loss: 7.436938176397234e-05, Learning Rate: 0.000329\n",
      "Epoch 15049/40000, Loss: 4.387554872664623e-05, Learning Rate: 0.000329\n",
      "Epoch 15050/40000, Loss: 7.224121509352699e-05, Learning Rate: 0.000329\n",
      "Epoch 15051/40000, Loss: 6.930633389856666e-05, Learning Rate: 0.000329\n",
      "Epoch 15052/40000, Loss: 5.78432809561491e-05, Learning Rate: 0.000329\n",
      "Epoch 15053/40000, Loss: 3.6149038351140916e-05, Learning Rate: 0.000328\n",
      "Epoch 15054/40000, Loss: 6.919266888871789e-05, Learning Rate: 0.000328\n",
      "Epoch 15055/40000, Loss: 6.867593037895858e-05, Learning Rate: 0.000328\n",
      "Epoch 15056/40000, Loss: 3.564528014976531e-05, Learning Rate: 0.000328\n",
      "Epoch 15057/40000, Loss: 4.358601654530503e-05, Learning Rate: 0.000328\n",
      "Epoch 15058/40000, Loss: 3.6098190321354195e-05, Learning Rate: 0.000328\n",
      "Epoch 15059/40000, Loss: 1.6240013792412356e-05, Learning Rate: 0.000328\n",
      "Epoch 15060/40000, Loss: 5.809552385471761e-05, Learning Rate: 0.000328\n",
      "Epoch 15061/40000, Loss: 4.165017162449658e-05, Learning Rate: 0.000328\n",
      "Epoch 15062/40000, Loss: 6.164994556456804e-05, Learning Rate: 0.000328\n",
      "Epoch 15063/40000, Loss: 1.6688903997419402e-05, Learning Rate: 0.000328\n",
      "Epoch 15064/40000, Loss: 7.021197961876169e-05, Learning Rate: 0.000328\n",
      "Epoch 15065/40000, Loss: 4.5558415877167135e-05, Learning Rate: 0.000328\n",
      "Epoch 15066/40000, Loss: 4.452802750165574e-05, Learning Rate: 0.000328\n",
      "Epoch 15067/40000, Loss: 6.074133125366643e-05, Learning Rate: 0.000328\n",
      "Epoch 15068/40000, Loss: 4.2650361137930304e-05, Learning Rate: 0.000328\n",
      "Epoch 15069/40000, Loss: 6.294492777669802e-05, Learning Rate: 0.000328\n",
      "Epoch 15070/40000, Loss: 4.629102113540284e-05, Learning Rate: 0.000328\n",
      "Epoch 15071/40000, Loss: 3.764267603401095e-05, Learning Rate: 0.000328\n",
      "Epoch 15072/40000, Loss: 3.6910776543663815e-05, Learning Rate: 0.000328\n",
      "Epoch 15073/40000, Loss: 2.0157704057055525e-05, Learning Rate: 0.000328\n",
      "Epoch 15074/40000, Loss: 2.0284946003812365e-05, Learning Rate: 0.000328\n",
      "Epoch 15075/40000, Loss: 1.893468106572982e-05, Learning Rate: 0.000328\n",
      "Epoch 15076/40000, Loss: 3.777996971621178e-05, Learning Rate: 0.000328\n",
      "Epoch 15077/40000, Loss: 6.982884951867163e-05, Learning Rate: 0.000328\n",
      "Epoch 15078/40000, Loss: 4.476965477806516e-05, Learning Rate: 0.000328\n",
      "Epoch 15079/40000, Loss: 3.613241278799251e-05, Learning Rate: 0.000327\n",
      "Epoch 15080/40000, Loss: 1.7403403035132214e-05, Learning Rate: 0.000327\n",
      "Epoch 15081/40000, Loss: 4.520154470810667e-05, Learning Rate: 0.000327\n",
      "Epoch 15082/40000, Loss: 3.644748358055949e-05, Learning Rate: 0.000327\n",
      "Epoch 15083/40000, Loss: 4.5272649003891274e-05, Learning Rate: 0.000327\n",
      "Epoch 15084/40000, Loss: 4.274164530215785e-05, Learning Rate: 0.000327\n",
      "Epoch 15085/40000, Loss: 3.699919761857018e-05, Learning Rate: 0.000327\n",
      "Epoch 15086/40000, Loss: 4.5410633902065456e-05, Learning Rate: 0.000327\n",
      "Epoch 15087/40000, Loss: 1.7165139070129953e-05, Learning Rate: 0.000327\n",
      "Epoch 15088/40000, Loss: 6.143251084722579e-05, Learning Rate: 0.000327\n",
      "Epoch 15089/40000, Loss: 5.968155892333016e-05, Learning Rate: 0.000327\n",
      "Epoch 15090/40000, Loss: 4.8031510232249275e-05, Learning Rate: 0.000327\n",
      "Epoch 15091/40000, Loss: 3.617398760979995e-05, Learning Rate: 0.000327\n",
      "Epoch 15092/40000, Loss: 6.969148671487346e-05, Learning Rate: 0.000327\n",
      "Epoch 15093/40000, Loss: 6.0506470617838204e-05, Learning Rate: 0.000327\n",
      "Epoch 15094/40000, Loss: 4.388759771245532e-05, Learning Rate: 0.000327\n",
      "Epoch 15095/40000, Loss: 5.9048019465990365e-05, Learning Rate: 0.000327\n",
      "Epoch 15096/40000, Loss: 4.185538273304701e-05, Learning Rate: 0.000327\n",
      "Epoch 15097/40000, Loss: 6.81390447425656e-05, Learning Rate: 0.000327\n",
      "Epoch 15098/40000, Loss: 5.83104592806194e-05, Learning Rate: 0.000327\n",
      "Epoch 15099/40000, Loss: 3.591191489249468e-05, Learning Rate: 0.000327\n",
      "Epoch 15100/40000, Loss: 5.8067413192475215e-05, Learning Rate: 0.000327\n",
      "Epoch 15101/40000, Loss: 3.627368278102949e-05, Learning Rate: 0.000327\n",
      "Epoch 15102/40000, Loss: 4.3690331949619576e-05, Learning Rate: 0.000327\n",
      "Epoch 15103/40000, Loss: 4.251666541676968e-05, Learning Rate: 0.000327\n",
      "Epoch 15104/40000, Loss: 4.167389124631882e-05, Learning Rate: 0.000326\n",
      "Epoch 15105/40000, Loss: 4.176280344836414e-05, Learning Rate: 0.000326\n",
      "Epoch 15106/40000, Loss: 4.3438652937766165e-05, Learning Rate: 0.000326\n",
      "Epoch 15107/40000, Loss: 4.34938701801002e-05, Learning Rate: 0.000326\n",
      "Epoch 15108/40000, Loss: 1.637873901927378e-05, Learning Rate: 0.000326\n",
      "Epoch 15109/40000, Loss: 5.816833800054155e-05, Learning Rate: 0.000326\n",
      "Epoch 15110/40000, Loss: 5.816691918880679e-05, Learning Rate: 0.000326\n",
      "Epoch 15111/40000, Loss: 3.5691758967004716e-05, Learning Rate: 0.000326\n",
      "Epoch 15112/40000, Loss: 4.257793989381753e-05, Learning Rate: 0.000326\n",
      "Epoch 15113/40000, Loss: 4.240337875671685e-05, Learning Rate: 0.000326\n",
      "Epoch 15114/40000, Loss: 6.792647764086723e-05, Learning Rate: 0.000326\n",
      "Epoch 15115/40000, Loss: 4.8128767957678065e-05, Learning Rate: 0.000326\n",
      "Epoch 15116/40000, Loss: 4.4904347305418923e-05, Learning Rate: 0.000326\n",
      "Epoch 15117/40000, Loss: 4.2808249418158084e-05, Learning Rate: 0.000326\n",
      "Epoch 15118/40000, Loss: 6.8682296841871e-05, Learning Rate: 0.000326\n",
      "Epoch 15119/40000, Loss: 4.26668266300112e-05, Learning Rate: 0.000326\n",
      "Epoch 15120/40000, Loss: 3.6419696698430926e-05, Learning Rate: 0.000326\n",
      "Epoch 15121/40000, Loss: 6.876293628010899e-05, Learning Rate: 0.000326\n",
      "Epoch 15122/40000, Loss: 3.6584930057870224e-05, Learning Rate: 0.000326\n",
      "Epoch 15123/40000, Loss: 3.605285382946022e-05, Learning Rate: 0.000326\n",
      "Epoch 15124/40000, Loss: 6.888337520649657e-05, Learning Rate: 0.000326\n",
      "Epoch 15125/40000, Loss: 3.9290451240958646e-05, Learning Rate: 0.000326\n",
      "Epoch 15126/40000, Loss: 1.9133374735247344e-05, Learning Rate: 0.000326\n",
      "Epoch 15127/40000, Loss: 4.671792339649983e-05, Learning Rate: 0.000326\n",
      "Epoch 15128/40000, Loss: 2.0686244170065038e-05, Learning Rate: 0.000326\n",
      "Epoch 15129/40000, Loss: 4.235701635479927e-05, Learning Rate: 0.000326\n",
      "Epoch 15130/40000, Loss: 2.492038402124308e-05, Learning Rate: 0.000325\n",
      "Epoch 15131/40000, Loss: 3.9637954614590853e-05, Learning Rate: 0.000325\n",
      "Epoch 15132/40000, Loss: 7.670852937735617e-05, Learning Rate: 0.000325\n",
      "Epoch 15133/40000, Loss: 6.656871119048446e-05, Learning Rate: 0.000325\n",
      "Epoch 15134/40000, Loss: 4.521165101323277e-05, Learning Rate: 0.000325\n",
      "Epoch 15135/40000, Loss: 5.0384103815304115e-05, Learning Rate: 0.000325\n",
      "Epoch 15136/40000, Loss: 4.1383158531971276e-05, Learning Rate: 0.000325\n",
      "Epoch 15137/40000, Loss: 7.25162826711312e-05, Learning Rate: 0.000325\n",
      "Epoch 15138/40000, Loss: 5.288240572554059e-05, Learning Rate: 0.000325\n",
      "Epoch 15139/40000, Loss: 5.088520629215054e-05, Learning Rate: 0.000325\n",
      "Epoch 15140/40000, Loss: 7.213145727291703e-05, Learning Rate: 0.000325\n",
      "Epoch 15141/40000, Loss: 2.453759043419268e-05, Learning Rate: 0.000325\n",
      "Epoch 15142/40000, Loss: 4.6491226385114715e-05, Learning Rate: 0.000325\n",
      "Epoch 15143/40000, Loss: 7.350363011937588e-05, Learning Rate: 0.000325\n",
      "Epoch 15144/40000, Loss: 3.884157194988802e-05, Learning Rate: 0.000325\n",
      "Epoch 15145/40000, Loss: 7.102871313691139e-05, Learning Rate: 0.000325\n",
      "Epoch 15146/40000, Loss: 1.8417837054585107e-05, Learning Rate: 0.000325\n",
      "Epoch 15147/40000, Loss: 3.7621288356604055e-05, Learning Rate: 0.000325\n",
      "Epoch 15148/40000, Loss: 1.9097409676760435e-05, Learning Rate: 0.000325\n",
      "Epoch 15149/40000, Loss: 1.7484264390077442e-05, Learning Rate: 0.000325\n",
      "Epoch 15150/40000, Loss: 4.667877874453552e-05, Learning Rate: 0.000325\n",
      "Epoch 15151/40000, Loss: 1.888553197204601e-05, Learning Rate: 0.000325\n",
      "Epoch 15152/40000, Loss: 4.330384035711177e-05, Learning Rate: 0.000325\n",
      "Epoch 15153/40000, Loss: 4.2351744923507795e-05, Learning Rate: 0.000325\n",
      "Epoch 15154/40000, Loss: 3.6937806726200506e-05, Learning Rate: 0.000325\n",
      "Epoch 15155/40000, Loss: 6.0930338804610074e-05, Learning Rate: 0.000324\n",
      "Epoch 15156/40000, Loss: 1.7285625290242024e-05, Learning Rate: 0.000324\n",
      "Epoch 15157/40000, Loss: 3.623085649451241e-05, Learning Rate: 0.000324\n",
      "Epoch 15158/40000, Loss: 1.755741141096223e-05, Learning Rate: 0.000324\n",
      "Epoch 15159/40000, Loss: 4.561100649880245e-05, Learning Rate: 0.000324\n",
      "Epoch 15160/40000, Loss: 5.853903712704778e-05, Learning Rate: 0.000324\n",
      "Epoch 15161/40000, Loss: 5.365788456401788e-05, Learning Rate: 0.000324\n",
      "Epoch 15162/40000, Loss: 3.6999823350925e-05, Learning Rate: 0.000324\n",
      "Epoch 15163/40000, Loss: 4.855095176026225e-05, Learning Rate: 0.000324\n",
      "Epoch 15164/40000, Loss: 6.954307900741696e-05, Learning Rate: 0.000324\n",
      "Epoch 15165/40000, Loss: 6.855242099845782e-05, Learning Rate: 0.000324\n",
      "Epoch 15166/40000, Loss: 3.7338359106797725e-05, Learning Rate: 0.000324\n",
      "Epoch 15167/40000, Loss: 4.573256956064142e-05, Learning Rate: 0.000324\n",
      "Epoch 15168/40000, Loss: 3.740108877536841e-05, Learning Rate: 0.000324\n",
      "Epoch 15169/40000, Loss: 4.5151962694944814e-05, Learning Rate: 0.000324\n",
      "Epoch 15170/40000, Loss: 5.9389829402789474e-05, Learning Rate: 0.000324\n",
      "Epoch 15171/40000, Loss: 3.71268397429958e-05, Learning Rate: 0.000324\n",
      "Epoch 15172/40000, Loss: 1.683856316958554e-05, Learning Rate: 0.000324\n",
      "Epoch 15173/40000, Loss: 6.948876398382708e-05, Learning Rate: 0.000324\n",
      "Epoch 15174/40000, Loss: 6.904907786520198e-05, Learning Rate: 0.000324\n",
      "Epoch 15175/40000, Loss: 1.7428956198273227e-05, Learning Rate: 0.000324\n",
      "Epoch 15176/40000, Loss: 1.7517950254841708e-05, Learning Rate: 0.000324\n",
      "Epoch 15177/40000, Loss: 1.6774820323917083e-05, Learning Rate: 0.000324\n",
      "Epoch 15178/40000, Loss: 6.961344479350373e-05, Learning Rate: 0.000324\n",
      "Epoch 15179/40000, Loss: 6.876164115965366e-05, Learning Rate: 0.000324\n",
      "Epoch 15180/40000, Loss: 5.8535475545795634e-05, Learning Rate: 0.000324\n",
      "Epoch 15181/40000, Loss: 3.733590347110294e-05, Learning Rate: 0.000323\n",
      "Epoch 15182/40000, Loss: 5.9534380852710456e-05, Learning Rate: 0.000323\n",
      "Epoch 15183/40000, Loss: 3.791291237575933e-05, Learning Rate: 0.000323\n",
      "Epoch 15184/40000, Loss: 4.215068111079745e-05, Learning Rate: 0.000323\n",
      "Epoch 15185/40000, Loss: 5.9722930018324405e-05, Learning Rate: 0.000323\n",
      "Epoch 15186/40000, Loss: 1.7269636373384856e-05, Learning Rate: 0.000323\n",
      "Epoch 15187/40000, Loss: 1.7030382878147066e-05, Learning Rate: 0.000323\n",
      "Epoch 15188/40000, Loss: 4.281007932149805e-05, Learning Rate: 0.000323\n",
      "Epoch 15189/40000, Loss: 4.421585617819801e-05, Learning Rate: 0.000323\n",
      "Epoch 15190/40000, Loss: 1.678968510532286e-05, Learning Rate: 0.000323\n",
      "Epoch 15191/40000, Loss: 4.252914368407801e-05, Learning Rate: 0.000323\n",
      "Epoch 15192/40000, Loss: 4.205773075227626e-05, Learning Rate: 0.000323\n",
      "Epoch 15193/40000, Loss: 3.687310891109519e-05, Learning Rate: 0.000323\n",
      "Epoch 15194/40000, Loss: 6.24847671133466e-05, Learning Rate: 0.000323\n",
      "Epoch 15195/40000, Loss: 4.26091137342155e-05, Learning Rate: 0.000323\n",
      "Epoch 15196/40000, Loss: 6.836079410277307e-05, Learning Rate: 0.000323\n",
      "Epoch 15197/40000, Loss: 6.801832932978868e-05, Learning Rate: 0.000323\n",
      "Epoch 15198/40000, Loss: 5.940803021076135e-05, Learning Rate: 0.000323\n",
      "Epoch 15199/40000, Loss: 1.6853071429068223e-05, Learning Rate: 0.000323\n",
      "Epoch 15200/40000, Loss: 4.178473318461329e-05, Learning Rate: 0.000323\n",
      "Epoch 15201/40000, Loss: 1.6622745533823036e-05, Learning Rate: 0.000323\n",
      "Epoch 15202/40000, Loss: 4.196114241494797e-05, Learning Rate: 0.000323\n",
      "Epoch 15203/40000, Loss: 4.386805812828243e-05, Learning Rate: 0.000323\n",
      "Epoch 15204/40000, Loss: 3.614215165725909e-05, Learning Rate: 0.000323\n",
      "Epoch 15205/40000, Loss: 1.6951324141700752e-05, Learning Rate: 0.000323\n",
      "Epoch 15206/40000, Loss: 5.8920701121678576e-05, Learning Rate: 0.000323\n",
      "Epoch 15207/40000, Loss: 5.877570583834313e-05, Learning Rate: 0.000322\n",
      "Epoch 15208/40000, Loss: 1.6312820662278682e-05, Learning Rate: 0.000322\n",
      "Epoch 15209/40000, Loss: 3.5774257412413135e-05, Learning Rate: 0.000322\n",
      "Epoch 15210/40000, Loss: 3.5581793781602755e-05, Learning Rate: 0.000322\n",
      "Epoch 15211/40000, Loss: 5.820227670483291e-05, Learning Rate: 0.000322\n",
      "Epoch 15212/40000, Loss: 5.8083223848370835e-05, Learning Rate: 0.000322\n",
      "Epoch 15213/40000, Loss: 6.872265657875687e-05, Learning Rate: 0.000322\n",
      "Epoch 15214/40000, Loss: 3.593385554268025e-05, Learning Rate: 0.000322\n",
      "Epoch 15215/40000, Loss: 4.346285277279094e-05, Learning Rate: 0.000322\n",
      "Epoch 15216/40000, Loss: 5.761209467891604e-05, Learning Rate: 0.000322\n",
      "Epoch 15217/40000, Loss: 4.3470750824781135e-05, Learning Rate: 0.000322\n",
      "Epoch 15218/40000, Loss: 1.625386175874155e-05, Learning Rate: 0.000322\n",
      "Epoch 15219/40000, Loss: 5.7650748203741387e-05, Learning Rate: 0.000322\n",
      "Epoch 15220/40000, Loss: 3.5615154047263786e-05, Learning Rate: 0.000322\n",
      "Epoch 15221/40000, Loss: 4.3137210013810545e-05, Learning Rate: 0.000322\n",
      "Epoch 15222/40000, Loss: 6.787992606405169e-05, Learning Rate: 0.000322\n",
      "Epoch 15223/40000, Loss: 4.3694875785149634e-05, Learning Rate: 0.000322\n",
      "Epoch 15224/40000, Loss: 5.7960161939263344e-05, Learning Rate: 0.000322\n",
      "Epoch 15225/40000, Loss: 5.798011989099905e-05, Learning Rate: 0.000322\n",
      "Epoch 15226/40000, Loss: 3.546698280842975e-05, Learning Rate: 0.000322\n",
      "Epoch 15227/40000, Loss: 4.3213789467699826e-05, Learning Rate: 0.000322\n",
      "Epoch 15228/40000, Loss: 3.5923741961596534e-05, Learning Rate: 0.000322\n",
      "Epoch 15229/40000, Loss: 1.649373552936595e-05, Learning Rate: 0.000322\n",
      "Epoch 15230/40000, Loss: 5.785171742900275e-05, Learning Rate: 0.000322\n",
      "Epoch 15231/40000, Loss: 6.799814582336694e-05, Learning Rate: 0.000322\n",
      "Epoch 15232/40000, Loss: 6.785178993595764e-05, Learning Rate: 0.000322\n",
      "Epoch 15233/40000, Loss: 5.7797129557002336e-05, Learning Rate: 0.000321\n",
      "Epoch 15234/40000, Loss: 4.13166890211869e-05, Learning Rate: 0.000321\n",
      "Epoch 15235/40000, Loss: 4.3920983443968e-05, Learning Rate: 0.000321\n",
      "Epoch 15236/40000, Loss: 1.6560466974624433e-05, Learning Rate: 0.000321\n",
      "Epoch 15237/40000, Loss: 6.790384213672951e-05, Learning Rate: 0.000321\n",
      "Epoch 15238/40000, Loss: 3.610155181377195e-05, Learning Rate: 0.000321\n",
      "Epoch 15239/40000, Loss: 4.1447081457590684e-05, Learning Rate: 0.000321\n",
      "Epoch 15240/40000, Loss: 6.815693632233888e-05, Learning Rate: 0.000321\n",
      "Epoch 15241/40000, Loss: 3.574753282009624e-05, Learning Rate: 0.000321\n",
      "Epoch 15242/40000, Loss: 4.396071381052025e-05, Learning Rate: 0.000321\n",
      "Epoch 15243/40000, Loss: 4.789690865436569e-05, Learning Rate: 0.000321\n",
      "Epoch 15244/40000, Loss: 6.0205838963156566e-05, Learning Rate: 0.000321\n",
      "Epoch 15245/40000, Loss: 4.3359494156902656e-05, Learning Rate: 0.000321\n",
      "Epoch 15246/40000, Loss: 4.1368111851625144e-05, Learning Rate: 0.000321\n",
      "Epoch 15247/40000, Loss: 5.110764323035255e-05, Learning Rate: 0.000321\n",
      "Epoch 15248/40000, Loss: 2.2716436433256604e-05, Learning Rate: 0.000321\n",
      "Epoch 15249/40000, Loss: 2.0786879758816212e-05, Learning Rate: 0.000321\n",
      "Epoch 15250/40000, Loss: 0.00015960916061885655, Learning Rate: 0.000321\n",
      "Epoch 15251/40000, Loss: 6.366647721733898e-05, Learning Rate: 0.000321\n",
      "Epoch 15252/40000, Loss: 9.453659731661901e-05, Learning Rate: 0.000321\n",
      "Epoch 15253/40000, Loss: 0.00010845383803825825, Learning Rate: 0.000321\n",
      "Epoch 15254/40000, Loss: 0.00018234257004223764, Learning Rate: 0.000321\n",
      "Epoch 15255/40000, Loss: 7.911360444268212e-05, Learning Rate: 0.000321\n",
      "Epoch 15256/40000, Loss: 0.0001154925994342193, Learning Rate: 0.000321\n",
      "Epoch 15257/40000, Loss: 0.00010247366299154237, Learning Rate: 0.000321\n",
      "Epoch 15258/40000, Loss: 4.996167263016105e-05, Learning Rate: 0.000321\n",
      "Epoch 15259/40000, Loss: 5.16644686285872e-05, Learning Rate: 0.000320\n",
      "Epoch 15260/40000, Loss: 7.826992805348709e-05, Learning Rate: 0.000320\n",
      "Epoch 15261/40000, Loss: 5.91297575738281e-05, Learning Rate: 0.000320\n",
      "Epoch 15262/40000, Loss: 4.31734551966656e-05, Learning Rate: 0.000320\n",
      "Epoch 15263/40000, Loss: 3.923236727132462e-05, Learning Rate: 0.000320\n",
      "Epoch 15264/40000, Loss: 4.462223296286538e-05, Learning Rate: 0.000320\n",
      "Epoch 15265/40000, Loss: 7.088485290296376e-05, Learning Rate: 0.000320\n",
      "Epoch 15266/40000, Loss: 1.7941962141776457e-05, Learning Rate: 0.000320\n",
      "Epoch 15267/40000, Loss: 4.6297023800434545e-05, Learning Rate: 0.000320\n",
      "Epoch 15268/40000, Loss: 4.403650382300839e-05, Learning Rate: 0.000320\n",
      "Epoch 15269/40000, Loss: 4.072852243552916e-05, Learning Rate: 0.000320\n",
      "Epoch 15270/40000, Loss: 4.624276334652677e-05, Learning Rate: 0.000320\n",
      "Epoch 15271/40000, Loss: 6.003005910315551e-05, Learning Rate: 0.000320\n",
      "Epoch 15272/40000, Loss: 1.8986938812304288e-05, Learning Rate: 0.000320\n",
      "Epoch 15273/40000, Loss: 4.20997675973922e-05, Learning Rate: 0.000320\n",
      "Epoch 15274/40000, Loss: 4.5313550799619406e-05, Learning Rate: 0.000320\n",
      "Epoch 15275/40000, Loss: 7.100048242136836e-05, Learning Rate: 0.000320\n",
      "Epoch 15276/40000, Loss: 6.781817501178011e-05, Learning Rate: 0.000320\n",
      "Epoch 15277/40000, Loss: 3.5888653656002134e-05, Learning Rate: 0.000320\n",
      "Epoch 15278/40000, Loss: 5.7731434935703874e-05, Learning Rate: 0.000320\n",
      "Epoch 15279/40000, Loss: 1.6398495063185692e-05, Learning Rate: 0.000320\n",
      "Epoch 15280/40000, Loss: 5.805576438433491e-05, Learning Rate: 0.000320\n",
      "Epoch 15281/40000, Loss: 4.1400071495445445e-05, Learning Rate: 0.000320\n",
      "Epoch 15282/40000, Loss: 1.6183097613975406e-05, Learning Rate: 0.000320\n",
      "Epoch 15283/40000, Loss: 3.5504439438227564e-05, Learning Rate: 0.000320\n",
      "Epoch 15284/40000, Loss: 6.743248377460986e-05, Learning Rate: 0.000320\n",
      "Epoch 15285/40000, Loss: 5.749804404331371e-05, Learning Rate: 0.000319\n",
      "Epoch 15286/40000, Loss: 4.329550210968591e-05, Learning Rate: 0.000319\n",
      "Epoch 15287/40000, Loss: 4.334234108682722e-05, Learning Rate: 0.000319\n",
      "Epoch 15288/40000, Loss: 6.76931013003923e-05, Learning Rate: 0.000319\n",
      "Epoch 15289/40000, Loss: 4.115458796150051e-05, Learning Rate: 0.000319\n",
      "Epoch 15290/40000, Loss: 4.126388375880197e-05, Learning Rate: 0.000319\n",
      "Epoch 15291/40000, Loss: 4.314517354941927e-05, Learning Rate: 0.000319\n",
      "Epoch 15292/40000, Loss: 1.6063364455476403e-05, Learning Rate: 0.000319\n",
      "Epoch 15293/40000, Loss: 3.5292709071654826e-05, Learning Rate: 0.000319\n",
      "Epoch 15294/40000, Loss: 1.59823102876544e-05, Learning Rate: 0.000319\n",
      "Epoch 15295/40000, Loss: 1.5889067071839236e-05, Learning Rate: 0.000319\n",
      "Epoch 15296/40000, Loss: 5.732178033213131e-05, Learning Rate: 0.000319\n",
      "Epoch 15297/40000, Loss: 6.701173697365448e-05, Learning Rate: 0.000319\n",
      "Epoch 15298/40000, Loss: 4.1178147512255237e-05, Learning Rate: 0.000319\n",
      "Epoch 15299/40000, Loss: 5.7356890465598553e-05, Learning Rate: 0.000319\n",
      "Epoch 15300/40000, Loss: 1.6015159417293034e-05, Learning Rate: 0.000319\n",
      "Epoch 15301/40000, Loss: 3.5450837458483875e-05, Learning Rate: 0.000319\n",
      "Epoch 15302/40000, Loss: 1.5963983969413675e-05, Learning Rate: 0.000319\n",
      "Epoch 15303/40000, Loss: 5.724916991312057e-05, Learning Rate: 0.000319\n",
      "Epoch 15304/40000, Loss: 3.5339358873898163e-05, Learning Rate: 0.000319\n",
      "Epoch 15305/40000, Loss: 1.5979972886270843e-05, Learning Rate: 0.000319\n",
      "Epoch 15306/40000, Loss: 6.735086935805157e-05, Learning Rate: 0.000319\n",
      "Epoch 15307/40000, Loss: 1.59993924171431e-05, Learning Rate: 0.000319\n",
      "Epoch 15308/40000, Loss: 1.5942730897222646e-05, Learning Rate: 0.000319\n",
      "Epoch 15309/40000, Loss: 6.73132817610167e-05, Learning Rate: 0.000319\n",
      "Epoch 15310/40000, Loss: 1.597685695742257e-05, Learning Rate: 0.000319\n",
      "Epoch 15311/40000, Loss: 3.529707100824453e-05, Learning Rate: 0.000318\n",
      "Epoch 15312/40000, Loss: 3.531395486788824e-05, Learning Rate: 0.000318\n",
      "Epoch 15313/40000, Loss: 4.2941355786751956e-05, Learning Rate: 0.000318\n",
      "Epoch 15314/40000, Loss: 4.1048257116926834e-05, Learning Rate: 0.000318\n",
      "Epoch 15315/40000, Loss: 4.303360037738457e-05, Learning Rate: 0.000318\n",
      "Epoch 15316/40000, Loss: 4.286248440621421e-05, Learning Rate: 0.000318\n",
      "Epoch 15317/40000, Loss: 6.720811506966129e-05, Learning Rate: 0.000318\n",
      "Epoch 15318/40000, Loss: 3.5234530514571816e-05, Learning Rate: 0.000318\n",
      "Epoch 15319/40000, Loss: 1.591100954101421e-05, Learning Rate: 0.000318\n",
      "Epoch 15320/40000, Loss: 1.600532777956687e-05, Learning Rate: 0.000318\n",
      "Epoch 15321/40000, Loss: 3.539602039381862e-05, Learning Rate: 0.000318\n",
      "Epoch 15322/40000, Loss: 3.541185287758708e-05, Learning Rate: 0.000318\n",
      "Epoch 15323/40000, Loss: 6.722032412653789e-05, Learning Rate: 0.000318\n",
      "Epoch 15324/40000, Loss: 5.724757647840306e-05, Learning Rate: 0.000318\n",
      "Epoch 15325/40000, Loss: 4.30140680691693e-05, Learning Rate: 0.000318\n",
      "Epoch 15326/40000, Loss: 1.5955909475451335e-05, Learning Rate: 0.000318\n",
      "Epoch 15327/40000, Loss: 6.701519305352122e-05, Learning Rate: 0.000318\n",
      "Epoch 15328/40000, Loss: 4.111921589355916e-05, Learning Rate: 0.000318\n",
      "Epoch 15329/40000, Loss: 1.5864598026382737e-05, Learning Rate: 0.000318\n",
      "Epoch 15330/40000, Loss: 1.5912324670352973e-05, Learning Rate: 0.000318\n",
      "Epoch 15331/40000, Loss: 3.525512875057757e-05, Learning Rate: 0.000318\n",
      "Epoch 15332/40000, Loss: 5.723848516936414e-05, Learning Rate: 0.000318\n",
      "Epoch 15333/40000, Loss: 4.1055824112845585e-05, Learning Rate: 0.000318\n",
      "Epoch 15334/40000, Loss: 1.5874717064434662e-05, Learning Rate: 0.000318\n",
      "Epoch 15335/40000, Loss: 1.592461376276333e-05, Learning Rate: 0.000318\n",
      "Epoch 15336/40000, Loss: 3.528107117745094e-05, Learning Rate: 0.000318\n",
      "Epoch 15337/40000, Loss: 6.676440534647554e-05, Learning Rate: 0.000317\n",
      "Epoch 15338/40000, Loss: 3.54182229784783e-05, Learning Rate: 0.000317\n",
      "Epoch 15339/40000, Loss: 6.705801206408069e-05, Learning Rate: 0.000317\n",
      "Epoch 15340/40000, Loss: 4.312930832384154e-05, Learning Rate: 0.000317\n",
      "Epoch 15341/40000, Loss: 1.5976045688148588e-05, Learning Rate: 0.000317\n",
      "Epoch 15342/40000, Loss: 4.128356522414833e-05, Learning Rate: 0.000317\n",
      "Epoch 15343/40000, Loss: 4.3158051994396374e-05, Learning Rate: 0.000317\n",
      "Epoch 15344/40000, Loss: 1.5884023014223203e-05, Learning Rate: 0.000317\n",
      "Epoch 15345/40000, Loss: 3.535375071805902e-05, Learning Rate: 0.000317\n",
      "Epoch 15346/40000, Loss: 6.796004163334146e-05, Learning Rate: 0.000317\n",
      "Epoch 15347/40000, Loss: 4.306737901060842e-05, Learning Rate: 0.000317\n",
      "Epoch 15348/40000, Loss: 4.314726538723335e-05, Learning Rate: 0.000317\n",
      "Epoch 15349/40000, Loss: 5.754162702942267e-05, Learning Rate: 0.000317\n",
      "Epoch 15350/40000, Loss: 5.739220432587899e-05, Learning Rate: 0.000317\n",
      "Epoch 15351/40000, Loss: 4.1126862925011665e-05, Learning Rate: 0.000317\n",
      "Epoch 15352/40000, Loss: 3.533143535605632e-05, Learning Rate: 0.000317\n",
      "Epoch 15353/40000, Loss: 3.52861279679928e-05, Learning Rate: 0.000317\n",
      "Epoch 15354/40000, Loss: 3.523937994032167e-05, Learning Rate: 0.000317\n",
      "Epoch 15355/40000, Loss: 3.537224984029308e-05, Learning Rate: 0.000317\n",
      "Epoch 15356/40000, Loss: 4.110315057914704e-05, Learning Rate: 0.000317\n",
      "Epoch 15357/40000, Loss: 4.293348320061341e-05, Learning Rate: 0.000317\n",
      "Epoch 15358/40000, Loss: 4.136538700549863e-05, Learning Rate: 0.000317\n",
      "Epoch 15359/40000, Loss: 4.334890400059521e-05, Learning Rate: 0.000317\n",
      "Epoch 15360/40000, Loss: 5.779891216661781e-05, Learning Rate: 0.000317\n",
      "Epoch 15361/40000, Loss: 4.1901359509211034e-05, Learning Rate: 0.000317\n",
      "Epoch 15362/40000, Loss: 5.7826426200335845e-05, Learning Rate: 0.000317\n",
      "Epoch 15363/40000, Loss: 3.549898974597454e-05, Learning Rate: 0.000317\n",
      "Epoch 15364/40000, Loss: 1.6099751519504935e-05, Learning Rate: 0.000316\n",
      "Epoch 15365/40000, Loss: 6.726584979332983e-05, Learning Rate: 0.000316\n",
      "Epoch 15366/40000, Loss: 4.136880306759849e-05, Learning Rate: 0.000316\n",
      "Epoch 15367/40000, Loss: 1.6208079614443704e-05, Learning Rate: 0.000316\n",
      "Epoch 15368/40000, Loss: 5.7601628213888034e-05, Learning Rate: 0.000316\n",
      "Epoch 15369/40000, Loss: 1.6211752154049464e-05, Learning Rate: 0.000316\n",
      "Epoch 15370/40000, Loss: 4.344290937297046e-05, Learning Rate: 0.000316\n",
      "Epoch 15371/40000, Loss: 4.151708344579674e-05, Learning Rate: 0.000316\n",
      "Epoch 15372/40000, Loss: 1.6122225133585744e-05, Learning Rate: 0.000316\n",
      "Epoch 15373/40000, Loss: 1.621051342226565e-05, Learning Rate: 0.000316\n",
      "Epoch 15374/40000, Loss: 3.5401215427555144e-05, Learning Rate: 0.000316\n",
      "Epoch 15375/40000, Loss: 4.166509097558446e-05, Learning Rate: 0.000316\n",
      "Epoch 15376/40000, Loss: 3.5409382689977065e-05, Learning Rate: 0.000316\n",
      "Epoch 15377/40000, Loss: 6.753765774192289e-05, Learning Rate: 0.000316\n",
      "Epoch 15378/40000, Loss: 4.15802096540574e-05, Learning Rate: 0.000316\n",
      "Epoch 15379/40000, Loss: 3.5553497582441196e-05, Learning Rate: 0.000316\n",
      "Epoch 15380/40000, Loss: 1.621163755771704e-05, Learning Rate: 0.000316\n",
      "Epoch 15381/40000, Loss: 1.616288864170201e-05, Learning Rate: 0.000316\n",
      "Epoch 15382/40000, Loss: 3.584562728065066e-05, Learning Rate: 0.000316\n",
      "Epoch 15383/40000, Loss: 3.570536500774324e-05, Learning Rate: 0.000316\n",
      "Epoch 15384/40000, Loss: 4.169305248069577e-05, Learning Rate: 0.000316\n",
      "Epoch 15385/40000, Loss: 6.719748489558697e-05, Learning Rate: 0.000316\n",
      "Epoch 15386/40000, Loss: 1.6404892448917963e-05, Learning Rate: 0.000316\n",
      "Epoch 15387/40000, Loss: 5.8634072047425434e-05, Learning Rate: 0.000316\n",
      "Epoch 15388/40000, Loss: 4.192294363747351e-05, Learning Rate: 0.000316\n",
      "Epoch 15389/40000, Loss: 4.5738182961940765e-05, Learning Rate: 0.000316\n",
      "Epoch 15390/40000, Loss: 6.957699952181429e-05, Learning Rate: 0.000315\n",
      "Epoch 15391/40000, Loss: 3.657037086668424e-05, Learning Rate: 0.000315\n",
      "Epoch 15392/40000, Loss: 3.6150504456600174e-05, Learning Rate: 0.000315\n",
      "Epoch 15393/40000, Loss: 4.817983790417202e-05, Learning Rate: 0.000315\n",
      "Epoch 15394/40000, Loss: 4.83070507470984e-05, Learning Rate: 0.000315\n",
      "Epoch 15395/40000, Loss: 6.18810736341402e-05, Learning Rate: 0.000315\n",
      "Epoch 15396/40000, Loss: 5.371762017603032e-05, Learning Rate: 0.000315\n",
      "Epoch 15397/40000, Loss: 4.955991607857868e-05, Learning Rate: 0.000315\n",
      "Epoch 15398/40000, Loss: 6.950415263418108e-05, Learning Rate: 0.000315\n",
      "Epoch 15399/40000, Loss: 6.502882752101868e-05, Learning Rate: 0.000315\n",
      "Epoch 15400/40000, Loss: 1.9421859178692102e-05, Learning Rate: 0.000315\n",
      "Epoch 15401/40000, Loss: 5.0091806770069525e-05, Learning Rate: 0.000315\n",
      "Epoch 15402/40000, Loss: 7.217723032226786e-05, Learning Rate: 0.000315\n",
      "Epoch 15403/40000, Loss: 4.911997893941589e-05, Learning Rate: 0.000315\n",
      "Epoch 15404/40000, Loss: 3.7517926102736965e-05, Learning Rate: 0.000315\n",
      "Epoch 15405/40000, Loss: 3.664404357550666e-05, Learning Rate: 0.000315\n",
      "Epoch 15406/40000, Loss: 8.38500345707871e-05, Learning Rate: 0.000315\n",
      "Epoch 15407/40000, Loss: 5.068006066721864e-05, Learning Rate: 0.000315\n",
      "Epoch 15408/40000, Loss: 6.0679620219161734e-05, Learning Rate: 0.000315\n",
      "Epoch 15409/40000, Loss: 6.900738662807271e-05, Learning Rate: 0.000315\n",
      "Epoch 15410/40000, Loss: 3.786509478231892e-05, Learning Rate: 0.000315\n",
      "Epoch 15411/40000, Loss: 4.690644709626213e-05, Learning Rate: 0.000315\n",
      "Epoch 15412/40000, Loss: 1.8311746316612698e-05, Learning Rate: 0.000315\n",
      "Epoch 15413/40000, Loss: 4.21824079239741e-05, Learning Rate: 0.000315\n",
      "Epoch 15414/40000, Loss: 7.032264693407342e-05, Learning Rate: 0.000315\n",
      "Epoch 15415/40000, Loss: 5.960633643553592e-05, Learning Rate: 0.000315\n",
      "Epoch 15416/40000, Loss: 7.220744737423956e-05, Learning Rate: 0.000314\n",
      "Epoch 15417/40000, Loss: 4.4136653741588816e-05, Learning Rate: 0.000314\n",
      "Epoch 15418/40000, Loss: 6.0680467868223786e-05, Learning Rate: 0.000314\n",
      "Epoch 15419/40000, Loss: 1.7296772057306953e-05, Learning Rate: 0.000314\n",
      "Epoch 15420/40000, Loss: 6.928945367690176e-05, Learning Rate: 0.000314\n",
      "Epoch 15421/40000, Loss: 6.85481500113383e-05, Learning Rate: 0.000314\n",
      "Epoch 15422/40000, Loss: 3.852473309962079e-05, Learning Rate: 0.000314\n",
      "Epoch 15423/40000, Loss: 4.645179069484584e-05, Learning Rate: 0.000314\n",
      "Epoch 15424/40000, Loss: 4.2711755668278784e-05, Learning Rate: 0.000314\n",
      "Epoch 15425/40000, Loss: 7.199846731964499e-05, Learning Rate: 0.000314\n",
      "Epoch 15426/40000, Loss: 1.971622259588912e-05, Learning Rate: 0.000314\n",
      "Epoch 15427/40000, Loss: 4.237494431436062e-05, Learning Rate: 0.000314\n",
      "Epoch 15428/40000, Loss: 3.834189192275517e-05, Learning Rate: 0.000314\n",
      "Epoch 15429/40000, Loss: 4.5407308789435774e-05, Learning Rate: 0.000314\n",
      "Epoch 15430/40000, Loss: 4.3409581849118695e-05, Learning Rate: 0.000314\n",
      "Epoch 15431/40000, Loss: 3.9216862205648795e-05, Learning Rate: 0.000314\n",
      "Epoch 15432/40000, Loss: 3.842953810817562e-05, Learning Rate: 0.000314\n",
      "Epoch 15433/40000, Loss: 6.209370621945709e-05, Learning Rate: 0.000314\n",
      "Epoch 15434/40000, Loss: 7.49904866097495e-05, Learning Rate: 0.000314\n",
      "Epoch 15435/40000, Loss: 6.150175613583997e-05, Learning Rate: 0.000314\n",
      "Epoch 15436/40000, Loss: 7.02821635059081e-05, Learning Rate: 0.000314\n",
      "Epoch 15437/40000, Loss: 1.8668564734980464e-05, Learning Rate: 0.000314\n",
      "Epoch 15438/40000, Loss: 4.207326492178254e-05, Learning Rate: 0.000314\n",
      "Epoch 15439/40000, Loss: 6.562183261848986e-05, Learning Rate: 0.000314\n",
      "Epoch 15440/40000, Loss: 1.839937067416031e-05, Learning Rate: 0.000314\n",
      "Epoch 15441/40000, Loss: 4.6136006858432665e-05, Learning Rate: 0.000314\n",
      "Epoch 15442/40000, Loss: 5.972176222712733e-05, Learning Rate: 0.000314\n",
      "Epoch 15443/40000, Loss: 4.0703984268475324e-05, Learning Rate: 0.000313\n",
      "Epoch 15444/40000, Loss: 6.327568553388119e-05, Learning Rate: 0.000313\n",
      "Epoch 15445/40000, Loss: 9.024320752359927e-05, Learning Rate: 0.000313\n",
      "Epoch 15446/40000, Loss: 3.937977089663036e-05, Learning Rate: 0.000313\n",
      "Epoch 15447/40000, Loss: 5.1423689001239836e-05, Learning Rate: 0.000313\n",
      "Epoch 15448/40000, Loss: 3.911771273124032e-05, Learning Rate: 0.000313\n",
      "Epoch 15449/40000, Loss: 8.168826752807945e-05, Learning Rate: 0.000313\n",
      "Epoch 15450/40000, Loss: 7.247859321068972e-05, Learning Rate: 0.000313\n",
      "Epoch 15451/40000, Loss: 3.8226848118938506e-05, Learning Rate: 0.000313\n",
      "Epoch 15452/40000, Loss: 1.75719633261906e-05, Learning Rate: 0.000313\n",
      "Epoch 15453/40000, Loss: 6.129414396127686e-05, Learning Rate: 0.000313\n",
      "Epoch 15454/40000, Loss: 4.302283196011558e-05, Learning Rate: 0.000313\n",
      "Epoch 15455/40000, Loss: 4.2294315790059045e-05, Learning Rate: 0.000313\n",
      "Epoch 15456/40000, Loss: 1.7215563275385648e-05, Learning Rate: 0.000313\n",
      "Epoch 15457/40000, Loss: 1.6148100257851183e-05, Learning Rate: 0.000313\n",
      "Epoch 15458/40000, Loss: 6.906108319526538e-05, Learning Rate: 0.000313\n",
      "Epoch 15459/40000, Loss: 6.886052869958803e-05, Learning Rate: 0.000313\n",
      "Epoch 15460/40000, Loss: 5.810963557451032e-05, Learning Rate: 0.000313\n",
      "Epoch 15461/40000, Loss: 1.6573745597270317e-05, Learning Rate: 0.000313\n",
      "Epoch 15462/40000, Loss: 6.880333967274055e-05, Learning Rate: 0.000313\n",
      "Epoch 15463/40000, Loss: 1.639481524762232e-05, Learning Rate: 0.000313\n",
      "Epoch 15464/40000, Loss: 5.743734072893858e-05, Learning Rate: 0.000313\n",
      "Epoch 15465/40000, Loss: 4.3604595703072846e-05, Learning Rate: 0.000313\n",
      "Epoch 15466/40000, Loss: 4.131855530431494e-05, Learning Rate: 0.000313\n",
      "Epoch 15467/40000, Loss: 3.5341883631190285e-05, Learning Rate: 0.000313\n",
      "Epoch 15468/40000, Loss: 6.705767009407282e-05, Learning Rate: 0.000313\n",
      "Epoch 15469/40000, Loss: 3.5274020774522796e-05, Learning Rate: 0.000312\n",
      "Epoch 15470/40000, Loss: 3.5294775443617254e-05, Learning Rate: 0.000312\n",
      "Epoch 15471/40000, Loss: 5.747517934651114e-05, Learning Rate: 0.000312\n",
      "Epoch 15472/40000, Loss: 1.64424109243555e-05, Learning Rate: 0.000312\n",
      "Epoch 15473/40000, Loss: 6.716392090311274e-05, Learning Rate: 0.000312\n",
      "Epoch 15474/40000, Loss: 4.304955655243248e-05, Learning Rate: 0.000312\n",
      "Epoch 15475/40000, Loss: 3.5900648072129115e-05, Learning Rate: 0.000312\n",
      "Epoch 15476/40000, Loss: 3.561965058906935e-05, Learning Rate: 0.000312\n",
      "Epoch 15477/40000, Loss: 5.7578174164518714e-05, Learning Rate: 0.000312\n",
      "Epoch 15478/40000, Loss: 3.5388642572797835e-05, Learning Rate: 0.000312\n",
      "Epoch 15479/40000, Loss: 1.5989427993190475e-05, Learning Rate: 0.000312\n",
      "Epoch 15480/40000, Loss: 1.621825504116714e-05, Learning Rate: 0.000312\n",
      "Epoch 15481/40000, Loss: 4.289405478630215e-05, Learning Rate: 0.000312\n",
      "Epoch 15482/40000, Loss: 3.51830167346634e-05, Learning Rate: 0.000312\n",
      "Epoch 15483/40000, Loss: 4.116015406907536e-05, Learning Rate: 0.000312\n",
      "Epoch 15484/40000, Loss: 6.724354170728475e-05, Learning Rate: 0.000312\n",
      "Epoch 15485/40000, Loss: 4.298486965126358e-05, Learning Rate: 0.000312\n",
      "Epoch 15486/40000, Loss: 4.301716762711294e-05, Learning Rate: 0.000312\n",
      "Epoch 15487/40000, Loss: 4.108690336579457e-05, Learning Rate: 0.000312\n",
      "Epoch 15488/40000, Loss: 3.5270459193270653e-05, Learning Rate: 0.000312\n",
      "Epoch 15489/40000, Loss: 3.523151099216193e-05, Learning Rate: 0.000312\n",
      "Epoch 15490/40000, Loss: 6.714540359098464e-05, Learning Rate: 0.000312\n",
      "Epoch 15491/40000, Loss: 6.721790123265237e-05, Learning Rate: 0.000312\n",
      "Epoch 15492/40000, Loss: 4.363085827208124e-05, Learning Rate: 0.000312\n",
      "Epoch 15493/40000, Loss: 4.3968397221760824e-05, Learning Rate: 0.000312\n",
      "Epoch 15494/40000, Loss: 4.3857940909219906e-05, Learning Rate: 0.000312\n",
      "Epoch 15495/40000, Loss: 3.619333801907487e-05, Learning Rate: 0.000312\n",
      "Epoch 15496/40000, Loss: 4.224843723932281e-05, Learning Rate: 0.000311\n",
      "Epoch 15497/40000, Loss: 3.596248279791325e-05, Learning Rate: 0.000311\n",
      "Epoch 15498/40000, Loss: 3.544334686012007e-05, Learning Rate: 0.000311\n",
      "Epoch 15499/40000, Loss: 3.538627061061561e-05, Learning Rate: 0.000311\n",
      "Epoch 15500/40000, Loss: 4.1518218495184556e-05, Learning Rate: 0.000311\n",
      "Epoch 15501/40000, Loss: 1.611304105608724e-05, Learning Rate: 0.000311\n",
      "Epoch 15502/40000, Loss: 4.1669016354717314e-05, Learning Rate: 0.000311\n",
      "Epoch 15503/40000, Loss: 5.882159894099459e-05, Learning Rate: 0.000311\n",
      "Epoch 15504/40000, Loss: 1.6353251339751296e-05, Learning Rate: 0.000311\n",
      "Epoch 15505/40000, Loss: 5.777850674348883e-05, Learning Rate: 0.000311\n",
      "Epoch 15506/40000, Loss: 4.301830267650075e-05, Learning Rate: 0.000311\n",
      "Epoch 15507/40000, Loss: 4.112075112061575e-05, Learning Rate: 0.000311\n",
      "Epoch 15508/40000, Loss: 5.757150211138651e-05, Learning Rate: 0.000311\n",
      "Epoch 15509/40000, Loss: 4.177528899163008e-05, Learning Rate: 0.000311\n",
      "Epoch 15510/40000, Loss: 4.163286939729005e-05, Learning Rate: 0.000311\n",
      "Epoch 15511/40000, Loss: 6.705256964778528e-05, Learning Rate: 0.000311\n",
      "Epoch 15512/40000, Loss: 4.1260635043727234e-05, Learning Rate: 0.000311\n",
      "Epoch 15513/40000, Loss: 6.728273729095235e-05, Learning Rate: 0.000311\n",
      "Epoch 15514/40000, Loss: 4.308341522119008e-05, Learning Rate: 0.000311\n",
      "Epoch 15515/40000, Loss: 5.7907196605810896e-05, Learning Rate: 0.000311\n",
      "Epoch 15516/40000, Loss: 5.743897054344416e-05, Learning Rate: 0.000311\n",
      "Epoch 15517/40000, Loss: 3.5400615161051974e-05, Learning Rate: 0.000311\n",
      "Epoch 15518/40000, Loss: 3.554858994903043e-05, Learning Rate: 0.000311\n",
      "Epoch 15519/40000, Loss: 3.526941509335302e-05, Learning Rate: 0.000311\n",
      "Epoch 15520/40000, Loss: 4.352019459474832e-05, Learning Rate: 0.000311\n",
      "Epoch 15521/40000, Loss: 1.6170804883586243e-05, Learning Rate: 0.000311\n",
      "Epoch 15522/40000, Loss: 4.111881571589038e-05, Learning Rate: 0.000311\n",
      "Epoch 15523/40000, Loss: 1.8123702830052935e-05, Learning Rate: 0.000310\n",
      "Epoch 15524/40000, Loss: 6.744640995748341e-05, Learning Rate: 0.000310\n",
      "Epoch 15525/40000, Loss: 4.121903111808933e-05, Learning Rate: 0.000310\n",
      "Epoch 15526/40000, Loss: 3.5491404560161754e-05, Learning Rate: 0.000310\n",
      "Epoch 15527/40000, Loss: 6.751046021236107e-05, Learning Rate: 0.000310\n",
      "Epoch 15528/40000, Loss: 4.160958997090347e-05, Learning Rate: 0.000310\n",
      "Epoch 15529/40000, Loss: 4.149667074671015e-05, Learning Rate: 0.000310\n",
      "Epoch 15530/40000, Loss: 5.765472087659873e-05, Learning Rate: 0.000310\n",
      "Epoch 15531/40000, Loss: 3.595784437493421e-05, Learning Rate: 0.000310\n",
      "Epoch 15532/40000, Loss: 6.7621331254486e-05, Learning Rate: 0.000310\n",
      "Epoch 15533/40000, Loss: 4.1386098018847406e-05, Learning Rate: 0.000310\n",
      "Epoch 15534/40000, Loss: 4.355326746008359e-05, Learning Rate: 0.000310\n",
      "Epoch 15535/40000, Loss: 5.851515379617922e-05, Learning Rate: 0.000310\n",
      "Epoch 15536/40000, Loss: 1.6982088709482923e-05, Learning Rate: 0.000310\n",
      "Epoch 15537/40000, Loss: 4.2603500332916155e-05, Learning Rate: 0.000310\n",
      "Epoch 15538/40000, Loss: 4.4444990635383874e-05, Learning Rate: 0.000310\n",
      "Epoch 15539/40000, Loss: 5.955090091447346e-05, Learning Rate: 0.000310\n",
      "Epoch 15540/40000, Loss: 4.471716601983644e-05, Learning Rate: 0.000310\n",
      "Epoch 15541/40000, Loss: 5.993844024487771e-05, Learning Rate: 0.000310\n",
      "Epoch 15542/40000, Loss: 1.7239086446352303e-05, Learning Rate: 0.000310\n",
      "Epoch 15543/40000, Loss: 6.742072582710534e-05, Learning Rate: 0.000310\n",
      "Epoch 15544/40000, Loss: 6.717661017319188e-05, Learning Rate: 0.000310\n",
      "Epoch 15545/40000, Loss: 5.9130365116288885e-05, Learning Rate: 0.000310\n",
      "Epoch 15546/40000, Loss: 4.237009852658957e-05, Learning Rate: 0.000310\n",
      "Epoch 15547/40000, Loss: 6.285099516389892e-05, Learning Rate: 0.000310\n",
      "Epoch 15548/40000, Loss: 4.4930013245902956e-05, Learning Rate: 0.000310\n",
      "Epoch 15549/40000, Loss: 6.994016439421102e-05, Learning Rate: 0.000310\n",
      "Epoch 15550/40000, Loss: 4.044158777105622e-05, Learning Rate: 0.000309\n",
      "Epoch 15551/40000, Loss: 7.45458819437772e-05, Learning Rate: 0.000309\n",
      "Epoch 15552/40000, Loss: 5.855692143086344e-05, Learning Rate: 0.000309\n",
      "Epoch 15553/40000, Loss: 2.3899334337329492e-05, Learning Rate: 0.000309\n",
      "Epoch 15554/40000, Loss: 4.369978705653921e-05, Learning Rate: 0.000309\n",
      "Epoch 15555/40000, Loss: 7.018776523182169e-05, Learning Rate: 0.000309\n",
      "Epoch 15556/40000, Loss: 4.571428871713579e-05, Learning Rate: 0.000309\n",
      "Epoch 15557/40000, Loss: 6.962334009585902e-05, Learning Rate: 0.000309\n",
      "Epoch 15558/40000, Loss: 7.499200728489086e-05, Learning Rate: 0.000309\n",
      "Epoch 15559/40000, Loss: 3.903740434907377e-05, Learning Rate: 0.000309\n",
      "Epoch 15560/40000, Loss: 7.434474537149072e-05, Learning Rate: 0.000309\n",
      "Epoch 15561/40000, Loss: 6.752396438969299e-05, Learning Rate: 0.000309\n",
      "Epoch 15562/40000, Loss: 2.8209638912812807e-05, Learning Rate: 0.000309\n",
      "Epoch 15563/40000, Loss: 7.077716873027384e-05, Learning Rate: 0.000309\n",
      "Epoch 15564/40000, Loss: 7.627201557625085e-05, Learning Rate: 0.000309\n",
      "Epoch 15565/40000, Loss: 7.333145185839385e-05, Learning Rate: 0.000309\n",
      "Epoch 15566/40000, Loss: 3.757332160603255e-05, Learning Rate: 0.000309\n",
      "Epoch 15567/40000, Loss: 7.24920755601488e-05, Learning Rate: 0.000309\n",
      "Epoch 15568/40000, Loss: 2.0324259821791202e-05, Learning Rate: 0.000309\n",
      "Epoch 15569/40000, Loss: 7.081181684043258e-05, Learning Rate: 0.000309\n",
      "Epoch 15570/40000, Loss: 4.712754889624193e-05, Learning Rate: 0.000309\n",
      "Epoch 15571/40000, Loss: 7.850167457945645e-05, Learning Rate: 0.000309\n",
      "Epoch 15572/40000, Loss: 6.187768303789198e-05, Learning Rate: 0.000309\n",
      "Epoch 15573/40000, Loss: 2.0692168618552387e-05, Learning Rate: 0.000309\n",
      "Epoch 15574/40000, Loss: 4.2112835217267275e-05, Learning Rate: 0.000309\n",
      "Epoch 15575/40000, Loss: 3.769670001929626e-05, Learning Rate: 0.000309\n",
      "Epoch 15576/40000, Loss: 3.679459041450173e-05, Learning Rate: 0.000309\n",
      "Epoch 15577/40000, Loss: 3.6165325582260266e-05, Learning Rate: 0.000308\n",
      "Epoch 15578/40000, Loss: 3.5500874218996614e-05, Learning Rate: 0.000308\n",
      "Epoch 15579/40000, Loss: 3.566822852008045e-05, Learning Rate: 0.000308\n",
      "Epoch 15580/40000, Loss: 3.5954235499957576e-05, Learning Rate: 0.000308\n",
      "Epoch 15581/40000, Loss: 6.720716191921383e-05, Learning Rate: 0.000308\n",
      "Epoch 15582/40000, Loss: 3.552233101800084e-05, Learning Rate: 0.000308\n",
      "Epoch 15583/40000, Loss: 4.345807246863842e-05, Learning Rate: 0.000308\n",
      "Epoch 15584/40000, Loss: 5.823833998874761e-05, Learning Rate: 0.000308\n",
      "Epoch 15585/40000, Loss: 6.688561552437022e-05, Learning Rate: 0.000308\n",
      "Epoch 15586/40000, Loss: 4.351852476247586e-05, Learning Rate: 0.000308\n",
      "Epoch 15587/40000, Loss: 6.67885469738394e-05, Learning Rate: 0.000308\n",
      "Epoch 15588/40000, Loss: 1.6194491763599217e-05, Learning Rate: 0.000308\n",
      "Epoch 15589/40000, Loss: 6.651779403910041e-05, Learning Rate: 0.000308\n",
      "Epoch 15590/40000, Loss: 6.636731268372387e-05, Learning Rate: 0.000308\n",
      "Epoch 15591/40000, Loss: 1.614283974049613e-05, Learning Rate: 0.000308\n",
      "Epoch 15592/40000, Loss: 4.343298860476352e-05, Learning Rate: 0.000308\n",
      "Epoch 15593/40000, Loss: 4.2915537051158026e-05, Learning Rate: 0.000308\n",
      "Epoch 15594/40000, Loss: 4.083033127244562e-05, Learning Rate: 0.000308\n",
      "Epoch 15595/40000, Loss: 4.290325887268409e-05, Learning Rate: 0.000308\n",
      "Epoch 15596/40000, Loss: 3.5212851798860356e-05, Learning Rate: 0.000308\n",
      "Epoch 15597/40000, Loss: 6.669027061434463e-05, Learning Rate: 0.000308\n",
      "Epoch 15598/40000, Loss: 4.0815924876369536e-05, Learning Rate: 0.000308\n",
      "Epoch 15599/40000, Loss: 4.2689724068623036e-05, Learning Rate: 0.000308\n",
      "Epoch 15600/40000, Loss: 4.092124436283484e-05, Learning Rate: 0.000308\n",
      "Epoch 15601/40000, Loss: 4.079252539668232e-05, Learning Rate: 0.000308\n",
      "Epoch 15602/40000, Loss: 4.08462256018538e-05, Learning Rate: 0.000308\n",
      "Epoch 15603/40000, Loss: 4.274063030607067e-05, Learning Rate: 0.000308\n",
      "Epoch 15604/40000, Loss: 5.704188151867129e-05, Learning Rate: 0.000307\n",
      "Epoch 15605/40000, Loss: 5.719704131479375e-05, Learning Rate: 0.000307\n",
      "Epoch 15606/40000, Loss: 5.714416329283267e-05, Learning Rate: 0.000307\n",
      "Epoch 15607/40000, Loss: 3.504535197862424e-05, Learning Rate: 0.000307\n",
      "Epoch 15608/40000, Loss: 6.656309415120631e-05, Learning Rate: 0.000307\n",
      "Epoch 15609/40000, Loss: 6.65503102936782e-05, Learning Rate: 0.000307\n",
      "Epoch 15610/40000, Loss: 4.0695740608498454e-05, Learning Rate: 0.000307\n",
      "Epoch 15611/40000, Loss: 6.741561810486019e-05, Learning Rate: 0.000307\n",
      "Epoch 15612/40000, Loss: 4.099923899048008e-05, Learning Rate: 0.000307\n",
      "Epoch 15613/40000, Loss: 6.674260657746345e-05, Learning Rate: 0.000307\n",
      "Epoch 15614/40000, Loss: 5.73994402657263e-05, Learning Rate: 0.000307\n",
      "Epoch 15615/40000, Loss: 4.291769437259063e-05, Learning Rate: 0.000307\n",
      "Epoch 15616/40000, Loss: 1.5758541849208996e-05, Learning Rate: 0.000307\n",
      "Epoch 15617/40000, Loss: 6.682159437332302e-05, Learning Rate: 0.000307\n",
      "Epoch 15618/40000, Loss: 6.657456833636388e-05, Learning Rate: 0.000307\n",
      "Epoch 15619/40000, Loss: 4.087418710696511e-05, Learning Rate: 0.000307\n",
      "Epoch 15620/40000, Loss: 6.657879566773772e-05, Learning Rate: 0.000307\n",
      "Epoch 15621/40000, Loss: 4.115926640224643e-05, Learning Rate: 0.000307\n",
      "Epoch 15622/40000, Loss: 5.758635961683467e-05, Learning Rate: 0.000307\n",
      "Epoch 15623/40000, Loss: 4.1765495552681386e-05, Learning Rate: 0.000307\n",
      "Epoch 15624/40000, Loss: 4.1350860556121916e-05, Learning Rate: 0.000307\n",
      "Epoch 15625/40000, Loss: 4.139249722356908e-05, Learning Rate: 0.000307\n",
      "Epoch 15626/40000, Loss: 1.6692772987880744e-05, Learning Rate: 0.000307\n",
      "Epoch 15627/40000, Loss: 5.934982618782669e-05, Learning Rate: 0.000307\n",
      "Epoch 15628/40000, Loss: 5.921983392909169e-05, Learning Rate: 0.000307\n",
      "Epoch 15629/40000, Loss: 1.700422762951348e-05, Learning Rate: 0.000307\n",
      "Epoch 15630/40000, Loss: 6.03819644311443e-05, Learning Rate: 0.000307\n",
      "Epoch 15631/40000, Loss: 7.714425737503916e-05, Learning Rate: 0.000306\n",
      "Epoch 15632/40000, Loss: 7.490409188903868e-05, Learning Rate: 0.000306\n",
      "Epoch 15633/40000, Loss: 0.00010379069863120094, Learning Rate: 0.000306\n",
      "Epoch 15634/40000, Loss: 7.917306356830522e-05, Learning Rate: 0.000306\n",
      "Epoch 15635/40000, Loss: 2.4334967747563496e-05, Learning Rate: 0.000306\n",
      "Epoch 15636/40000, Loss: 0.0001633010688237846, Learning Rate: 0.000306\n",
      "Epoch 15637/40000, Loss: 2.541968206060119e-05, Learning Rate: 0.000306\n",
      "Epoch 15638/40000, Loss: 6.624046363867819e-05, Learning Rate: 0.000306\n",
      "Epoch 15639/40000, Loss: 7.296211697394028e-05, Learning Rate: 0.000306\n",
      "Epoch 15640/40000, Loss: 2.052007221209351e-05, Learning Rate: 0.000306\n",
      "Epoch 15641/40000, Loss: 4.5695891458308324e-05, Learning Rate: 0.000306\n",
      "Epoch 15642/40000, Loss: 4.854060534853488e-05, Learning Rate: 0.000306\n",
      "Epoch 15643/40000, Loss: 2.127773768734187e-05, Learning Rate: 0.000306\n",
      "Epoch 15644/40000, Loss: 3.673663013614714e-05, Learning Rate: 0.000306\n",
      "Epoch 15645/40000, Loss: 7.136659405659884e-05, Learning Rate: 0.000306\n",
      "Epoch 15646/40000, Loss: 4.7453231672989205e-05, Learning Rate: 0.000306\n",
      "Epoch 15647/40000, Loss: 6.896946433698758e-05, Learning Rate: 0.000306\n",
      "Epoch 15648/40000, Loss: 6.063482942408882e-05, Learning Rate: 0.000306\n",
      "Epoch 15649/40000, Loss: 4.6674194891238585e-05, Learning Rate: 0.000306\n",
      "Epoch 15650/40000, Loss: 3.664397809188813e-05, Learning Rate: 0.000306\n",
      "Epoch 15651/40000, Loss: 4.830840043723583e-05, Learning Rate: 0.000306\n",
      "Epoch 15652/40000, Loss: 6.004034003126435e-05, Learning Rate: 0.000306\n",
      "Epoch 15653/40000, Loss: 4.3396110413596034e-05, Learning Rate: 0.000306\n",
      "Epoch 15654/40000, Loss: 6.778991519240662e-05, Learning Rate: 0.000306\n",
      "Epoch 15655/40000, Loss: 1.675688872637693e-05, Learning Rate: 0.000306\n",
      "Epoch 15656/40000, Loss: 4.387180524645373e-05, Learning Rate: 0.000306\n",
      "Epoch 15657/40000, Loss: 5.8040943258674815e-05, Learning Rate: 0.000306\n",
      "Epoch 15658/40000, Loss: 1.648325996939093e-05, Learning Rate: 0.000305\n",
      "Epoch 15659/40000, Loss: 1.6314730601152405e-05, Learning Rate: 0.000305\n",
      "Epoch 15660/40000, Loss: 5.789032729808241e-05, Learning Rate: 0.000305\n",
      "Epoch 15661/40000, Loss: 5.7508059398969635e-05, Learning Rate: 0.000305\n",
      "Epoch 15662/40000, Loss: 3.584273508749902e-05, Learning Rate: 0.000305\n",
      "Epoch 15663/40000, Loss: 1.614494431123603e-05, Learning Rate: 0.000305\n",
      "Epoch 15664/40000, Loss: 7.014012226136401e-05, Learning Rate: 0.000305\n",
      "Epoch 15665/40000, Loss: 4.3514181015780196e-05, Learning Rate: 0.000305\n",
      "Epoch 15666/40000, Loss: 3.5424654925009236e-05, Learning Rate: 0.000305\n",
      "Epoch 15667/40000, Loss: 1.6280991985695437e-05, Learning Rate: 0.000305\n",
      "Epoch 15668/40000, Loss: 3.570938133634627e-05, Learning Rate: 0.000305\n",
      "Epoch 15669/40000, Loss: 6.983554339967668e-05, Learning Rate: 0.000305\n",
      "Epoch 15670/40000, Loss: 3.737841689144261e-05, Learning Rate: 0.000305\n",
      "Epoch 15671/40000, Loss: 5.8574063587002456e-05, Learning Rate: 0.000305\n",
      "Epoch 15672/40000, Loss: 6.822135765105486e-05, Learning Rate: 0.000305\n",
      "Epoch 15673/40000, Loss: 1.674520899541676e-05, Learning Rate: 0.000305\n",
      "Epoch 15674/40000, Loss: 7.084282697178423e-05, Learning Rate: 0.000305\n",
      "Epoch 15675/40000, Loss: 6.770389154553413e-05, Learning Rate: 0.000305\n",
      "Epoch 15676/40000, Loss: 1.904272721731104e-05, Learning Rate: 0.000305\n",
      "Epoch 15677/40000, Loss: 7.317630661418661e-05, Learning Rate: 0.000305\n",
      "Epoch 15678/40000, Loss: 1.7603515516384505e-05, Learning Rate: 0.000305\n",
      "Epoch 15679/40000, Loss: 6.752312765456736e-05, Learning Rate: 0.000305\n",
      "Epoch 15680/40000, Loss: 3.5926379496231675e-05, Learning Rate: 0.000305\n",
      "Epoch 15681/40000, Loss: 4.5569060603156686e-05, Learning Rate: 0.000305\n",
      "Epoch 15682/40000, Loss: 5.967904144199565e-05, Learning Rate: 0.000305\n",
      "Epoch 15683/40000, Loss: 5.882949335500598e-05, Learning Rate: 0.000305\n",
      "Epoch 15684/40000, Loss: 3.579371332307346e-05, Learning Rate: 0.000305\n",
      "Epoch 15685/40000, Loss: 6.41948136035353e-05, Learning Rate: 0.000305\n",
      "Epoch 15686/40000, Loss: 6.71830348437652e-05, Learning Rate: 0.000304\n",
      "Epoch 15687/40000, Loss: 6.67214480927214e-05, Learning Rate: 0.000304\n",
      "Epoch 15688/40000, Loss: 6.656362529611215e-05, Learning Rate: 0.000304\n",
      "Epoch 15689/40000, Loss: 4.1284263716079295e-05, Learning Rate: 0.000304\n",
      "Epoch 15690/40000, Loss: 4.099912621313706e-05, Learning Rate: 0.000304\n",
      "Epoch 15691/40000, Loss: 6.62683232803829e-05, Learning Rate: 0.000304\n",
      "Epoch 15692/40000, Loss: 4.314405668992549e-05, Learning Rate: 0.000304\n",
      "Epoch 15693/40000, Loss: 1.599010283825919e-05, Learning Rate: 0.000304\n",
      "Epoch 15694/40000, Loss: 4.10166394431144e-05, Learning Rate: 0.000304\n",
      "Epoch 15695/40000, Loss: 4.318714854889549e-05, Learning Rate: 0.000304\n",
      "Epoch 15696/40000, Loss: 1.631368104426656e-05, Learning Rate: 0.000304\n",
      "Epoch 15697/40000, Loss: 3.522114275256172e-05, Learning Rate: 0.000304\n",
      "Epoch 15698/40000, Loss: 1.6694015357643366e-05, Learning Rate: 0.000304\n",
      "Epoch 15699/40000, Loss: 6.63118262309581e-05, Learning Rate: 0.000304\n",
      "Epoch 15700/40000, Loss: 3.517551886034198e-05, Learning Rate: 0.000304\n",
      "Epoch 15701/40000, Loss: 1.601979420229327e-05, Learning Rate: 0.000304\n",
      "Epoch 15702/40000, Loss: 4.312416785978712e-05, Learning Rate: 0.000304\n",
      "Epoch 15703/40000, Loss: 4.0951472328742966e-05, Learning Rate: 0.000304\n",
      "Epoch 15704/40000, Loss: 4.363560219644569e-05, Learning Rate: 0.000304\n",
      "Epoch 15705/40000, Loss: 1.6598040019744076e-05, Learning Rate: 0.000304\n",
      "Epoch 15706/40000, Loss: 1.6143883840413764e-05, Learning Rate: 0.000304\n",
      "Epoch 15707/40000, Loss: 3.514962736517191e-05, Learning Rate: 0.000304\n",
      "Epoch 15708/40000, Loss: 6.6386055550538e-05, Learning Rate: 0.000304\n",
      "Epoch 15709/40000, Loss: 5.7284873037133366e-05, Learning Rate: 0.000304\n",
      "Epoch 15710/40000, Loss: 4.102842285647057e-05, Learning Rate: 0.000304\n",
      "Epoch 15711/40000, Loss: 1.577976581756957e-05, Learning Rate: 0.000304\n",
      "Epoch 15712/40000, Loss: 4.1387389501323923e-05, Learning Rate: 0.000304\n",
      "Epoch 15713/40000, Loss: 4.096818884136155e-05, Learning Rate: 0.000303\n",
      "Epoch 15714/40000, Loss: 6.638390914304182e-05, Learning Rate: 0.000303\n",
      "Epoch 15715/40000, Loss: 6.620430940529332e-05, Learning Rate: 0.000303\n",
      "Epoch 15716/40000, Loss: 5.7175431720679626e-05, Learning Rate: 0.000303\n",
      "Epoch 15717/40000, Loss: 4.099269426660612e-05, Learning Rate: 0.000303\n",
      "Epoch 15718/40000, Loss: 5.737975152442232e-05, Learning Rate: 0.000303\n",
      "Epoch 15719/40000, Loss: 6.675697659375146e-05, Learning Rate: 0.000303\n",
      "Epoch 15720/40000, Loss: 6.629872950725257e-05, Learning Rate: 0.000303\n",
      "Epoch 15721/40000, Loss: 6.705081614200026e-05, Learning Rate: 0.000303\n",
      "Epoch 15722/40000, Loss: 1.6043153664213605e-05, Learning Rate: 0.000303\n",
      "Epoch 15723/40000, Loss: 3.5631241189548746e-05, Learning Rate: 0.000303\n",
      "Epoch 15724/40000, Loss: 5.7710694818524644e-05, Learning Rate: 0.000303\n",
      "Epoch 15725/40000, Loss: 6.7598040914163e-05, Learning Rate: 0.000303\n",
      "Epoch 15726/40000, Loss: 3.599267438403331e-05, Learning Rate: 0.000303\n",
      "Epoch 15727/40000, Loss: 6.684505933662876e-05, Learning Rate: 0.000303\n",
      "Epoch 15728/40000, Loss: 3.5638986446429044e-05, Learning Rate: 0.000303\n",
      "Epoch 15729/40000, Loss: 5.916180816711858e-05, Learning Rate: 0.000303\n",
      "Epoch 15730/40000, Loss: 5.8173114666715264e-05, Learning Rate: 0.000303\n",
      "Epoch 15731/40000, Loss: 5.79687075514812e-05, Learning Rate: 0.000303\n",
      "Epoch 15732/40000, Loss: 3.8192949432414025e-05, Learning Rate: 0.000303\n",
      "Epoch 15733/40000, Loss: 6.155880691949278e-05, Learning Rate: 0.000303\n",
      "Epoch 15734/40000, Loss: 5.869928645552136e-05, Learning Rate: 0.000303\n",
      "Epoch 15735/40000, Loss: 3.685225601657294e-05, Learning Rate: 0.000303\n",
      "Epoch 15736/40000, Loss: 3.58788893208839e-05, Learning Rate: 0.000303\n",
      "Epoch 15737/40000, Loss: 5.8643163356464356e-05, Learning Rate: 0.000303\n",
      "Epoch 15738/40000, Loss: 5.8068326325155795e-05, Learning Rate: 0.000303\n",
      "Epoch 15739/40000, Loss: 7.212232594611123e-05, Learning Rate: 0.000303\n",
      "Epoch 15740/40000, Loss: 6.208795093698427e-05, Learning Rate: 0.000303\n",
      "Epoch 15741/40000, Loss: 4.791860919794999e-05, Learning Rate: 0.000302\n",
      "Epoch 15742/40000, Loss: 3.75427734979894e-05, Learning Rate: 0.000302\n",
      "Epoch 15743/40000, Loss: 7.412446575472131e-05, Learning Rate: 0.000302\n",
      "Epoch 15744/40000, Loss: 2.2530148271471262e-05, Learning Rate: 0.000302\n",
      "Epoch 15745/40000, Loss: 2.0022915123263374e-05, Learning Rate: 0.000302\n",
      "Epoch 15746/40000, Loss: 3.798412944888696e-05, Learning Rate: 0.000302\n",
      "Epoch 15747/40000, Loss: 3.839678174699657e-05, Learning Rate: 0.000302\n",
      "Epoch 15748/40000, Loss: 3.655859472928569e-05, Learning Rate: 0.000302\n",
      "Epoch 15749/40000, Loss: 6.647793634328991e-05, Learning Rate: 0.000302\n",
      "Epoch 15750/40000, Loss: 6.363162538036704e-05, Learning Rate: 0.000302\n",
      "Epoch 15751/40000, Loss: 8.567223267164081e-05, Learning Rate: 0.000302\n",
      "Epoch 15752/40000, Loss: 3.942248076782562e-05, Learning Rate: 0.000302\n",
      "Epoch 15753/40000, Loss: 4.407732194522396e-05, Learning Rate: 0.000302\n",
      "Epoch 15754/40000, Loss: 6.321902765193954e-05, Learning Rate: 0.000302\n",
      "Epoch 15755/40000, Loss: 1.8629398255143315e-05, Learning Rate: 0.000302\n",
      "Epoch 15756/40000, Loss: 6.758189556421712e-05, Learning Rate: 0.000302\n",
      "Epoch 15757/40000, Loss: 9.047271305462345e-05, Learning Rate: 0.000302\n",
      "Epoch 15758/40000, Loss: 3.7958816392347217e-05, Learning Rate: 0.000302\n",
      "Epoch 15759/40000, Loss: 7.677995745325461e-05, Learning Rate: 0.000302\n",
      "Epoch 15760/40000, Loss: 6.158106407383457e-05, Learning Rate: 0.000302\n",
      "Epoch 15761/40000, Loss: 7.443740469170734e-05, Learning Rate: 0.000302\n",
      "Epoch 15762/40000, Loss: 5.1899271056754515e-05, Learning Rate: 0.000302\n",
      "Epoch 15763/40000, Loss: 7.092782470863312e-05, Learning Rate: 0.000302\n",
      "Epoch 15764/40000, Loss: 4.366950088297017e-05, Learning Rate: 0.000302\n",
      "Epoch 15765/40000, Loss: 7.211398769868538e-05, Learning Rate: 0.000302\n",
      "Epoch 15766/40000, Loss: 1.9790109945461154e-05, Learning Rate: 0.000302\n",
      "Epoch 15767/40000, Loss: 6.999153265496716e-05, Learning Rate: 0.000302\n",
      "Epoch 15768/40000, Loss: 6.453024980146438e-05, Learning Rate: 0.000301\n",
      "Epoch 15769/40000, Loss: 5.159140346222557e-05, Learning Rate: 0.000301\n",
      "Epoch 15770/40000, Loss: 6.643240340054035e-05, Learning Rate: 0.000301\n",
      "Epoch 15771/40000, Loss: 3.98858028347604e-05, Learning Rate: 0.000301\n",
      "Epoch 15772/40000, Loss: 4.424334110808559e-05, Learning Rate: 0.000301\n",
      "Epoch 15773/40000, Loss: 7.007172825979069e-05, Learning Rate: 0.000301\n",
      "Epoch 15774/40000, Loss: 6.0774123994633555e-05, Learning Rate: 0.000301\n",
      "Epoch 15775/40000, Loss: 4.29221399826929e-05, Learning Rate: 0.000301\n",
      "Epoch 15776/40000, Loss: 4.1921073716366664e-05, Learning Rate: 0.000301\n",
      "Epoch 15777/40000, Loss: 3.8781308830948547e-05, Learning Rate: 0.000301\n",
      "Epoch 15778/40000, Loss: 5.236314973444678e-05, Learning Rate: 0.000301\n",
      "Epoch 15779/40000, Loss: 7.043332880130038e-05, Learning Rate: 0.000301\n",
      "Epoch 15780/40000, Loss: 6.934341217856854e-05, Learning Rate: 0.000301\n",
      "Epoch 15781/40000, Loss: 6.091021714382805e-05, Learning Rate: 0.000301\n",
      "Epoch 15782/40000, Loss: 4.037977851112373e-05, Learning Rate: 0.000301\n",
      "Epoch 15783/40000, Loss: 4.2071693314937875e-05, Learning Rate: 0.000301\n",
      "Epoch 15784/40000, Loss: 6.906449561938643e-05, Learning Rate: 0.000301\n",
      "Epoch 15785/40000, Loss: 4.471707870834507e-05, Learning Rate: 0.000301\n",
      "Epoch 15786/40000, Loss: 4.391334732645191e-05, Learning Rate: 0.000301\n",
      "Epoch 15787/40000, Loss: 4.508982601691969e-05, Learning Rate: 0.000301\n",
      "Epoch 15788/40000, Loss: 4.3535485019674525e-05, Learning Rate: 0.000301\n",
      "Epoch 15789/40000, Loss: 6.684074469376355e-05, Learning Rate: 0.000301\n",
      "Epoch 15790/40000, Loss: 4.221796916681342e-05, Learning Rate: 0.000301\n",
      "Epoch 15791/40000, Loss: 1.6403011613874696e-05, Learning Rate: 0.000301\n",
      "Epoch 15792/40000, Loss: 5.7630422816146165e-05, Learning Rate: 0.000301\n",
      "Epoch 15793/40000, Loss: 5.7198511058231816e-05, Learning Rate: 0.000301\n",
      "Epoch 15794/40000, Loss: 6.624568050028756e-05, Learning Rate: 0.000301\n",
      "Epoch 15795/40000, Loss: 4.102026287000626e-05, Learning Rate: 0.000301\n",
      "Epoch 15796/40000, Loss: 6.604756345041096e-05, Learning Rate: 0.000300\n",
      "Epoch 15797/40000, Loss: 4.135262861382216e-05, Learning Rate: 0.000300\n",
      "Epoch 15798/40000, Loss: 4.0663166146259755e-05, Learning Rate: 0.000300\n",
      "Epoch 15799/40000, Loss: 5.696695370716043e-05, Learning Rate: 0.000300\n",
      "Epoch 15800/40000, Loss: 4.089231515536085e-05, Learning Rate: 0.000300\n",
      "Epoch 15801/40000, Loss: 4.266568066668697e-05, Learning Rate: 0.000300\n",
      "Epoch 15802/40000, Loss: 3.496865974739194e-05, Learning Rate: 0.000300\n",
      "Epoch 15803/40000, Loss: 1.5750270904391073e-05, Learning Rate: 0.000300\n",
      "Epoch 15804/40000, Loss: 1.561376848258078e-05, Learning Rate: 0.000300\n",
      "Epoch 15805/40000, Loss: 3.502428808133118e-05, Learning Rate: 0.000300\n",
      "Epoch 15806/40000, Loss: 5.6847082305466756e-05, Learning Rate: 0.000300\n",
      "Epoch 15807/40000, Loss: 4.04312158934772e-05, Learning Rate: 0.000300\n",
      "Epoch 15808/40000, Loss: 5.7001008826773614e-05, Learning Rate: 0.000300\n",
      "Epoch 15809/40000, Loss: 5.6808254157658666e-05, Learning Rate: 0.000300\n",
      "Epoch 15810/40000, Loss: 4.2588788346620277e-05, Learning Rate: 0.000300\n",
      "Epoch 15811/40000, Loss: 5.70107476960402e-05, Learning Rate: 0.000300\n",
      "Epoch 15812/40000, Loss: 4.2663243220886216e-05, Learning Rate: 0.000300\n",
      "Epoch 15813/40000, Loss: 1.562974284752272e-05, Learning Rate: 0.000300\n",
      "Epoch 15814/40000, Loss: 1.565624370414298e-05, Learning Rate: 0.000300\n",
      "Epoch 15815/40000, Loss: 5.690826947102323e-05, Learning Rate: 0.000300\n",
      "Epoch 15816/40000, Loss: 4.2711453716037795e-05, Learning Rate: 0.000300\n",
      "Epoch 15817/40000, Loss: 1.5460344002349302e-05, Learning Rate: 0.000300\n",
      "Epoch 15818/40000, Loss: 5.6780587328830734e-05, Learning Rate: 0.000300\n",
      "Epoch 15819/40000, Loss: 4.286303374101408e-05, Learning Rate: 0.000300\n",
      "Epoch 15820/40000, Loss: 5.667516961693764e-05, Learning Rate: 0.000300\n",
      "Epoch 15821/40000, Loss: 1.547633291920647e-05, Learning Rate: 0.000300\n",
      "Epoch 15822/40000, Loss: 6.643751839874312e-05, Learning Rate: 0.000300\n",
      "Epoch 15823/40000, Loss: 4.265166353434324e-05, Learning Rate: 0.000300\n",
      "Epoch 15824/40000, Loss: 6.832133658463135e-05, Learning Rate: 0.000299\n",
      "Epoch 15825/40000, Loss: 3.5109780583297834e-05, Learning Rate: 0.000299\n",
      "Epoch 15826/40000, Loss: 4.265168900019489e-05, Learning Rate: 0.000299\n",
      "Epoch 15827/40000, Loss: 3.503932384774089e-05, Learning Rate: 0.000299\n",
      "Epoch 15828/40000, Loss: 4.2457395466044545e-05, Learning Rate: 0.000299\n",
      "Epoch 15829/40000, Loss: 6.629205745412037e-05, Learning Rate: 0.000299\n",
      "Epoch 15830/40000, Loss: 3.496215504128486e-05, Learning Rate: 0.000299\n",
      "Epoch 15831/40000, Loss: 4.054511373396963e-05, Learning Rate: 0.000299\n",
      "Epoch 15832/40000, Loss: 4.266728137736209e-05, Learning Rate: 0.000299\n",
      "Epoch 15833/40000, Loss: 3.497576108202338e-05, Learning Rate: 0.000299\n",
      "Epoch 15834/40000, Loss: 4.049509152537212e-05, Learning Rate: 0.000299\n",
      "Epoch 15835/40000, Loss: 4.361859828350134e-05, Learning Rate: 0.000299\n",
      "Epoch 15836/40000, Loss: 3.4994322049897164e-05, Learning Rate: 0.000299\n",
      "Epoch 15837/40000, Loss: 5.694338688044809e-05, Learning Rate: 0.000299\n",
      "Epoch 15838/40000, Loss: 6.593684520339593e-05, Learning Rate: 0.000299\n",
      "Epoch 15839/40000, Loss: 4.042761793243699e-05, Learning Rate: 0.000299\n",
      "Epoch 15840/40000, Loss: 4.05088321713265e-05, Learning Rate: 0.000299\n",
      "Epoch 15841/40000, Loss: 3.487101275823079e-05, Learning Rate: 0.000299\n",
      "Epoch 15842/40000, Loss: 1.5586985682602972e-05, Learning Rate: 0.000299\n",
      "Epoch 15843/40000, Loss: 4.264367089490406e-05, Learning Rate: 0.000299\n",
      "Epoch 15844/40000, Loss: 4.0559301851317286e-05, Learning Rate: 0.000299\n",
      "Epoch 15845/40000, Loss: 3.4966014936799183e-05, Learning Rate: 0.000299\n",
      "Epoch 15846/40000, Loss: 6.584904622286558e-05, Learning Rate: 0.000299\n",
      "Epoch 15847/40000, Loss: 3.484958506305702e-05, Learning Rate: 0.000299\n",
      "Epoch 15848/40000, Loss: 6.585881783394143e-05, Learning Rate: 0.000299\n",
      "Epoch 15849/40000, Loss: 3.502390973153524e-05, Learning Rate: 0.000299\n",
      "Epoch 15850/40000, Loss: 1.5658051779610105e-05, Learning Rate: 0.000299\n",
      "Epoch 15851/40000, Loss: 5.690219040843658e-05, Learning Rate: 0.000298\n",
      "Epoch 15852/40000, Loss: 5.704733484890312e-05, Learning Rate: 0.000298\n",
      "Epoch 15853/40000, Loss: 1.576437171024736e-05, Learning Rate: 0.000298\n",
      "Epoch 15854/40000, Loss: 1.5585213986923918e-05, Learning Rate: 0.000298\n",
      "Epoch 15855/40000, Loss: 6.615262100240216e-05, Learning Rate: 0.000298\n",
      "Epoch 15856/40000, Loss: 4.291452933102846e-05, Learning Rate: 0.000298\n",
      "Epoch 15857/40000, Loss: 6.743517587892711e-05, Learning Rate: 0.000298\n",
      "Epoch 15858/40000, Loss: 6.705884879920632e-05, Learning Rate: 0.000298\n",
      "Epoch 15859/40000, Loss: 4.071080911671743e-05, Learning Rate: 0.000298\n",
      "Epoch 15860/40000, Loss: 1.577785224071704e-05, Learning Rate: 0.000298\n",
      "Epoch 15861/40000, Loss: 6.632691656704992e-05, Learning Rate: 0.000298\n",
      "Epoch 15862/40000, Loss: 4.3638745410135016e-05, Learning Rate: 0.000298\n",
      "Epoch 15863/40000, Loss: 3.5148768802173436e-05, Learning Rate: 0.000298\n",
      "Epoch 15864/40000, Loss: 1.6604801203357056e-05, Learning Rate: 0.000298\n",
      "Epoch 15865/40000, Loss: 4.495826215134002e-05, Learning Rate: 0.000298\n",
      "Epoch 15866/40000, Loss: 3.551878398866393e-05, Learning Rate: 0.000298\n",
      "Epoch 15867/40000, Loss: 4.1112951294053346e-05, Learning Rate: 0.000298\n",
      "Epoch 15868/40000, Loss: 4.563828406389803e-05, Learning Rate: 0.000298\n",
      "Epoch 15869/40000, Loss: 4.538526991382241e-05, Learning Rate: 0.000298\n",
      "Epoch 15870/40000, Loss: 4.614347926690243e-05, Learning Rate: 0.000298\n",
      "Epoch 15871/40000, Loss: 4.648376489058137e-05, Learning Rate: 0.000298\n",
      "Epoch 15872/40000, Loss: 4.194529537926428e-05, Learning Rate: 0.000298\n",
      "Epoch 15873/40000, Loss: 3.549221219145693e-05, Learning Rate: 0.000298\n",
      "Epoch 15874/40000, Loss: 4.159193849773146e-05, Learning Rate: 0.000298\n",
      "Epoch 15875/40000, Loss: 6.21174112893641e-05, Learning Rate: 0.000298\n",
      "Epoch 15876/40000, Loss: 5.963463263469748e-05, Learning Rate: 0.000298\n",
      "Epoch 15877/40000, Loss: 1.9018394596059807e-05, Learning Rate: 0.000298\n",
      "Epoch 15878/40000, Loss: 6.405271415133029e-05, Learning Rate: 0.000298\n",
      "Epoch 15879/40000, Loss: 1.7446483980165794e-05, Learning Rate: 0.000297\n",
      "Epoch 15880/40000, Loss: 4.6362019929802045e-05, Learning Rate: 0.000297\n",
      "Epoch 15881/40000, Loss: 3.6402510886546224e-05, Learning Rate: 0.000297\n",
      "Epoch 15882/40000, Loss: 3.6401634133653715e-05, Learning Rate: 0.000297\n",
      "Epoch 15883/40000, Loss: 4.2105693864868954e-05, Learning Rate: 0.000297\n",
      "Epoch 15884/40000, Loss: 6.990344991208985e-05, Learning Rate: 0.000297\n",
      "Epoch 15885/40000, Loss: 4.4526273995870724e-05, Learning Rate: 0.000297\n",
      "Epoch 15886/40000, Loss: 4.477307811612263e-05, Learning Rate: 0.000297\n",
      "Epoch 15887/40000, Loss: 1.872606844699476e-05, Learning Rate: 0.000297\n",
      "Epoch 15888/40000, Loss: 4.581906250678003e-05, Learning Rate: 0.000297\n",
      "Epoch 15889/40000, Loss: 4.638480822904967e-05, Learning Rate: 0.000297\n",
      "Epoch 15890/40000, Loss: 1.9335097022121772e-05, Learning Rate: 0.000297\n",
      "Epoch 15891/40000, Loss: 4.626873851520941e-05, Learning Rate: 0.000297\n",
      "Epoch 15892/40000, Loss: 6.982680497458205e-05, Learning Rate: 0.000297\n",
      "Epoch 15893/40000, Loss: 4.335836274549365e-05, Learning Rate: 0.000297\n",
      "Epoch 15894/40000, Loss: 3.593781002564356e-05, Learning Rate: 0.000297\n",
      "Epoch 15895/40000, Loss: 5.895317008253187e-05, Learning Rate: 0.000297\n",
      "Epoch 15896/40000, Loss: 7.085817196639255e-05, Learning Rate: 0.000297\n",
      "Epoch 15897/40000, Loss: 6.0662718169623986e-05, Learning Rate: 0.000297\n",
      "Epoch 15898/40000, Loss: 4.349923983681947e-05, Learning Rate: 0.000297\n",
      "Epoch 15899/40000, Loss: 1.7414637113688514e-05, Learning Rate: 0.000297\n",
      "Epoch 15900/40000, Loss: 3.615592868300155e-05, Learning Rate: 0.000297\n",
      "Epoch 15901/40000, Loss: 4.113071918254718e-05, Learning Rate: 0.000297\n",
      "Epoch 15902/40000, Loss: 1.6580999727011658e-05, Learning Rate: 0.000297\n",
      "Epoch 15903/40000, Loss: 4.5329739805310965e-05, Learning Rate: 0.000297\n",
      "Epoch 15904/40000, Loss: 4.1446419345447794e-05, Learning Rate: 0.000297\n",
      "Epoch 15905/40000, Loss: 4.4400996557669714e-05, Learning Rate: 0.000297\n",
      "Epoch 15906/40000, Loss: 4.12568733736407e-05, Learning Rate: 0.000297\n",
      "Epoch 15907/40000, Loss: 3.540475881891325e-05, Learning Rate: 0.000296\n",
      "Epoch 15908/40000, Loss: 1.626422999834176e-05, Learning Rate: 0.000296\n",
      "Epoch 15909/40000, Loss: 3.5040342481806874e-05, Learning Rate: 0.000296\n",
      "Epoch 15910/40000, Loss: 5.7530713093001395e-05, Learning Rate: 0.000296\n",
      "Epoch 15911/40000, Loss: 4.349459049990401e-05, Learning Rate: 0.000296\n",
      "Epoch 15912/40000, Loss: 4.422492565936409e-05, Learning Rate: 0.000296\n",
      "Epoch 15913/40000, Loss: 6.357451638905331e-05, Learning Rate: 0.000296\n",
      "Epoch 15914/40000, Loss: 6.219445640454069e-05, Learning Rate: 0.000296\n",
      "Epoch 15915/40000, Loss: 1.6477421013405547e-05, Learning Rate: 0.000296\n",
      "Epoch 15916/40000, Loss: 3.563541395124048e-05, Learning Rate: 0.000296\n",
      "Epoch 15917/40000, Loss: 3.60461235686671e-05, Learning Rate: 0.000296\n",
      "Epoch 15918/40000, Loss: 5.9791054809466004e-05, Learning Rate: 0.000296\n",
      "Epoch 15919/40000, Loss: 5.844885890837759e-05, Learning Rate: 0.000296\n",
      "Epoch 15920/40000, Loss: 4.2866628064075485e-05, Learning Rate: 0.000296\n",
      "Epoch 15921/40000, Loss: 1.6770245565567166e-05, Learning Rate: 0.000296\n",
      "Epoch 15922/40000, Loss: 6.791658961446956e-05, Learning Rate: 0.000296\n",
      "Epoch 15923/40000, Loss: 4.38964634668082e-05, Learning Rate: 0.000296\n",
      "Epoch 15924/40000, Loss: 7.257291144924238e-05, Learning Rate: 0.000296\n",
      "Epoch 15925/40000, Loss: 5.831772796227597e-05, Learning Rate: 0.000296\n",
      "Epoch 15926/40000, Loss: 5.7709235989023e-05, Learning Rate: 0.000296\n",
      "Epoch 15927/40000, Loss: 6.932521500857547e-05, Learning Rate: 0.000296\n",
      "Epoch 15928/40000, Loss: 3.5630178899737075e-05, Learning Rate: 0.000296\n",
      "Epoch 15929/40000, Loss: 4.444272053660825e-05, Learning Rate: 0.000296\n",
      "Epoch 15930/40000, Loss: 3.585066588129848e-05, Learning Rate: 0.000296\n",
      "Epoch 15931/40000, Loss: 6.785971345379949e-05, Learning Rate: 0.000296\n",
      "Epoch 15932/40000, Loss: 5.773104203399271e-05, Learning Rate: 0.000296\n",
      "Epoch 15933/40000, Loss: 1.6290021449094638e-05, Learning Rate: 0.000296\n",
      "Epoch 15934/40000, Loss: 4.099609577679075e-05, Learning Rate: 0.000296\n",
      "Epoch 15935/40000, Loss: 4.065621760673821e-05, Learning Rate: 0.000296\n",
      "Epoch 15936/40000, Loss: 4.0943406929727644e-05, Learning Rate: 0.000295\n",
      "Epoch 15937/40000, Loss: 3.5506564017850906e-05, Learning Rate: 0.000295\n",
      "Epoch 15938/40000, Loss: 5.737324318033643e-05, Learning Rate: 0.000295\n",
      "Epoch 15939/40000, Loss: 4.342777901911177e-05, Learning Rate: 0.000295\n",
      "Epoch 15940/40000, Loss: 1.623280877538491e-05, Learning Rate: 0.000295\n",
      "Epoch 15941/40000, Loss: 3.5815999581245705e-05, Learning Rate: 0.000295\n",
      "Epoch 15942/40000, Loss: 5.8338631788501516e-05, Learning Rate: 0.000295\n",
      "Epoch 15943/40000, Loss: 3.689661389216781e-05, Learning Rate: 0.000295\n",
      "Epoch 15944/40000, Loss: 5.8372119383420795e-05, Learning Rate: 0.000295\n",
      "Epoch 15945/40000, Loss: 6.695197953376919e-05, Learning Rate: 0.000295\n",
      "Epoch 15946/40000, Loss: 4.348805668996647e-05, Learning Rate: 0.000295\n",
      "Epoch 15947/40000, Loss: 4.3829033529618755e-05, Learning Rate: 0.000295\n",
      "Epoch 15948/40000, Loss: 5.9892950957873836e-05, Learning Rate: 0.000295\n",
      "Epoch 15949/40000, Loss: 5.946219607722014e-05, Learning Rate: 0.000295\n",
      "Epoch 15950/40000, Loss: 6.764382851542905e-05, Learning Rate: 0.000295\n",
      "Epoch 15951/40000, Loss: 6.842649600002915e-05, Learning Rate: 0.000295\n",
      "Epoch 15952/40000, Loss: 4.457000250113197e-05, Learning Rate: 0.000295\n",
      "Epoch 15953/40000, Loss: 4.401024125399999e-05, Learning Rate: 0.000295\n",
      "Epoch 15954/40000, Loss: 7.036482566036284e-05, Learning Rate: 0.000295\n",
      "Epoch 15955/40000, Loss: 5.8908932260237634e-05, Learning Rate: 0.000295\n",
      "Epoch 15956/40000, Loss: 7.63272400945425e-05, Learning Rate: 0.000295\n",
      "Epoch 15957/40000, Loss: 3.869400097755715e-05, Learning Rate: 0.000295\n",
      "Epoch 15958/40000, Loss: 2.031913390965201e-05, Learning Rate: 0.000295\n",
      "Epoch 15959/40000, Loss: 5.9744445024989545e-05, Learning Rate: 0.000295\n",
      "Epoch 15960/40000, Loss: 4.209042162983678e-05, Learning Rate: 0.000295\n",
      "Epoch 15961/40000, Loss: 4.1694533138070256e-05, Learning Rate: 0.000295\n",
      "Epoch 15962/40000, Loss: 6.966909859329462e-05, Learning Rate: 0.000295\n",
      "Epoch 15963/40000, Loss: 1.9437144146650098e-05, Learning Rate: 0.000295\n",
      "Epoch 15964/40000, Loss: 1.6788588254712522e-05, Learning Rate: 0.000294\n",
      "Epoch 15965/40000, Loss: 6.955425487831235e-05, Learning Rate: 0.000294\n",
      "Epoch 15966/40000, Loss: 5.360909563023597e-05, Learning Rate: 0.000294\n",
      "Epoch 15967/40000, Loss: 5.9414309362182394e-05, Learning Rate: 0.000294\n",
      "Epoch 15968/40000, Loss: 4.320049629313871e-05, Learning Rate: 0.000294\n",
      "Epoch 15969/40000, Loss: 3.7325651646824554e-05, Learning Rate: 0.000294\n",
      "Epoch 15970/40000, Loss: 5.0772283429978415e-05, Learning Rate: 0.000294\n",
      "Epoch 15971/40000, Loss: 4.223700670991093e-05, Learning Rate: 0.000294\n",
      "Epoch 15972/40000, Loss: 2.2403322873287834e-05, Learning Rate: 0.000294\n",
      "Epoch 15973/40000, Loss: 7.31559848645702e-05, Learning Rate: 0.000294\n",
      "Epoch 15974/40000, Loss: 5.351332947611809e-05, Learning Rate: 0.000294\n",
      "Epoch 15975/40000, Loss: 4.2969841160811484e-05, Learning Rate: 0.000294\n",
      "Epoch 15976/40000, Loss: 1.7759350157575682e-05, Learning Rate: 0.000294\n",
      "Epoch 15977/40000, Loss: 6.87358042341657e-05, Learning Rate: 0.000294\n",
      "Epoch 15978/40000, Loss: 6.194254820002243e-05, Learning Rate: 0.000294\n",
      "Epoch 15979/40000, Loss: 1.7676278730505146e-05, Learning Rate: 0.000294\n",
      "Epoch 15980/40000, Loss: 6.932608812348917e-05, Learning Rate: 0.000294\n",
      "Epoch 15981/40000, Loss: 6.021256558597088e-05, Learning Rate: 0.000294\n",
      "Epoch 15982/40000, Loss: 1.728056486172136e-05, Learning Rate: 0.000294\n",
      "Epoch 15983/40000, Loss: 3.64425613952335e-05, Learning Rate: 0.000294\n",
      "Epoch 15984/40000, Loss: 4.575254934024997e-05, Learning Rate: 0.000294\n",
      "Epoch 15985/40000, Loss: 5.9510715800570324e-05, Learning Rate: 0.000294\n",
      "Epoch 15986/40000, Loss: 3.6426616134122014e-05, Learning Rate: 0.000294\n",
      "Epoch 15987/40000, Loss: 6.754542846465483e-05, Learning Rate: 0.000294\n",
      "Epoch 15988/40000, Loss: 6.6396314650774e-05, Learning Rate: 0.000294\n",
      "Epoch 15989/40000, Loss: 3.799725527642295e-05, Learning Rate: 0.000294\n",
      "Epoch 15990/40000, Loss: 4.198791430098936e-05, Learning Rate: 0.000294\n",
      "Epoch 15991/40000, Loss: 5.906700971536338e-05, Learning Rate: 0.000294\n",
      "Epoch 15992/40000, Loss: 4.451522545423359e-05, Learning Rate: 0.000293\n",
      "Epoch 15993/40000, Loss: 3.585973900044337e-05, Learning Rate: 0.000293\n",
      "Epoch 15994/40000, Loss: 1.7966111045097932e-05, Learning Rate: 0.000293\n",
      "Epoch 15995/40000, Loss: 6.22053921688348e-05, Learning Rate: 0.000293\n",
      "Epoch 15996/40000, Loss: 3.853689122479409e-05, Learning Rate: 0.000293\n",
      "Epoch 15997/40000, Loss: 3.732258119271137e-05, Learning Rate: 0.000293\n",
      "Epoch 15998/40000, Loss: 4.21374716097489e-05, Learning Rate: 0.000293\n",
      "Epoch 15999/40000, Loss: 4.728646672447212e-05, Learning Rate: 0.000293\n",
      "Epoch 16000/40000, Loss: 6.833743827883154e-05, Learning Rate: 0.000293\n",
      "Epoch 16001/40000, Loss: 6.612393190152943e-05, Learning Rate: 0.000293\n",
      "Epoch 16002/40000, Loss: 7.776067650411278e-05, Learning Rate: 0.000293\n",
      "Epoch 16003/40000, Loss: 4.294270547688939e-05, Learning Rate: 0.000293\n",
      "Epoch 16004/40000, Loss: 1.7192078303196467e-05, Learning Rate: 0.000293\n",
      "Epoch 16005/40000, Loss: 4.759017610922456e-05, Learning Rate: 0.000293\n",
      "Epoch 16006/40000, Loss: 4.1624356526881456e-05, Learning Rate: 0.000293\n",
      "Epoch 16007/40000, Loss: 6.0112510254839435e-05, Learning Rate: 0.000293\n",
      "Epoch 16008/40000, Loss: 1.7138479961431585e-05, Learning Rate: 0.000293\n",
      "Epoch 16009/40000, Loss: 3.636692781583406e-05, Learning Rate: 0.000293\n",
      "Epoch 16010/40000, Loss: 5.946818055235781e-05, Learning Rate: 0.000293\n",
      "Epoch 16011/40000, Loss: 4.200141120236367e-05, Learning Rate: 0.000293\n",
      "Epoch 16012/40000, Loss: 4.115693809580989e-05, Learning Rate: 0.000293\n",
      "Epoch 16013/40000, Loss: 4.377995355753228e-05, Learning Rate: 0.000293\n",
      "Epoch 16014/40000, Loss: 6.215744360815734e-05, Learning Rate: 0.000293\n",
      "Epoch 16015/40000, Loss: 4.101542435819283e-05, Learning Rate: 0.000293\n",
      "Epoch 16016/40000, Loss: 6.437417323468253e-05, Learning Rate: 0.000293\n",
      "Epoch 16017/40000, Loss: 4.1533719922881573e-05, Learning Rate: 0.000293\n",
      "Epoch 16018/40000, Loss: 5.155756662134081e-05, Learning Rate: 0.000293\n",
      "Epoch 16019/40000, Loss: 6.476760609075427e-05, Learning Rate: 0.000293\n",
      "Epoch 16020/40000, Loss: 4.143632031627931e-05, Learning Rate: 0.000293\n",
      "Epoch 16021/40000, Loss: 4.7116627683863044e-05, Learning Rate: 0.000292\n",
      "Epoch 16022/40000, Loss: 6.777842645533383e-05, Learning Rate: 0.000292\n",
      "Epoch 16023/40000, Loss: 4.220224218443036e-05, Learning Rate: 0.000292\n",
      "Epoch 16024/40000, Loss: 6.797789683332667e-05, Learning Rate: 0.000292\n",
      "Epoch 16025/40000, Loss: 6.699994264636189e-05, Learning Rate: 0.000292\n",
      "Epoch 16026/40000, Loss: 6.714068149449304e-05, Learning Rate: 0.000292\n",
      "Epoch 16027/40000, Loss: 3.575103255570866e-05, Learning Rate: 0.000292\n",
      "Epoch 16028/40000, Loss: 1.7303496861131862e-05, Learning Rate: 0.000292\n",
      "Epoch 16029/40000, Loss: 4.119873483432457e-05, Learning Rate: 0.000292\n",
      "Epoch 16030/40000, Loss: 7.18524242984131e-05, Learning Rate: 0.000292\n",
      "Epoch 16031/40000, Loss: 8.065863221418113e-05, Learning Rate: 0.000292\n",
      "Epoch 16032/40000, Loss: 3.788132016779855e-05, Learning Rate: 0.000292\n",
      "Epoch 16033/40000, Loss: 7.321043085539714e-05, Learning Rate: 0.000292\n",
      "Epoch 16034/40000, Loss: 2.479126487742178e-05, Learning Rate: 0.000292\n",
      "Epoch 16035/40000, Loss: 2.201352072006557e-05, Learning Rate: 0.000292\n",
      "Epoch 16036/40000, Loss: 9.394266817253083e-05, Learning Rate: 0.000292\n",
      "Epoch 16037/40000, Loss: 8.166907355189323e-05, Learning Rate: 0.000292\n",
      "Epoch 16038/40000, Loss: 2.5657311198301613e-05, Learning Rate: 0.000292\n",
      "Epoch 16039/40000, Loss: 7.485038076993078e-05, Learning Rate: 0.000292\n",
      "Epoch 16040/40000, Loss: 5.347815022105351e-05, Learning Rate: 0.000292\n",
      "Epoch 16041/40000, Loss: 6.651290459558368e-05, Learning Rate: 0.000292\n",
      "Epoch 16042/40000, Loss: 4.009678013972007e-05, Learning Rate: 0.000292\n",
      "Epoch 16043/40000, Loss: 4.109455403522588e-05, Learning Rate: 0.000292\n",
      "Epoch 16044/40000, Loss: 7.130411540856585e-05, Learning Rate: 0.000292\n",
      "Epoch 16045/40000, Loss: 4.9452370149083436e-05, Learning Rate: 0.000292\n",
      "Epoch 16046/40000, Loss: 7.449672557413578e-05, Learning Rate: 0.000292\n",
      "Epoch 16047/40000, Loss: 6.382016727002338e-05, Learning Rate: 0.000292\n",
      "Epoch 16048/40000, Loss: 3.8228114135563374e-05, Learning Rate: 0.000292\n",
      "Epoch 16049/40000, Loss: 4.5854318159399554e-05, Learning Rate: 0.000291\n",
      "Epoch 16050/40000, Loss: 1.8298043869435787e-05, Learning Rate: 0.000291\n",
      "Epoch 16051/40000, Loss: 4.600091415341012e-05, Learning Rate: 0.000291\n",
      "Epoch 16052/40000, Loss: 6.787727033952251e-05, Learning Rate: 0.000291\n",
      "Epoch 16053/40000, Loss: 6.066070272936486e-05, Learning Rate: 0.000291\n",
      "Epoch 16054/40000, Loss: 5.871483517694287e-05, Learning Rate: 0.000291\n",
      "Epoch 16055/40000, Loss: 5.8051740779774264e-05, Learning Rate: 0.000291\n",
      "Epoch 16056/40000, Loss: 4.1704217437654734e-05, Learning Rate: 0.000291\n",
      "Epoch 16057/40000, Loss: 6.935869168955833e-05, Learning Rate: 0.000291\n",
      "Epoch 16058/40000, Loss: 3.543710772646591e-05, Learning Rate: 0.000291\n",
      "Epoch 16059/40000, Loss: 7.404460484394804e-05, Learning Rate: 0.000291\n",
      "Epoch 16060/40000, Loss: 1.6708549082977697e-05, Learning Rate: 0.000291\n",
      "Epoch 16061/40000, Loss: 0.00010924916568910703, Learning Rate: 0.000291\n",
      "Epoch 16062/40000, Loss: 1.7153304725070484e-05, Learning Rate: 0.000291\n",
      "Epoch 16063/40000, Loss: 4.6456425479846075e-05, Learning Rate: 0.000291\n",
      "Epoch 16064/40000, Loss: 7.535598706454039e-05, Learning Rate: 0.000291\n",
      "Epoch 16065/40000, Loss: 4.29777501267381e-05, Learning Rate: 0.000291\n",
      "Epoch 16066/40000, Loss: 1.928840356413275e-05, Learning Rate: 0.000291\n",
      "Epoch 16067/40000, Loss: 1.7320227925665677e-05, Learning Rate: 0.000291\n",
      "Epoch 16068/40000, Loss: 1.6615696949884295e-05, Learning Rate: 0.000291\n",
      "Epoch 16069/40000, Loss: 4.108712892048061e-05, Learning Rate: 0.000291\n",
      "Epoch 16070/40000, Loss: 7.008724060142413e-05, Learning Rate: 0.000291\n",
      "Epoch 16071/40000, Loss: 1.6413399862358347e-05, Learning Rate: 0.000291\n",
      "Epoch 16072/40000, Loss: 6.831657810835168e-05, Learning Rate: 0.000291\n",
      "Epoch 16073/40000, Loss: 4.3534033466130495e-05, Learning Rate: 0.000291\n",
      "Epoch 16074/40000, Loss: 1.5664099919376895e-05, Learning Rate: 0.000291\n",
      "Epoch 16075/40000, Loss: 5.755665188189596e-05, Learning Rate: 0.000291\n",
      "Epoch 16076/40000, Loss: 6.600306369364262e-05, Learning Rate: 0.000291\n",
      "Epoch 16077/40000, Loss: 3.493706026347354e-05, Learning Rate: 0.000291\n",
      "Epoch 16078/40000, Loss: 4.063961387146264e-05, Learning Rate: 0.000290\n",
      "Epoch 16079/40000, Loss: 5.7043951528612524e-05, Learning Rate: 0.000290\n",
      "Epoch 16080/40000, Loss: 5.676366708939895e-05, Learning Rate: 0.000290\n",
      "Epoch 16081/40000, Loss: 4.0673374314792454e-05, Learning Rate: 0.000290\n",
      "Epoch 16082/40000, Loss: 4.24678364652209e-05, Learning Rate: 0.000290\n",
      "Epoch 16083/40000, Loss: 4.075613833265379e-05, Learning Rate: 0.000290\n",
      "Epoch 16084/40000, Loss: 4.3003539758501574e-05, Learning Rate: 0.000290\n",
      "Epoch 16085/40000, Loss: 4.3081545300083235e-05, Learning Rate: 0.000290\n",
      "Epoch 16086/40000, Loss: 4.075716424267739e-05, Learning Rate: 0.000290\n",
      "Epoch 16087/40000, Loss: 5.6709035561652854e-05, Learning Rate: 0.000290\n",
      "Epoch 16088/40000, Loss: 5.6473552831448615e-05, Learning Rate: 0.000290\n",
      "Epoch 16089/40000, Loss: 1.5315989003283903e-05, Learning Rate: 0.000290\n",
      "Epoch 16090/40000, Loss: 1.5313085896195844e-05, Learning Rate: 0.000290\n",
      "Epoch 16091/40000, Loss: 5.663965566782281e-05, Learning Rate: 0.000290\n",
      "Epoch 16092/40000, Loss: 3.469529838184826e-05, Learning Rate: 0.000290\n",
      "Epoch 16093/40000, Loss: 6.558698078151792e-05, Learning Rate: 0.000290\n",
      "Epoch 16094/40000, Loss: 4.255223393556662e-05, Learning Rate: 0.000290\n",
      "Epoch 16095/40000, Loss: 1.5287700080079958e-05, Learning Rate: 0.000290\n",
      "Epoch 16096/40000, Loss: 3.459108484094031e-05, Learning Rate: 0.000290\n",
      "Epoch 16097/40000, Loss: 6.542710616486147e-05, Learning Rate: 0.000290\n",
      "Epoch 16098/40000, Loss: 1.5253374840540346e-05, Learning Rate: 0.000290\n",
      "Epoch 16099/40000, Loss: 5.633436740026809e-05, Learning Rate: 0.000290\n",
      "Epoch 16100/40000, Loss: 3.465073314146139e-05, Learning Rate: 0.000290\n",
      "Epoch 16101/40000, Loss: 1.527345011709258e-05, Learning Rate: 0.000290\n",
      "Epoch 16102/40000, Loss: 4.003235881100409e-05, Learning Rate: 0.000290\n",
      "Epoch 16103/40000, Loss: 4.2256600863765925e-05, Learning Rate: 0.000290\n",
      "Epoch 16104/40000, Loss: 3.4632510505616665e-05, Learning Rate: 0.000290\n",
      "Epoch 16105/40000, Loss: 4.0172348235500976e-05, Learning Rate: 0.000290\n",
      "Epoch 16106/40000, Loss: 4.236223321640864e-05, Learning Rate: 0.000290\n",
      "Epoch 16107/40000, Loss: 4.238267501932569e-05, Learning Rate: 0.000289\n",
      "Epoch 16108/40000, Loss: 4.2246530938427895e-05, Learning Rate: 0.000289\n",
      "Epoch 16109/40000, Loss: 4.2228242818964645e-05, Learning Rate: 0.000289\n",
      "Epoch 16110/40000, Loss: 3.457787897787057e-05, Learning Rate: 0.000289\n",
      "Epoch 16111/40000, Loss: 6.529826350742951e-05, Learning Rate: 0.000289\n",
      "Epoch 16112/40000, Loss: 5.64407637284603e-05, Learning Rate: 0.000289\n",
      "Epoch 16113/40000, Loss: 4.2342977394582704e-05, Learning Rate: 0.000289\n",
      "Epoch 16114/40000, Loss: 4.2347430280642584e-05, Learning Rate: 0.000289\n",
      "Epoch 16115/40000, Loss: 4.013592842966318e-05, Learning Rate: 0.000289\n",
      "Epoch 16116/40000, Loss: 5.634684202959761e-05, Learning Rate: 0.000289\n",
      "Epoch 16117/40000, Loss: 3.459821891738102e-05, Learning Rate: 0.000289\n",
      "Epoch 16118/40000, Loss: 5.638765651383437e-05, Learning Rate: 0.000289\n",
      "Epoch 16119/40000, Loss: 3.460639709373936e-05, Learning Rate: 0.000289\n",
      "Epoch 16120/40000, Loss: 1.536006857350003e-05, Learning Rate: 0.000289\n",
      "Epoch 16121/40000, Loss: 6.518253212561831e-05, Learning Rate: 0.000289\n",
      "Epoch 16122/40000, Loss: 4.232804349157959e-05, Learning Rate: 0.000289\n",
      "Epoch 16123/40000, Loss: 1.5279176295734942e-05, Learning Rate: 0.000289\n",
      "Epoch 16124/40000, Loss: 3.4609969588927925e-05, Learning Rate: 0.000289\n",
      "Epoch 16125/40000, Loss: 4.0057428122963756e-05, Learning Rate: 0.000289\n",
      "Epoch 16126/40000, Loss: 5.6348249927395955e-05, Learning Rate: 0.000289\n",
      "Epoch 16127/40000, Loss: 3.446158370934427e-05, Learning Rate: 0.000289\n",
      "Epoch 16128/40000, Loss: 4.232459468767047e-05, Learning Rate: 0.000289\n",
      "Epoch 16129/40000, Loss: 4.2377036152174696e-05, Learning Rate: 0.000289\n",
      "Epoch 16130/40000, Loss: 4.226102100801654e-05, Learning Rate: 0.000289\n",
      "Epoch 16131/40000, Loss: 1.5309886293835007e-05, Learning Rate: 0.000289\n",
      "Epoch 16132/40000, Loss: 5.64145520911552e-05, Learning Rate: 0.000289\n",
      "Epoch 16133/40000, Loss: 3.456898048170842e-05, Learning Rate: 0.000289\n",
      "Epoch 16134/40000, Loss: 6.521686009364203e-05, Learning Rate: 0.000289\n",
      "Epoch 16135/40000, Loss: 3.45916960213799e-05, Learning Rate: 0.000288\n",
      "Epoch 16136/40000, Loss: 5.635391062241979e-05, Learning Rate: 0.000288\n",
      "Epoch 16137/40000, Loss: 4.001704655820504e-05, Learning Rate: 0.000288\n",
      "Epoch 16138/40000, Loss: 6.521534669445828e-05, Learning Rate: 0.000288\n",
      "Epoch 16139/40000, Loss: 4.015650483779609e-05, Learning Rate: 0.000288\n",
      "Epoch 16140/40000, Loss: 4.247629476594739e-05, Learning Rate: 0.000288\n",
      "Epoch 16141/40000, Loss: 3.46856213582214e-05, Learning Rate: 0.000288\n",
      "Epoch 16142/40000, Loss: 5.6760698498692364e-05, Learning Rate: 0.000288\n",
      "Epoch 16143/40000, Loss: 4.025159796583466e-05, Learning Rate: 0.000288\n",
      "Epoch 16144/40000, Loss: 4.256780812283978e-05, Learning Rate: 0.000288\n",
      "Epoch 16145/40000, Loss: 1.5582298146910034e-05, Learning Rate: 0.000288\n",
      "Epoch 16146/40000, Loss: 1.568352490721736e-05, Learning Rate: 0.000288\n",
      "Epoch 16147/40000, Loss: 1.5520099623245187e-05, Learning Rate: 0.000288\n",
      "Epoch 16148/40000, Loss: 3.477343125268817e-05, Learning Rate: 0.000288\n",
      "Epoch 16149/40000, Loss: 6.578314787475392e-05, Learning Rate: 0.000288\n",
      "Epoch 16150/40000, Loss: 4.044316301587969e-05, Learning Rate: 0.000288\n",
      "Epoch 16151/40000, Loss: 3.492449104669504e-05, Learning Rate: 0.000288\n",
      "Epoch 16152/40000, Loss: 6.581045454367995e-05, Learning Rate: 0.000288\n",
      "Epoch 16153/40000, Loss: 4.481777432374656e-05, Learning Rate: 0.000288\n",
      "Epoch 16154/40000, Loss: 4.577331492328085e-05, Learning Rate: 0.000288\n",
      "Epoch 16155/40000, Loss: 1.714029349386692e-05, Learning Rate: 0.000288\n",
      "Epoch 16156/40000, Loss: 6.741794641129673e-05, Learning Rate: 0.000288\n",
      "Epoch 16157/40000, Loss: 2.3723549020360224e-05, Learning Rate: 0.000288\n",
      "Epoch 16158/40000, Loss: 7.160182576626539e-05, Learning Rate: 0.000288\n",
      "Epoch 16159/40000, Loss: 4.521219671005383e-05, Learning Rate: 0.000288\n",
      "Epoch 16160/40000, Loss: 5.514128497452475e-05, Learning Rate: 0.000288\n",
      "Epoch 16161/40000, Loss: 8.619151049060747e-05, Learning Rate: 0.000288\n",
      "Epoch 16162/40000, Loss: 7.464148075086996e-05, Learning Rate: 0.000288\n",
      "Epoch 16163/40000, Loss: 4.8600657464703545e-05, Learning Rate: 0.000288\n",
      "Epoch 16164/40000, Loss: 8.645088382763788e-05, Learning Rate: 0.000287\n",
      "Epoch 16165/40000, Loss: 5.1956561947008595e-05, Learning Rate: 0.000287\n",
      "Epoch 16166/40000, Loss: 4.638668542611413e-05, Learning Rate: 0.000287\n",
      "Epoch 16167/40000, Loss: 6.797759124310687e-05, Learning Rate: 0.000287\n",
      "Epoch 16168/40000, Loss: 4.915242607239634e-05, Learning Rate: 0.000287\n",
      "Epoch 16169/40000, Loss: 5.7173794630216435e-05, Learning Rate: 0.000287\n",
      "Epoch 16170/40000, Loss: 3.088202720391564e-05, Learning Rate: 0.000287\n",
      "Epoch 16171/40000, Loss: 7.453405851265416e-05, Learning Rate: 0.000287\n",
      "Epoch 16172/40000, Loss: 3.765380824916065e-05, Learning Rate: 0.000287\n",
      "Epoch 16173/40000, Loss: 6.118707096902654e-05, Learning Rate: 0.000287\n",
      "Epoch 16174/40000, Loss: 3.8644473534077406e-05, Learning Rate: 0.000287\n",
      "Epoch 16175/40000, Loss: 4.541369708022103e-05, Learning Rate: 0.000287\n",
      "Epoch 16176/40000, Loss: 5.9579302615020424e-05, Learning Rate: 0.000287\n",
      "Epoch 16177/40000, Loss: 4.58028371213004e-05, Learning Rate: 0.000287\n",
      "Epoch 16178/40000, Loss: 4.56602247140836e-05, Learning Rate: 0.000287\n",
      "Epoch 16179/40000, Loss: 4.378946323413402e-05, Learning Rate: 0.000287\n",
      "Epoch 16180/40000, Loss: 4.368128065834753e-05, Learning Rate: 0.000287\n",
      "Epoch 16181/40000, Loss: 4.135955896344967e-05, Learning Rate: 0.000287\n",
      "Epoch 16182/40000, Loss: 3.499159720377065e-05, Learning Rate: 0.000287\n",
      "Epoch 16183/40000, Loss: 4.3066735088359565e-05, Learning Rate: 0.000287\n",
      "Epoch 16184/40000, Loss: 5.702169437427074e-05, Learning Rate: 0.000287\n",
      "Epoch 16185/40000, Loss: 6.590894918190315e-05, Learning Rate: 0.000287\n",
      "Epoch 16186/40000, Loss: 4.284768147044815e-05, Learning Rate: 0.000287\n",
      "Epoch 16187/40000, Loss: 6.567377567989752e-05, Learning Rate: 0.000287\n",
      "Epoch 16188/40000, Loss: 4.3597541662165895e-05, Learning Rate: 0.000287\n",
      "Epoch 16189/40000, Loss: 5.696708831237629e-05, Learning Rate: 0.000287\n",
      "Epoch 16190/40000, Loss: 4.122136306250468e-05, Learning Rate: 0.000287\n",
      "Epoch 16191/40000, Loss: 1.604208409844432e-05, Learning Rate: 0.000287\n",
      "Epoch 16192/40000, Loss: 3.493425901979208e-05, Learning Rate: 0.000287\n",
      "Epoch 16193/40000, Loss: 5.687220982508734e-05, Learning Rate: 0.000286\n",
      "Epoch 16194/40000, Loss: 1.579131821927149e-05, Learning Rate: 0.000286\n",
      "Epoch 16195/40000, Loss: 6.65895568090491e-05, Learning Rate: 0.000286\n",
      "Epoch 16196/40000, Loss: 3.6961075238650665e-05, Learning Rate: 0.000286\n",
      "Epoch 16197/40000, Loss: 6.592262798221782e-05, Learning Rate: 0.000286\n",
      "Epoch 16198/40000, Loss: 1.5717465430498123e-05, Learning Rate: 0.000286\n",
      "Epoch 16199/40000, Loss: 3.509340967866592e-05, Learning Rate: 0.000286\n",
      "Epoch 16200/40000, Loss: 1.5675837857997976e-05, Learning Rate: 0.000286\n",
      "Epoch 16201/40000, Loss: 6.649830174865201e-05, Learning Rate: 0.000286\n",
      "Epoch 16202/40000, Loss: 4.020571941509843e-05, Learning Rate: 0.000286\n",
      "Epoch 16203/40000, Loss: 3.5048771678702906e-05, Learning Rate: 0.000286\n",
      "Epoch 16204/40000, Loss: 4.0712162444833666e-05, Learning Rate: 0.000286\n",
      "Epoch 16205/40000, Loss: 6.5914515289478e-05, Learning Rate: 0.000286\n",
      "Epoch 16206/40000, Loss: 6.576576561201364e-05, Learning Rate: 0.000286\n",
      "Epoch 16207/40000, Loss: 4.104225445189513e-05, Learning Rate: 0.000286\n",
      "Epoch 16208/40000, Loss: 1.5452260413439944e-05, Learning Rate: 0.000286\n",
      "Epoch 16209/40000, Loss: 5.67457391298376e-05, Learning Rate: 0.000286\n",
      "Epoch 16210/40000, Loss: 4.262340007699095e-05, Learning Rate: 0.000286\n",
      "Epoch 16211/40000, Loss: 3.4462187613826245e-05, Learning Rate: 0.000286\n",
      "Epoch 16212/40000, Loss: 6.541796756209806e-05, Learning Rate: 0.000286\n",
      "Epoch 16213/40000, Loss: 3.467191709205508e-05, Learning Rate: 0.000286\n",
      "Epoch 16214/40000, Loss: 1.534357579657808e-05, Learning Rate: 0.000286\n",
      "Epoch 16215/40000, Loss: 1.5270861695171334e-05, Learning Rate: 0.000286\n",
      "Epoch 16216/40000, Loss: 6.517081055790186e-05, Learning Rate: 0.000286\n",
      "Epoch 16217/40000, Loss: 5.623374818242155e-05, Learning Rate: 0.000286\n",
      "Epoch 16218/40000, Loss: 5.631664680549875e-05, Learning Rate: 0.000286\n",
      "Epoch 16219/40000, Loss: 5.63315479666926e-05, Learning Rate: 0.000286\n",
      "Epoch 16220/40000, Loss: 3.4551489079603925e-05, Learning Rate: 0.000286\n",
      "Epoch 16221/40000, Loss: 6.512808613479137e-05, Learning Rate: 0.000286\n",
      "Epoch 16222/40000, Loss: 4.2382078390801325e-05, Learning Rate: 0.000286\n",
      "Epoch 16223/40000, Loss: 4.238204928697087e-05, Learning Rate: 0.000285\n",
      "Epoch 16224/40000, Loss: 4.0095543226925656e-05, Learning Rate: 0.000285\n",
      "Epoch 16225/40000, Loss: 4.2233554268023e-05, Learning Rate: 0.000285\n",
      "Epoch 16226/40000, Loss: 4.234422158333473e-05, Learning Rate: 0.000285\n",
      "Epoch 16227/40000, Loss: 3.453568569966592e-05, Learning Rate: 0.000285\n",
      "Epoch 16228/40000, Loss: 4.230611375533044e-05, Learning Rate: 0.000285\n",
      "Epoch 16229/40000, Loss: 6.511365063488483e-05, Learning Rate: 0.000285\n",
      "Epoch 16230/40000, Loss: 6.502673932118341e-05, Learning Rate: 0.000285\n",
      "Epoch 16231/40000, Loss: 1.5238467312883586e-05, Learning Rate: 0.000285\n",
      "Epoch 16232/40000, Loss: 3.4293418138986453e-05, Learning Rate: 0.000285\n",
      "Epoch 16233/40000, Loss: 1.5228832126013003e-05, Learning Rate: 0.000285\n",
      "Epoch 16234/40000, Loss: 3.999907858087681e-05, Learning Rate: 0.000285\n",
      "Epoch 16235/40000, Loss: 3.448332063271664e-05, Learning Rate: 0.000285\n",
      "Epoch 16236/40000, Loss: 3.998541069449857e-05, Learning Rate: 0.000285\n",
      "Epoch 16237/40000, Loss: 4.224486110615544e-05, Learning Rate: 0.000285\n",
      "Epoch 16238/40000, Loss: 3.454396937740967e-05, Learning Rate: 0.000285\n",
      "Epoch 16239/40000, Loss: 6.522766489069909e-05, Learning Rate: 0.000285\n",
      "Epoch 16240/40000, Loss: 3.4539218177087605e-05, Learning Rate: 0.000285\n",
      "Epoch 16241/40000, Loss: 1.5229766177071724e-05, Learning Rate: 0.000285\n",
      "Epoch 16242/40000, Loss: 3.42877465300262e-05, Learning Rate: 0.000285\n",
      "Epoch 16243/40000, Loss: 4.220015398459509e-05, Learning Rate: 0.000285\n",
      "Epoch 16244/40000, Loss: 4.224891745252535e-05, Learning Rate: 0.000285\n",
      "Epoch 16245/40000, Loss: 3.997905514552258e-05, Learning Rate: 0.000285\n",
      "Epoch 16246/40000, Loss: 3.999510227004066e-05, Learning Rate: 0.000285\n",
      "Epoch 16247/40000, Loss: 6.506752833956853e-05, Learning Rate: 0.000285\n",
      "Epoch 16248/40000, Loss: 6.506194768007845e-05, Learning Rate: 0.000285\n",
      "Epoch 16249/40000, Loss: 5.6266264436999336e-05, Learning Rate: 0.000285\n",
      "Epoch 16250/40000, Loss: 1.5123679077078123e-05, Learning Rate: 0.000285\n",
      "Epoch 16251/40000, Loss: 6.505403871415183e-05, Learning Rate: 0.000285\n",
      "Epoch 16252/40000, Loss: 3.9778176869731396e-05, Learning Rate: 0.000284\n",
      "Epoch 16253/40000, Loss: 5.625179619528353e-05, Learning Rate: 0.000284\n",
      "Epoch 16254/40000, Loss: 1.5170310689427424e-05, Learning Rate: 0.000284\n",
      "Epoch 16255/40000, Loss: 3.4323264117119834e-05, Learning Rate: 0.000284\n",
      "Epoch 16256/40000, Loss: 5.624199184239842e-05, Learning Rate: 0.000284\n",
      "Epoch 16257/40000, Loss: 4.232487845001742e-05, Learning Rate: 0.000284\n",
      "Epoch 16258/40000, Loss: 1.5211590834951494e-05, Learning Rate: 0.000284\n",
      "Epoch 16259/40000, Loss: 5.631690873997286e-05, Learning Rate: 0.000284\n",
      "Epoch 16260/40000, Loss: 5.620636511594057e-05, Learning Rate: 0.000284\n",
      "Epoch 16261/40000, Loss: 1.5234287275234237e-05, Learning Rate: 0.000284\n",
      "Epoch 16262/40000, Loss: 3.438568819547072e-05, Learning Rate: 0.000284\n",
      "Epoch 16263/40000, Loss: 3.988356911577284e-05, Learning Rate: 0.000284\n",
      "Epoch 16264/40000, Loss: 5.6252993090311065e-05, Learning Rate: 0.000284\n",
      "Epoch 16265/40000, Loss: 6.515711720567197e-05, Learning Rate: 0.000284\n",
      "Epoch 16266/40000, Loss: 3.438190833549015e-05, Learning Rate: 0.000284\n",
      "Epoch 16267/40000, Loss: 6.530326209031045e-05, Learning Rate: 0.000284\n",
      "Epoch 16268/40000, Loss: 3.9999798900680616e-05, Learning Rate: 0.000284\n",
      "Epoch 16269/40000, Loss: 5.6396896980004385e-05, Learning Rate: 0.000284\n",
      "Epoch 16270/40000, Loss: 6.517079600598663e-05, Learning Rate: 0.000284\n",
      "Epoch 16271/40000, Loss: 4.0218896174337715e-05, Learning Rate: 0.000284\n",
      "Epoch 16272/40000, Loss: 4.243803778081201e-05, Learning Rate: 0.000284\n",
      "Epoch 16273/40000, Loss: 4.243075454724021e-05, Learning Rate: 0.000284\n",
      "Epoch 16274/40000, Loss: 4.2397190554765984e-05, Learning Rate: 0.000284\n",
      "Epoch 16275/40000, Loss: 1.5444411474163644e-05, Learning Rate: 0.000284\n",
      "Epoch 16276/40000, Loss: 1.5382036508526653e-05, Learning Rate: 0.000284\n",
      "Epoch 16277/40000, Loss: 3.457935235928744e-05, Learning Rate: 0.000284\n",
      "Epoch 16278/40000, Loss: 1.5219658962450922e-05, Learning Rate: 0.000284\n",
      "Epoch 16279/40000, Loss: 4.026162423542701e-05, Learning Rate: 0.000284\n",
      "Epoch 16280/40000, Loss: 1.5122945114853792e-05, Learning Rate: 0.000284\n",
      "Epoch 16281/40000, Loss: 3.4416352718835697e-05, Learning Rate: 0.000283\n",
      "Epoch 16282/40000, Loss: 1.5512017853325233e-05, Learning Rate: 0.000283\n",
      "Epoch 16283/40000, Loss: 1.5632274880772457e-05, Learning Rate: 0.000283\n",
      "Epoch 16284/40000, Loss: 4.241579517838545e-05, Learning Rate: 0.000283\n",
      "Epoch 16285/40000, Loss: 5.6860331824282184e-05, Learning Rate: 0.000283\n",
      "Epoch 16286/40000, Loss: 1.5381679986603558e-05, Learning Rate: 0.000283\n",
      "Epoch 16287/40000, Loss: 3.491360621410422e-05, Learning Rate: 0.000283\n",
      "Epoch 16288/40000, Loss: 5.826317283208482e-05, Learning Rate: 0.000283\n",
      "Epoch 16289/40000, Loss: 6.580965418834239e-05, Learning Rate: 0.000283\n",
      "Epoch 16290/40000, Loss: 4.035487290821038e-05, Learning Rate: 0.000283\n",
      "Epoch 16291/40000, Loss: 3.515433854772709e-05, Learning Rate: 0.000283\n",
      "Epoch 16292/40000, Loss: 4.082900704815984e-05, Learning Rate: 0.000283\n",
      "Epoch 16293/40000, Loss: 3.4821820008801296e-05, Learning Rate: 0.000283\n",
      "Epoch 16294/40000, Loss: 4.434859147295356e-05, Learning Rate: 0.000283\n",
      "Epoch 16295/40000, Loss: 5.898228118894622e-05, Learning Rate: 0.000283\n",
      "Epoch 16296/40000, Loss: 6.793305510655046e-05, Learning Rate: 0.000283\n",
      "Epoch 16297/40000, Loss: 4.739470386994071e-05, Learning Rate: 0.000283\n",
      "Epoch 16298/40000, Loss: 4.821434777113609e-05, Learning Rate: 0.000283\n",
      "Epoch 16299/40000, Loss: 2.043530548689887e-05, Learning Rate: 0.000283\n",
      "Epoch 16300/40000, Loss: 4.244445517542772e-05, Learning Rate: 0.000283\n",
      "Epoch 16301/40000, Loss: 4.219605034450069e-05, Learning Rate: 0.000283\n",
      "Epoch 16302/40000, Loss: 4.2158644646406174e-05, Learning Rate: 0.000283\n",
      "Epoch 16303/40000, Loss: 3.969619137933478e-05, Learning Rate: 0.000283\n",
      "Epoch 16304/40000, Loss: 5.466637230711058e-05, Learning Rate: 0.000283\n",
      "Epoch 16305/40000, Loss: 9.689114085631445e-05, Learning Rate: 0.000283\n",
      "Epoch 16306/40000, Loss: 7.87766621215269e-05, Learning Rate: 0.000283\n",
      "Epoch 16307/40000, Loss: 2.0061583200003952e-05, Learning Rate: 0.000283\n",
      "Epoch 16308/40000, Loss: 4.630904004443437e-05, Learning Rate: 0.000283\n",
      "Epoch 16309/40000, Loss: 1.9090146452072076e-05, Learning Rate: 0.000283\n",
      "Epoch 16310/40000, Loss: 1.6997126294882037e-05, Learning Rate: 0.000283\n",
      "Epoch 16311/40000, Loss: 3.550795008777641e-05, Learning Rate: 0.000282\n",
      "Epoch 16312/40000, Loss: 1.755581070028711e-05, Learning Rate: 0.000282\n",
      "Epoch 16313/40000, Loss: 6.684915570076555e-05, Learning Rate: 0.000282\n",
      "Epoch 16314/40000, Loss: 5.800100916530937e-05, Learning Rate: 0.000282\n",
      "Epoch 16315/40000, Loss: 6.657865014858544e-05, Learning Rate: 0.000282\n",
      "Epoch 16316/40000, Loss: 4.336694109952077e-05, Learning Rate: 0.000282\n",
      "Epoch 16317/40000, Loss: 5.6916585890576243e-05, Learning Rate: 0.000282\n",
      "Epoch 16318/40000, Loss: 4.0415488911094144e-05, Learning Rate: 0.000282\n",
      "Epoch 16319/40000, Loss: 4.3147829273948446e-05, Learning Rate: 0.000282\n",
      "Epoch 16320/40000, Loss: 6.584271613974124e-05, Learning Rate: 0.000282\n",
      "Epoch 16321/40000, Loss: 5.642258111038245e-05, Learning Rate: 0.000282\n",
      "Epoch 16322/40000, Loss: 1.543331927678082e-05, Learning Rate: 0.000282\n",
      "Epoch 16323/40000, Loss: 4.0177517803385854e-05, Learning Rate: 0.000282\n",
      "Epoch 16324/40000, Loss: 3.447138806222938e-05, Learning Rate: 0.000282\n",
      "Epoch 16325/40000, Loss: 5.68169925827533e-05, Learning Rate: 0.000282\n",
      "Epoch 16326/40000, Loss: 1.5511253877775744e-05, Learning Rate: 0.000282\n",
      "Epoch 16327/40000, Loss: 4.286900002625771e-05, Learning Rate: 0.000282\n",
      "Epoch 16328/40000, Loss: 1.5890052964095958e-05, Learning Rate: 0.000282\n",
      "Epoch 16329/40000, Loss: 5.661121031153016e-05, Learning Rate: 0.000282\n",
      "Epoch 16330/40000, Loss: 4.4197524402989075e-05, Learning Rate: 0.000282\n",
      "Epoch 16331/40000, Loss: 4.3634245230350643e-05, Learning Rate: 0.000282\n",
      "Epoch 16332/40000, Loss: 4.030660056741908e-05, Learning Rate: 0.000282\n",
      "Epoch 16333/40000, Loss: 5.6847882660804316e-05, Learning Rate: 0.000282\n",
      "Epoch 16334/40000, Loss: 4.0180217183660716e-05, Learning Rate: 0.000282\n",
      "Epoch 16335/40000, Loss: 4.330277442932129e-05, Learning Rate: 0.000282\n",
      "Epoch 16336/40000, Loss: 1.5584986613248475e-05, Learning Rate: 0.000282\n",
      "Epoch 16337/40000, Loss: 3.531827314873226e-05, Learning Rate: 0.000282\n",
      "Epoch 16338/40000, Loss: 6.618020415771753e-05, Learning Rate: 0.000282\n",
      "Epoch 16339/40000, Loss: 1.5646664905943908e-05, Learning Rate: 0.000282\n",
      "Epoch 16340/40000, Loss: 3.5290282539790496e-05, Learning Rate: 0.000281\n",
      "Epoch 16341/40000, Loss: 1.571521715959534e-05, Learning Rate: 0.000281\n",
      "Epoch 16342/40000, Loss: 1.541450183140114e-05, Learning Rate: 0.000281\n",
      "Epoch 16343/40000, Loss: 1.5424902812810615e-05, Learning Rate: 0.000281\n",
      "Epoch 16344/40000, Loss: 4.3205986003158614e-05, Learning Rate: 0.000281\n",
      "Epoch 16345/40000, Loss: 4.286084367777221e-05, Learning Rate: 0.000281\n",
      "Epoch 16346/40000, Loss: 6.511761603178456e-05, Learning Rate: 0.000281\n",
      "Epoch 16347/40000, Loss: 4.0169259591493756e-05, Learning Rate: 0.000281\n",
      "Epoch 16348/40000, Loss: 5.6657561799511313e-05, Learning Rate: 0.000281\n",
      "Epoch 16349/40000, Loss: 4.2549512727418914e-05, Learning Rate: 0.000281\n",
      "Epoch 16350/40000, Loss: 3.440490036155097e-05, Learning Rate: 0.000281\n",
      "Epoch 16351/40000, Loss: 3.444620597292669e-05, Learning Rate: 0.000281\n",
      "Epoch 16352/40000, Loss: 4.262923903297633e-05, Learning Rate: 0.000281\n",
      "Epoch 16353/40000, Loss: 5.65497248317115e-05, Learning Rate: 0.000281\n",
      "Epoch 16354/40000, Loss: 3.4342712751822546e-05, Learning Rate: 0.000281\n",
      "Epoch 16355/40000, Loss: 1.5455614629900083e-05, Learning Rate: 0.000281\n",
      "Epoch 16356/40000, Loss: 4.2988529457943514e-05, Learning Rate: 0.000281\n",
      "Epoch 16357/40000, Loss: 6.694832700304687e-05, Learning Rate: 0.000281\n",
      "Epoch 16358/40000, Loss: 3.472551179584116e-05, Learning Rate: 0.000281\n",
      "Epoch 16359/40000, Loss: 1.5598317986587062e-05, Learning Rate: 0.000281\n",
      "Epoch 16360/40000, Loss: 1.5474010069738142e-05, Learning Rate: 0.000281\n",
      "Epoch 16361/40000, Loss: 4.0288185118697584e-05, Learning Rate: 0.000281\n",
      "Epoch 16362/40000, Loss: 4.012638964923099e-05, Learning Rate: 0.000281\n",
      "Epoch 16363/40000, Loss: 5.743657311541028e-05, Learning Rate: 0.000281\n",
      "Epoch 16364/40000, Loss: 5.707202581106685e-05, Learning Rate: 0.000281\n",
      "Epoch 16365/40000, Loss: 5.713129212381318e-05, Learning Rate: 0.000281\n",
      "Epoch 16366/40000, Loss: 4.2684521758928895e-05, Learning Rate: 0.000281\n",
      "Epoch 16367/40000, Loss: 4.016736784251407e-05, Learning Rate: 0.000281\n",
      "Epoch 16368/40000, Loss: 5.6856730225263163e-05, Learning Rate: 0.000281\n",
      "Epoch 16369/40000, Loss: 6.574590224772692e-05, Learning Rate: 0.000281\n",
      "Epoch 16370/40000, Loss: 3.4699682146310806e-05, Learning Rate: 0.000280\n",
      "Epoch 16371/40000, Loss: 1.6622379916952923e-05, Learning Rate: 0.000280\n",
      "Epoch 16372/40000, Loss: 5.7582277804613113e-05, Learning Rate: 0.000280\n",
      "Epoch 16373/40000, Loss: 5.7778368500294164e-05, Learning Rate: 0.000280\n",
      "Epoch 16374/40000, Loss: 1.7077845768653788e-05, Learning Rate: 0.000280\n",
      "Epoch 16375/40000, Loss: 6.49698922643438e-05, Learning Rate: 0.000280\n",
      "Epoch 16376/40000, Loss: 6.764820864191279e-05, Learning Rate: 0.000280\n",
      "Epoch 16377/40000, Loss: 3.6241617635823786e-05, Learning Rate: 0.000280\n",
      "Epoch 16378/40000, Loss: 1.7679936718195677e-05, Learning Rate: 0.000280\n",
      "Epoch 16379/40000, Loss: 4.177646042080596e-05, Learning Rate: 0.000280\n",
      "Epoch 16380/40000, Loss: 4.073206946486607e-05, Learning Rate: 0.000280\n",
      "Epoch 16381/40000, Loss: 6.646795372944325e-05, Learning Rate: 0.000280\n",
      "Epoch 16382/40000, Loss: 4.226177770760842e-05, Learning Rate: 0.000280\n",
      "Epoch 16383/40000, Loss: 3.5783003113465384e-05, Learning Rate: 0.000280\n",
      "Epoch 16384/40000, Loss: 1.7621930965106003e-05, Learning Rate: 0.000280\n",
      "Epoch 16385/40000, Loss: 4.591167817125097e-05, Learning Rate: 0.000280\n",
      "Epoch 16386/40000, Loss: 6.095448770793155e-05, Learning Rate: 0.000280\n",
      "Epoch 16387/40000, Loss: 4.689577326644212e-05, Learning Rate: 0.000280\n",
      "Epoch 16388/40000, Loss: 3.575588925741613e-05, Learning Rate: 0.000280\n",
      "Epoch 16389/40000, Loss: 3.5037352063227445e-05, Learning Rate: 0.000280\n",
      "Epoch 16390/40000, Loss: 6.797158130211756e-05, Learning Rate: 0.000280\n",
      "Epoch 16391/40000, Loss: 4.7303048631874844e-05, Learning Rate: 0.000280\n",
      "Epoch 16392/40000, Loss: 6.880536238895729e-05, Learning Rate: 0.000280\n",
      "Epoch 16393/40000, Loss: 5.594344111159444e-05, Learning Rate: 0.000280\n",
      "Epoch 16394/40000, Loss: 3.715050115715712e-05, Learning Rate: 0.000280\n",
      "Epoch 16395/40000, Loss: 5.115017120260745e-05, Learning Rate: 0.000280\n",
      "Epoch 16396/40000, Loss: 4.339277802500874e-05, Learning Rate: 0.000280\n",
      "Epoch 16397/40000, Loss: 6.14869495620951e-05, Learning Rate: 0.000280\n",
      "Epoch 16398/40000, Loss: 6.953523552510887e-05, Learning Rate: 0.000280\n",
      "Epoch 16399/40000, Loss: 3.859064236166887e-05, Learning Rate: 0.000280\n",
      "Epoch 16400/40000, Loss: 3.70983689208515e-05, Learning Rate: 0.000279\n",
      "Epoch 16401/40000, Loss: 3.636462861322798e-05, Learning Rate: 0.000279\n",
      "Epoch 16402/40000, Loss: 6.870134529890493e-05, Learning Rate: 0.000279\n",
      "Epoch 16403/40000, Loss: 4.21808181272354e-05, Learning Rate: 0.000279\n",
      "Epoch 16404/40000, Loss: 4.069086571689695e-05, Learning Rate: 0.000279\n",
      "Epoch 16405/40000, Loss: 4.0506878576707095e-05, Learning Rate: 0.000279\n",
      "Epoch 16406/40000, Loss: 5.722758578485809e-05, Learning Rate: 0.000279\n",
      "Epoch 16407/40000, Loss: 6.625608511967584e-05, Learning Rate: 0.000279\n",
      "Epoch 16408/40000, Loss: 1.567353683640249e-05, Learning Rate: 0.000279\n",
      "Epoch 16409/40000, Loss: 3.446943446760997e-05, Learning Rate: 0.000279\n",
      "Epoch 16410/40000, Loss: 3.444106550887227e-05, Learning Rate: 0.000279\n",
      "Epoch 16411/40000, Loss: 3.999042382929474e-05, Learning Rate: 0.000279\n",
      "Epoch 16412/40000, Loss: 5.721890556742437e-05, Learning Rate: 0.000279\n",
      "Epoch 16413/40000, Loss: 4.048127811984159e-05, Learning Rate: 0.000279\n",
      "Epoch 16414/40000, Loss: 6.62207166897133e-05, Learning Rate: 0.000279\n",
      "Epoch 16415/40000, Loss: 5.6871387641876936e-05, Learning Rate: 0.000279\n",
      "Epoch 16416/40000, Loss: 6.973982090130448e-05, Learning Rate: 0.000279\n",
      "Epoch 16417/40000, Loss: 4.0150745917344466e-05, Learning Rate: 0.000279\n",
      "Epoch 16418/40000, Loss: 3.661573646240868e-05, Learning Rate: 0.000279\n",
      "Epoch 16419/40000, Loss: 5.9439975302666426e-05, Learning Rate: 0.000279\n",
      "Epoch 16420/40000, Loss: 3.9934930100571364e-05, Learning Rate: 0.000279\n",
      "Epoch 16421/40000, Loss: 5.0461123464629054e-05, Learning Rate: 0.000279\n",
      "Epoch 16422/40000, Loss: 7.805743371136487e-05, Learning Rate: 0.000279\n",
      "Epoch 16423/40000, Loss: 6.366534216795117e-05, Learning Rate: 0.000279\n",
      "Epoch 16424/40000, Loss: 5.3649968322133645e-05, Learning Rate: 0.000279\n",
      "Epoch 16425/40000, Loss: 4.731565059046261e-05, Learning Rate: 0.000279\n",
      "Epoch 16426/40000, Loss: 7.249995542224497e-05, Learning Rate: 0.000279\n",
      "Epoch 16427/40000, Loss: 6.222935189725831e-05, Learning Rate: 0.000279\n",
      "Epoch 16428/40000, Loss: 1.7201151422341354e-05, Learning Rate: 0.000279\n",
      "Epoch 16429/40000, Loss: 6.840096466476098e-05, Learning Rate: 0.000278\n",
      "Epoch 16430/40000, Loss: 4.085692125954665e-05, Learning Rate: 0.000278\n",
      "Epoch 16431/40000, Loss: 3.5116485378239304e-05, Learning Rate: 0.000278\n",
      "Epoch 16432/40000, Loss: 4.4662047002930194e-05, Learning Rate: 0.000278\n",
      "Epoch 16433/40000, Loss: 6.574757571797818e-05, Learning Rate: 0.000278\n",
      "Epoch 16434/40000, Loss: 6.518787995446473e-05, Learning Rate: 0.000278\n",
      "Epoch 16435/40000, Loss: 4.303795503801666e-05, Learning Rate: 0.000278\n",
      "Epoch 16436/40000, Loss: 6.597957690246403e-05, Learning Rate: 0.000278\n",
      "Epoch 16437/40000, Loss: 6.533954001497477e-05, Learning Rate: 0.000278\n",
      "Epoch 16438/40000, Loss: 1.564944068377372e-05, Learning Rate: 0.000278\n",
      "Epoch 16439/40000, Loss: 1.546699604659807e-05, Learning Rate: 0.000278\n",
      "Epoch 16440/40000, Loss: 4.296486804378219e-05, Learning Rate: 0.000278\n",
      "Epoch 16441/40000, Loss: 3.441372246015817e-05, Learning Rate: 0.000278\n",
      "Epoch 16442/40000, Loss: 1.545368468214292e-05, Learning Rate: 0.000278\n",
      "Epoch 16443/40000, Loss: 1.5394740330521017e-05, Learning Rate: 0.000278\n",
      "Epoch 16444/40000, Loss: 6.556606240337715e-05, Learning Rate: 0.000278\n",
      "Epoch 16445/40000, Loss: 3.983442729804665e-05, Learning Rate: 0.000278\n",
      "Epoch 16446/40000, Loss: 6.703533290419728e-05, Learning Rate: 0.000278\n",
      "Epoch 16447/40000, Loss: 4.289950811653398e-05, Learning Rate: 0.000278\n",
      "Epoch 16448/40000, Loss: 7.294637180166319e-05, Learning Rate: 0.000278\n",
      "Epoch 16449/40000, Loss: 1.5792207705089822e-05, Learning Rate: 0.000278\n",
      "Epoch 16450/40000, Loss: 6.688691792078316e-05, Learning Rate: 0.000278\n",
      "Epoch 16451/40000, Loss: 1.633402098377701e-05, Learning Rate: 0.000278\n",
      "Epoch 16452/40000, Loss: 4.10604989156127e-05, Learning Rate: 0.000278\n",
      "Epoch 16453/40000, Loss: 5.805394903291017e-05, Learning Rate: 0.000278\n",
      "Epoch 16454/40000, Loss: 6.538780144182965e-05, Learning Rate: 0.000278\n",
      "Epoch 16455/40000, Loss: 3.466175985522568e-05, Learning Rate: 0.000278\n",
      "Epoch 16456/40000, Loss: 4.2649771785363555e-05, Learning Rate: 0.000278\n",
      "Epoch 16457/40000, Loss: 5.668298035743646e-05, Learning Rate: 0.000278\n",
      "Epoch 16458/40000, Loss: 6.492462853202596e-05, Learning Rate: 0.000278\n",
      "Epoch 16459/40000, Loss: 3.425233444431797e-05, Learning Rate: 0.000277\n",
      "Epoch 16460/40000, Loss: 3.9723712689010426e-05, Learning Rate: 0.000277\n",
      "Epoch 16461/40000, Loss: 1.5289781003957614e-05, Learning Rate: 0.000277\n",
      "Epoch 16462/40000, Loss: 4.23474375566002e-05, Learning Rate: 0.000277\n",
      "Epoch 16463/40000, Loss: 1.515736585133709e-05, Learning Rate: 0.000277\n",
      "Epoch 16464/40000, Loss: 1.5173284737102222e-05, Learning Rate: 0.000277\n",
      "Epoch 16465/40000, Loss: 6.522058538394049e-05, Learning Rate: 0.000277\n",
      "Epoch 16466/40000, Loss: 5.637570211547427e-05, Learning Rate: 0.000277\n",
      "Epoch 16467/40000, Loss: 4.2484676669118926e-05, Learning Rate: 0.000277\n",
      "Epoch 16468/40000, Loss: 3.462403401499614e-05, Learning Rate: 0.000277\n",
      "Epoch 16469/40000, Loss: 1.547117244626861e-05, Learning Rate: 0.000277\n",
      "Epoch 16470/40000, Loss: 4.270096906111576e-05, Learning Rate: 0.000277\n",
      "Epoch 16471/40000, Loss: 4.2373798351036385e-05, Learning Rate: 0.000277\n",
      "Epoch 16472/40000, Loss: 3.4371510992059484e-05, Learning Rate: 0.000277\n",
      "Epoch 16473/40000, Loss: 3.974765058956109e-05, Learning Rate: 0.000277\n",
      "Epoch 16474/40000, Loss: 3.966479198425077e-05, Learning Rate: 0.000277\n",
      "Epoch 16475/40000, Loss: 6.482852040790021e-05, Learning Rate: 0.000277\n",
      "Epoch 16476/40000, Loss: 3.4404845791868865e-05, Learning Rate: 0.000277\n",
      "Epoch 16477/40000, Loss: 4.264018571120687e-05, Learning Rate: 0.000277\n",
      "Epoch 16478/40000, Loss: 1.528911707282532e-05, Learning Rate: 0.000277\n",
      "Epoch 16479/40000, Loss: 6.505016790470108e-05, Learning Rate: 0.000277\n",
      "Epoch 16480/40000, Loss: 4.290397191653028e-05, Learning Rate: 0.000277\n",
      "Epoch 16481/40000, Loss: 3.458492938079871e-05, Learning Rate: 0.000277\n",
      "Epoch 16482/40000, Loss: 1.5461402654182166e-05, Learning Rate: 0.000277\n",
      "Epoch 16483/40000, Loss: 6.527218647534028e-05, Learning Rate: 0.000277\n",
      "Epoch 16484/40000, Loss: 3.534351344569586e-05, Learning Rate: 0.000277\n",
      "Epoch 16485/40000, Loss: 3.555536750354804e-05, Learning Rate: 0.000277\n",
      "Epoch 16486/40000, Loss: 3.9990569348447025e-05, Learning Rate: 0.000277\n",
      "Epoch 16487/40000, Loss: 3.988832759205252e-05, Learning Rate: 0.000277\n",
      "Epoch 16488/40000, Loss: 1.6526713807252236e-05, Learning Rate: 0.000277\n",
      "Epoch 16489/40000, Loss: 6.571414996869862e-05, Learning Rate: 0.000276\n",
      "Epoch 16490/40000, Loss: 6.598069739993662e-05, Learning Rate: 0.000276\n",
      "Epoch 16491/40000, Loss: 1.8462080333847553e-05, Learning Rate: 0.000276\n",
      "Epoch 16492/40000, Loss: 6.328703602775931e-05, Learning Rate: 0.000276\n",
      "Epoch 16493/40000, Loss: 4.0884264308260754e-05, Learning Rate: 0.000276\n",
      "Epoch 16494/40000, Loss: 5.085706652607769e-05, Learning Rate: 0.000276\n",
      "Epoch 16495/40000, Loss: 7.118229405023158e-05, Learning Rate: 0.000276\n",
      "Epoch 16496/40000, Loss: 7.110164733603597e-05, Learning Rate: 0.000276\n",
      "Epoch 16497/40000, Loss: 6.992607086431235e-05, Learning Rate: 0.000276\n",
      "Epoch 16498/40000, Loss: 1.965956653293688e-05, Learning Rate: 0.000276\n",
      "Epoch 16499/40000, Loss: 3.965255382354371e-05, Learning Rate: 0.000276\n",
      "Epoch 16500/40000, Loss: 6.318577652564272e-05, Learning Rate: 0.000276\n",
      "Epoch 16501/40000, Loss: 1.699840504443273e-05, Learning Rate: 0.000276\n",
      "Epoch 16502/40000, Loss: 7.481524517061189e-05, Learning Rate: 0.000276\n",
      "Epoch 16503/40000, Loss: 6.947429210413247e-05, Learning Rate: 0.000276\n",
      "Epoch 16504/40000, Loss: 3.601350908866152e-05, Learning Rate: 0.000276\n",
      "Epoch 16505/40000, Loss: 7.954192551551387e-05, Learning Rate: 0.000276\n",
      "Epoch 16506/40000, Loss: 4.140705641475506e-05, Learning Rate: 0.000276\n",
      "Epoch 16507/40000, Loss: 3.5715878766495734e-05, Learning Rate: 0.000276\n",
      "Epoch 16508/40000, Loss: 4.270516365068033e-05, Learning Rate: 0.000276\n",
      "Epoch 16509/40000, Loss: 4.552766404231079e-05, Learning Rate: 0.000276\n",
      "Epoch 16510/40000, Loss: 1.7182192095788196e-05, Learning Rate: 0.000276\n",
      "Epoch 16511/40000, Loss: 3.5182169085601345e-05, Learning Rate: 0.000276\n",
      "Epoch 16512/40000, Loss: 3.499417653074488e-05, Learning Rate: 0.000276\n",
      "Epoch 16513/40000, Loss: 6.993104034336284e-05, Learning Rate: 0.000276\n",
      "Epoch 16514/40000, Loss: 6.771522748749703e-05, Learning Rate: 0.000276\n",
      "Epoch 16515/40000, Loss: 1.7149892300949432e-05, Learning Rate: 0.000276\n",
      "Epoch 16516/40000, Loss: 1.6568712453590706e-05, Learning Rate: 0.000276\n",
      "Epoch 16517/40000, Loss: 4.121623715036549e-05, Learning Rate: 0.000276\n",
      "Epoch 16518/40000, Loss: 3.5411245335126296e-05, Learning Rate: 0.000276\n",
      "Epoch 16519/40000, Loss: 1.6748008420108818e-05, Learning Rate: 0.000276\n",
      "Epoch 16520/40000, Loss: 6.954134005354717e-05, Learning Rate: 0.000275\n",
      "Epoch 16521/40000, Loss: 3.601907883421518e-05, Learning Rate: 0.000275\n",
      "Epoch 16522/40000, Loss: 3.4936372685479e-05, Learning Rate: 0.000275\n",
      "Epoch 16523/40000, Loss: 1.613412496226374e-05, Learning Rate: 0.000275\n",
      "Epoch 16524/40000, Loss: 4.041333158966154e-05, Learning Rate: 0.000275\n",
      "Epoch 16525/40000, Loss: 3.981988993473351e-05, Learning Rate: 0.000275\n",
      "Epoch 16526/40000, Loss: 6.561119516845793e-05, Learning Rate: 0.000275\n",
      "Epoch 16527/40000, Loss: 5.7274402934126556e-05, Learning Rate: 0.000275\n",
      "Epoch 16528/40000, Loss: 6.546301301568747e-05, Learning Rate: 0.000275\n",
      "Epoch 16529/40000, Loss: 4.250751590006985e-05, Learning Rate: 0.000275\n",
      "Epoch 16530/40000, Loss: 1.539885124657303e-05, Learning Rate: 0.000275\n",
      "Epoch 16531/40000, Loss: 3.424093301873654e-05, Learning Rate: 0.000275\n",
      "Epoch 16532/40000, Loss: 1.510378933744505e-05, Learning Rate: 0.000275\n",
      "Epoch 16533/40000, Loss: 5.6811197282513604e-05, Learning Rate: 0.000275\n",
      "Epoch 16534/40000, Loss: 1.529602195660118e-05, Learning Rate: 0.000275\n",
      "Epoch 16535/40000, Loss: 1.5143999917199835e-05, Learning Rate: 0.000275\n",
      "Epoch 16536/40000, Loss: 5.67452771065291e-05, Learning Rate: 0.000275\n",
      "Epoch 16537/40000, Loss: 3.980181281804107e-05, Learning Rate: 0.000275\n",
      "Epoch 16538/40000, Loss: 3.421964356675744e-05, Learning Rate: 0.000275\n",
      "Epoch 16539/40000, Loss: 3.966693839174695e-05, Learning Rate: 0.000275\n",
      "Epoch 16540/40000, Loss: 5.6522410886827856e-05, Learning Rate: 0.000275\n",
      "Epoch 16541/40000, Loss: 5.641207826556638e-05, Learning Rate: 0.000275\n",
      "Epoch 16542/40000, Loss: 1.5429110135301016e-05, Learning Rate: 0.000275\n",
      "Epoch 16543/40000, Loss: 6.548842793563381e-05, Learning Rate: 0.000275\n",
      "Epoch 16544/40000, Loss: 5.708949902327731e-05, Learning Rate: 0.000275\n",
      "Epoch 16545/40000, Loss: 6.816330278525129e-05, Learning Rate: 0.000275\n",
      "Epoch 16546/40000, Loss: 1.5372856069006957e-05, Learning Rate: 0.000275\n",
      "Epoch 16547/40000, Loss: 6.890291842864826e-05, Learning Rate: 0.000275\n",
      "Epoch 16548/40000, Loss: 4.074297612532973e-05, Learning Rate: 0.000275\n",
      "Epoch 16549/40000, Loss: 7.750315853627399e-05, Learning Rate: 0.000275\n",
      "Epoch 16550/40000, Loss: 6.681598461000249e-05, Learning Rate: 0.000274\n",
      "Epoch 16551/40000, Loss: 6.704145926050842e-05, Learning Rate: 0.000274\n",
      "Epoch 16552/40000, Loss: 1.5623958461219445e-05, Learning Rate: 0.000274\n",
      "Epoch 16553/40000, Loss: 1.5189605619525537e-05, Learning Rate: 0.000274\n",
      "Epoch 16554/40000, Loss: 4.024359805043787e-05, Learning Rate: 0.000274\n",
      "Epoch 16555/40000, Loss: 5.788748603663407e-05, Learning Rate: 0.000274\n",
      "Epoch 16556/40000, Loss: 4.055245517520234e-05, Learning Rate: 0.000274\n",
      "Epoch 16557/40000, Loss: 1.567313665873371e-05, Learning Rate: 0.000274\n",
      "Epoch 16558/40000, Loss: 3.426562761887908e-05, Learning Rate: 0.000274\n",
      "Epoch 16559/40000, Loss: 4.259250272298232e-05, Learning Rate: 0.000274\n",
      "Epoch 16560/40000, Loss: 6.528441736008972e-05, Learning Rate: 0.000274\n",
      "Epoch 16561/40000, Loss: 5.6723296438576654e-05, Learning Rate: 0.000274\n",
      "Epoch 16562/40000, Loss: 6.49325447739102e-05, Learning Rate: 0.000274\n",
      "Epoch 16563/40000, Loss: 6.018546628183685e-05, Learning Rate: 0.000274\n",
      "Epoch 16564/40000, Loss: 5.9630750911310315e-05, Learning Rate: 0.000274\n",
      "Epoch 16565/40000, Loss: 5.747978502768092e-05, Learning Rate: 0.000274\n",
      "Epoch 16566/40000, Loss: 5.7032419135794044e-05, Learning Rate: 0.000274\n",
      "Epoch 16567/40000, Loss: 6.503966869786382e-05, Learning Rate: 0.000274\n",
      "Epoch 16568/40000, Loss: 4.258410626789555e-05, Learning Rate: 0.000274\n",
      "Epoch 16569/40000, Loss: 6.481906893895939e-05, Learning Rate: 0.000274\n",
      "Epoch 16570/40000, Loss: 6.486527126980945e-05, Learning Rate: 0.000274\n",
      "Epoch 16571/40000, Loss: 4.311340308049694e-05, Learning Rate: 0.000274\n",
      "Epoch 16572/40000, Loss: 6.622556247748435e-05, Learning Rate: 0.000274\n",
      "Epoch 16573/40000, Loss: 5.7865730923367664e-05, Learning Rate: 0.000274\n",
      "Epoch 16574/40000, Loss: 3.5299479350214824e-05, Learning Rate: 0.000274\n",
      "Epoch 16575/40000, Loss: 3.549071334418841e-05, Learning Rate: 0.000274\n",
      "Epoch 16576/40000, Loss: 4.575432831188664e-05, Learning Rate: 0.000274\n",
      "Epoch 16577/40000, Loss: 1.782848448783625e-05, Learning Rate: 0.000274\n",
      "Epoch 16578/40000, Loss: 4.879745392827317e-05, Learning Rate: 0.000274\n",
      "Epoch 16579/40000, Loss: 1.778658406692557e-05, Learning Rate: 0.000274\n",
      "Epoch 16580/40000, Loss: 5.8479949075262994e-05, Learning Rate: 0.000273\n",
      "Epoch 16581/40000, Loss: 1.818157033994794e-05, Learning Rate: 0.000273\n",
      "Epoch 16582/40000, Loss: 4.684180021286011e-05, Learning Rate: 0.000273\n",
      "Epoch 16583/40000, Loss: 4.596334110829048e-05, Learning Rate: 0.000273\n",
      "Epoch 16584/40000, Loss: 6.158735777717084e-05, Learning Rate: 0.000273\n",
      "Epoch 16585/40000, Loss: 6.819002737756819e-05, Learning Rate: 0.000273\n",
      "Epoch 16586/40000, Loss: 4.931097646476701e-05, Learning Rate: 0.000273\n",
      "Epoch 16587/40000, Loss: 7.039730553515255e-05, Learning Rate: 0.000273\n",
      "Epoch 16588/40000, Loss: 1.8911090592155233e-05, Learning Rate: 0.000273\n",
      "Epoch 16589/40000, Loss: 5.305293598212302e-05, Learning Rate: 0.000273\n",
      "Epoch 16590/40000, Loss: 7.102268864400685e-05, Learning Rate: 0.000273\n",
      "Epoch 16591/40000, Loss: 5.506489105755463e-05, Learning Rate: 0.000273\n",
      "Epoch 16592/40000, Loss: 2.2288098989520222e-05, Learning Rate: 0.000273\n",
      "Epoch 16593/40000, Loss: 3.955341162509285e-05, Learning Rate: 0.000273\n",
      "Epoch 16594/40000, Loss: 2.5397401259397157e-05, Learning Rate: 0.000273\n",
      "Epoch 16595/40000, Loss: 4.485595491132699e-05, Learning Rate: 0.000273\n",
      "Epoch 16596/40000, Loss: 6.328219023998827e-05, Learning Rate: 0.000273\n",
      "Epoch 16597/40000, Loss: 4.580389941111207e-05, Learning Rate: 0.000273\n",
      "Epoch 16598/40000, Loss: 2.0501498511293903e-05, Learning Rate: 0.000273\n",
      "Epoch 16599/40000, Loss: 4.233188519719988e-05, Learning Rate: 0.000273\n",
      "Epoch 16600/40000, Loss: 6.944626511540264e-05, Learning Rate: 0.000273\n",
      "Epoch 16601/40000, Loss: 5.011946996091865e-05, Learning Rate: 0.000273\n",
      "Epoch 16602/40000, Loss: 2.229055462521501e-05, Learning Rate: 0.000273\n",
      "Epoch 16603/40000, Loss: 4.3878120777662843e-05, Learning Rate: 0.000273\n",
      "Epoch 16604/40000, Loss: 5.310165215632878e-05, Learning Rate: 0.000273\n",
      "Epoch 16605/40000, Loss: 1.987019095395226e-05, Learning Rate: 0.000273\n",
      "Epoch 16606/40000, Loss: 6.905265763634816e-05, Learning Rate: 0.000273\n",
      "Epoch 16607/40000, Loss: 2.3337837774306536e-05, Learning Rate: 0.000273\n",
      "Epoch 16608/40000, Loss: 1.6505537132616155e-05, Learning Rate: 0.000273\n",
      "Epoch 16609/40000, Loss: 5.859031807631254e-05, Learning Rate: 0.000273\n",
      "Epoch 16610/40000, Loss: 5.7182798627763987e-05, Learning Rate: 0.000273\n",
      "Epoch 16611/40000, Loss: 4.3087038648081943e-05, Learning Rate: 0.000272\n",
      "Epoch 16612/40000, Loss: 3.431524964980781e-05, Learning Rate: 0.000272\n",
      "Epoch 16613/40000, Loss: 1.5821353372302838e-05, Learning Rate: 0.000272\n",
      "Epoch 16614/40000, Loss: 5.7711607951205224e-05, Learning Rate: 0.000272\n",
      "Epoch 16615/40000, Loss: 4.329165676608682e-05, Learning Rate: 0.000272\n",
      "Epoch 16616/40000, Loss: 3.95714450860396e-05, Learning Rate: 0.000272\n",
      "Epoch 16617/40000, Loss: 1.5415074813063256e-05, Learning Rate: 0.000272\n",
      "Epoch 16618/40000, Loss: 5.63846951990854e-05, Learning Rate: 0.000272\n",
      "Epoch 16619/40000, Loss: 3.411076249903999e-05, Learning Rate: 0.000272\n",
      "Epoch 16620/40000, Loss: 6.47405322524719e-05, Learning Rate: 0.000272\n",
      "Epoch 16621/40000, Loss: 3.4060813050018623e-05, Learning Rate: 0.000272\n",
      "Epoch 16622/40000, Loss: 5.643772237817757e-05, Learning Rate: 0.000272\n",
      "Epoch 16623/40000, Loss: 3.945503704017028e-05, Learning Rate: 0.000272\n",
      "Epoch 16624/40000, Loss: 1.5359311873908155e-05, Learning Rate: 0.000272\n",
      "Epoch 16625/40000, Loss: 3.9427250158041716e-05, Learning Rate: 0.000272\n",
      "Epoch 16626/40000, Loss: 5.6171011237893254e-05, Learning Rate: 0.000272\n",
      "Epoch 16627/40000, Loss: 6.452946399804205e-05, Learning Rate: 0.000272\n",
      "Epoch 16628/40000, Loss: 4.271199577488005e-05, Learning Rate: 0.000272\n",
      "Epoch 16629/40000, Loss: 3.9552378439111635e-05, Learning Rate: 0.000272\n",
      "Epoch 16630/40000, Loss: 3.395102976355702e-05, Learning Rate: 0.000272\n",
      "Epoch 16631/40000, Loss: 6.466345803346485e-05, Learning Rate: 0.000272\n",
      "Epoch 16632/40000, Loss: 4.2992443923139945e-05, Learning Rate: 0.000272\n",
      "Epoch 16633/40000, Loss: 5.7858072977978736e-05, Learning Rate: 0.000272\n",
      "Epoch 16634/40000, Loss: 5.771948417532258e-05, Learning Rate: 0.000272\n",
      "Epoch 16635/40000, Loss: 5.705411604139954e-05, Learning Rate: 0.000272\n",
      "Epoch 16636/40000, Loss: 3.9751095755491406e-05, Learning Rate: 0.000272\n",
      "Epoch 16637/40000, Loss: 1.5602274288539775e-05, Learning Rate: 0.000272\n",
      "Epoch 16638/40000, Loss: 5.654325650539249e-05, Learning Rate: 0.000272\n",
      "Epoch 16639/40000, Loss: 5.632357715512626e-05, Learning Rate: 0.000272\n",
      "Epoch 16640/40000, Loss: 1.539603363198694e-05, Learning Rate: 0.000272\n",
      "Epoch 16641/40000, Loss: 1.505665022705216e-05, Learning Rate: 0.000272\n",
      "Epoch 16642/40000, Loss: 3.4892760595539585e-05, Learning Rate: 0.000271\n",
      "Epoch 16643/40000, Loss: 3.92790898331441e-05, Learning Rate: 0.000271\n",
      "Epoch 16644/40000, Loss: 4.252809958416037e-05, Learning Rate: 0.000271\n",
      "Epoch 16645/40000, Loss: 5.64569236303214e-05, Learning Rate: 0.000271\n",
      "Epoch 16646/40000, Loss: 5.623750257655047e-05, Learning Rate: 0.000271\n",
      "Epoch 16647/40000, Loss: 6.487155769718811e-05, Learning Rate: 0.000271\n",
      "Epoch 16648/40000, Loss: 6.477769784396514e-05, Learning Rate: 0.000271\n",
      "Epoch 16649/40000, Loss: 4.22340672230348e-05, Learning Rate: 0.000271\n",
      "Epoch 16650/40000, Loss: 1.5078440810611937e-05, Learning Rate: 0.000271\n",
      "Epoch 16651/40000, Loss: 6.444226892199367e-05, Learning Rate: 0.000271\n",
      "Epoch 16652/40000, Loss: 3.9815524360165e-05, Learning Rate: 0.000271\n",
      "Epoch 16653/40000, Loss: 3.391617792658508e-05, Learning Rate: 0.000271\n",
      "Epoch 16654/40000, Loss: 3.4011151001323014e-05, Learning Rate: 0.000271\n",
      "Epoch 16655/40000, Loss: 4.2509018385317177e-05, Learning Rate: 0.000271\n",
      "Epoch 16656/40000, Loss: 1.5715568224550225e-05, Learning Rate: 0.000271\n",
      "Epoch 16657/40000, Loss: 1.5373887435998768e-05, Learning Rate: 0.000271\n",
      "Epoch 16658/40000, Loss: 4.2248932004440576e-05, Learning Rate: 0.000271\n",
      "Epoch 16659/40000, Loss: 4.286190960556269e-05, Learning Rate: 0.000271\n",
      "Epoch 16660/40000, Loss: 1.534482180431951e-05, Learning Rate: 0.000271\n",
      "Epoch 16661/40000, Loss: 6.45884865662083e-05, Learning Rate: 0.000271\n",
      "Epoch 16662/40000, Loss: 5.654199412674643e-05, Learning Rate: 0.000271\n",
      "Epoch 16663/40000, Loss: 5.6298576964763924e-05, Learning Rate: 0.000271\n",
      "Epoch 16664/40000, Loss: 1.5237968909787014e-05, Learning Rate: 0.000271\n",
      "Epoch 16665/40000, Loss: 6.471890083048493e-05, Learning Rate: 0.000271\n",
      "Epoch 16666/40000, Loss: 1.531045745650772e-05, Learning Rate: 0.000271\n",
      "Epoch 16667/40000, Loss: 5.615235204459168e-05, Learning Rate: 0.000271\n",
      "Epoch 16668/40000, Loss: 1.5121010619623121e-05, Learning Rate: 0.000271\n",
      "Epoch 16669/40000, Loss: 6.441472214646637e-05, Learning Rate: 0.000271\n",
      "Epoch 16670/40000, Loss: 6.430006033042446e-05, Learning Rate: 0.000271\n",
      "Epoch 16671/40000, Loss: 5.61056294827722e-05, Learning Rate: 0.000271\n",
      "Epoch 16672/40000, Loss: 4.2295552702853456e-05, Learning Rate: 0.000270\n",
      "Epoch 16673/40000, Loss: 3.408053089515306e-05, Learning Rate: 0.000270\n",
      "Epoch 16674/40000, Loss: 1.5065760635479819e-05, Learning Rate: 0.000270\n",
      "Epoch 16675/40000, Loss: 5.632514148601331e-05, Learning Rate: 0.000270\n",
      "Epoch 16676/40000, Loss: 4.261929643689655e-05, Learning Rate: 0.000270\n",
      "Epoch 16677/40000, Loss: 3.4259155654581264e-05, Learning Rate: 0.000270\n",
      "Epoch 16678/40000, Loss: 3.401553112780675e-05, Learning Rate: 0.000270\n",
      "Epoch 16679/40000, Loss: 5.676804357790388e-05, Learning Rate: 0.000270\n",
      "Epoch 16680/40000, Loss: 4.4508546125143766e-05, Learning Rate: 0.000270\n",
      "Epoch 16681/40000, Loss: 3.9788097637938336e-05, Learning Rate: 0.000270\n",
      "Epoch 16682/40000, Loss: 6.460236181737855e-05, Learning Rate: 0.000270\n",
      "Epoch 16683/40000, Loss: 5.6794931879267097e-05, Learning Rate: 0.000270\n",
      "Epoch 16684/40000, Loss: 3.9384845877066255e-05, Learning Rate: 0.000270\n",
      "Epoch 16685/40000, Loss: 3.401431968086399e-05, Learning Rate: 0.000270\n",
      "Epoch 16686/40000, Loss: 4.287893898435868e-05, Learning Rate: 0.000270\n",
      "Epoch 16687/40000, Loss: 6.494348053820431e-05, Learning Rate: 0.000270\n",
      "Epoch 16688/40000, Loss: 3.447310155024752e-05, Learning Rate: 0.000270\n",
      "Epoch 16689/40000, Loss: 5.6545868574175984e-05, Learning Rate: 0.000270\n",
      "Epoch 16690/40000, Loss: 6.465964543167502e-05, Learning Rate: 0.000270\n",
      "Epoch 16691/40000, Loss: 4.0069939132081345e-05, Learning Rate: 0.000270\n",
      "Epoch 16692/40000, Loss: 4.2731167923193425e-05, Learning Rate: 0.000270\n",
      "Epoch 16693/40000, Loss: 6.517880683531985e-05, Learning Rate: 0.000270\n",
      "Epoch 16694/40000, Loss: 3.519686288200319e-05, Learning Rate: 0.000270\n",
      "Epoch 16695/40000, Loss: 5.415450141299516e-05, Learning Rate: 0.000270\n",
      "Epoch 16696/40000, Loss: 5.963203147985041e-05, Learning Rate: 0.000270\n",
      "Epoch 16697/40000, Loss: 6.770683103241026e-05, Learning Rate: 0.000270\n",
      "Epoch 16698/40000, Loss: 1.964874900295399e-05, Learning Rate: 0.000270\n",
      "Epoch 16699/40000, Loss: 1.7348524124827236e-05, Learning Rate: 0.000270\n",
      "Epoch 16700/40000, Loss: 1.9389115550438873e-05, Learning Rate: 0.000270\n",
      "Epoch 16701/40000, Loss: 3.561412449926138e-05, Learning Rate: 0.000270\n",
      "Epoch 16702/40000, Loss: 6.669110734947026e-05, Learning Rate: 0.000270\n",
      "Epoch 16703/40000, Loss: 4.134525443078019e-05, Learning Rate: 0.000269\n",
      "Epoch 16704/40000, Loss: 1.5933204849716276e-05, Learning Rate: 0.000269\n",
      "Epoch 16705/40000, Loss: 6.59896686556749e-05, Learning Rate: 0.000269\n",
      "Epoch 16706/40000, Loss: 4.3326119339326397e-05, Learning Rate: 0.000269\n",
      "Epoch 16707/40000, Loss: 3.45608132192865e-05, Learning Rate: 0.000269\n",
      "Epoch 16708/40000, Loss: 3.407580152270384e-05, Learning Rate: 0.000269\n",
      "Epoch 16709/40000, Loss: 6.544104689965025e-05, Learning Rate: 0.000269\n",
      "Epoch 16710/40000, Loss: 5.679063542629592e-05, Learning Rate: 0.000269\n",
      "Epoch 16711/40000, Loss: 1.5556550351902843e-05, Learning Rate: 0.000269\n",
      "Epoch 16712/40000, Loss: 1.5336803699028678e-05, Learning Rate: 0.000269\n",
      "Epoch 16713/40000, Loss: 3.972347258240916e-05, Learning Rate: 0.000269\n",
      "Epoch 16714/40000, Loss: 4.316238482715562e-05, Learning Rate: 0.000269\n",
      "Epoch 16715/40000, Loss: 4.3722357077058405e-05, Learning Rate: 0.000269\n",
      "Epoch 16716/40000, Loss: 1.531899397377856e-05, Learning Rate: 0.000269\n",
      "Epoch 16717/40000, Loss: 6.776341615477577e-05, Learning Rate: 0.000269\n",
      "Epoch 16718/40000, Loss: 6.789195322198793e-05, Learning Rate: 0.000269\n",
      "Epoch 16719/40000, Loss: 6.683146784780547e-05, Learning Rate: 0.000269\n",
      "Epoch 16720/40000, Loss: 6.626819231314585e-05, Learning Rate: 0.000269\n",
      "Epoch 16721/40000, Loss: 4.0298189560417086e-05, Learning Rate: 0.000269\n",
      "Epoch 16722/40000, Loss: 1.7168638805742376e-05, Learning Rate: 0.000269\n",
      "Epoch 16723/40000, Loss: 6.866828334750608e-05, Learning Rate: 0.000269\n",
      "Epoch 16724/40000, Loss: 4.792930121766403e-05, Learning Rate: 0.000269\n",
      "Epoch 16725/40000, Loss: 4.192604319541715e-05, Learning Rate: 0.000269\n",
      "Epoch 16726/40000, Loss: 7.092775194905698e-05, Learning Rate: 0.000269\n",
      "Epoch 16727/40000, Loss: 6.920799933141097e-05, Learning Rate: 0.000269\n",
      "Epoch 16728/40000, Loss: 3.6005625588586554e-05, Learning Rate: 0.000269\n",
      "Epoch 16729/40000, Loss: 4.616843943949789e-05, Learning Rate: 0.000269\n",
      "Epoch 16730/40000, Loss: 3.718750667758286e-05, Learning Rate: 0.000269\n",
      "Epoch 16731/40000, Loss: 4.231962520861998e-05, Learning Rate: 0.000269\n",
      "Epoch 16732/40000, Loss: 4.6021676098462194e-05, Learning Rate: 0.000269\n",
      "Epoch 16733/40000, Loss: 4.342014653957449e-05, Learning Rate: 0.000269\n",
      "Epoch 16734/40000, Loss: 4.5449101889971644e-05, Learning Rate: 0.000268\n",
      "Epoch 16735/40000, Loss: 1.8249123968416825e-05, Learning Rate: 0.000268\n",
      "Epoch 16736/40000, Loss: 6.164197839098051e-05, Learning Rate: 0.000268\n",
      "Epoch 16737/40000, Loss: 6.126506923465058e-05, Learning Rate: 0.000268\n",
      "Epoch 16738/40000, Loss: 6.105530337663367e-05, Learning Rate: 0.000268\n",
      "Epoch 16739/40000, Loss: 5.855719427927397e-05, Learning Rate: 0.000268\n",
      "Epoch 16740/40000, Loss: 4.05944992962759e-05, Learning Rate: 0.000268\n",
      "Epoch 16741/40000, Loss: 6.163900980027393e-05, Learning Rate: 0.000268\n",
      "Epoch 16742/40000, Loss: 6.866123294457793e-05, Learning Rate: 0.000268\n",
      "Epoch 16743/40000, Loss: 6.892107194289565e-05, Learning Rate: 0.000268\n",
      "Epoch 16744/40000, Loss: 3.553917849785648e-05, Learning Rate: 0.000268\n",
      "Epoch 16745/40000, Loss: 3.476485289866105e-05, Learning Rate: 0.000268\n",
      "Epoch 16746/40000, Loss: 5.712131314794533e-05, Learning Rate: 0.000268\n",
      "Epoch 16747/40000, Loss: 3.975141953560524e-05, Learning Rate: 0.000268\n",
      "Epoch 16748/40000, Loss: 4.4198535761097446e-05, Learning Rate: 0.000268\n",
      "Epoch 16749/40000, Loss: 3.490327799227089e-05, Learning Rate: 0.000268\n",
      "Epoch 16750/40000, Loss: 1.5810539480298758e-05, Learning Rate: 0.000268\n",
      "Epoch 16751/40000, Loss: 1.5966903447406366e-05, Learning Rate: 0.000268\n",
      "Epoch 16752/40000, Loss: 3.9384318370139226e-05, Learning Rate: 0.000268\n",
      "Epoch 16753/40000, Loss: 1.6707341274013743e-05, Learning Rate: 0.000268\n",
      "Epoch 16754/40000, Loss: 4.330174488131888e-05, Learning Rate: 0.000268\n",
      "Epoch 16755/40000, Loss: 5.6832959671737626e-05, Learning Rate: 0.000268\n",
      "Epoch 16756/40000, Loss: 5.664943819283508e-05, Learning Rate: 0.000268\n",
      "Epoch 16757/40000, Loss: 5.642514588544145e-05, Learning Rate: 0.000268\n",
      "Epoch 16758/40000, Loss: 1.5696476111770608e-05, Learning Rate: 0.000268\n",
      "Epoch 16759/40000, Loss: 4.496269684750587e-05, Learning Rate: 0.000268\n",
      "Epoch 16760/40000, Loss: 5.902281918679364e-05, Learning Rate: 0.000268\n",
      "Epoch 16761/40000, Loss: 5.9450958360685036e-05, Learning Rate: 0.000268\n",
      "Epoch 16762/40000, Loss: 7.095625915098935e-05, Learning Rate: 0.000268\n",
      "Epoch 16763/40000, Loss: 3.5682427551364526e-05, Learning Rate: 0.000268\n",
      "Epoch 16764/40000, Loss: 7.022984209470451e-05, Learning Rate: 0.000268\n",
      "Epoch 16765/40000, Loss: 4.681838254327886e-05, Learning Rate: 0.000267\n",
      "Epoch 16766/40000, Loss: 5.963517105556093e-05, Learning Rate: 0.000267\n",
      "Epoch 16767/40000, Loss: 3.586723323678598e-05, Learning Rate: 0.000267\n",
      "Epoch 16768/40000, Loss: 1.615582368685864e-05, Learning Rate: 0.000267\n",
      "Epoch 16769/40000, Loss: 1.6944917661021464e-05, Learning Rate: 0.000267\n",
      "Epoch 16770/40000, Loss: 4.662790888687596e-05, Learning Rate: 0.000267\n",
      "Epoch 16771/40000, Loss: 6.892641249578446e-05, Learning Rate: 0.000267\n",
      "Epoch 16772/40000, Loss: 4.062516018166207e-05, Learning Rate: 0.000267\n",
      "Epoch 16773/40000, Loss: 6.929531082278118e-05, Learning Rate: 0.000267\n",
      "Epoch 16774/40000, Loss: 6.703655526507646e-05, Learning Rate: 0.000267\n",
      "Epoch 16775/40000, Loss: 6.658276834059507e-05, Learning Rate: 0.000267\n",
      "Epoch 16776/40000, Loss: 3.4241566027048975e-05, Learning Rate: 0.000267\n",
      "Epoch 16777/40000, Loss: 4.293027086532675e-05, Learning Rate: 0.000267\n",
      "Epoch 16778/40000, Loss: 7.010022090980783e-05, Learning Rate: 0.000267\n",
      "Epoch 16779/40000, Loss: 4.0144994272850454e-05, Learning Rate: 0.000267\n",
      "Epoch 16780/40000, Loss: 4.040359999635257e-05, Learning Rate: 0.000267\n",
      "Epoch 16781/40000, Loss: 7.760414882795885e-05, Learning Rate: 0.000267\n",
      "Epoch 16782/40000, Loss: 6.0183549067005515e-05, Learning Rate: 0.000267\n",
      "Epoch 16783/40000, Loss: 4.0428159991279244e-05, Learning Rate: 0.000267\n",
      "Epoch 16784/40000, Loss: 4.51885316579137e-05, Learning Rate: 0.000267\n",
      "Epoch 16785/40000, Loss: 4.6100820327410474e-05, Learning Rate: 0.000267\n",
      "Epoch 16786/40000, Loss: 5.881044125999324e-05, Learning Rate: 0.000267\n",
      "Epoch 16787/40000, Loss: 4.019130574306473e-05, Learning Rate: 0.000267\n",
      "Epoch 16788/40000, Loss: 6.595408194698393e-05, Learning Rate: 0.000267\n",
      "Epoch 16789/40000, Loss: 4.0373957745032385e-05, Learning Rate: 0.000267\n",
      "Epoch 16790/40000, Loss: 1.5538587831542827e-05, Learning Rate: 0.000267\n",
      "Epoch 16791/40000, Loss: 5.7097677199635655e-05, Learning Rate: 0.000267\n",
      "Epoch 16792/40000, Loss: 6.572687561856583e-05, Learning Rate: 0.000267\n",
      "Epoch 16793/40000, Loss: 4.287285264581442e-05, Learning Rate: 0.000267\n",
      "Epoch 16794/40000, Loss: 3.6264485970605165e-05, Learning Rate: 0.000267\n",
      "Epoch 16795/40000, Loss: 1.5842158973100595e-05, Learning Rate: 0.000267\n",
      "Epoch 16796/40000, Loss: 3.8705882616341114e-05, Learning Rate: 0.000266\n",
      "Epoch 16797/40000, Loss: 6.619031773880124e-05, Learning Rate: 0.000266\n",
      "Epoch 16798/40000, Loss: 3.852739973808639e-05, Learning Rate: 0.000266\n",
      "Epoch 16799/40000, Loss: 6.568764365511015e-05, Learning Rate: 0.000266\n",
      "Epoch 16800/40000, Loss: 4.137836003792472e-05, Learning Rate: 0.000266\n",
      "Epoch 16801/40000, Loss: 3.976321386289783e-05, Learning Rate: 0.000266\n",
      "Epoch 16802/40000, Loss: 3.867891791742295e-05, Learning Rate: 0.000266\n",
      "Epoch 16803/40000, Loss: 6.0364440287230536e-05, Learning Rate: 0.000266\n",
      "Epoch 16804/40000, Loss: 4.303687819628976e-05, Learning Rate: 0.000266\n",
      "Epoch 16805/40000, Loss: 4.2264538933523e-05, Learning Rate: 0.000266\n",
      "Epoch 16806/40000, Loss: 1.530317240394652e-05, Learning Rate: 0.000266\n",
      "Epoch 16807/40000, Loss: 4.2366555135231465e-05, Learning Rate: 0.000266\n",
      "Epoch 16808/40000, Loss: 6.426269101211801e-05, Learning Rate: 0.000266\n",
      "Epoch 16809/40000, Loss: 1.507126034994144e-05, Learning Rate: 0.000266\n",
      "Epoch 16810/40000, Loss: 3.91217035939917e-05, Learning Rate: 0.000266\n",
      "Epoch 16811/40000, Loss: 3.9033766370266676e-05, Learning Rate: 0.000266\n",
      "Epoch 16812/40000, Loss: 5.594447793555446e-05, Learning Rate: 0.000266\n",
      "Epoch 16813/40000, Loss: 3.919858136214316e-05, Learning Rate: 0.000266\n",
      "Epoch 16814/40000, Loss: 3.900940646417439e-05, Learning Rate: 0.000266\n",
      "Epoch 16815/40000, Loss: 4.201448246021755e-05, Learning Rate: 0.000266\n",
      "Epoch 16816/40000, Loss: 4.198931492283009e-05, Learning Rate: 0.000266\n",
      "Epoch 16817/40000, Loss: 6.403704901458696e-05, Learning Rate: 0.000266\n",
      "Epoch 16818/40000, Loss: 6.398407276719809e-05, Learning Rate: 0.000266\n",
      "Epoch 16819/40000, Loss: 6.38432102277875e-05, Learning Rate: 0.000266\n",
      "Epoch 16820/40000, Loss: 1.4885136806697119e-05, Learning Rate: 0.000266\n",
      "Epoch 16821/40000, Loss: 6.534313433803618e-05, Learning Rate: 0.000266\n",
      "Epoch 16822/40000, Loss: 3.3824104320956394e-05, Learning Rate: 0.000266\n",
      "Epoch 16823/40000, Loss: 6.564143404830247e-05, Learning Rate: 0.000266\n",
      "Epoch 16824/40000, Loss: 4.226015153108165e-05, Learning Rate: 0.000266\n",
      "Epoch 16825/40000, Loss: 3.9063925214577466e-05, Learning Rate: 0.000266\n",
      "Epoch 16826/40000, Loss: 3.383021612535231e-05, Learning Rate: 0.000266\n",
      "Epoch 16827/40000, Loss: 1.4871946405037306e-05, Learning Rate: 0.000266\n",
      "Epoch 16828/40000, Loss: 3.413350714254193e-05, Learning Rate: 0.000265\n",
      "Epoch 16829/40000, Loss: 4.225183147354983e-05, Learning Rate: 0.000265\n",
      "Epoch 16830/40000, Loss: 4.234183506923728e-05, Learning Rate: 0.000265\n",
      "Epoch 16831/40000, Loss: 3.924603515770286e-05, Learning Rate: 0.000265\n",
      "Epoch 16832/40000, Loss: 6.43812381895259e-05, Learning Rate: 0.000265\n",
      "Epoch 16833/40000, Loss: 6.42934610368684e-05, Learning Rate: 0.000265\n",
      "Epoch 16834/40000, Loss: 4.201051342533901e-05, Learning Rate: 0.000265\n",
      "Epoch 16835/40000, Loss: 3.381268834345974e-05, Learning Rate: 0.000265\n",
      "Epoch 16836/40000, Loss: 1.5066210835357197e-05, Learning Rate: 0.000265\n",
      "Epoch 16837/40000, Loss: 5.592160960077308e-05, Learning Rate: 0.000265\n",
      "Epoch 16838/40000, Loss: 3.392883081687614e-05, Learning Rate: 0.000265\n",
      "Epoch 16839/40000, Loss: 3.907017890014686e-05, Learning Rate: 0.000265\n",
      "Epoch 16840/40000, Loss: 6.434804527089e-05, Learning Rate: 0.000265\n",
      "Epoch 16841/40000, Loss: 6.440615834435448e-05, Learning Rate: 0.000265\n",
      "Epoch 16842/40000, Loss: 6.430850044125691e-05, Learning Rate: 0.000265\n",
      "Epoch 16843/40000, Loss: 6.431696965591982e-05, Learning Rate: 0.000265\n",
      "Epoch 16844/40000, Loss: 4.314962643547915e-05, Learning Rate: 0.000265\n",
      "Epoch 16845/40000, Loss: 6.558550376212224e-05, Learning Rate: 0.000265\n",
      "Epoch 16846/40000, Loss: 1.5421503121615387e-05, Learning Rate: 0.000265\n",
      "Epoch 16847/40000, Loss: 5.725224036723375e-05, Learning Rate: 0.000265\n",
      "Epoch 16848/40000, Loss: 1.5679413991165347e-05, Learning Rate: 0.000265\n",
      "Epoch 16849/40000, Loss: 3.3958986023208126e-05, Learning Rate: 0.000265\n",
      "Epoch 16850/40000, Loss: 6.508738442789763e-05, Learning Rate: 0.000265\n",
      "Epoch 16851/40000, Loss: 3.925876444554888e-05, Learning Rate: 0.000265\n",
      "Epoch 16852/40000, Loss: 3.399539127713069e-05, Learning Rate: 0.000265\n",
      "Epoch 16853/40000, Loss: 3.3937260013772175e-05, Learning Rate: 0.000265\n",
      "Epoch 16854/40000, Loss: 3.405733878025785e-05, Learning Rate: 0.000265\n",
      "Epoch 16855/40000, Loss: 1.5263280147337355e-05, Learning Rate: 0.000265\n",
      "Epoch 16856/40000, Loss: 3.389751145732589e-05, Learning Rate: 0.000265\n",
      "Epoch 16857/40000, Loss: 5.654350024997257e-05, Learning Rate: 0.000265\n",
      "Epoch 16858/40000, Loss: 3.9246995584107935e-05, Learning Rate: 0.000265\n",
      "Epoch 16859/40000, Loss: 4.307089693611488e-05, Learning Rate: 0.000264\n",
      "Epoch 16860/40000, Loss: 5.726320159737952e-05, Learning Rate: 0.000264\n",
      "Epoch 16861/40000, Loss: 4.4589080061996356e-05, Learning Rate: 0.000264\n",
      "Epoch 16862/40000, Loss: 4.333907054387964e-05, Learning Rate: 0.000264\n",
      "Epoch 16863/40000, Loss: 1.5637300748494454e-05, Learning Rate: 0.000264\n",
      "Epoch 16864/40000, Loss: 6.484283949248493e-05, Learning Rate: 0.000264\n",
      "Epoch 16865/40000, Loss: 6.497157301055267e-05, Learning Rate: 0.000264\n",
      "Epoch 16866/40000, Loss: 5.870973109267652e-05, Learning Rate: 0.000264\n",
      "Epoch 16867/40000, Loss: 5.908002276555635e-05, Learning Rate: 0.000264\n",
      "Epoch 16868/40000, Loss: 3.515498974593356e-05, Learning Rate: 0.000264\n",
      "Epoch 16869/40000, Loss: 5.989464989397675e-05, Learning Rate: 0.000264\n",
      "Epoch 16870/40000, Loss: 3.6281260690884665e-05, Learning Rate: 0.000264\n",
      "Epoch 16871/40000, Loss: 2.0285890059312806e-05, Learning Rate: 0.000264\n",
      "Epoch 16872/40000, Loss: 6.169025436975062e-05, Learning Rate: 0.000264\n",
      "Epoch 16873/40000, Loss: 3.648774145403877e-05, Learning Rate: 0.000264\n",
      "Epoch 16874/40000, Loss: 1.8482438463252038e-05, Learning Rate: 0.000264\n",
      "Epoch 16875/40000, Loss: 4.1584167774999514e-05, Learning Rate: 0.000264\n",
      "Epoch 16876/40000, Loss: 3.6344019463285804e-05, Learning Rate: 0.000264\n",
      "Epoch 16877/40000, Loss: 2.1656862372765318e-05, Learning Rate: 0.000264\n",
      "Epoch 16878/40000, Loss: 4.5600532757816836e-05, Learning Rate: 0.000264\n",
      "Epoch 16879/40000, Loss: 6.735583883710206e-05, Learning Rate: 0.000264\n",
      "Epoch 16880/40000, Loss: 6.667531124548987e-05, Learning Rate: 0.000264\n",
      "Epoch 16881/40000, Loss: 4.642193016479723e-05, Learning Rate: 0.000264\n",
      "Epoch 16882/40000, Loss: 4.538522625807673e-05, Learning Rate: 0.000264\n",
      "Epoch 16883/40000, Loss: 4.0287595766130835e-05, Learning Rate: 0.000264\n",
      "Epoch 16884/40000, Loss: 4.5358774514170364e-05, Learning Rate: 0.000264\n",
      "Epoch 16885/40000, Loss: 3.991647099610418e-05, Learning Rate: 0.000264\n",
      "Epoch 16886/40000, Loss: 6.608863623114303e-05, Learning Rate: 0.000264\n",
      "Epoch 16887/40000, Loss: 4.5118511479813606e-05, Learning Rate: 0.000264\n",
      "Epoch 16888/40000, Loss: 4.287126648705453e-05, Learning Rate: 0.000264\n",
      "Epoch 16889/40000, Loss: 6.477419810835272e-05, Learning Rate: 0.000264\n",
      "Epoch 16890/40000, Loss: 5.678642992279492e-05, Learning Rate: 0.000264\n",
      "Epoch 16891/40000, Loss: 5.642052201437764e-05, Learning Rate: 0.000263\n",
      "Epoch 16892/40000, Loss: 4.254946543369442e-05, Learning Rate: 0.000263\n",
      "Epoch 16893/40000, Loss: 5.7214572734665126e-05, Learning Rate: 0.000263\n",
      "Epoch 16894/40000, Loss: 5.627095015370287e-05, Learning Rate: 0.000263\n",
      "Epoch 16895/40000, Loss: 6.467117054853588e-05, Learning Rate: 0.000263\n",
      "Epoch 16896/40000, Loss: 3.900282536051236e-05, Learning Rate: 0.000263\n",
      "Epoch 16897/40000, Loss: 3.392126382095739e-05, Learning Rate: 0.000263\n",
      "Epoch 16898/40000, Loss: 4.285087197786197e-05, Learning Rate: 0.000263\n",
      "Epoch 16899/40000, Loss: 3.4214808692922816e-05, Learning Rate: 0.000263\n",
      "Epoch 16900/40000, Loss: 3.4049979149131104e-05, Learning Rate: 0.000263\n",
      "Epoch 16901/40000, Loss: 3.447814378887415e-05, Learning Rate: 0.000263\n",
      "Epoch 16902/40000, Loss: 6.456701521528885e-05, Learning Rate: 0.000263\n",
      "Epoch 16903/40000, Loss: 3.4259679523529485e-05, Learning Rate: 0.000263\n",
      "Epoch 16904/40000, Loss: 6.550551916006953e-05, Learning Rate: 0.000263\n",
      "Epoch 16905/40000, Loss: 3.4176893677795306e-05, Learning Rate: 0.000263\n",
      "Epoch 16906/40000, Loss: 5.696568769053556e-05, Learning Rate: 0.000263\n",
      "Epoch 16907/40000, Loss: 5.7311932323500514e-05, Learning Rate: 0.000263\n",
      "Epoch 16908/40000, Loss: 3.458757419139147e-05, Learning Rate: 0.000263\n",
      "Epoch 16909/40000, Loss: 6.479586591012776e-05, Learning Rate: 0.000263\n",
      "Epoch 16910/40000, Loss: 4.416815863805823e-05, Learning Rate: 0.000263\n",
      "Epoch 16911/40000, Loss: 1.6928897821344435e-05, Learning Rate: 0.000263\n",
      "Epoch 16912/40000, Loss: 6.486348866019398e-05, Learning Rate: 0.000263\n",
      "Epoch 16913/40000, Loss: 1.5633524526492693e-05, Learning Rate: 0.000263\n",
      "Epoch 16914/40000, Loss: 4.360107777756639e-05, Learning Rate: 0.000263\n",
      "Epoch 16915/40000, Loss: 3.4535016311565414e-05, Learning Rate: 0.000263\n",
      "Epoch 16916/40000, Loss: 5.6584507547086105e-05, Learning Rate: 0.000263\n",
      "Epoch 16917/40000, Loss: 1.5472442100872286e-05, Learning Rate: 0.000263\n",
      "Epoch 16918/40000, Loss: 6.464355828939006e-05, Learning Rate: 0.000263\n",
      "Epoch 16919/40000, Loss: 4.107555287191644e-05, Learning Rate: 0.000263\n",
      "Epoch 16920/40000, Loss: 4.327155329519883e-05, Learning Rate: 0.000263\n",
      "Epoch 16921/40000, Loss: 3.439172724029049e-05, Learning Rate: 0.000263\n",
      "Epoch 16922/40000, Loss: 1.5804709619260393e-05, Learning Rate: 0.000262\n",
      "Epoch 16923/40000, Loss: 5.7137454859912395e-05, Learning Rate: 0.000262\n",
      "Epoch 16924/40000, Loss: 1.620203875063453e-05, Learning Rate: 0.000262\n",
      "Epoch 16925/40000, Loss: 1.5954336049617268e-05, Learning Rate: 0.000262\n",
      "Epoch 16926/40000, Loss: 6.539553578477353e-05, Learning Rate: 0.000262\n",
      "Epoch 16927/40000, Loss: 3.470416777417995e-05, Learning Rate: 0.000262\n",
      "Epoch 16928/40000, Loss: 5.873869304195978e-05, Learning Rate: 0.000262\n",
      "Epoch 16929/40000, Loss: 5.775871977675706e-05, Learning Rate: 0.000262\n",
      "Epoch 16930/40000, Loss: 5.835296178702265e-05, Learning Rate: 0.000262\n",
      "Epoch 16931/40000, Loss: 4.143212936469354e-05, Learning Rate: 0.000262\n",
      "Epoch 16932/40000, Loss: 1.6141502783284523e-05, Learning Rate: 0.000262\n",
      "Epoch 16933/40000, Loss: 6.026568735251203e-05, Learning Rate: 0.000262\n",
      "Epoch 16934/40000, Loss: 6.668116839136928e-05, Learning Rate: 0.000262\n",
      "Epoch 16935/40000, Loss: 4.287137562641874e-05, Learning Rate: 0.000262\n",
      "Epoch 16936/40000, Loss: 1.973629514395725e-05, Learning Rate: 0.000262\n",
      "Epoch 16937/40000, Loss: 4.280404755263589e-05, Learning Rate: 0.000262\n",
      "Epoch 16938/40000, Loss: 4.593089397531003e-05, Learning Rate: 0.000262\n",
      "Epoch 16939/40000, Loss: 4.511733277468011e-05, Learning Rate: 0.000262\n",
      "Epoch 16940/40000, Loss: 4.216406523482874e-05, Learning Rate: 0.000262\n",
      "Epoch 16941/40000, Loss: 6.950600072741508e-05, Learning Rate: 0.000262\n",
      "Epoch 16942/40000, Loss: 4.164709025644697e-05, Learning Rate: 0.000262\n",
      "Epoch 16943/40000, Loss: 2.103810584230814e-05, Learning Rate: 0.000262\n",
      "Epoch 16944/40000, Loss: 1.6481748389196582e-05, Learning Rate: 0.000262\n",
      "Epoch 16945/40000, Loss: 1.9303986846352927e-05, Learning Rate: 0.000262\n",
      "Epoch 16946/40000, Loss: 3.589600237319246e-05, Learning Rate: 0.000262\n",
      "Epoch 16947/40000, Loss: 7.302285666810349e-05, Learning Rate: 0.000262\n",
      "Epoch 16948/40000, Loss: 1.7787901015253738e-05, Learning Rate: 0.000262\n",
      "Epoch 16949/40000, Loss: 3.9996284613152966e-05, Learning Rate: 0.000262\n",
      "Epoch 16950/40000, Loss: 4.995503695681691e-05, Learning Rate: 0.000262\n",
      "Epoch 16951/40000, Loss: 5.3199419198790565e-05, Learning Rate: 0.000262\n",
      "Epoch 16952/40000, Loss: 4.336879283073358e-05, Learning Rate: 0.000262\n",
      "Epoch 16953/40000, Loss: 6.400507845683023e-05, Learning Rate: 0.000262\n",
      "Epoch 16954/40000, Loss: 6.786547601222992e-05, Learning Rate: 0.000261\n",
      "Epoch 16955/40000, Loss: 8.933993376558647e-05, Learning Rate: 0.000261\n",
      "Epoch 16956/40000, Loss: 4.670481212087907e-05, Learning Rate: 0.000261\n",
      "Epoch 16957/40000, Loss: 1.803449231374543e-05, Learning Rate: 0.000261\n",
      "Epoch 16958/40000, Loss: 1.5863464795984328e-05, Learning Rate: 0.000261\n",
      "Epoch 16959/40000, Loss: 4.4986656575929374e-05, Learning Rate: 0.000261\n",
      "Epoch 16960/40000, Loss: 6.162028876133263e-05, Learning Rate: 0.000261\n",
      "Epoch 16961/40000, Loss: 4.036932296003215e-05, Learning Rate: 0.000261\n",
      "Epoch 16962/40000, Loss: 4.059958882862702e-05, Learning Rate: 0.000261\n",
      "Epoch 16963/40000, Loss: 3.9808062865631655e-05, Learning Rate: 0.000261\n",
      "Epoch 16964/40000, Loss: 5.7149700296577066e-05, Learning Rate: 0.000261\n",
      "Epoch 16965/40000, Loss: 3.443556488491595e-05, Learning Rate: 0.000261\n",
      "Epoch 16966/40000, Loss: 3.3914206142071635e-05, Learning Rate: 0.000261\n",
      "Epoch 16967/40000, Loss: 6.413270602934062e-05, Learning Rate: 0.000261\n",
      "Epoch 16968/40000, Loss: 3.4233373298775405e-05, Learning Rate: 0.000261\n",
      "Epoch 16969/40000, Loss: 3.377460961928591e-05, Learning Rate: 0.000261\n",
      "Epoch 16970/40000, Loss: 6.39895151834935e-05, Learning Rate: 0.000261\n",
      "Epoch 16971/40000, Loss: 6.406789907487109e-05, Learning Rate: 0.000261\n",
      "Epoch 16972/40000, Loss: 3.369638216099702e-05, Learning Rate: 0.000261\n",
      "Epoch 16973/40000, Loss: 6.401358405128121e-05, Learning Rate: 0.000261\n",
      "Epoch 16974/40000, Loss: 3.8900783692952245e-05, Learning Rate: 0.000261\n",
      "Epoch 16975/40000, Loss: 6.373836367856711e-05, Learning Rate: 0.000261\n",
      "Epoch 16976/40000, Loss: 3.350980841787532e-05, Learning Rate: 0.000261\n",
      "Epoch 16977/40000, Loss: 3.369472688063979e-05, Learning Rate: 0.000261\n",
      "Epoch 16978/40000, Loss: 3.8976413634372875e-05, Learning Rate: 0.000261\n",
      "Epoch 16979/40000, Loss: 6.433311500586569e-05, Learning Rate: 0.000261\n",
      "Epoch 16980/40000, Loss: 5.597440031124279e-05, Learning Rate: 0.000261\n",
      "Epoch 16981/40000, Loss: 3.896253474522382e-05, Learning Rate: 0.000261\n",
      "Epoch 16982/40000, Loss: 5.6253029470099136e-05, Learning Rate: 0.000261\n",
      "Epoch 16983/40000, Loss: 1.5055045878398232e-05, Learning Rate: 0.000261\n",
      "Epoch 16984/40000, Loss: 1.5283467291737907e-05, Learning Rate: 0.000261\n",
      "Epoch 16985/40000, Loss: 1.4903182091074996e-05, Learning Rate: 0.000261\n",
      "Epoch 16986/40000, Loss: 3.894732071785256e-05, Learning Rate: 0.000260\n",
      "Epoch 16987/40000, Loss: 4.232770879752934e-05, Learning Rate: 0.000260\n",
      "Epoch 16988/40000, Loss: 5.6496042816434056e-05, Learning Rate: 0.000260\n",
      "Epoch 16989/40000, Loss: 3.394002123968676e-05, Learning Rate: 0.000260\n",
      "Epoch 16990/40000, Loss: 3.382171053090133e-05, Learning Rate: 0.000260\n",
      "Epoch 16991/40000, Loss: 4.217653622617945e-05, Learning Rate: 0.000260\n",
      "Epoch 16992/40000, Loss: 4.202012496534735e-05, Learning Rate: 0.000260\n",
      "Epoch 16993/40000, Loss: 4.2062922148033977e-05, Learning Rate: 0.000260\n",
      "Epoch 16994/40000, Loss: 5.594879257841967e-05, Learning Rate: 0.000260\n",
      "Epoch 16995/40000, Loss: 5.583219535765238e-05, Learning Rate: 0.000260\n",
      "Epoch 16996/40000, Loss: 6.403852603398263e-05, Learning Rate: 0.000260\n",
      "Epoch 16997/40000, Loss: 4.202352283755317e-05, Learning Rate: 0.000260\n",
      "Epoch 16998/40000, Loss: 5.580586730502546e-05, Learning Rate: 0.000260\n",
      "Epoch 16999/40000, Loss: 1.487658573751105e-05, Learning Rate: 0.000260\n",
      "Epoch 17000/40000, Loss: 3.3631276892265305e-05, Learning Rate: 0.000260\n",
      "Epoch 17001/40000, Loss: 1.4988601833465509e-05, Learning Rate: 0.000260\n",
      "Epoch 17002/40000, Loss: 6.367830792441964e-05, Learning Rate: 0.000260\n",
      "Epoch 17003/40000, Loss: 1.4932287740521133e-05, Learning Rate: 0.000260\n",
      "Epoch 17004/40000, Loss: 4.211001214571297e-05, Learning Rate: 0.000260\n",
      "Epoch 17005/40000, Loss: 3.875608672387898e-05, Learning Rate: 0.000260\n",
      "Epoch 17006/40000, Loss: 4.239653571858071e-05, Learning Rate: 0.000260\n",
      "Epoch 17007/40000, Loss: 6.395418313331902e-05, Learning Rate: 0.000260\n",
      "Epoch 17008/40000, Loss: 5.591935769189149e-05, Learning Rate: 0.000260\n",
      "Epoch 17009/40000, Loss: 5.581441291724332e-05, Learning Rate: 0.000260\n",
      "Epoch 17010/40000, Loss: 1.5288267604773864e-05, Learning Rate: 0.000260\n",
      "Epoch 17011/40000, Loss: 6.410778587451205e-05, Learning Rate: 0.000260\n",
      "Epoch 17012/40000, Loss: 4.345573688624427e-05, Learning Rate: 0.000260\n",
      "Epoch 17013/40000, Loss: 4.271751458873041e-05, Learning Rate: 0.000260\n",
      "Epoch 17014/40000, Loss: 5.586736369878054e-05, Learning Rate: 0.000260\n",
      "Epoch 17015/40000, Loss: 3.377625034772791e-05, Learning Rate: 0.000260\n",
      "Epoch 17016/40000, Loss: 6.396318349288777e-05, Learning Rate: 0.000260\n",
      "Epoch 17017/40000, Loss: 3.870772707159631e-05, Learning Rate: 0.000260\n",
      "Epoch 17018/40000, Loss: 3.369886690052226e-05, Learning Rate: 0.000259\n",
      "Epoch 17019/40000, Loss: 3.924309567082673e-05, Learning Rate: 0.000259\n",
      "Epoch 17020/40000, Loss: 5.5993594287429005e-05, Learning Rate: 0.000259\n",
      "Epoch 17021/40000, Loss: 4.284396709408611e-05, Learning Rate: 0.000259\n",
      "Epoch 17022/40000, Loss: 4.250043639331125e-05, Learning Rate: 0.000259\n",
      "Epoch 17023/40000, Loss: 4.227811950840987e-05, Learning Rate: 0.000259\n",
      "Epoch 17024/40000, Loss: 5.590245928033255e-05, Learning Rate: 0.000259\n",
      "Epoch 17025/40000, Loss: 3.344647484482266e-05, Learning Rate: 0.000259\n",
      "Epoch 17026/40000, Loss: 6.46593834972009e-05, Learning Rate: 0.000259\n",
      "Epoch 17027/40000, Loss: 3.8892267184564844e-05, Learning Rate: 0.000259\n",
      "Epoch 17028/40000, Loss: 3.894081964972429e-05, Learning Rate: 0.000259\n",
      "Epoch 17029/40000, Loss: 5.59828695259057e-05, Learning Rate: 0.000259\n",
      "Epoch 17030/40000, Loss: 3.865222242893651e-05, Learning Rate: 0.000259\n",
      "Epoch 17031/40000, Loss: 5.6268141634063795e-05, Learning Rate: 0.000259\n",
      "Epoch 17032/40000, Loss: 1.4968258255976252e-05, Learning Rate: 0.000259\n",
      "Epoch 17033/40000, Loss: 4.2790710722329095e-05, Learning Rate: 0.000259\n",
      "Epoch 17034/40000, Loss: 1.5361652913270518e-05, Learning Rate: 0.000259\n",
      "Epoch 17035/40000, Loss: 6.423467129934579e-05, Learning Rate: 0.000259\n",
      "Epoch 17036/40000, Loss: 1.6992527889669873e-05, Learning Rate: 0.000259\n",
      "Epoch 17037/40000, Loss: 3.940548776881769e-05, Learning Rate: 0.000259\n",
      "Epoch 17038/40000, Loss: 1.5627492757630534e-05, Learning Rate: 0.000259\n",
      "Epoch 17039/40000, Loss: 6.465925980592147e-05, Learning Rate: 0.000259\n",
      "Epoch 17040/40000, Loss: 3.407957046874799e-05, Learning Rate: 0.000259\n",
      "Epoch 17041/40000, Loss: 3.934892447432503e-05, Learning Rate: 0.000259\n",
      "Epoch 17042/40000, Loss: 6.516520079458132e-05, Learning Rate: 0.000259\n",
      "Epoch 17043/40000, Loss: 3.9674127037869766e-05, Learning Rate: 0.000259\n",
      "Epoch 17044/40000, Loss: 3.934997585020028e-05, Learning Rate: 0.000259\n",
      "Epoch 17045/40000, Loss: 4.243160583428107e-05, Learning Rate: 0.000259\n",
      "Epoch 17046/40000, Loss: 1.532641726953443e-05, Learning Rate: 0.000259\n",
      "Epoch 17047/40000, Loss: 5.597285780822858e-05, Learning Rate: 0.000259\n",
      "Epoch 17048/40000, Loss: 4.2281255446141586e-05, Learning Rate: 0.000259\n",
      "Epoch 17049/40000, Loss: 3.8829395634820685e-05, Learning Rate: 0.000259\n",
      "Epoch 17050/40000, Loss: 6.408533226931468e-05, Learning Rate: 0.000258\n",
      "Epoch 17051/40000, Loss: 4.229378828313202e-05, Learning Rate: 0.000258\n",
      "Epoch 17052/40000, Loss: 5.645137935061939e-05, Learning Rate: 0.000258\n",
      "Epoch 17053/40000, Loss: 3.396070678718388e-05, Learning Rate: 0.000258\n",
      "Epoch 17054/40000, Loss: 3.893621760653332e-05, Learning Rate: 0.000258\n",
      "Epoch 17055/40000, Loss: 6.503737677121535e-05, Learning Rate: 0.000258\n",
      "Epoch 17056/40000, Loss: 3.434922109590843e-05, Learning Rate: 0.000258\n",
      "Epoch 17057/40000, Loss: 1.5182643437583465e-05, Learning Rate: 0.000258\n",
      "Epoch 17058/40000, Loss: 6.474647670984268e-05, Learning Rate: 0.000258\n",
      "Epoch 17059/40000, Loss: 4.247408651281148e-05, Learning Rate: 0.000258\n",
      "Epoch 17060/40000, Loss: 3.920869130524807e-05, Learning Rate: 0.000258\n",
      "Epoch 17061/40000, Loss: 5.599989526672289e-05, Learning Rate: 0.000258\n",
      "Epoch 17062/40000, Loss: 1.5344307030318305e-05, Learning Rate: 0.000258\n",
      "Epoch 17063/40000, Loss: 6.545103678945452e-05, Learning Rate: 0.000258\n",
      "Epoch 17064/40000, Loss: 6.493338150903583e-05, Learning Rate: 0.000258\n",
      "Epoch 17065/40000, Loss: 4.071796502103098e-05, Learning Rate: 0.000258\n",
      "Epoch 17066/40000, Loss: 6.431831570807844e-05, Learning Rate: 0.000258\n",
      "Epoch 17067/40000, Loss: 6.437629781430587e-05, Learning Rate: 0.000258\n",
      "Epoch 17068/40000, Loss: 4.269788769306615e-05, Learning Rate: 0.000258\n",
      "Epoch 17069/40000, Loss: 5.664167110808194e-05, Learning Rate: 0.000258\n",
      "Epoch 17070/40000, Loss: 4.3970198021270335e-05, Learning Rate: 0.000258\n",
      "Epoch 17071/40000, Loss: 1.7113994545070454e-05, Learning Rate: 0.000258\n",
      "Epoch 17072/40000, Loss: 4.446033199201338e-05, Learning Rate: 0.000258\n",
      "Epoch 17073/40000, Loss: 3.5547545849112794e-05, Learning Rate: 0.000258\n",
      "Epoch 17074/40000, Loss: 4.04958518629428e-05, Learning Rate: 0.000258\n",
      "Epoch 17075/40000, Loss: 2.1964940970065072e-05, Learning Rate: 0.000258\n",
      "Epoch 17076/40000, Loss: 1.792153125279583e-05, Learning Rate: 0.000258\n",
      "Epoch 17077/40000, Loss: 4.1593473724788055e-05, Learning Rate: 0.000258\n",
      "Epoch 17078/40000, Loss: 4.003908907179721e-05, Learning Rate: 0.000258\n",
      "Epoch 17079/40000, Loss: 3.576088056433946e-05, Learning Rate: 0.000258\n",
      "Epoch 17080/40000, Loss: 4.428089960129e-05, Learning Rate: 0.000258\n",
      "Epoch 17081/40000, Loss: 5.906445949221961e-05, Learning Rate: 0.000258\n",
      "Epoch 17082/40000, Loss: 3.678682696772739e-05, Learning Rate: 0.000258\n",
      "Epoch 17083/40000, Loss: 5.976287866360508e-05, Learning Rate: 0.000257\n",
      "Epoch 17084/40000, Loss: 6.968826346565038e-05, Learning Rate: 0.000257\n",
      "Epoch 17085/40000, Loss: 6.267295975703746e-05, Learning Rate: 0.000257\n",
      "Epoch 17086/40000, Loss: 7.44384087738581e-05, Learning Rate: 0.000257\n",
      "Epoch 17087/40000, Loss: 2.0631541701732203e-05, Learning Rate: 0.000257\n",
      "Epoch 17088/40000, Loss: 4.748829087475315e-05, Learning Rate: 0.000257\n",
      "Epoch 17089/40000, Loss: 6.838305853307247e-05, Learning Rate: 0.000257\n",
      "Epoch 17090/40000, Loss: 6.485041376436129e-05, Learning Rate: 0.000257\n",
      "Epoch 17091/40000, Loss: 4.334113691584207e-05, Learning Rate: 0.000257\n",
      "Epoch 17092/40000, Loss: 4.8635520215611905e-05, Learning Rate: 0.000257\n",
      "Epoch 17093/40000, Loss: 1.975453778868541e-05, Learning Rate: 0.000257\n",
      "Epoch 17094/40000, Loss: 4.5897773816250265e-05, Learning Rate: 0.000257\n",
      "Epoch 17095/40000, Loss: 6.117273005656898e-05, Learning Rate: 0.000257\n",
      "Epoch 17096/40000, Loss: 1.7972584828385152e-05, Learning Rate: 0.000257\n",
      "Epoch 17097/40000, Loss: 4.535866901278496e-05, Learning Rate: 0.000257\n",
      "Epoch 17098/40000, Loss: 4.4649848859990016e-05, Learning Rate: 0.000257\n",
      "Epoch 17099/40000, Loss: 6.637510523432866e-05, Learning Rate: 0.000257\n",
      "Epoch 17100/40000, Loss: 3.507403380353935e-05, Learning Rate: 0.000257\n",
      "Epoch 17101/40000, Loss: 6.774815847165883e-05, Learning Rate: 0.000257\n",
      "Epoch 17102/40000, Loss: 4.449253174243495e-05, Learning Rate: 0.000257\n",
      "Epoch 17103/40000, Loss: 4.347799767856486e-05, Learning Rate: 0.000257\n",
      "Epoch 17104/40000, Loss: 3.939179805456661e-05, Learning Rate: 0.000257\n",
      "Epoch 17105/40000, Loss: 3.43281208188273e-05, Learning Rate: 0.000257\n",
      "Epoch 17106/40000, Loss: 6.508402293547988e-05, Learning Rate: 0.000257\n",
      "Epoch 17107/40000, Loss: 3.9157683204393834e-05, Learning Rate: 0.000257\n",
      "Epoch 17108/40000, Loss: 1.4999901395640336e-05, Learning Rate: 0.000257\n",
      "Epoch 17109/40000, Loss: 1.4941231711418368e-05, Learning Rate: 0.000257\n",
      "Epoch 17110/40000, Loss: 6.450639921240509e-05, Learning Rate: 0.000257\n",
      "Epoch 17111/40000, Loss: 3.955152351409197e-05, Learning Rate: 0.000257\n",
      "Epoch 17112/40000, Loss: 4.307586277718656e-05, Learning Rate: 0.000257\n",
      "Epoch 17113/40000, Loss: 4.3183204979868606e-05, Learning Rate: 0.000257\n",
      "Epoch 17114/40000, Loss: 5.7373188610654324e-05, Learning Rate: 0.000257\n",
      "Epoch 17115/40000, Loss: 3.970093166572042e-05, Learning Rate: 0.000256\n",
      "Epoch 17116/40000, Loss: 3.933691914426163e-05, Learning Rate: 0.000256\n",
      "Epoch 17117/40000, Loss: 1.5415653251693584e-05, Learning Rate: 0.000256\n",
      "Epoch 17118/40000, Loss: 4.3082596675958484e-05, Learning Rate: 0.000256\n",
      "Epoch 17119/40000, Loss: 3.955587453674525e-05, Learning Rate: 0.000256\n",
      "Epoch 17120/40000, Loss: 1.5249672287609428e-05, Learning Rate: 0.000256\n",
      "Epoch 17121/40000, Loss: 1.520202658866765e-05, Learning Rate: 0.000256\n",
      "Epoch 17122/40000, Loss: 3.3538359275553375e-05, Learning Rate: 0.000256\n",
      "Epoch 17123/40000, Loss: 3.9093934901757166e-05, Learning Rate: 0.000256\n",
      "Epoch 17124/40000, Loss: 5.746933675254695e-05, Learning Rate: 0.000256\n",
      "Epoch 17125/40000, Loss: 6.428731285268441e-05, Learning Rate: 0.000256\n",
      "Epoch 17126/40000, Loss: 1.5874837117735296e-05, Learning Rate: 0.000256\n",
      "Epoch 17127/40000, Loss: 1.577736111357808e-05, Learning Rate: 0.000256\n",
      "Epoch 17128/40000, Loss: 6.428585038520396e-05, Learning Rate: 0.000256\n",
      "Epoch 17129/40000, Loss: 5.8626614190870896e-05, Learning Rate: 0.000256\n",
      "Epoch 17130/40000, Loss: 5.704492286895402e-05, Learning Rate: 0.000256\n",
      "Epoch 17131/40000, Loss: 6.58769640722312e-05, Learning Rate: 0.000256\n",
      "Epoch 17132/40000, Loss: 3.9345002733170986e-05, Learning Rate: 0.000256\n",
      "Epoch 17133/40000, Loss: 4.490793799050152e-05, Learning Rate: 0.000256\n",
      "Epoch 17134/40000, Loss: 6.727618892909959e-05, Learning Rate: 0.000256\n",
      "Epoch 17135/40000, Loss: 5.8093457482755184e-05, Learning Rate: 0.000256\n",
      "Epoch 17136/40000, Loss: 3.9762493543094024e-05, Learning Rate: 0.000256\n",
      "Epoch 17137/40000, Loss: 7.047895633149892e-05, Learning Rate: 0.000256\n",
      "Epoch 17138/40000, Loss: 1.7432781532988884e-05, Learning Rate: 0.000256\n",
      "Epoch 17139/40000, Loss: 6.185307574924082e-05, Learning Rate: 0.000256\n",
      "Epoch 17140/40000, Loss: 7.78760077082552e-05, Learning Rate: 0.000256\n",
      "Epoch 17141/40000, Loss: 4.800968599738553e-05, Learning Rate: 0.000256\n",
      "Epoch 17142/40000, Loss: 4.4902993977302685e-05, Learning Rate: 0.000256\n",
      "Epoch 17143/40000, Loss: 6.571769336005673e-05, Learning Rate: 0.000256\n",
      "Epoch 17144/40000, Loss: 7.802603067830205e-05, Learning Rate: 0.000256\n",
      "Epoch 17145/40000, Loss: 0.00010715859389165416, Learning Rate: 0.000256\n",
      "Epoch 17146/40000, Loss: 0.00013449999096337706, Learning Rate: 0.000256\n",
      "Epoch 17147/40000, Loss: 7.16510257916525e-05, Learning Rate: 0.000256\n",
      "Epoch 17148/40000, Loss: 0.00011089392501162365, Learning Rate: 0.000255\n",
      "Epoch 17149/40000, Loss: 4.6590936108259484e-05, Learning Rate: 0.000255\n",
      "Epoch 17150/40000, Loss: 7.804418419254944e-05, Learning Rate: 0.000255\n",
      "Epoch 17151/40000, Loss: 7.703567098360509e-05, Learning Rate: 0.000255\n",
      "Epoch 17152/40000, Loss: 2.50916818913538e-05, Learning Rate: 0.000255\n",
      "Epoch 17153/40000, Loss: 7.589432061649859e-05, Learning Rate: 0.000255\n",
      "Epoch 17154/40000, Loss: 5.114825034979731e-05, Learning Rate: 0.000255\n",
      "Epoch 17155/40000, Loss: 5.1108712796121836e-05, Learning Rate: 0.000255\n",
      "Epoch 17156/40000, Loss: 7.315764378290623e-05, Learning Rate: 0.000255\n",
      "Epoch 17157/40000, Loss: 6.923354521859437e-05, Learning Rate: 0.000255\n",
      "Epoch 17158/40000, Loss: 1.82983076228993e-05, Learning Rate: 0.000255\n",
      "Epoch 17159/40000, Loss: 1.781857827154454e-05, Learning Rate: 0.000255\n",
      "Epoch 17160/40000, Loss: 4.094336327398196e-05, Learning Rate: 0.000255\n",
      "Epoch 17161/40000, Loss: 3.6166376958135515e-05, Learning Rate: 0.000255\n",
      "Epoch 17162/40000, Loss: 3.563013160601258e-05, Learning Rate: 0.000255\n",
      "Epoch 17163/40000, Loss: 4.111612361157313e-05, Learning Rate: 0.000255\n",
      "Epoch 17164/40000, Loss: 5.8885150792775676e-05, Learning Rate: 0.000255\n",
      "Epoch 17165/40000, Loss: 4.0329901821678504e-05, Learning Rate: 0.000255\n",
      "Epoch 17166/40000, Loss: 6.081437095417641e-05, Learning Rate: 0.000255\n",
      "Epoch 17167/40000, Loss: 4.045217428938486e-05, Learning Rate: 0.000255\n",
      "Epoch 17168/40000, Loss: 3.3894502848852426e-05, Learning Rate: 0.000255\n",
      "Epoch 17169/40000, Loss: 1.5759675079607405e-05, Learning Rate: 0.000255\n",
      "Epoch 17170/40000, Loss: 4.295550024835393e-05, Learning Rate: 0.000255\n",
      "Epoch 17171/40000, Loss: 1.606811383680906e-05, Learning Rate: 0.000255\n",
      "Epoch 17172/40000, Loss: 4.4478889321908355e-05, Learning Rate: 0.000255\n",
      "Epoch 17173/40000, Loss: 6.419129204005003e-05, Learning Rate: 0.000255\n",
      "Epoch 17174/40000, Loss: 6.401996506610885e-05, Learning Rate: 0.000255\n",
      "Epoch 17175/40000, Loss: 5.585172402788885e-05, Learning Rate: 0.000255\n",
      "Epoch 17176/40000, Loss: 4.267843905836344e-05, Learning Rate: 0.000255\n",
      "Epoch 17177/40000, Loss: 4.207321035210043e-05, Learning Rate: 0.000255\n",
      "Epoch 17178/40000, Loss: 1.4910072422935627e-05, Learning Rate: 0.000255\n",
      "Epoch 17179/40000, Loss: 4.233650179230608e-05, Learning Rate: 0.000255\n",
      "Epoch 17180/40000, Loss: 4.18961608374957e-05, Learning Rate: 0.000254\n",
      "Epoch 17181/40000, Loss: 3.8492828025482595e-05, Learning Rate: 0.000254\n",
      "Epoch 17182/40000, Loss: 6.362651038216427e-05, Learning Rate: 0.000254\n",
      "Epoch 17183/40000, Loss: 5.549396155402064e-05, Learning Rate: 0.000254\n",
      "Epoch 17184/40000, Loss: 5.5512384278699756e-05, Learning Rate: 0.000254\n",
      "Epoch 17185/40000, Loss: 1.4726019799127243e-05, Learning Rate: 0.000254\n",
      "Epoch 17186/40000, Loss: 5.558962584473193e-05, Learning Rate: 0.000254\n",
      "Epoch 17187/40000, Loss: 3.844095044769347e-05, Learning Rate: 0.000254\n",
      "Epoch 17188/40000, Loss: 3.343281423440203e-05, Learning Rate: 0.000254\n",
      "Epoch 17189/40000, Loss: 3.8451937143690884e-05, Learning Rate: 0.000254\n",
      "Epoch 17190/40000, Loss: 6.352630589390174e-05, Learning Rate: 0.000254\n",
      "Epoch 17191/40000, Loss: 3.8503774703713134e-05, Learning Rate: 0.000254\n",
      "Epoch 17192/40000, Loss: 3.833125811070204e-05, Learning Rate: 0.000254\n",
      "Epoch 17193/40000, Loss: 1.5149356840993278e-05, Learning Rate: 0.000254\n",
      "Epoch 17194/40000, Loss: 4.2171053792117164e-05, Learning Rate: 0.000254\n",
      "Epoch 17195/40000, Loss: 1.4680821550427936e-05, Learning Rate: 0.000254\n",
      "Epoch 17196/40000, Loss: 3.923601616406813e-05, Learning Rate: 0.000254\n",
      "Epoch 17197/40000, Loss: 1.4721868865308352e-05, Learning Rate: 0.000254\n",
      "Epoch 17198/40000, Loss: 4.035219535580836e-05, Learning Rate: 0.000254\n",
      "Epoch 17199/40000, Loss: 1.4849164472252596e-05, Learning Rate: 0.000254\n",
      "Epoch 17200/40000, Loss: 4.1691280785016716e-05, Learning Rate: 0.000254\n",
      "Epoch 17201/40000, Loss: 3.331647167215124e-05, Learning Rate: 0.000254\n",
      "Epoch 17202/40000, Loss: 4.2159204895142466e-05, Learning Rate: 0.000254\n",
      "Epoch 17203/40000, Loss: 4.200153489364311e-05, Learning Rate: 0.000254\n",
      "Epoch 17204/40000, Loss: 1.4935398212401196e-05, Learning Rate: 0.000254\n",
      "Epoch 17205/40000, Loss: 3.3283064112765715e-05, Learning Rate: 0.000254\n",
      "Epoch 17206/40000, Loss: 1.504386636952404e-05, Learning Rate: 0.000254\n",
      "Epoch 17207/40000, Loss: 4.208061727695167e-05, Learning Rate: 0.000254\n",
      "Epoch 17208/40000, Loss: 6.357960228342563e-05, Learning Rate: 0.000254\n",
      "Epoch 17209/40000, Loss: 1.502732266089879e-05, Learning Rate: 0.000254\n",
      "Epoch 17210/40000, Loss: 3.864592144964263e-05, Learning Rate: 0.000254\n",
      "Epoch 17211/40000, Loss: 4.185326906736009e-05, Learning Rate: 0.000254\n",
      "Epoch 17212/40000, Loss: 3.349109829287045e-05, Learning Rate: 0.000254\n",
      "Epoch 17213/40000, Loss: 3.843413287540898e-05, Learning Rate: 0.000253\n",
      "Epoch 17214/40000, Loss: 5.5590317060705274e-05, Learning Rate: 0.000253\n",
      "Epoch 17215/40000, Loss: 5.550105925067328e-05, Learning Rate: 0.000253\n",
      "Epoch 17216/40000, Loss: 3.3490927307866514e-05, Learning Rate: 0.000253\n",
      "Epoch 17217/40000, Loss: 3.3466632885392755e-05, Learning Rate: 0.000253\n",
      "Epoch 17218/40000, Loss: 4.181429176242091e-05, Learning Rate: 0.000253\n",
      "Epoch 17219/40000, Loss: 3.3869637263705954e-05, Learning Rate: 0.000253\n",
      "Epoch 17220/40000, Loss: 5.570755456574261e-05, Learning Rate: 0.000253\n",
      "Epoch 17221/40000, Loss: 5.5524920753668994e-05, Learning Rate: 0.000253\n",
      "Epoch 17222/40000, Loss: 4.194783468847163e-05, Learning Rate: 0.000253\n",
      "Epoch 17223/40000, Loss: 1.4700101019116119e-05, Learning Rate: 0.000253\n",
      "Epoch 17224/40000, Loss: 3.344029391882941e-05, Learning Rate: 0.000253\n",
      "Epoch 17225/40000, Loss: 6.334819772746414e-05, Learning Rate: 0.000253\n",
      "Epoch 17226/40000, Loss: 6.330612814053893e-05, Learning Rate: 0.000253\n",
      "Epoch 17227/40000, Loss: 3.3479205740150064e-05, Learning Rate: 0.000253\n",
      "Epoch 17228/40000, Loss: 1.481842809880618e-05, Learning Rate: 0.000253\n",
      "Epoch 17229/40000, Loss: 3.837298936559819e-05, Learning Rate: 0.000253\n",
      "Epoch 17230/40000, Loss: 4.18872878071852e-05, Learning Rate: 0.000253\n",
      "Epoch 17231/40000, Loss: 3.864511018036865e-05, Learning Rate: 0.000253\n",
      "Epoch 17232/40000, Loss: 4.2074869270436466e-05, Learning Rate: 0.000253\n",
      "Epoch 17233/40000, Loss: 1.493859053880442e-05, Learning Rate: 0.000253\n",
      "Epoch 17234/40000, Loss: 5.561211582971737e-05, Learning Rate: 0.000253\n",
      "Epoch 17235/40000, Loss: 1.4831548469373956e-05, Learning Rate: 0.000253\n",
      "Epoch 17236/40000, Loss: 3.3473737858003005e-05, Learning Rate: 0.000253\n",
      "Epoch 17237/40000, Loss: 3.336459121783264e-05, Learning Rate: 0.000253\n",
      "Epoch 17238/40000, Loss: 5.580759170698002e-05, Learning Rate: 0.000253\n",
      "Epoch 17239/40000, Loss: 1.4915160136297345e-05, Learning Rate: 0.000253\n",
      "Epoch 17240/40000, Loss: 4.186212390777655e-05, Learning Rate: 0.000253\n",
      "Epoch 17241/40000, Loss: 3.836225005215965e-05, Learning Rate: 0.000253\n",
      "Epoch 17242/40000, Loss: 6.329976167762652e-05, Learning Rate: 0.000253\n",
      "Epoch 17243/40000, Loss: 5.651013634633273e-05, Learning Rate: 0.000253\n",
      "Epoch 17244/40000, Loss: 5.6333650718443096e-05, Learning Rate: 0.000253\n",
      "Epoch 17245/40000, Loss: 1.502324994362425e-05, Learning Rate: 0.000253\n",
      "Epoch 17246/40000, Loss: 5.8064699260285124e-05, Learning Rate: 0.000252\n",
      "Epoch 17247/40000, Loss: 3.473345350357704e-05, Learning Rate: 0.000252\n",
      "Epoch 17248/40000, Loss: 3.451450538705103e-05, Learning Rate: 0.000252\n",
      "Epoch 17249/40000, Loss: 6.422418664442375e-05, Learning Rate: 0.000252\n",
      "Epoch 17250/40000, Loss: 5.931497071287595e-05, Learning Rate: 0.000252\n",
      "Epoch 17251/40000, Loss: 5.6519871577620506e-05, Learning Rate: 0.000252\n",
      "Epoch 17252/40000, Loss: 3.345133882248774e-05, Learning Rate: 0.000252\n",
      "Epoch 17253/40000, Loss: 1.5113174413272645e-05, Learning Rate: 0.000252\n",
      "Epoch 17254/40000, Loss: 6.329108873615041e-05, Learning Rate: 0.000252\n",
      "Epoch 17255/40000, Loss: 4.2188916268059984e-05, Learning Rate: 0.000252\n",
      "Epoch 17256/40000, Loss: 3.861988443532027e-05, Learning Rate: 0.000252\n",
      "Epoch 17257/40000, Loss: 4.311006341595203e-05, Learning Rate: 0.000252\n",
      "Epoch 17258/40000, Loss: 3.435262624407187e-05, Learning Rate: 0.000252\n",
      "Epoch 17259/40000, Loss: 4.3758231186075136e-05, Learning Rate: 0.000252\n",
      "Epoch 17260/40000, Loss: 4.237844404997304e-05, Learning Rate: 0.000252\n",
      "Epoch 17261/40000, Loss: 3.3809894375735894e-05, Learning Rate: 0.000252\n",
      "Epoch 17262/40000, Loss: 3.845978790195659e-05, Learning Rate: 0.000252\n",
      "Epoch 17263/40000, Loss: 1.530110057501588e-05, Learning Rate: 0.000252\n",
      "Epoch 17264/40000, Loss: 4.2432351619936526e-05, Learning Rate: 0.000252\n",
      "Epoch 17265/40000, Loss: 5.5765452998457476e-05, Learning Rate: 0.000252\n",
      "Epoch 17266/40000, Loss: 6.347159069264308e-05, Learning Rate: 0.000252\n",
      "Epoch 17267/40000, Loss: 3.8580092223128304e-05, Learning Rate: 0.000252\n",
      "Epoch 17268/40000, Loss: 6.341830885503441e-05, Learning Rate: 0.000252\n",
      "Epoch 17269/40000, Loss: 4.197933230898343e-05, Learning Rate: 0.000252\n",
      "Epoch 17270/40000, Loss: 5.564134335145354e-05, Learning Rate: 0.000252\n",
      "Epoch 17271/40000, Loss: 1.487947884015739e-05, Learning Rate: 0.000252\n",
      "Epoch 17272/40000, Loss: 3.335982182761654e-05, Learning Rate: 0.000252\n",
      "Epoch 17273/40000, Loss: 3.84002341888845e-05, Learning Rate: 0.000252\n",
      "Epoch 17274/40000, Loss: 5.552303991862573e-05, Learning Rate: 0.000252\n",
      "Epoch 17275/40000, Loss: 3.332996493554674e-05, Learning Rate: 0.000252\n",
      "Epoch 17276/40000, Loss: 6.344607390929013e-05, Learning Rate: 0.000252\n",
      "Epoch 17277/40000, Loss: 1.4759139958187006e-05, Learning Rate: 0.000252\n",
      "Epoch 17278/40000, Loss: 3.82336183974985e-05, Learning Rate: 0.000252\n",
      "Epoch 17279/40000, Loss: 5.538634650292806e-05, Learning Rate: 0.000251\n",
      "Epoch 17280/40000, Loss: 1.4690514944959432e-05, Learning Rate: 0.000251\n",
      "Epoch 17281/40000, Loss: 3.3260810596402735e-05, Learning Rate: 0.000251\n",
      "Epoch 17282/40000, Loss: 6.340941763482988e-05, Learning Rate: 0.000251\n",
      "Epoch 17283/40000, Loss: 5.546323518501595e-05, Learning Rate: 0.000251\n",
      "Epoch 17284/40000, Loss: 4.1619277908466756e-05, Learning Rate: 0.000251\n",
      "Epoch 17285/40000, Loss: 6.34091193205677e-05, Learning Rate: 0.000251\n",
      "Epoch 17286/40000, Loss: 5.588046769844368e-05, Learning Rate: 0.000251\n",
      "Epoch 17287/40000, Loss: 4.172168701188639e-05, Learning Rate: 0.000251\n",
      "Epoch 17288/40000, Loss: 5.5539716413477436e-05, Learning Rate: 0.000251\n",
      "Epoch 17289/40000, Loss: 3.32181625708472e-05, Learning Rate: 0.000251\n",
      "Epoch 17290/40000, Loss: 5.560008139582351e-05, Learning Rate: 0.000251\n",
      "Epoch 17291/40000, Loss: 4.182638076599687e-05, Learning Rate: 0.000251\n",
      "Epoch 17292/40000, Loss: 3.338655733386986e-05, Learning Rate: 0.000251\n",
      "Epoch 17293/40000, Loss: 3.3372391044395044e-05, Learning Rate: 0.000251\n",
      "Epoch 17294/40000, Loss: 4.212007115711458e-05, Learning Rate: 0.000251\n",
      "Epoch 17295/40000, Loss: 5.614994734060019e-05, Learning Rate: 0.000251\n",
      "Epoch 17296/40000, Loss: 6.376675446517766e-05, Learning Rate: 0.000251\n",
      "Epoch 17297/40000, Loss: 5.8422705478733405e-05, Learning Rate: 0.000251\n",
      "Epoch 17298/40000, Loss: 5.73093093407806e-05, Learning Rate: 0.000251\n",
      "Epoch 17299/40000, Loss: 6.376430246746168e-05, Learning Rate: 0.000251\n",
      "Epoch 17300/40000, Loss: 3.876640403177589e-05, Learning Rate: 0.000251\n",
      "Epoch 17301/40000, Loss: 4.317415732657537e-05, Learning Rate: 0.000251\n",
      "Epoch 17302/40000, Loss: 3.4536464227130637e-05, Learning Rate: 0.000251\n",
      "Epoch 17303/40000, Loss: 4.5298605982679874e-05, Learning Rate: 0.000251\n",
      "Epoch 17304/40000, Loss: 6.813573418185115e-05, Learning Rate: 0.000251\n",
      "Epoch 17305/40000, Loss: 4.516830085776746e-05, Learning Rate: 0.000251\n",
      "Epoch 17306/40000, Loss: 6.736503564752638e-05, Learning Rate: 0.000251\n",
      "Epoch 17307/40000, Loss: 5.670056998496875e-05, Learning Rate: 0.000251\n",
      "Epoch 17308/40000, Loss: 3.4470507671358064e-05, Learning Rate: 0.000251\n",
      "Epoch 17309/40000, Loss: 5.908773891860619e-05, Learning Rate: 0.000251\n",
      "Epoch 17310/40000, Loss: 3.940684473491274e-05, Learning Rate: 0.000251\n",
      "Epoch 17311/40000, Loss: 6.466468767030165e-05, Learning Rate: 0.000251\n",
      "Epoch 17312/40000, Loss: 4.375422577140853e-05, Learning Rate: 0.000250\n",
      "Epoch 17313/40000, Loss: 4.2840347305173054e-05, Learning Rate: 0.000250\n",
      "Epoch 17314/40000, Loss: 6.402776489267126e-05, Learning Rate: 0.000250\n",
      "Epoch 17315/40000, Loss: 3.866643237415701e-05, Learning Rate: 0.000250\n",
      "Epoch 17316/40000, Loss: 3.3676220482448116e-05, Learning Rate: 0.000250\n",
      "Epoch 17317/40000, Loss: 6.398099503712729e-05, Learning Rate: 0.000250\n",
      "Epoch 17318/40000, Loss: 3.87218278774526e-05, Learning Rate: 0.000250\n",
      "Epoch 17319/40000, Loss: 6.850765930721536e-05, Learning Rate: 0.000250\n",
      "Epoch 17320/40000, Loss: 3.877821291098371e-05, Learning Rate: 0.000250\n",
      "Epoch 17321/40000, Loss: 4.322799577494152e-05, Learning Rate: 0.000250\n",
      "Epoch 17322/40000, Loss: 7.133693725336343e-05, Learning Rate: 0.000250\n",
      "Epoch 17323/40000, Loss: 3.48251560353674e-05, Learning Rate: 0.000250\n",
      "Epoch 17324/40000, Loss: 1.6314183085341938e-05, Learning Rate: 0.000250\n",
      "Epoch 17325/40000, Loss: 3.469601142569445e-05, Learning Rate: 0.000250\n",
      "Epoch 17326/40000, Loss: 4.0040828025667e-05, Learning Rate: 0.000250\n",
      "Epoch 17327/40000, Loss: 6.518921145470813e-05, Learning Rate: 0.000250\n",
      "Epoch 17328/40000, Loss: 5.699077155441046e-05, Learning Rate: 0.000250\n",
      "Epoch 17329/40000, Loss: 3.437069244682789e-05, Learning Rate: 0.000250\n",
      "Epoch 17330/40000, Loss: 1.6218202290474437e-05, Learning Rate: 0.000250\n",
      "Epoch 17331/40000, Loss: 6.421939906431362e-05, Learning Rate: 0.000250\n",
      "Epoch 17332/40000, Loss: 5.853688344359398e-05, Learning Rate: 0.000250\n",
      "Epoch 17333/40000, Loss: 3.55140000465326e-05, Learning Rate: 0.000250\n",
      "Epoch 17334/40000, Loss: 6.480191223090515e-05, Learning Rate: 0.000250\n",
      "Epoch 17335/40000, Loss: 1.9312779841129668e-05, Learning Rate: 0.000250\n",
      "Epoch 17336/40000, Loss: 6.508061051135883e-05, Learning Rate: 0.000250\n",
      "Epoch 17337/40000, Loss: 2.147117629647255e-05, Learning Rate: 0.000250\n",
      "Epoch 17338/40000, Loss: 1.6463372958241962e-05, Learning Rate: 0.000250\n",
      "Epoch 17339/40000, Loss: 1.6340965885319747e-05, Learning Rate: 0.000250\n",
      "Epoch 17340/40000, Loss: 1.5889554560999386e-05, Learning Rate: 0.000250\n",
      "Epoch 17341/40000, Loss: 4.364231426734477e-05, Learning Rate: 0.000250\n",
      "Epoch 17342/40000, Loss: 4.264785093255341e-05, Learning Rate: 0.000250\n",
      "Epoch 17343/40000, Loss: 1.5179436559265014e-05, Learning Rate: 0.000250\n",
      "Epoch 17344/40000, Loss: 3.541745900292881e-05, Learning Rate: 0.000250\n",
      "Epoch 17345/40000, Loss: 6.389601912815124e-05, Learning Rate: 0.000250\n",
      "Epoch 17346/40000, Loss: 4.273376544006169e-05, Learning Rate: 0.000249\n",
      "Epoch 17347/40000, Loss: 4.272817386663519e-05, Learning Rate: 0.000249\n",
      "Epoch 17348/40000, Loss: 6.378938996931538e-05, Learning Rate: 0.000249\n",
      "Epoch 17349/40000, Loss: 5.5901491577969864e-05, Learning Rate: 0.000249\n",
      "Epoch 17350/40000, Loss: 3.885109617840499e-05, Learning Rate: 0.000249\n",
      "Epoch 17351/40000, Loss: 3.354146610945463e-05, Learning Rate: 0.000249\n",
      "Epoch 17352/40000, Loss: 4.276263643987477e-05, Learning Rate: 0.000249\n",
      "Epoch 17353/40000, Loss: 6.396049866452813e-05, Learning Rate: 0.000249\n",
      "Epoch 17354/40000, Loss: 6.360081897582859e-05, Learning Rate: 0.000249\n",
      "Epoch 17355/40000, Loss: 5.64054207643494e-05, Learning Rate: 0.000249\n",
      "Epoch 17356/40000, Loss: 4.2533862142590806e-05, Learning Rate: 0.000249\n",
      "Epoch 17357/40000, Loss: 1.4915760402800515e-05, Learning Rate: 0.000249\n",
      "Epoch 17358/40000, Loss: 3.8428719562944025e-05, Learning Rate: 0.000249\n",
      "Epoch 17359/40000, Loss: 5.668836092809215e-05, Learning Rate: 0.000249\n",
      "Epoch 17360/40000, Loss: 5.583786696661264e-05, Learning Rate: 0.000249\n",
      "Epoch 17361/40000, Loss: 3.3361458918079734e-05, Learning Rate: 0.000249\n",
      "Epoch 17362/40000, Loss: 3.832197035080753e-05, Learning Rate: 0.000249\n",
      "Epoch 17363/40000, Loss: 5.585825419984758e-05, Learning Rate: 0.000249\n",
      "Epoch 17364/40000, Loss: 4.182472184766084e-05, Learning Rate: 0.000249\n",
      "Epoch 17365/40000, Loss: 5.659912130795419e-05, Learning Rate: 0.000249\n",
      "Epoch 17366/40000, Loss: 4.1873536247294396e-05, Learning Rate: 0.000249\n",
      "Epoch 17367/40000, Loss: 5.860096280230209e-05, Learning Rate: 0.000249\n",
      "Epoch 17368/40000, Loss: 5.574847455136478e-05, Learning Rate: 0.000249\n",
      "Epoch 17369/40000, Loss: 4.2030536860693246e-05, Learning Rate: 0.000249\n",
      "Epoch 17370/40000, Loss: 5.5789147154428065e-05, Learning Rate: 0.000249\n",
      "Epoch 17371/40000, Loss: 6.422366277547553e-05, Learning Rate: 0.000249\n",
      "Epoch 17372/40000, Loss: 3.844771708827466e-05, Learning Rate: 0.000249\n",
      "Epoch 17373/40000, Loss: 3.858808122458868e-05, Learning Rate: 0.000249\n",
      "Epoch 17374/40000, Loss: 3.391720383660868e-05, Learning Rate: 0.000249\n",
      "Epoch 17375/40000, Loss: 3.8331032556016e-05, Learning Rate: 0.000249\n",
      "Epoch 17376/40000, Loss: 4.22384291596245e-05, Learning Rate: 0.000249\n",
      "Epoch 17377/40000, Loss: 6.475778354797512e-05, Learning Rate: 0.000249\n",
      "Epoch 17378/40000, Loss: 6.428763299481943e-05, Learning Rate: 0.000249\n",
      "Epoch 17379/40000, Loss: 1.533760405436624e-05, Learning Rate: 0.000248\n",
      "Epoch 17380/40000, Loss: 4.35134643339552e-05, Learning Rate: 0.000248\n",
      "Epoch 17381/40000, Loss: 6.416412361431867e-05, Learning Rate: 0.000248\n",
      "Epoch 17382/40000, Loss: 4.271979196346365e-05, Learning Rate: 0.000248\n",
      "Epoch 17383/40000, Loss: 1.579779745952692e-05, Learning Rate: 0.000248\n",
      "Epoch 17384/40000, Loss: 5.666404831572436e-05, Learning Rate: 0.000248\n",
      "Epoch 17385/40000, Loss: 3.348106474732049e-05, Learning Rate: 0.000248\n",
      "Epoch 17386/40000, Loss: 3.3224194339709356e-05, Learning Rate: 0.000248\n",
      "Epoch 17387/40000, Loss: 3.838850534521043e-05, Learning Rate: 0.000248\n",
      "Epoch 17388/40000, Loss: 3.3314612664980814e-05, Learning Rate: 0.000248\n",
      "Epoch 17389/40000, Loss: 3.318104427307844e-05, Learning Rate: 0.000248\n",
      "Epoch 17390/40000, Loss: 1.5095486560312565e-05, Learning Rate: 0.000248\n",
      "Epoch 17391/40000, Loss: 6.340081745292991e-05, Learning Rate: 0.000248\n",
      "Epoch 17392/40000, Loss: 3.852964800898917e-05, Learning Rate: 0.000248\n",
      "Epoch 17393/40000, Loss: 6.480056617874652e-05, Learning Rate: 0.000248\n",
      "Epoch 17394/40000, Loss: 6.39180580037646e-05, Learning Rate: 0.000248\n",
      "Epoch 17395/40000, Loss: 3.90540408261586e-05, Learning Rate: 0.000248\n",
      "Epoch 17396/40000, Loss: 4.26694423367735e-05, Learning Rate: 0.000248\n",
      "Epoch 17397/40000, Loss: 6.449057400459424e-05, Learning Rate: 0.000248\n",
      "Epoch 17398/40000, Loss: 3.967428710893728e-05, Learning Rate: 0.000248\n",
      "Epoch 17399/40000, Loss: 7.046722021186724e-05, Learning Rate: 0.000248\n",
      "Epoch 17400/40000, Loss: 1.713131132419221e-05, Learning Rate: 0.000248\n",
      "Epoch 17401/40000, Loss: 5.838846482220106e-05, Learning Rate: 0.000248\n",
      "Epoch 17402/40000, Loss: 4.683730730903335e-05, Learning Rate: 0.000248\n",
      "Epoch 17403/40000, Loss: 4.6068995288806036e-05, Learning Rate: 0.000248\n",
      "Epoch 17404/40000, Loss: 4.451614950085059e-05, Learning Rate: 0.000248\n",
      "Epoch 17405/40000, Loss: 6.606849638046697e-05, Learning Rate: 0.000248\n",
      "Epoch 17406/40000, Loss: 5.899853931623511e-05, Learning Rate: 0.000248\n",
      "Epoch 17407/40000, Loss: 1.8193935829913244e-05, Learning Rate: 0.000248\n",
      "Epoch 17408/40000, Loss: 3.469017246970907e-05, Learning Rate: 0.000248\n",
      "Epoch 17409/40000, Loss: 4.3697102228179574e-05, Learning Rate: 0.000248\n",
      "Epoch 17410/40000, Loss: 6.006130206515081e-05, Learning Rate: 0.000248\n",
      "Epoch 17411/40000, Loss: 6.552862032549456e-05, Learning Rate: 0.000248\n",
      "Epoch 17412/40000, Loss: 4.132487447350286e-05, Learning Rate: 0.000248\n",
      "Epoch 17413/40000, Loss: 1.736273043206893e-05, Learning Rate: 0.000247\n",
      "Epoch 17414/40000, Loss: 7.558480137959123e-05, Learning Rate: 0.000247\n",
      "Epoch 17415/40000, Loss: 5.834701732965186e-05, Learning Rate: 0.000247\n",
      "Epoch 17416/40000, Loss: 5.235450225882232e-05, Learning Rate: 0.000247\n",
      "Epoch 17417/40000, Loss: 4.21713302785065e-05, Learning Rate: 0.000247\n",
      "Epoch 17418/40000, Loss: 3.800571721512824e-05, Learning Rate: 0.000247\n",
      "Epoch 17419/40000, Loss: 4.144577178522013e-05, Learning Rate: 0.000247\n",
      "Epoch 17420/40000, Loss: 6.630083225900307e-05, Learning Rate: 0.000247\n",
      "Epoch 17421/40000, Loss: 2.0750239855260588e-05, Learning Rate: 0.000247\n",
      "Epoch 17422/40000, Loss: 4.995678682462312e-05, Learning Rate: 0.000247\n",
      "Epoch 17423/40000, Loss: 6.67886997689493e-05, Learning Rate: 0.000247\n",
      "Epoch 17424/40000, Loss: 4.443347643245943e-05, Learning Rate: 0.000247\n",
      "Epoch 17425/40000, Loss: 3.438108251430094e-05, Learning Rate: 0.000247\n",
      "Epoch 17426/40000, Loss: 5.2564773795893416e-05, Learning Rate: 0.000247\n",
      "Epoch 17427/40000, Loss: 3.91153444070369e-05, Learning Rate: 0.000247\n",
      "Epoch 17428/40000, Loss: 6.425500032491982e-05, Learning Rate: 0.000247\n",
      "Epoch 17429/40000, Loss: 3.9149912481661886e-05, Learning Rate: 0.000247\n",
      "Epoch 17430/40000, Loss: 3.4201224480057135e-05, Learning Rate: 0.000247\n",
      "Epoch 17431/40000, Loss: 3.96306422771886e-05, Learning Rate: 0.000247\n",
      "Epoch 17432/40000, Loss: 1.5152421838138252e-05, Learning Rate: 0.000247\n",
      "Epoch 17433/40000, Loss: 5.609738582279533e-05, Learning Rate: 0.000247\n",
      "Epoch 17434/40000, Loss: 3.867676787194796e-05, Learning Rate: 0.000247\n",
      "Epoch 17435/40000, Loss: 6.338409730233252e-05, Learning Rate: 0.000247\n",
      "Epoch 17436/40000, Loss: 5.75787817069795e-05, Learning Rate: 0.000247\n",
      "Epoch 17437/40000, Loss: 3.417594052734785e-05, Learning Rate: 0.000247\n",
      "Epoch 17438/40000, Loss: 3.359399488545023e-05, Learning Rate: 0.000247\n",
      "Epoch 17439/40000, Loss: 4.082590021425858e-05, Learning Rate: 0.000247\n",
      "Epoch 17440/40000, Loss: 3.513528281473555e-05, Learning Rate: 0.000247\n",
      "Epoch 17441/40000, Loss: 6.432334339478984e-05, Learning Rate: 0.000247\n",
      "Epoch 17442/40000, Loss: 6.391524220816791e-05, Learning Rate: 0.000247\n",
      "Epoch 17443/40000, Loss: 5.616762791760266e-05, Learning Rate: 0.000247\n",
      "Epoch 17444/40000, Loss: 3.869373176712543e-05, Learning Rate: 0.000247\n",
      "Epoch 17445/40000, Loss: 5.720785338780843e-05, Learning Rate: 0.000247\n",
      "Epoch 17446/40000, Loss: 4.0436290873913094e-05, Learning Rate: 0.000246\n",
      "Epoch 17447/40000, Loss: 5.959227564744651e-05, Learning Rate: 0.000246\n",
      "Epoch 17448/40000, Loss: 6.46262924419716e-05, Learning Rate: 0.000246\n",
      "Epoch 17449/40000, Loss: 4.0081133192870766e-05, Learning Rate: 0.000246\n",
      "Epoch 17450/40000, Loss: 5.7602403103373945e-05, Learning Rate: 0.000246\n",
      "Epoch 17451/40000, Loss: 6.441212462959811e-05, Learning Rate: 0.000246\n",
      "Epoch 17452/40000, Loss: 1.5737170542706735e-05, Learning Rate: 0.000246\n",
      "Epoch 17453/40000, Loss: 4.345779962022789e-05, Learning Rate: 0.000246\n",
      "Epoch 17454/40000, Loss: 3.859463322442025e-05, Learning Rate: 0.000246\n",
      "Epoch 17455/40000, Loss: 5.626693382509984e-05, Learning Rate: 0.000246\n",
      "Epoch 17456/40000, Loss: 6.355845835059881e-05, Learning Rate: 0.000246\n",
      "Epoch 17457/40000, Loss: 3.850200664601289e-05, Learning Rate: 0.000246\n",
      "Epoch 17458/40000, Loss: 3.370233389432542e-05, Learning Rate: 0.000246\n",
      "Epoch 17459/40000, Loss: 4.232541687088087e-05, Learning Rate: 0.000246\n",
      "Epoch 17460/40000, Loss: 3.411078068893403e-05, Learning Rate: 0.000246\n",
      "Epoch 17461/40000, Loss: 3.8857640902278945e-05, Learning Rate: 0.000246\n",
      "Epoch 17462/40000, Loss: 5.6152872275561094e-05, Learning Rate: 0.000246\n",
      "Epoch 17463/40000, Loss: 1.5490673831664026e-05, Learning Rate: 0.000246\n",
      "Epoch 17464/40000, Loss: 5.899039751966484e-05, Learning Rate: 0.000246\n",
      "Epoch 17465/40000, Loss: 6.410144851543009e-05, Learning Rate: 0.000246\n",
      "Epoch 17466/40000, Loss: 4.20451833633706e-05, Learning Rate: 0.000246\n",
      "Epoch 17467/40000, Loss: 1.502472150605172e-05, Learning Rate: 0.000246\n",
      "Epoch 17468/40000, Loss: 6.324881542241201e-05, Learning Rate: 0.000246\n",
      "Epoch 17469/40000, Loss: 6.312569894362241e-05, Learning Rate: 0.000246\n",
      "Epoch 17470/40000, Loss: 3.825293242698535e-05, Learning Rate: 0.000246\n",
      "Epoch 17471/40000, Loss: 6.356311496347189e-05, Learning Rate: 0.000246\n",
      "Epoch 17472/40000, Loss: 3.840698263957165e-05, Learning Rate: 0.000246\n",
      "Epoch 17473/40000, Loss: 3.3244687074329704e-05, Learning Rate: 0.000246\n",
      "Epoch 17474/40000, Loss: 5.561491707339883e-05, Learning Rate: 0.000246\n",
      "Epoch 17475/40000, Loss: 3.366836972418241e-05, Learning Rate: 0.000246\n",
      "Epoch 17476/40000, Loss: 3.828468106803484e-05, Learning Rate: 0.000246\n",
      "Epoch 17477/40000, Loss: 6.334632053039968e-05, Learning Rate: 0.000246\n",
      "Epoch 17478/40000, Loss: 3.340672992635518e-05, Learning Rate: 0.000246\n",
      "Epoch 17479/40000, Loss: 4.172843546257354e-05, Learning Rate: 0.000246\n",
      "Epoch 17480/40000, Loss: 4.1815670556388795e-05, Learning Rate: 0.000245\n",
      "Epoch 17481/40000, Loss: 3.8433485315181315e-05, Learning Rate: 0.000245\n",
      "Epoch 17482/40000, Loss: 3.332659980515018e-05, Learning Rate: 0.000245\n",
      "Epoch 17483/40000, Loss: 6.319306703517213e-05, Learning Rate: 0.000245\n",
      "Epoch 17484/40000, Loss: 3.8339567254297435e-05, Learning Rate: 0.000245\n",
      "Epoch 17485/40000, Loss: 5.5549786338815466e-05, Learning Rate: 0.000245\n",
      "Epoch 17486/40000, Loss: 4.2395713535370305e-05, Learning Rate: 0.000245\n",
      "Epoch 17487/40000, Loss: 3.342944182804786e-05, Learning Rate: 0.000245\n",
      "Epoch 17488/40000, Loss: 5.596897244686261e-05, Learning Rate: 0.000245\n",
      "Epoch 17489/40000, Loss: 6.453532841987908e-05, Learning Rate: 0.000245\n",
      "Epoch 17490/40000, Loss: 5.5861724831629544e-05, Learning Rate: 0.000245\n",
      "Epoch 17491/40000, Loss: 1.4901638678566087e-05, Learning Rate: 0.000245\n",
      "Epoch 17492/40000, Loss: 6.471260712714866e-05, Learning Rate: 0.000245\n",
      "Epoch 17493/40000, Loss: 5.630359737551771e-05, Learning Rate: 0.000245\n",
      "Epoch 17494/40000, Loss: 6.808277976233512e-05, Learning Rate: 0.000245\n",
      "Epoch 17495/40000, Loss: 6.598269828828052e-05, Learning Rate: 0.000245\n",
      "Epoch 17496/40000, Loss: 3.374530569999479e-05, Learning Rate: 0.000245\n",
      "Epoch 17497/40000, Loss: 3.3508425985928625e-05, Learning Rate: 0.000245\n",
      "Epoch 17498/40000, Loss: 4.1961702663684264e-05, Learning Rate: 0.000245\n",
      "Epoch 17499/40000, Loss: 5.633878754451871e-05, Learning Rate: 0.000245\n",
      "Epoch 17500/40000, Loss: 3.9326088881352916e-05, Learning Rate: 0.000245\n",
      "Epoch 17501/40000, Loss: 3.424180977162905e-05, Learning Rate: 0.000245\n",
      "Epoch 17502/40000, Loss: 5.658930240315385e-05, Learning Rate: 0.000245\n",
      "Epoch 17503/40000, Loss: 5.6018237955868244e-05, Learning Rate: 0.000245\n",
      "Epoch 17504/40000, Loss: 5.618079740088433e-05, Learning Rate: 0.000245\n",
      "Epoch 17505/40000, Loss: 1.548010368424002e-05, Learning Rate: 0.000245\n",
      "Epoch 17506/40000, Loss: 5.6650154874660075e-05, Learning Rate: 0.000245\n",
      "Epoch 17507/40000, Loss: 6.798830145271495e-05, Learning Rate: 0.000245\n",
      "Epoch 17508/40000, Loss: 3.369594196556136e-05, Learning Rate: 0.000245\n",
      "Epoch 17509/40000, Loss: 1.6406356735387817e-05, Learning Rate: 0.000245\n",
      "Epoch 17510/40000, Loss: 6.384035805240273e-05, Learning Rate: 0.000245\n",
      "Epoch 17511/40000, Loss: 4.350162998889573e-05, Learning Rate: 0.000245\n",
      "Epoch 17512/40000, Loss: 6.60377336316742e-05, Learning Rate: 0.000245\n",
      "Epoch 17513/40000, Loss: 4.396430085762404e-05, Learning Rate: 0.000245\n",
      "Epoch 17514/40000, Loss: 3.511644899845123e-05, Learning Rate: 0.000244\n",
      "Epoch 17515/40000, Loss: 3.3841013646451756e-05, Learning Rate: 0.000244\n",
      "Epoch 17516/40000, Loss: 4.378882294986397e-05, Learning Rate: 0.000244\n",
      "Epoch 17517/40000, Loss: 6.474901601905003e-05, Learning Rate: 0.000244\n",
      "Epoch 17518/40000, Loss: 6.38532656012103e-05, Learning Rate: 0.000244\n",
      "Epoch 17519/40000, Loss: 4.232164428685792e-05, Learning Rate: 0.000244\n",
      "Epoch 17520/40000, Loss: 6.351653428282589e-05, Learning Rate: 0.000244\n",
      "Epoch 17521/40000, Loss: 5.59500913368538e-05, Learning Rate: 0.000244\n",
      "Epoch 17522/40000, Loss: 4.233388972352259e-05, Learning Rate: 0.000244\n",
      "Epoch 17523/40000, Loss: 3.354635555297136e-05, Learning Rate: 0.000244\n",
      "Epoch 17524/40000, Loss: 3.8358481106115505e-05, Learning Rate: 0.000244\n",
      "Epoch 17525/40000, Loss: 6.399590347427875e-05, Learning Rate: 0.000244\n",
      "Epoch 17526/40000, Loss: 4.2405157728353515e-05, Learning Rate: 0.000244\n",
      "Epoch 17527/40000, Loss: 4.233537401887588e-05, Learning Rate: 0.000244\n",
      "Epoch 17528/40000, Loss: 6.377236422849819e-05, Learning Rate: 0.000244\n",
      "Epoch 17529/40000, Loss: 3.826430474873632e-05, Learning Rate: 0.000244\n",
      "Epoch 17530/40000, Loss: 3.323790951981209e-05, Learning Rate: 0.000244\n",
      "Epoch 17531/40000, Loss: 6.355164805427194e-05, Learning Rate: 0.000244\n",
      "Epoch 17532/40000, Loss: 4.229795013088733e-05, Learning Rate: 0.000244\n",
      "Epoch 17533/40000, Loss: 5.576658804784529e-05, Learning Rate: 0.000244\n",
      "Epoch 17534/40000, Loss: 4.206477024126798e-05, Learning Rate: 0.000244\n",
      "Epoch 17535/40000, Loss: 5.577165575232357e-05, Learning Rate: 0.000244\n",
      "Epoch 17536/40000, Loss: 6.32257288089022e-05, Learning Rate: 0.000244\n",
      "Epoch 17537/40000, Loss: 5.600377335213125e-05, Learning Rate: 0.000244\n",
      "Epoch 17538/40000, Loss: 4.1859515476971865e-05, Learning Rate: 0.000244\n",
      "Epoch 17539/40000, Loss: 3.8631438656011596e-05, Learning Rate: 0.000244\n",
      "Epoch 17540/40000, Loss: 5.5646181863266975e-05, Learning Rate: 0.000244\n",
      "Epoch 17541/40000, Loss: 3.341317642480135e-05, Learning Rate: 0.000244\n",
      "Epoch 17542/40000, Loss: 5.590653381659649e-05, Learning Rate: 0.000244\n",
      "Epoch 17543/40000, Loss: 3.813104922301136e-05, Learning Rate: 0.000244\n",
      "Epoch 17544/40000, Loss: 4.186344085610472e-05, Learning Rate: 0.000244\n",
      "Epoch 17545/40000, Loss: 4.1968331061070785e-05, Learning Rate: 0.000244\n",
      "Epoch 17546/40000, Loss: 3.8177498936420307e-05, Learning Rate: 0.000244\n",
      "Epoch 17547/40000, Loss: 6.314161146292463e-05, Learning Rate: 0.000244\n",
      "Epoch 17548/40000, Loss: 6.331938493531197e-05, Learning Rate: 0.000244\n",
      "Epoch 17549/40000, Loss: 6.331504846457392e-05, Learning Rate: 0.000243\n",
      "Epoch 17550/40000, Loss: 3.327662852825597e-05, Learning Rate: 0.000243\n",
      "Epoch 17551/40000, Loss: 4.2053117795148864e-05, Learning Rate: 0.000243\n",
      "Epoch 17552/40000, Loss: 1.5128556697163731e-05, Learning Rate: 0.000243\n",
      "Epoch 17553/40000, Loss: 5.5601874919375405e-05, Learning Rate: 0.000243\n",
      "Epoch 17554/40000, Loss: 5.5407159379683435e-05, Learning Rate: 0.000243\n",
      "Epoch 17555/40000, Loss: 3.8054942706367e-05, Learning Rate: 0.000243\n",
      "Epoch 17556/40000, Loss: 1.4810755601502024e-05, Learning Rate: 0.000243\n",
      "Epoch 17557/40000, Loss: 4.179232564638369e-05, Learning Rate: 0.000243\n",
      "Epoch 17558/40000, Loss: 3.3282001822954044e-05, Learning Rate: 0.000243\n",
      "Epoch 17559/40000, Loss: 6.35774340480566e-05, Learning Rate: 0.000243\n",
      "Epoch 17560/40000, Loss: 6.372426287271082e-05, Learning Rate: 0.000243\n",
      "Epoch 17561/40000, Loss: 4.2741776269394904e-05, Learning Rate: 0.000243\n",
      "Epoch 17562/40000, Loss: 5.659328598994762e-05, Learning Rate: 0.000243\n",
      "Epoch 17563/40000, Loss: 3.381965507287532e-05, Learning Rate: 0.000243\n",
      "Epoch 17564/40000, Loss: 3.372649007360451e-05, Learning Rate: 0.000243\n",
      "Epoch 17565/40000, Loss: 3.903414835804142e-05, Learning Rate: 0.000243\n",
      "Epoch 17566/40000, Loss: 3.4067590604536235e-05, Learning Rate: 0.000243\n",
      "Epoch 17567/40000, Loss: 4.026311944471672e-05, Learning Rate: 0.000243\n",
      "Epoch 17568/40000, Loss: 1.5241551409417298e-05, Learning Rate: 0.000243\n",
      "Epoch 17569/40000, Loss: 4.258201806806028e-05, Learning Rate: 0.000243\n",
      "Epoch 17570/40000, Loss: 6.400483835022897e-05, Learning Rate: 0.000243\n",
      "Epoch 17571/40000, Loss: 3.457341517787427e-05, Learning Rate: 0.000243\n",
      "Epoch 17572/40000, Loss: 3.376872336957604e-05, Learning Rate: 0.000243\n",
      "Epoch 17573/40000, Loss: 3.380540510988794e-05, Learning Rate: 0.000243\n",
      "Epoch 17574/40000, Loss: 3.343170101288706e-05, Learning Rate: 0.000243\n",
      "Epoch 17575/40000, Loss: 3.848264896078035e-05, Learning Rate: 0.000243\n",
      "Epoch 17576/40000, Loss: 3.3508389606140554e-05, Learning Rate: 0.000243\n",
      "Epoch 17577/40000, Loss: 3.79499324481003e-05, Learning Rate: 0.000243\n",
      "Epoch 17578/40000, Loss: 3.449368159635924e-05, Learning Rate: 0.000243\n",
      "Epoch 17579/40000, Loss: 3.412346268305555e-05, Learning Rate: 0.000243\n",
      "Epoch 17580/40000, Loss: 6.371525523718446e-05, Learning Rate: 0.000243\n",
      "Epoch 17581/40000, Loss: 5.65186383028049e-05, Learning Rate: 0.000243\n",
      "Epoch 17582/40000, Loss: 3.405139432288706e-05, Learning Rate: 0.000243\n",
      "Epoch 17583/40000, Loss: 4.2606465285643935e-05, Learning Rate: 0.000242\n",
      "Epoch 17584/40000, Loss: 3.872716843034141e-05, Learning Rate: 0.000242\n",
      "Epoch 17585/40000, Loss: 3.856271359836683e-05, Learning Rate: 0.000242\n",
      "Epoch 17586/40000, Loss: 6.41873775748536e-05, Learning Rate: 0.000242\n",
      "Epoch 17587/40000, Loss: 1.5591635019518435e-05, Learning Rate: 0.000242\n",
      "Epoch 17588/40000, Loss: 3.366047894814983e-05, Learning Rate: 0.000242\n",
      "Epoch 17589/40000, Loss: 3.80675119231455e-05, Learning Rate: 0.000242\n",
      "Epoch 17590/40000, Loss: 3.797313183895312e-05, Learning Rate: 0.000242\n",
      "Epoch 17591/40000, Loss: 3.3422751585021615e-05, Learning Rate: 0.000242\n",
      "Epoch 17592/40000, Loss: 3.32918789354153e-05, Learning Rate: 0.000242\n",
      "Epoch 17593/40000, Loss: 3.843346348730847e-05, Learning Rate: 0.000242\n",
      "Epoch 17594/40000, Loss: 5.6499869970139116e-05, Learning Rate: 0.000242\n",
      "Epoch 17595/40000, Loss: 4.218700996716507e-05, Learning Rate: 0.000242\n",
      "Epoch 17596/40000, Loss: 6.360401312122121e-05, Learning Rate: 0.000242\n",
      "Epoch 17597/40000, Loss: 3.3963628084165975e-05, Learning Rate: 0.000242\n",
      "Epoch 17598/40000, Loss: 4.312521923566237e-05, Learning Rate: 0.000242\n",
      "Epoch 17599/40000, Loss: 4.2083775042556226e-05, Learning Rate: 0.000242\n",
      "Epoch 17600/40000, Loss: 5.731310011469759e-05, Learning Rate: 0.000242\n",
      "Epoch 17601/40000, Loss: 4.3739080865634605e-05, Learning Rate: 0.000242\n",
      "Epoch 17602/40000, Loss: 6.442404264817014e-05, Learning Rate: 0.000242\n",
      "Epoch 17603/40000, Loss: 3.915154229616746e-05, Learning Rate: 0.000242\n",
      "Epoch 17604/40000, Loss: 3.435523831285536e-05, Learning Rate: 0.000242\n",
      "Epoch 17605/40000, Loss: 4.503072705119848e-05, Learning Rate: 0.000242\n",
      "Epoch 17606/40000, Loss: 3.504191408865154e-05, Learning Rate: 0.000242\n",
      "Epoch 17607/40000, Loss: 1.6724157831049524e-05, Learning Rate: 0.000242\n",
      "Epoch 17608/40000, Loss: 3.9154867408797145e-05, Learning Rate: 0.000242\n",
      "Epoch 17609/40000, Loss: 4.5991855586180463e-05, Learning Rate: 0.000242\n",
      "Epoch 17610/40000, Loss: 7.2640432335902e-05, Learning Rate: 0.000242\n",
      "Epoch 17611/40000, Loss: 1.72636064235121e-05, Learning Rate: 0.000242\n",
      "Epoch 17612/40000, Loss: 3.6989007639931515e-05, Learning Rate: 0.000242\n",
      "Epoch 17613/40000, Loss: 7.408872625092044e-05, Learning Rate: 0.000242\n",
      "Epoch 17614/40000, Loss: 4.093006646144204e-05, Learning Rate: 0.000242\n",
      "Epoch 17615/40000, Loss: 4.401130718179047e-05, Learning Rate: 0.000242\n",
      "Epoch 17616/40000, Loss: 6.9695663114544e-05, Learning Rate: 0.000242\n",
      "Epoch 17617/40000, Loss: 3.839573764707893e-05, Learning Rate: 0.000241\n",
      "Epoch 17618/40000, Loss: 6.56168776913546e-05, Learning Rate: 0.000241\n",
      "Epoch 17619/40000, Loss: 4.5350043365033343e-05, Learning Rate: 0.000241\n",
      "Epoch 17620/40000, Loss: 7.431479752995074e-05, Learning Rate: 0.000241\n",
      "Epoch 17621/40000, Loss: 5.88316579523962e-05, Learning Rate: 0.000241\n",
      "Epoch 17622/40000, Loss: 5.3328498324844986e-05, Learning Rate: 0.000241\n",
      "Epoch 17623/40000, Loss: 6.36657714494504e-05, Learning Rate: 0.000241\n",
      "Epoch 17624/40000, Loss: 6.537080480484292e-05, Learning Rate: 0.000241\n",
      "Epoch 17625/40000, Loss: 6.971226684981957e-05, Learning Rate: 0.000241\n",
      "Epoch 17626/40000, Loss: 1.7907241272041574e-05, Learning Rate: 0.000241\n",
      "Epoch 17627/40000, Loss: 4.021597123937681e-05, Learning Rate: 0.000241\n",
      "Epoch 17628/40000, Loss: 6.175720773171633e-05, Learning Rate: 0.000241\n",
      "Epoch 17629/40000, Loss: 4.6889759687474e-05, Learning Rate: 0.000241\n",
      "Epoch 17630/40000, Loss: 6.620990461669862e-05, Learning Rate: 0.000241\n",
      "Epoch 17631/40000, Loss: 1.6472038623760454e-05, Learning Rate: 0.000241\n",
      "Epoch 17632/40000, Loss: 1.8354498024564236e-05, Learning Rate: 0.000241\n",
      "Epoch 17633/40000, Loss: 3.7600821087835357e-05, Learning Rate: 0.000241\n",
      "Epoch 17634/40000, Loss: 6.009356729919091e-05, Learning Rate: 0.000241\n",
      "Epoch 17635/40000, Loss: 4.062759762746282e-05, Learning Rate: 0.000241\n",
      "Epoch 17636/40000, Loss: 5.852308458997868e-05, Learning Rate: 0.000241\n",
      "Epoch 17637/40000, Loss: 6.642713560722768e-05, Learning Rate: 0.000241\n",
      "Epoch 17638/40000, Loss: 1.6646896256133914e-05, Learning Rate: 0.000241\n",
      "Epoch 17639/40000, Loss: 3.427683259360492e-05, Learning Rate: 0.000241\n",
      "Epoch 17640/40000, Loss: 5.7203891628887504e-05, Learning Rate: 0.000241\n",
      "Epoch 17641/40000, Loss: 3.852526788250543e-05, Learning Rate: 0.000241\n",
      "Epoch 17642/40000, Loss: 6.420392310246825e-05, Learning Rate: 0.000241\n",
      "Epoch 17643/40000, Loss: 1.5622768842149526e-05, Learning Rate: 0.000241\n",
      "Epoch 17644/40000, Loss: 6.596323510166258e-05, Learning Rate: 0.000241\n",
      "Epoch 17645/40000, Loss: 3.9397618820657954e-05, Learning Rate: 0.000241\n",
      "Epoch 17646/40000, Loss: 5.620545198325999e-05, Learning Rate: 0.000241\n",
      "Epoch 17647/40000, Loss: 3.3394568163203076e-05, Learning Rate: 0.000241\n",
      "Epoch 17648/40000, Loss: 3.8740807212889194e-05, Learning Rate: 0.000241\n",
      "Epoch 17649/40000, Loss: 5.6512079027015716e-05, Learning Rate: 0.000241\n",
      "Epoch 17650/40000, Loss: 1.4998398000898305e-05, Learning Rate: 0.000241\n",
      "Epoch 17651/40000, Loss: 3.8775582652306184e-05, Learning Rate: 0.000241\n",
      "Epoch 17652/40000, Loss: 4.211076520732604e-05, Learning Rate: 0.000240\n",
      "Epoch 17653/40000, Loss: 4.0975264710141346e-05, Learning Rate: 0.000240\n",
      "Epoch 17654/40000, Loss: 5.580533616011962e-05, Learning Rate: 0.000240\n",
      "Epoch 17655/40000, Loss: 6.310155731625855e-05, Learning Rate: 0.000240\n",
      "Epoch 17656/40000, Loss: 6.272720929700881e-05, Learning Rate: 0.000240\n",
      "Epoch 17657/40000, Loss: 5.566222171182744e-05, Learning Rate: 0.000240\n",
      "Epoch 17658/40000, Loss: 4.1881707147695124e-05, Learning Rate: 0.000240\n",
      "Epoch 17659/40000, Loss: 3.827321415883489e-05, Learning Rate: 0.000240\n",
      "Epoch 17660/40000, Loss: 3.787923924392089e-05, Learning Rate: 0.000240\n",
      "Epoch 17661/40000, Loss: 6.275768100749701e-05, Learning Rate: 0.000240\n",
      "Epoch 17662/40000, Loss: 6.267933349590749e-05, Learning Rate: 0.000240\n",
      "Epoch 17663/40000, Loss: 4.172767876298167e-05, Learning Rate: 0.000240\n",
      "Epoch 17664/40000, Loss: 6.256326014408842e-05, Learning Rate: 0.000240\n",
      "Epoch 17665/40000, Loss: 3.769785325857811e-05, Learning Rate: 0.000240\n",
      "Epoch 17666/40000, Loss: 3.2993117201840505e-05, Learning Rate: 0.000240\n",
      "Epoch 17667/40000, Loss: 5.511931158252992e-05, Learning Rate: 0.000240\n",
      "Epoch 17668/40000, Loss: 3.7681631511077285e-05, Learning Rate: 0.000240\n",
      "Epoch 17669/40000, Loss: 4.146861829212867e-05, Learning Rate: 0.000240\n",
      "Epoch 17670/40000, Loss: 3.772006675717421e-05, Learning Rate: 0.000240\n",
      "Epoch 17671/40000, Loss: 3.75784256902989e-05, Learning Rate: 0.000240\n",
      "Epoch 17672/40000, Loss: 3.2862673833733425e-05, Learning Rate: 0.000240\n",
      "Epoch 17673/40000, Loss: 5.518299440154806e-05, Learning Rate: 0.000240\n",
      "Epoch 17674/40000, Loss: 6.339154788292944e-05, Learning Rate: 0.000240\n",
      "Epoch 17675/40000, Loss: 4.170524698565714e-05, Learning Rate: 0.000240\n",
      "Epoch 17676/40000, Loss: 6.462397868745029e-05, Learning Rate: 0.000240\n",
      "Epoch 17677/40000, Loss: 6.34934040135704e-05, Learning Rate: 0.000240\n",
      "Epoch 17678/40000, Loss: 5.5165197409223765e-05, Learning Rate: 0.000240\n",
      "Epoch 17679/40000, Loss: 5.5104217608459294e-05, Learning Rate: 0.000240\n",
      "Epoch 17680/40000, Loss: 4.143768092035316e-05, Learning Rate: 0.000240\n",
      "Epoch 17681/40000, Loss: 6.281521200435236e-05, Learning Rate: 0.000240\n",
      "Epoch 17682/40000, Loss: 6.27450062893331e-05, Learning Rate: 0.000240\n",
      "Epoch 17683/40000, Loss: 1.4643684153270442e-05, Learning Rate: 0.000240\n",
      "Epoch 17684/40000, Loss: 3.7583184166578576e-05, Learning Rate: 0.000240\n",
      "Epoch 17685/40000, Loss: 3.290338281658478e-05, Learning Rate: 0.000240\n",
      "Epoch 17686/40000, Loss: 1.4505267245112918e-05, Learning Rate: 0.000240\n",
      "Epoch 17687/40000, Loss: 1.4555331290466711e-05, Learning Rate: 0.000239\n",
      "Epoch 17688/40000, Loss: 5.5206852266564965e-05, Learning Rate: 0.000239\n",
      "Epoch 17689/40000, Loss: 5.531018177862279e-05, Learning Rate: 0.000239\n",
      "Epoch 17690/40000, Loss: 3.7598434573737904e-05, Learning Rate: 0.000239\n",
      "Epoch 17691/40000, Loss: 1.4509865650325082e-05, Learning Rate: 0.000239\n",
      "Epoch 17692/40000, Loss: 5.525352753465995e-05, Learning Rate: 0.000239\n",
      "Epoch 17693/40000, Loss: 3.759638639166951e-05, Learning Rate: 0.000239\n",
      "Epoch 17694/40000, Loss: 3.289376763859764e-05, Learning Rate: 0.000239\n",
      "Epoch 17695/40000, Loss: 3.754684439627454e-05, Learning Rate: 0.000239\n",
      "Epoch 17696/40000, Loss: 6.25977772870101e-05, Learning Rate: 0.000239\n",
      "Epoch 17697/40000, Loss: 3.773870048462413e-05, Learning Rate: 0.000239\n",
      "Epoch 17698/40000, Loss: 5.511118069989607e-05, Learning Rate: 0.000239\n",
      "Epoch 17699/40000, Loss: 1.4572380678146146e-05, Learning Rate: 0.000239\n",
      "Epoch 17700/40000, Loss: 3.760031540878117e-05, Learning Rate: 0.000239\n",
      "Epoch 17701/40000, Loss: 3.758222737815231e-05, Learning Rate: 0.000239\n",
      "Epoch 17702/40000, Loss: 3.749374081962742e-05, Learning Rate: 0.000239\n",
      "Epoch 17703/40000, Loss: 3.287609069957398e-05, Learning Rate: 0.000239\n",
      "Epoch 17704/40000, Loss: 3.28709720633924e-05, Learning Rate: 0.000239\n",
      "Epoch 17705/40000, Loss: 4.139129669056274e-05, Learning Rate: 0.000239\n",
      "Epoch 17706/40000, Loss: 6.254062463995069e-05, Learning Rate: 0.000239\n",
      "Epoch 17707/40000, Loss: 1.4655777704319917e-05, Learning Rate: 0.000239\n",
      "Epoch 17708/40000, Loss: 4.1405211959499866e-05, Learning Rate: 0.000239\n",
      "Epoch 17709/40000, Loss: 4.1376719309482723e-05, Learning Rate: 0.000239\n",
      "Epoch 17710/40000, Loss: 3.291550456197001e-05, Learning Rate: 0.000239\n",
      "Epoch 17711/40000, Loss: 3.75465169781819e-05, Learning Rate: 0.000239\n",
      "Epoch 17712/40000, Loss: 3.755652505788021e-05, Learning Rate: 0.000239\n",
      "Epoch 17713/40000, Loss: 1.4739845028088894e-05, Learning Rate: 0.000239\n",
      "Epoch 17714/40000, Loss: 1.477557816542685e-05, Learning Rate: 0.000239\n",
      "Epoch 17715/40000, Loss: 3.766058216569945e-05, Learning Rate: 0.000239\n",
      "Epoch 17716/40000, Loss: 3.749214738490991e-05, Learning Rate: 0.000239\n",
      "Epoch 17717/40000, Loss: 3.755458601517603e-05, Learning Rate: 0.000239\n",
      "Epoch 17718/40000, Loss: 6.251073500607163e-05, Learning Rate: 0.000239\n",
      "Epoch 17719/40000, Loss: 3.752821430680342e-05, Learning Rate: 0.000239\n",
      "Epoch 17720/40000, Loss: 4.170087049715221e-05, Learning Rate: 0.000239\n",
      "Epoch 17721/40000, Loss: 3.30118382407818e-05, Learning Rate: 0.000238\n",
      "Epoch 17722/40000, Loss: 3.2825271773617715e-05, Learning Rate: 0.000238\n",
      "Epoch 17723/40000, Loss: 3.7711415643570945e-05, Learning Rate: 0.000238\n",
      "Epoch 17724/40000, Loss: 1.4754721632925794e-05, Learning Rate: 0.000238\n",
      "Epoch 17725/40000, Loss: 5.512228744919412e-05, Learning Rate: 0.000238\n",
      "Epoch 17726/40000, Loss: 4.155205897404812e-05, Learning Rate: 0.000238\n",
      "Epoch 17727/40000, Loss: 3.285752245574258e-05, Learning Rate: 0.000238\n",
      "Epoch 17728/40000, Loss: 1.479087040934246e-05, Learning Rate: 0.000238\n",
      "Epoch 17729/40000, Loss: 3.7521054764511064e-05, Learning Rate: 0.000238\n",
      "Epoch 17730/40000, Loss: 1.4820339856669307e-05, Learning Rate: 0.000238\n",
      "Epoch 17731/40000, Loss: 6.254398613236845e-05, Learning Rate: 0.000238\n",
      "Epoch 17732/40000, Loss: 3.295598071417771e-05, Learning Rate: 0.000238\n",
      "Epoch 17733/40000, Loss: 3.290293534519151e-05, Learning Rate: 0.000238\n",
      "Epoch 17734/40000, Loss: 3.7753194192191586e-05, Learning Rate: 0.000238\n",
      "Epoch 17735/40000, Loss: 5.5077998695196584e-05, Learning Rate: 0.000238\n",
      "Epoch 17736/40000, Loss: 3.298120282124728e-05, Learning Rate: 0.000238\n",
      "Epoch 17737/40000, Loss: 6.26083419774659e-05, Learning Rate: 0.000238\n",
      "Epoch 17738/40000, Loss: 6.260596273932606e-05, Learning Rate: 0.000238\n",
      "Epoch 17739/40000, Loss: 4.183925921097398e-05, Learning Rate: 0.000238\n",
      "Epoch 17740/40000, Loss: 3.323559576529078e-05, Learning Rate: 0.000238\n",
      "Epoch 17741/40000, Loss: 6.269699224503711e-05, Learning Rate: 0.000238\n",
      "Epoch 17742/40000, Loss: 6.272023892961442e-05, Learning Rate: 0.000238\n",
      "Epoch 17743/40000, Loss: 4.162177356192842e-05, Learning Rate: 0.000238\n",
      "Epoch 17744/40000, Loss: 3.313821434858255e-05, Learning Rate: 0.000238\n",
      "Epoch 17745/40000, Loss: 3.30656512232963e-05, Learning Rate: 0.000238\n",
      "Epoch 17746/40000, Loss: 5.535978561965749e-05, Learning Rate: 0.000238\n",
      "Epoch 17747/40000, Loss: 5.5293439800152555e-05, Learning Rate: 0.000238\n",
      "Epoch 17748/40000, Loss: 5.5304837587755173e-05, Learning Rate: 0.000238\n",
      "Epoch 17749/40000, Loss: 6.300269160419703e-05, Learning Rate: 0.000238\n",
      "Epoch 17750/40000, Loss: 6.285215931711718e-05, Learning Rate: 0.000238\n",
      "Epoch 17751/40000, Loss: 1.4837652997812256e-05, Learning Rate: 0.000238\n",
      "Epoch 17752/40000, Loss: 3.780522456509061e-05, Learning Rate: 0.000238\n",
      "Epoch 17753/40000, Loss: 6.28280031378381e-05, Learning Rate: 0.000238\n",
      "Epoch 17754/40000, Loss: 4.228068428346887e-05, Learning Rate: 0.000238\n",
      "Epoch 17755/40000, Loss: 6.31409784546122e-05, Learning Rate: 0.000238\n",
      "Epoch 17756/40000, Loss: 6.280194065766409e-05, Learning Rate: 0.000237\n",
      "Epoch 17757/40000, Loss: 5.5684784456389025e-05, Learning Rate: 0.000237\n",
      "Epoch 17758/40000, Loss: 5.540020720218308e-05, Learning Rate: 0.000237\n",
      "Epoch 17759/40000, Loss: 3.760299296118319e-05, Learning Rate: 0.000237\n",
      "Epoch 17760/40000, Loss: 3.765216752071865e-05, Learning Rate: 0.000237\n",
      "Epoch 17761/40000, Loss: 3.30004004354123e-05, Learning Rate: 0.000237\n",
      "Epoch 17762/40000, Loss: 5.543767110793851e-05, Learning Rate: 0.000237\n",
      "Epoch 17763/40000, Loss: 6.275322812143713e-05, Learning Rate: 0.000237\n",
      "Epoch 17764/40000, Loss: 3.327231388539076e-05, Learning Rate: 0.000237\n",
      "Epoch 17765/40000, Loss: 6.336405931506306e-05, Learning Rate: 0.000237\n",
      "Epoch 17766/40000, Loss: 1.5477453416679054e-05, Learning Rate: 0.000237\n",
      "Epoch 17767/40000, Loss: 5.6295662943739444e-05, Learning Rate: 0.000237\n",
      "Epoch 17768/40000, Loss: 3.796775854425505e-05, Learning Rate: 0.000237\n",
      "Epoch 17769/40000, Loss: 1.5513327525695786e-05, Learning Rate: 0.000237\n",
      "Epoch 17770/40000, Loss: 5.955352389719337e-05, Learning Rate: 0.000237\n",
      "Epoch 17771/40000, Loss: 1.5844540030229837e-05, Learning Rate: 0.000237\n",
      "Epoch 17772/40000, Loss: 5.650736056850292e-05, Learning Rate: 0.000237\n",
      "Epoch 17773/40000, Loss: 6.346127338474616e-05, Learning Rate: 0.000237\n",
      "Epoch 17774/40000, Loss: 3.794844087678939e-05, Learning Rate: 0.000237\n",
      "Epoch 17775/40000, Loss: 6.324363494059071e-05, Learning Rate: 0.000237\n",
      "Epoch 17776/40000, Loss: 1.5398447430925444e-05, Learning Rate: 0.000237\n",
      "Epoch 17777/40000, Loss: 1.4992154319770634e-05, Learning Rate: 0.000237\n",
      "Epoch 17778/40000, Loss: 6.320488319033757e-05, Learning Rate: 0.000237\n",
      "Epoch 17779/40000, Loss: 3.28648166032508e-05, Learning Rate: 0.000237\n",
      "Epoch 17780/40000, Loss: 1.4643783288192935e-05, Learning Rate: 0.000237\n",
      "Epoch 17781/40000, Loss: 3.2938540243776515e-05, Learning Rate: 0.000237\n",
      "Epoch 17782/40000, Loss: 6.298038351815194e-05, Learning Rate: 0.000237\n",
      "Epoch 17783/40000, Loss: 3.76731222786475e-05, Learning Rate: 0.000237\n",
      "Epoch 17784/40000, Loss: 4.237239409121685e-05, Learning Rate: 0.000237\n",
      "Epoch 17785/40000, Loss: 6.3691521063447e-05, Learning Rate: 0.000237\n",
      "Epoch 17786/40000, Loss: 3.3666063245618716e-05, Learning Rate: 0.000237\n",
      "Epoch 17787/40000, Loss: 4.1994786442955956e-05, Learning Rate: 0.000237\n",
      "Epoch 17788/40000, Loss: 1.538349170004949e-05, Learning Rate: 0.000237\n",
      "Epoch 17789/40000, Loss: 3.8212412619031966e-05, Learning Rate: 0.000237\n",
      "Epoch 17790/40000, Loss: 3.404723611311056e-05, Learning Rate: 0.000237\n",
      "Epoch 17791/40000, Loss: 5.746098395320587e-05, Learning Rate: 0.000237\n",
      "Epoch 17792/40000, Loss: 5.6870689149945974e-05, Learning Rate: 0.000236\n",
      "Epoch 17793/40000, Loss: 4.483058728510514e-05, Learning Rate: 0.000236\n",
      "Epoch 17794/40000, Loss: 3.577588722691871e-05, Learning Rate: 0.000236\n",
      "Epoch 17795/40000, Loss: 3.5043092793785036e-05, Learning Rate: 0.000236\n",
      "Epoch 17796/40000, Loss: 3.415655737626366e-05, Learning Rate: 0.000236\n",
      "Epoch 17797/40000, Loss: 3.9674167055636644e-05, Learning Rate: 0.000236\n",
      "Epoch 17798/40000, Loss: 5.831402086187154e-05, Learning Rate: 0.000236\n",
      "Epoch 17799/40000, Loss: 4.454532972886227e-05, Learning Rate: 0.000236\n",
      "Epoch 17800/40000, Loss: 4.42234959336929e-05, Learning Rate: 0.000236\n",
      "Epoch 17801/40000, Loss: 2.0253206457709894e-05, Learning Rate: 0.000236\n",
      "Epoch 17802/40000, Loss: 3.473735705483705e-05, Learning Rate: 0.000236\n",
      "Epoch 17803/40000, Loss: 4.173737397650257e-05, Learning Rate: 0.000236\n",
      "Epoch 17804/40000, Loss: 1.609297760296613e-05, Learning Rate: 0.000236\n",
      "Epoch 17805/40000, Loss: 1.5456225810339674e-05, Learning Rate: 0.000236\n",
      "Epoch 17806/40000, Loss: 3.446860137046315e-05, Learning Rate: 0.000236\n",
      "Epoch 17807/40000, Loss: 4.007620009360835e-05, Learning Rate: 0.000236\n",
      "Epoch 17808/40000, Loss: 5.685779979103245e-05, Learning Rate: 0.000236\n",
      "Epoch 17809/40000, Loss: 5.710023106075823e-05, Learning Rate: 0.000236\n",
      "Epoch 17810/40000, Loss: 3.370330887264572e-05, Learning Rate: 0.000236\n",
      "Epoch 17811/40000, Loss: 3.32615636580158e-05, Learning Rate: 0.000236\n",
      "Epoch 17812/40000, Loss: 6.341822154354304e-05, Learning Rate: 0.000236\n",
      "Epoch 17813/40000, Loss: 5.618229988613166e-05, Learning Rate: 0.000236\n",
      "Epoch 17814/40000, Loss: 4.216126035316847e-05, Learning Rate: 0.000236\n",
      "Epoch 17815/40000, Loss: 3.8498008507303894e-05, Learning Rate: 0.000236\n",
      "Epoch 17816/40000, Loss: 5.580047218245454e-05, Learning Rate: 0.000236\n",
      "Epoch 17817/40000, Loss: 3.816385651589371e-05, Learning Rate: 0.000236\n",
      "Epoch 17818/40000, Loss: 6.344114081002772e-05, Learning Rate: 0.000236\n",
      "Epoch 17819/40000, Loss: 6.347420276142657e-05, Learning Rate: 0.000236\n",
      "Epoch 17820/40000, Loss: 3.354294676682912e-05, Learning Rate: 0.000236\n",
      "Epoch 17821/40000, Loss: 4.283822636352852e-05, Learning Rate: 0.000236\n",
      "Epoch 17822/40000, Loss: 1.776604403858073e-05, Learning Rate: 0.000236\n",
      "Epoch 17823/40000, Loss: 5.9062680520582944e-05, Learning Rate: 0.000236\n",
      "Epoch 17824/40000, Loss: 5.832027454744093e-05, Learning Rate: 0.000236\n",
      "Epoch 17825/40000, Loss: 4.086269473191351e-05, Learning Rate: 0.000236\n",
      "Epoch 17826/40000, Loss: 6.13936354056932e-05, Learning Rate: 0.000236\n",
      "Epoch 17827/40000, Loss: 7.049238047329709e-05, Learning Rate: 0.000235\n",
      "Epoch 17828/40000, Loss: 8.553023508284241e-05, Learning Rate: 0.000235\n",
      "Epoch 17829/40000, Loss: 9.576133015798405e-05, Learning Rate: 0.000235\n",
      "Epoch 17830/40000, Loss: 4.9154645239468664e-05, Learning Rate: 0.000235\n",
      "Epoch 17831/40000, Loss: 4.926098699797876e-05, Learning Rate: 0.000235\n",
      "Epoch 17832/40000, Loss: 2.508954821678344e-05, Learning Rate: 0.000235\n",
      "Epoch 17833/40000, Loss: 4.60907795059029e-05, Learning Rate: 0.000235\n",
      "Epoch 17834/40000, Loss: 5.423344919108786e-05, Learning Rate: 0.000235\n",
      "Epoch 17835/40000, Loss: 4.150814129388891e-05, Learning Rate: 0.000235\n",
      "Epoch 17836/40000, Loss: 6.995598232606426e-05, Learning Rate: 0.000235\n",
      "Epoch 17837/40000, Loss: 1.831178815336898e-05, Learning Rate: 0.000235\n",
      "Epoch 17838/40000, Loss: 6.04237757215742e-05, Learning Rate: 0.000235\n",
      "Epoch 17839/40000, Loss: 6.437450792873278e-05, Learning Rate: 0.000235\n",
      "Epoch 17840/40000, Loss: 6.886950723128393e-05, Learning Rate: 0.000235\n",
      "Epoch 17841/40000, Loss: 6.841614958830178e-05, Learning Rate: 0.000235\n",
      "Epoch 17842/40000, Loss: 1.639576476009097e-05, Learning Rate: 0.000235\n",
      "Epoch 17843/40000, Loss: 6.701257370878011e-05, Learning Rate: 0.000235\n",
      "Epoch 17844/40000, Loss: 3.923499753000215e-05, Learning Rate: 0.000235\n",
      "Epoch 17845/40000, Loss: 3.824103623628616e-05, Learning Rate: 0.000235\n",
      "Epoch 17846/40000, Loss: 6.461951124947518e-05, Learning Rate: 0.000235\n",
      "Epoch 17847/40000, Loss: 5.738091203966178e-05, Learning Rate: 0.000235\n",
      "Epoch 17848/40000, Loss: 1.581051583343651e-05, Learning Rate: 0.000235\n",
      "Epoch 17849/40000, Loss: 3.389973790035583e-05, Learning Rate: 0.000235\n",
      "Epoch 17850/40000, Loss: 5.8064535551238805e-05, Learning Rate: 0.000235\n",
      "Epoch 17851/40000, Loss: 5.725544906454161e-05, Learning Rate: 0.000235\n",
      "Epoch 17852/40000, Loss: 4.2586761992424726e-05, Learning Rate: 0.000235\n",
      "Epoch 17853/40000, Loss: 3.780002589337528e-05, Learning Rate: 0.000235\n",
      "Epoch 17854/40000, Loss: 3.84857012249995e-05, Learning Rate: 0.000235\n",
      "Epoch 17855/40000, Loss: 6.346173904603347e-05, Learning Rate: 0.000235\n",
      "Epoch 17856/40000, Loss: 3.753309283638373e-05, Learning Rate: 0.000235\n",
      "Epoch 17857/40000, Loss: 3.756723890546709e-05, Learning Rate: 0.000235\n",
      "Epoch 17858/40000, Loss: 3.839213968603872e-05, Learning Rate: 0.000235\n",
      "Epoch 17859/40000, Loss: 3.8926638808334246e-05, Learning Rate: 0.000235\n",
      "Epoch 17860/40000, Loss: 5.705860530724749e-05, Learning Rate: 0.000235\n",
      "Epoch 17861/40000, Loss: 6.31373404758051e-05, Learning Rate: 0.000235\n",
      "Epoch 17862/40000, Loss: 6.283335824264213e-05, Learning Rate: 0.000234\n",
      "Epoch 17863/40000, Loss: 6.447301711887121e-05, Learning Rate: 0.000234\n",
      "Epoch 17864/40000, Loss: 4.348066431703046e-05, Learning Rate: 0.000234\n",
      "Epoch 17865/40000, Loss: 1.5202831491478719e-05, Learning Rate: 0.000234\n",
      "Epoch 17866/40000, Loss: 3.763283893931657e-05, Learning Rate: 0.000234\n",
      "Epoch 17867/40000, Loss: 3.3136569982161745e-05, Learning Rate: 0.000234\n",
      "Epoch 17868/40000, Loss: 3.300464595668018e-05, Learning Rate: 0.000234\n",
      "Epoch 17869/40000, Loss: 3.3003874705173075e-05, Learning Rate: 0.000234\n",
      "Epoch 17870/40000, Loss: 1.4875684428261593e-05, Learning Rate: 0.000234\n",
      "Epoch 17871/40000, Loss: 3.273166294093244e-05, Learning Rate: 0.000234\n",
      "Epoch 17872/40000, Loss: 1.4631563317379914e-05, Learning Rate: 0.000234\n",
      "Epoch 17873/40000, Loss: 5.5303022236330435e-05, Learning Rate: 0.000234\n",
      "Epoch 17874/40000, Loss: 1.4991568605182692e-05, Learning Rate: 0.000234\n",
      "Epoch 17875/40000, Loss: 3.7550875276792794e-05, Learning Rate: 0.000234\n",
      "Epoch 17876/40000, Loss: 3.275766357546672e-05, Learning Rate: 0.000234\n",
      "Epoch 17877/40000, Loss: 1.4568997357855551e-05, Learning Rate: 0.000234\n",
      "Epoch 17878/40000, Loss: 3.300108801340684e-05, Learning Rate: 0.000234\n",
      "Epoch 17879/40000, Loss: 5.488323586178012e-05, Learning Rate: 0.000234\n",
      "Epoch 17880/40000, Loss: 6.243002280825749e-05, Learning Rate: 0.000234\n",
      "Epoch 17881/40000, Loss: 3.7573179724859074e-05, Learning Rate: 0.000234\n",
      "Epoch 17882/40000, Loss: 6.246166594792157e-05, Learning Rate: 0.000234\n",
      "Epoch 17883/40000, Loss: 4.141191311646253e-05, Learning Rate: 0.000234\n",
      "Epoch 17884/40000, Loss: 6.260519148781896e-05, Learning Rate: 0.000234\n",
      "Epoch 17885/40000, Loss: 4.146063656662591e-05, Learning Rate: 0.000234\n",
      "Epoch 17886/40000, Loss: 3.732897312147543e-05, Learning Rate: 0.000234\n",
      "Epoch 17887/40000, Loss: 3.276159623055719e-05, Learning Rate: 0.000234\n",
      "Epoch 17888/40000, Loss: 5.5242679081857204e-05, Learning Rate: 0.000234\n",
      "Epoch 17889/40000, Loss: 1.4588558769901283e-05, Learning Rate: 0.000234\n",
      "Epoch 17890/40000, Loss: 1.4741319319000468e-05, Learning Rate: 0.000234\n",
      "Epoch 17891/40000, Loss: 3.731809192686342e-05, Learning Rate: 0.000234\n",
      "Epoch 17892/40000, Loss: 5.5170239647850394e-05, Learning Rate: 0.000234\n",
      "Epoch 17893/40000, Loss: 3.270865272497758e-05, Learning Rate: 0.000234\n",
      "Epoch 17894/40000, Loss: 6.228801066754386e-05, Learning Rate: 0.000234\n",
      "Epoch 17895/40000, Loss: 4.117290518479422e-05, Learning Rate: 0.000234\n",
      "Epoch 17896/40000, Loss: 4.1256931581301615e-05, Learning Rate: 0.000234\n",
      "Epoch 17897/40000, Loss: 5.488221358973533e-05, Learning Rate: 0.000234\n",
      "Epoch 17898/40000, Loss: 6.244704854907468e-05, Learning Rate: 0.000233\n",
      "Epoch 17899/40000, Loss: 1.4611849110224284e-05, Learning Rate: 0.000233\n",
      "Epoch 17900/40000, Loss: 6.26793917035684e-05, Learning Rate: 0.000233\n",
      "Epoch 17901/40000, Loss: 4.143326077610254e-05, Learning Rate: 0.000233\n",
      "Epoch 17902/40000, Loss: 6.30688518867828e-05, Learning Rate: 0.000233\n",
      "Epoch 17903/40000, Loss: 5.498649989021942e-05, Learning Rate: 0.000233\n",
      "Epoch 17904/40000, Loss: 3.7333793443394825e-05, Learning Rate: 0.000233\n",
      "Epoch 17905/40000, Loss: 3.276468487456441e-05, Learning Rate: 0.000233\n",
      "Epoch 17906/40000, Loss: 3.262402242398821e-05, Learning Rate: 0.000233\n",
      "Epoch 17907/40000, Loss: 4.128709406359121e-05, Learning Rate: 0.000233\n",
      "Epoch 17908/40000, Loss: 6.261475937208161e-05, Learning Rate: 0.000233\n",
      "Epoch 17909/40000, Loss: 3.726448630914092e-05, Learning Rate: 0.000233\n",
      "Epoch 17910/40000, Loss: 3.7297089875210077e-05, Learning Rate: 0.000233\n",
      "Epoch 17911/40000, Loss: 1.4562829164788127e-05, Learning Rate: 0.000233\n",
      "Epoch 17912/40000, Loss: 1.4526781342283357e-05, Learning Rate: 0.000233\n",
      "Epoch 17913/40000, Loss: 3.269221997470595e-05, Learning Rate: 0.000233\n",
      "Epoch 17914/40000, Loss: 1.4490821740764659e-05, Learning Rate: 0.000233\n",
      "Epoch 17915/40000, Loss: 4.118424476473592e-05, Learning Rate: 0.000233\n",
      "Epoch 17916/40000, Loss: 6.23275336693041e-05, Learning Rate: 0.000233\n",
      "Epoch 17917/40000, Loss: 6.223290256457403e-05, Learning Rate: 0.000233\n",
      "Epoch 17918/40000, Loss: 4.1400457121199e-05, Learning Rate: 0.000233\n",
      "Epoch 17919/40000, Loss: 5.5033753596944734e-05, Learning Rate: 0.000233\n",
      "Epoch 17920/40000, Loss: 4.1361479816259816e-05, Learning Rate: 0.000233\n",
      "Epoch 17921/40000, Loss: 5.487334419740364e-05, Learning Rate: 0.000233\n",
      "Epoch 17922/40000, Loss: 5.489836257766001e-05, Learning Rate: 0.000233\n",
      "Epoch 17923/40000, Loss: 6.211618892848492e-05, Learning Rate: 0.000233\n",
      "Epoch 17924/40000, Loss: 5.4970358178252354e-05, Learning Rate: 0.000233\n",
      "Epoch 17925/40000, Loss: 5.494111974257976e-05, Learning Rate: 0.000233\n",
      "Epoch 17926/40000, Loss: 3.261801248299889e-05, Learning Rate: 0.000233\n",
      "Epoch 17927/40000, Loss: 3.719605228980072e-05, Learning Rate: 0.000233\n",
      "Epoch 17928/40000, Loss: 4.132088361075148e-05, Learning Rate: 0.000233\n",
      "Epoch 17929/40000, Loss: 3.267443025833927e-05, Learning Rate: 0.000233\n",
      "Epoch 17930/40000, Loss: 6.221797229954973e-05, Learning Rate: 0.000233\n",
      "Epoch 17931/40000, Loss: 5.5002929002512246e-05, Learning Rate: 0.000233\n",
      "Epoch 17932/40000, Loss: 3.765228029806167e-05, Learning Rate: 0.000233\n",
      "Epoch 17933/40000, Loss: 3.745016874745488e-05, Learning Rate: 0.000233\n",
      "Epoch 17934/40000, Loss: 4.175648791715503e-05, Learning Rate: 0.000232\n",
      "Epoch 17935/40000, Loss: 4.1490588046144694e-05, Learning Rate: 0.000232\n",
      "Epoch 17936/40000, Loss: 5.503010834218003e-05, Learning Rate: 0.000232\n",
      "Epoch 17937/40000, Loss: 3.741665568668395e-05, Learning Rate: 0.000232\n",
      "Epoch 17938/40000, Loss: 3.735010250238702e-05, Learning Rate: 0.000232\n",
      "Epoch 17939/40000, Loss: 6.246254633879289e-05, Learning Rate: 0.000232\n",
      "Epoch 17940/40000, Loss: 6.269705772865564e-05, Learning Rate: 0.000232\n",
      "Epoch 17941/40000, Loss: 3.7521480408031493e-05, Learning Rate: 0.000232\n",
      "Epoch 17942/40000, Loss: 1.4711253243149258e-05, Learning Rate: 0.000232\n",
      "Epoch 17943/40000, Loss: 5.5276115745073184e-05, Learning Rate: 0.000232\n",
      "Epoch 17944/40000, Loss: 3.777393067139201e-05, Learning Rate: 0.000232\n",
      "Epoch 17945/40000, Loss: 4.2233670683344826e-05, Learning Rate: 0.000232\n",
      "Epoch 17946/40000, Loss: 4.223957148496993e-05, Learning Rate: 0.000232\n",
      "Epoch 17947/40000, Loss: 4.211175473756157e-05, Learning Rate: 0.000232\n",
      "Epoch 17948/40000, Loss: 4.2008421587524936e-05, Learning Rate: 0.000232\n",
      "Epoch 17949/40000, Loss: 5.546318425331265e-05, Learning Rate: 0.000232\n",
      "Epoch 17950/40000, Loss: 4.2133462557103485e-05, Learning Rate: 0.000232\n",
      "Epoch 17951/40000, Loss: 1.527327003714163e-05, Learning Rate: 0.000232\n",
      "Epoch 17952/40000, Loss: 4.473219087230973e-05, Learning Rate: 0.000232\n",
      "Epoch 17953/40000, Loss: 4.2507770558586344e-05, Learning Rate: 0.000232\n",
      "Epoch 17954/40000, Loss: 3.294656198704615e-05, Learning Rate: 0.000232\n",
      "Epoch 17955/40000, Loss: 4.3153733713552356e-05, Learning Rate: 0.000232\n",
      "Epoch 17956/40000, Loss: 3.427817500778474e-05, Learning Rate: 0.000232\n",
      "Epoch 17957/40000, Loss: 5.659230737364851e-05, Learning Rate: 0.000232\n",
      "Epoch 17958/40000, Loss: 5.611387314274907e-05, Learning Rate: 0.000232\n",
      "Epoch 17959/40000, Loss: 5.5860975407995284e-05, Learning Rate: 0.000232\n",
      "Epoch 17960/40000, Loss: 6.349196337396279e-05, Learning Rate: 0.000232\n",
      "Epoch 17961/40000, Loss: 1.5510537195950747e-05, Learning Rate: 0.000232\n",
      "Epoch 17962/40000, Loss: 5.705067451344803e-05, Learning Rate: 0.000232\n",
      "Epoch 17963/40000, Loss: 4.386489308672026e-05, Learning Rate: 0.000232\n",
      "Epoch 17964/40000, Loss: 4.4446940592024475e-05, Learning Rate: 0.000232\n",
      "Epoch 17965/40000, Loss: 1.72539621416945e-05, Learning Rate: 0.000232\n",
      "Epoch 17966/40000, Loss: 3.481383464531973e-05, Learning Rate: 0.000232\n",
      "Epoch 17967/40000, Loss: 3.935313725378364e-05, Learning Rate: 0.000232\n",
      "Epoch 17968/40000, Loss: 6.550196121679619e-05, Learning Rate: 0.000232\n",
      "Epoch 17969/40000, Loss: 3.49663823726587e-05, Learning Rate: 0.000232\n",
      "Epoch 17970/40000, Loss: 3.9988404751056805e-05, Learning Rate: 0.000231\n",
      "Epoch 17971/40000, Loss: 4.373579940875061e-05, Learning Rate: 0.000231\n",
      "Epoch 17972/40000, Loss: 6.433702219510451e-05, Learning Rate: 0.000231\n",
      "Epoch 17973/40000, Loss: 4.0394283132627606e-05, Learning Rate: 0.000231\n",
      "Epoch 17974/40000, Loss: 1.75756067619659e-05, Learning Rate: 0.000231\n",
      "Epoch 17975/40000, Loss: 6.466623744927347e-05, Learning Rate: 0.000231\n",
      "Epoch 17976/40000, Loss: 3.485257911961526e-05, Learning Rate: 0.000231\n",
      "Epoch 17977/40000, Loss: 1.6128833522088826e-05, Learning Rate: 0.000231\n",
      "Epoch 17978/40000, Loss: 5.7133791415253654e-05, Learning Rate: 0.000231\n",
      "Epoch 17979/40000, Loss: 3.8393554859794676e-05, Learning Rate: 0.000231\n",
      "Epoch 17980/40000, Loss: 3.7507055822061375e-05, Learning Rate: 0.000231\n",
      "Epoch 17981/40000, Loss: 6.27155095571652e-05, Learning Rate: 0.000231\n",
      "Epoch 17982/40000, Loss: 3.28318819811102e-05, Learning Rate: 0.000231\n",
      "Epoch 17983/40000, Loss: 4.175887806923129e-05, Learning Rate: 0.000231\n",
      "Epoch 17984/40000, Loss: 4.154159614699893e-05, Learning Rate: 0.000231\n",
      "Epoch 17985/40000, Loss: 4.1651903302408755e-05, Learning Rate: 0.000231\n",
      "Epoch 17986/40000, Loss: 3.295291026006453e-05, Learning Rate: 0.000231\n",
      "Epoch 17987/40000, Loss: 4.148374864598736e-05, Learning Rate: 0.000231\n",
      "Epoch 17988/40000, Loss: 5.5416079703718424e-05, Learning Rate: 0.000231\n",
      "Epoch 17989/40000, Loss: 4.228678153594956e-05, Learning Rate: 0.000231\n",
      "Epoch 17990/40000, Loss: 1.490760132583091e-05, Learning Rate: 0.000231\n",
      "Epoch 17991/40000, Loss: 6.248192221391946e-05, Learning Rate: 0.000231\n",
      "Epoch 17992/40000, Loss: 3.759159517358057e-05, Learning Rate: 0.000231\n",
      "Epoch 17993/40000, Loss: 4.671674469136633e-05, Learning Rate: 0.000231\n",
      "Epoch 17994/40000, Loss: 3.7900088500464335e-05, Learning Rate: 0.000231\n",
      "Epoch 17995/40000, Loss: 5.526926543097943e-05, Learning Rate: 0.000231\n",
      "Epoch 17996/40000, Loss: 6.330510950647295e-05, Learning Rate: 0.000231\n",
      "Epoch 17997/40000, Loss: 1.5860287021496333e-05, Learning Rate: 0.000231\n",
      "Epoch 17998/40000, Loss: 5.602512464975007e-05, Learning Rate: 0.000231\n",
      "Epoch 17999/40000, Loss: 3.8665901229251176e-05, Learning Rate: 0.000231\n",
      "Epoch 18000/40000, Loss: 1.6338024579454213e-05, Learning Rate: 0.000231\n",
      "Epoch 18001/40000, Loss: 3.3042641007341444e-05, Learning Rate: 0.000231\n",
      "Epoch 18002/40000, Loss: 3.781761915888637e-05, Learning Rate: 0.000231\n",
      "Epoch 18003/40000, Loss: 5.568366759689525e-05, Learning Rate: 0.000231\n",
      "Epoch 18004/40000, Loss: 3.792150891968049e-05, Learning Rate: 0.000231\n",
      "Epoch 18005/40000, Loss: 3.760528488783166e-05, Learning Rate: 0.000231\n",
      "Epoch 18006/40000, Loss: 6.439507706090808e-05, Learning Rate: 0.000230\n",
      "Epoch 18007/40000, Loss: 3.737442602869123e-05, Learning Rate: 0.000230\n",
      "Epoch 18008/40000, Loss: 5.6046737881843e-05, Learning Rate: 0.000230\n",
      "Epoch 18009/40000, Loss: 4.247831384418532e-05, Learning Rate: 0.000230\n",
      "Epoch 18010/40000, Loss: 6.480691081378609e-05, Learning Rate: 0.000230\n",
      "Epoch 18011/40000, Loss: 5.680109461536631e-05, Learning Rate: 0.000230\n",
      "Epoch 18012/40000, Loss: 1.5576315490761772e-05, Learning Rate: 0.000230\n",
      "Epoch 18013/40000, Loss: 4.426527812029235e-05, Learning Rate: 0.000230\n",
      "Epoch 18014/40000, Loss: 4.37850430898834e-05, Learning Rate: 0.000230\n",
      "Epoch 18015/40000, Loss: 3.803723302553408e-05, Learning Rate: 0.000230\n",
      "Epoch 18016/40000, Loss: 5.717822932638228e-05, Learning Rate: 0.000230\n",
      "Epoch 18017/40000, Loss: 6.382239371305332e-05, Learning Rate: 0.000230\n",
      "Epoch 18018/40000, Loss: 3.788110916502774e-05, Learning Rate: 0.000230\n",
      "Epoch 18019/40000, Loss: 1.5820776752661914e-05, Learning Rate: 0.000230\n",
      "Epoch 18020/40000, Loss: 5.614649489871226e-05, Learning Rate: 0.000230\n",
      "Epoch 18021/40000, Loss: 5.587346458924003e-05, Learning Rate: 0.000230\n",
      "Epoch 18022/40000, Loss: 3.397948239580728e-05, Learning Rate: 0.000230\n",
      "Epoch 18023/40000, Loss: 3.793234282056801e-05, Learning Rate: 0.000230\n",
      "Epoch 18024/40000, Loss: 6.734083581250161e-05, Learning Rate: 0.000230\n",
      "Epoch 18025/40000, Loss: 3.331649350002408e-05, Learning Rate: 0.000230\n",
      "Epoch 18026/40000, Loss: 5.608880746876821e-05, Learning Rate: 0.000230\n",
      "Epoch 18027/40000, Loss: 5.5577154853381217e-05, Learning Rate: 0.000230\n",
      "Epoch 18028/40000, Loss: 5.5589465773664415e-05, Learning Rate: 0.000230\n",
      "Epoch 18029/40000, Loss: 1.556421375425998e-05, Learning Rate: 0.000230\n",
      "Epoch 18030/40000, Loss: 1.527890344732441e-05, Learning Rate: 0.000230\n",
      "Epoch 18031/40000, Loss: 1.4990108866186347e-05, Learning Rate: 0.000230\n",
      "Epoch 18032/40000, Loss: 5.559021155931987e-05, Learning Rate: 0.000230\n",
      "Epoch 18033/40000, Loss: 3.832716174656525e-05, Learning Rate: 0.000230\n",
      "Epoch 18034/40000, Loss: 5.6333487009396777e-05, Learning Rate: 0.000230\n",
      "Epoch 18035/40000, Loss: 6.27937915851362e-05, Learning Rate: 0.000230\n",
      "Epoch 18036/40000, Loss: 3.798838224611245e-05, Learning Rate: 0.000230\n",
      "Epoch 18037/40000, Loss: 6.419489363906905e-05, Learning Rate: 0.000230\n",
      "Epoch 18038/40000, Loss: 1.582976801728364e-05, Learning Rate: 0.000230\n",
      "Epoch 18039/40000, Loss: 6.40053185634315e-05, Learning Rate: 0.000230\n",
      "Epoch 18040/40000, Loss: 6.38754281681031e-05, Learning Rate: 0.000230\n",
      "Epoch 18041/40000, Loss: 6.402170401997864e-05, Learning Rate: 0.000230\n",
      "Epoch 18042/40000, Loss: 5.58587271370925e-05, Learning Rate: 0.000229\n",
      "Epoch 18043/40000, Loss: 6.342172855511308e-05, Learning Rate: 0.000229\n",
      "Epoch 18044/40000, Loss: 3.311644104542211e-05, Learning Rate: 0.000229\n",
      "Epoch 18045/40000, Loss: 6.31031725788489e-05, Learning Rate: 0.000229\n",
      "Epoch 18046/40000, Loss: 3.75301024178043e-05, Learning Rate: 0.000229\n",
      "Epoch 18047/40000, Loss: 3.729010859387927e-05, Learning Rate: 0.000229\n",
      "Epoch 18048/40000, Loss: 3.758336970349774e-05, Learning Rate: 0.000229\n",
      "Epoch 18049/40000, Loss: 3.298743104096502e-05, Learning Rate: 0.000229\n",
      "Epoch 18050/40000, Loss: 1.4771970199944917e-05, Learning Rate: 0.000229\n",
      "Epoch 18051/40000, Loss: 6.270597077673301e-05, Learning Rate: 0.000229\n",
      "Epoch 18052/40000, Loss: 5.5968899687286466e-05, Learning Rate: 0.000229\n",
      "Epoch 18053/40000, Loss: 4.286382318241522e-05, Learning Rate: 0.000229\n",
      "Epoch 18054/40000, Loss: 6.0285397921688855e-05, Learning Rate: 0.000229\n",
      "Epoch 18055/40000, Loss: 3.756769365281798e-05, Learning Rate: 0.000229\n",
      "Epoch 18056/40000, Loss: 6.352529453579336e-05, Learning Rate: 0.000229\n",
      "Epoch 18057/40000, Loss: 3.823872248176485e-05, Learning Rate: 0.000229\n",
      "Epoch 18058/40000, Loss: 1.5146561054280028e-05, Learning Rate: 0.000229\n",
      "Epoch 18059/40000, Loss: 1.4817948795098346e-05, Learning Rate: 0.000229\n",
      "Epoch 18060/40000, Loss: 5.6183005654020235e-05, Learning Rate: 0.000229\n",
      "Epoch 18061/40000, Loss: 6.246002885745838e-05, Learning Rate: 0.000229\n",
      "Epoch 18062/40000, Loss: 3.2918636861722916e-05, Learning Rate: 0.000229\n",
      "Epoch 18063/40000, Loss: 3.7358735426096246e-05, Learning Rate: 0.000229\n",
      "Epoch 18064/40000, Loss: 4.147132858633995e-05, Learning Rate: 0.000229\n",
      "Epoch 18065/40000, Loss: 6.230330473044887e-05, Learning Rate: 0.000229\n",
      "Epoch 18066/40000, Loss: 1.4961373381083831e-05, Learning Rate: 0.000229\n",
      "Epoch 18067/40000, Loss: 1.5072784663061611e-05, Learning Rate: 0.000229\n",
      "Epoch 18068/40000, Loss: 3.7406032788567245e-05, Learning Rate: 0.000229\n",
      "Epoch 18069/40000, Loss: 4.2549738282104954e-05, Learning Rate: 0.000229\n",
      "Epoch 18070/40000, Loss: 3.298280716990121e-05, Learning Rate: 0.000229\n",
      "Epoch 18071/40000, Loss: 6.309390300884843e-05, Learning Rate: 0.000229\n",
      "Epoch 18072/40000, Loss: 6.304091948550195e-05, Learning Rate: 0.000229\n",
      "Epoch 18073/40000, Loss: 3.8294398109428585e-05, Learning Rate: 0.000229\n",
      "Epoch 18074/40000, Loss: 1.5228937627398409e-05, Learning Rate: 0.000229\n",
      "Epoch 18075/40000, Loss: 3.3425079891458154e-05, Learning Rate: 0.000229\n",
      "Epoch 18076/40000, Loss: 1.7163862139568664e-05, Learning Rate: 0.000229\n",
      "Epoch 18077/40000, Loss: 5.924435390625149e-05, Learning Rate: 0.000229\n",
      "Epoch 18078/40000, Loss: 3.8873200537636876e-05, Learning Rate: 0.000228\n",
      "Epoch 18079/40000, Loss: 6.0443675465648994e-05, Learning Rate: 0.000228\n",
      "Epoch 18080/40000, Loss: 4.357092984719202e-05, Learning Rate: 0.000228\n",
      "Epoch 18081/40000, Loss: 6.71331217745319e-05, Learning Rate: 0.000228\n",
      "Epoch 18082/40000, Loss: 6.0354373999871314e-05, Learning Rate: 0.000228\n",
      "Epoch 18083/40000, Loss: 5.791626244899817e-05, Learning Rate: 0.000228\n",
      "Epoch 18084/40000, Loss: 3.364324584254064e-05, Learning Rate: 0.000228\n",
      "Epoch 18085/40000, Loss: 6.192029832163826e-05, Learning Rate: 0.000228\n",
      "Epoch 18086/40000, Loss: 3.969973113271408e-05, Learning Rate: 0.000228\n",
      "Epoch 18087/40000, Loss: 9.099506860366091e-05, Learning Rate: 0.000228\n",
      "Epoch 18088/40000, Loss: 3.553959322744049e-05, Learning Rate: 0.000228\n",
      "Epoch 18089/40000, Loss: 4.484350574784912e-05, Learning Rate: 0.000228\n",
      "Epoch 18090/40000, Loss: 4.032978540635668e-05, Learning Rate: 0.000228\n",
      "Epoch 18091/40000, Loss: 6.488927465397865e-05, Learning Rate: 0.000228\n",
      "Epoch 18092/40000, Loss: 1.657430584600661e-05, Learning Rate: 0.000228\n",
      "Epoch 18093/40000, Loss: 3.885954720317386e-05, Learning Rate: 0.000228\n",
      "Epoch 18094/40000, Loss: 4.381023973110132e-05, Learning Rate: 0.000228\n",
      "Epoch 18095/40000, Loss: 6.493357795989141e-05, Learning Rate: 0.000228\n",
      "Epoch 18096/40000, Loss: 1.697840889391955e-05, Learning Rate: 0.000228\n",
      "Epoch 18097/40000, Loss: 6.476186536019668e-05, Learning Rate: 0.000228\n",
      "Epoch 18098/40000, Loss: 6.347549788188189e-05, Learning Rate: 0.000228\n",
      "Epoch 18099/40000, Loss: 5.6601718824822456e-05, Learning Rate: 0.000228\n",
      "Epoch 18100/40000, Loss: 5.624504046863876e-05, Learning Rate: 0.000228\n",
      "Epoch 18101/40000, Loss: 6.291552563197911e-05, Learning Rate: 0.000228\n",
      "Epoch 18102/40000, Loss: 4.306182381696999e-05, Learning Rate: 0.000228\n",
      "Epoch 18103/40000, Loss: 3.7919864553259686e-05, Learning Rate: 0.000228\n",
      "Epoch 18104/40000, Loss: 3.3096264814957976e-05, Learning Rate: 0.000228\n",
      "Epoch 18105/40000, Loss: 4.2052452045027167e-05, Learning Rate: 0.000228\n",
      "Epoch 18106/40000, Loss: 3.775435106945224e-05, Learning Rate: 0.000228\n",
      "Epoch 18107/40000, Loss: 3.287506478955038e-05, Learning Rate: 0.000228\n",
      "Epoch 18108/40000, Loss: 3.283932528574951e-05, Learning Rate: 0.000228\n",
      "Epoch 18109/40000, Loss: 1.4894056221237406e-05, Learning Rate: 0.000228\n",
      "Epoch 18110/40000, Loss: 6.305514398263767e-05, Learning Rate: 0.000228\n",
      "Epoch 18111/40000, Loss: 4.1997413063654676e-05, Learning Rate: 0.000228\n",
      "Epoch 18112/40000, Loss: 1.4951211596780922e-05, Learning Rate: 0.000228\n",
      "Epoch 18113/40000, Loss: 6.272942118812352e-05, Learning Rate: 0.000228\n",
      "Epoch 18114/40000, Loss: 4.14658643421717e-05, Learning Rate: 0.000228\n",
      "Epoch 18115/40000, Loss: 4.173525303485803e-05, Learning Rate: 0.000227\n",
      "Epoch 18116/40000, Loss: 1.4809107597102411e-05, Learning Rate: 0.000227\n",
      "Epoch 18117/40000, Loss: 4.160510070505552e-05, Learning Rate: 0.000227\n",
      "Epoch 18118/40000, Loss: 6.53103124932386e-05, Learning Rate: 0.000227\n",
      "Epoch 18119/40000, Loss: 1.4803019439568743e-05, Learning Rate: 0.000227\n",
      "Epoch 18120/40000, Loss: 1.4740691767656244e-05, Learning Rate: 0.000227\n",
      "Epoch 18121/40000, Loss: 4.160601747571491e-05, Learning Rate: 0.000227\n",
      "Epoch 18122/40000, Loss: 4.157558214501478e-05, Learning Rate: 0.000227\n",
      "Epoch 18123/40000, Loss: 1.4877537978463806e-05, Learning Rate: 0.000227\n",
      "Epoch 18124/40000, Loss: 3.2968087907647714e-05, Learning Rate: 0.000227\n",
      "Epoch 18125/40000, Loss: 3.732775803655386e-05, Learning Rate: 0.000227\n",
      "Epoch 18126/40000, Loss: 3.717510844580829e-05, Learning Rate: 0.000227\n",
      "Epoch 18127/40000, Loss: 6.223891978152096e-05, Learning Rate: 0.000227\n",
      "Epoch 18128/40000, Loss: 3.718418156495318e-05, Learning Rate: 0.000227\n",
      "Epoch 18129/40000, Loss: 3.2761574402684346e-05, Learning Rate: 0.000227\n",
      "Epoch 18130/40000, Loss: 1.4649758668383583e-05, Learning Rate: 0.000227\n",
      "Epoch 18131/40000, Loss: 6.262279202928767e-05, Learning Rate: 0.000227\n",
      "Epoch 18132/40000, Loss: 3.7184847315074876e-05, Learning Rate: 0.000227\n",
      "Epoch 18133/40000, Loss: 1.4665440176031552e-05, Learning Rate: 0.000227\n",
      "Epoch 18134/40000, Loss: 1.4698688573844265e-05, Learning Rate: 0.000227\n",
      "Epoch 18135/40000, Loss: 1.4661356544820592e-05, Learning Rate: 0.000227\n",
      "Epoch 18136/40000, Loss: 4.130875095142983e-05, Learning Rate: 0.000227\n",
      "Epoch 18137/40000, Loss: 3.257641947129741e-05, Learning Rate: 0.000227\n",
      "Epoch 18138/40000, Loss: 3.258153810747899e-05, Learning Rate: 0.000227\n",
      "Epoch 18139/40000, Loss: 3.703728361870162e-05, Learning Rate: 0.000227\n",
      "Epoch 18140/40000, Loss: 4.133605762035586e-05, Learning Rate: 0.000227\n",
      "Epoch 18141/40000, Loss: 1.4668482435808983e-05, Learning Rate: 0.000227\n",
      "Epoch 18142/40000, Loss: 5.488936585607007e-05, Learning Rate: 0.000227\n",
      "Epoch 18143/40000, Loss: 4.134144546696916e-05, Learning Rate: 0.000227\n",
      "Epoch 18144/40000, Loss: 1.4665877642983105e-05, Learning Rate: 0.000227\n",
      "Epoch 18145/40000, Loss: 5.495249934028834e-05, Learning Rate: 0.000227\n",
      "Epoch 18146/40000, Loss: 1.5332439943449572e-05, Learning Rate: 0.000227\n",
      "Epoch 18147/40000, Loss: 3.704413757077418e-05, Learning Rate: 0.000227\n",
      "Epoch 18148/40000, Loss: 4.182877455605194e-05, Learning Rate: 0.000227\n",
      "Epoch 18149/40000, Loss: 1.4787287000217475e-05, Learning Rate: 0.000227\n",
      "Epoch 18150/40000, Loss: 5.485736619448289e-05, Learning Rate: 0.000227\n",
      "Epoch 18151/40000, Loss: 4.1519884689478204e-05, Learning Rate: 0.000227\n",
      "Epoch 18152/40000, Loss: 4.131358218728565e-05, Learning Rate: 0.000226\n",
      "Epoch 18153/40000, Loss: 3.282864417997189e-05, Learning Rate: 0.000226\n",
      "Epoch 18154/40000, Loss: 4.153833651798777e-05, Learning Rate: 0.000226\n",
      "Epoch 18155/40000, Loss: 6.22502775513567e-05, Learning Rate: 0.000226\n",
      "Epoch 18156/40000, Loss: 3.311343607492745e-05, Learning Rate: 0.000226\n",
      "Epoch 18157/40000, Loss: 3.314508649054915e-05, Learning Rate: 0.000226\n",
      "Epoch 18158/40000, Loss: 3.741058026207611e-05, Learning Rate: 0.000226\n",
      "Epoch 18159/40000, Loss: 6.285675044637173e-05, Learning Rate: 0.000226\n",
      "Epoch 18160/40000, Loss: 3.738547093234956e-05, Learning Rate: 0.000226\n",
      "Epoch 18161/40000, Loss: 5.627028440358117e-05, Learning Rate: 0.000226\n",
      "Epoch 18162/40000, Loss: 4.28584280598443e-05, Learning Rate: 0.000226\n",
      "Epoch 18163/40000, Loss: 3.4610355214681476e-05, Learning Rate: 0.000226\n",
      "Epoch 18164/40000, Loss: 3.8064004911575466e-05, Learning Rate: 0.000226\n",
      "Epoch 18165/40000, Loss: 4.359906233730726e-05, Learning Rate: 0.000226\n",
      "Epoch 18166/40000, Loss: 3.5249940992798656e-05, Learning Rate: 0.000226\n",
      "Epoch 18167/40000, Loss: 3.311409818707034e-05, Learning Rate: 0.000226\n",
      "Epoch 18168/40000, Loss: 1.6175243217730895e-05, Learning Rate: 0.000226\n",
      "Epoch 18169/40000, Loss: 4.311362499720417e-05, Learning Rate: 0.000226\n",
      "Epoch 18170/40000, Loss: 5.744054578826763e-05, Learning Rate: 0.000226\n",
      "Epoch 18171/40000, Loss: 3.370855483808555e-05, Learning Rate: 0.000226\n",
      "Epoch 18172/40000, Loss: 1.700926986814011e-05, Learning Rate: 0.000226\n",
      "Epoch 18173/40000, Loss: 3.3242122299270704e-05, Learning Rate: 0.000226\n",
      "Epoch 18174/40000, Loss: 3.336984809720889e-05, Learning Rate: 0.000226\n",
      "Epoch 18175/40000, Loss: 3.750277028302662e-05, Learning Rate: 0.000226\n",
      "Epoch 18176/40000, Loss: 3.420373104745522e-05, Learning Rate: 0.000226\n",
      "Epoch 18177/40000, Loss: 1.5232812074827962e-05, Learning Rate: 0.000226\n",
      "Epoch 18178/40000, Loss: 3.286724313511513e-05, Learning Rate: 0.000226\n",
      "Epoch 18179/40000, Loss: 3.753127020900138e-05, Learning Rate: 0.000226\n",
      "Epoch 18180/40000, Loss: 6.405165186151862e-05, Learning Rate: 0.000226\n",
      "Epoch 18181/40000, Loss: 6.384012522175908e-05, Learning Rate: 0.000226\n",
      "Epoch 18182/40000, Loss: 3.393342194613069e-05, Learning Rate: 0.000226\n",
      "Epoch 18183/40000, Loss: 1.5969391824910417e-05, Learning Rate: 0.000226\n",
      "Epoch 18184/40000, Loss: 5.780311767011881e-05, Learning Rate: 0.000226\n",
      "Epoch 18185/40000, Loss: 3.4389224310871214e-05, Learning Rate: 0.000226\n",
      "Epoch 18186/40000, Loss: 3.997132807853632e-05, Learning Rate: 0.000226\n",
      "Epoch 18187/40000, Loss: 5.8293549955124035e-05, Learning Rate: 0.000226\n",
      "Epoch 18188/40000, Loss: 4.6151919377734885e-05, Learning Rate: 0.000226\n",
      "Epoch 18189/40000, Loss: 3.4644042898435146e-05, Learning Rate: 0.000225\n",
      "Epoch 18190/40000, Loss: 4.538624489214271e-05, Learning Rate: 0.000225\n",
      "Epoch 18191/40000, Loss: 1.678944317973219e-05, Learning Rate: 0.000225\n",
      "Epoch 18192/40000, Loss: 6.652232696069404e-05, Learning Rate: 0.000225\n",
      "Epoch 18193/40000, Loss: 1.7699974705465138e-05, Learning Rate: 0.000225\n",
      "Epoch 18194/40000, Loss: 3.3691783755784854e-05, Learning Rate: 0.000225\n",
      "Epoch 18195/40000, Loss: 3.784691580221988e-05, Learning Rate: 0.000225\n",
      "Epoch 18196/40000, Loss: 6.446889165090397e-05, Learning Rate: 0.000225\n",
      "Epoch 18197/40000, Loss: 1.567963590787258e-05, Learning Rate: 0.000225\n",
      "Epoch 18198/40000, Loss: 3.3018386602634564e-05, Learning Rate: 0.000225\n",
      "Epoch 18199/40000, Loss: 4.158486990490928e-05, Learning Rate: 0.000225\n",
      "Epoch 18200/40000, Loss: 6.40817015664652e-05, Learning Rate: 0.000225\n",
      "Epoch 18201/40000, Loss: 3.757773447432555e-05, Learning Rate: 0.000225\n",
      "Epoch 18202/40000, Loss: 5.517890167539008e-05, Learning Rate: 0.000225\n",
      "Epoch 18203/40000, Loss: 6.304723501671106e-05, Learning Rate: 0.000225\n",
      "Epoch 18204/40000, Loss: 1.4989908777351957e-05, Learning Rate: 0.000225\n",
      "Epoch 18205/40000, Loss: 5.56118757231161e-05, Learning Rate: 0.000225\n",
      "Epoch 18206/40000, Loss: 5.511301424121484e-05, Learning Rate: 0.000225\n",
      "Epoch 18207/40000, Loss: 3.7315243389457464e-05, Learning Rate: 0.000225\n",
      "Epoch 18208/40000, Loss: 5.517177487490699e-05, Learning Rate: 0.000225\n",
      "Epoch 18209/40000, Loss: 3.2505355193279684e-05, Learning Rate: 0.000225\n",
      "Epoch 18210/40000, Loss: 3.69167210010346e-05, Learning Rate: 0.000225\n",
      "Epoch 18211/40000, Loss: 3.288436710136011e-05, Learning Rate: 0.000225\n",
      "Epoch 18212/40000, Loss: 1.4698328413942363e-05, Learning Rate: 0.000225\n",
      "Epoch 18213/40000, Loss: 5.489937757374719e-05, Learning Rate: 0.000225\n",
      "Epoch 18214/40000, Loss: 3.259538789279759e-05, Learning Rate: 0.000225\n",
      "Epoch 18215/40000, Loss: 4.13706547988113e-05, Learning Rate: 0.000225\n",
      "Epoch 18216/40000, Loss: 5.48049429198727e-05, Learning Rate: 0.000225\n",
      "Epoch 18217/40000, Loss: 5.480398249346763e-05, Learning Rate: 0.000225\n",
      "Epoch 18218/40000, Loss: 5.478266757563688e-05, Learning Rate: 0.000225\n",
      "Epoch 18219/40000, Loss: 4.1322429751744494e-05, Learning Rate: 0.000225\n",
      "Epoch 18220/40000, Loss: 3.685965566546656e-05, Learning Rate: 0.000225\n",
      "Epoch 18221/40000, Loss: 3.6756664485437796e-05, Learning Rate: 0.000225\n",
      "Epoch 18222/40000, Loss: 4.11421169701498e-05, Learning Rate: 0.000225\n",
      "Epoch 18223/40000, Loss: 1.4619075955124572e-05, Learning Rate: 0.000225\n",
      "Epoch 18224/40000, Loss: 3.6840214306721464e-05, Learning Rate: 0.000225\n",
      "Epoch 18225/40000, Loss: 1.4781314348510932e-05, Learning Rate: 0.000225\n",
      "Epoch 18226/40000, Loss: 4.2109681089641526e-05, Learning Rate: 0.000224\n",
      "Epoch 18227/40000, Loss: 3.2563646527705714e-05, Learning Rate: 0.000224\n",
      "Epoch 18228/40000, Loss: 3.2651096262270585e-05, Learning Rate: 0.000224\n",
      "Epoch 18229/40000, Loss: 4.1257295379182324e-05, Learning Rate: 0.000224\n",
      "Epoch 18230/40000, Loss: 6.217933696461841e-05, Learning Rate: 0.000224\n",
      "Epoch 18231/40000, Loss: 6.22077495791018e-05, Learning Rate: 0.000224\n",
      "Epoch 18232/40000, Loss: 1.4750181435374543e-05, Learning Rate: 0.000224\n",
      "Epoch 18233/40000, Loss: 4.1323881305288523e-05, Learning Rate: 0.000224\n",
      "Epoch 18234/40000, Loss: 5.534164665732533e-05, Learning Rate: 0.000224\n",
      "Epoch 18235/40000, Loss: 5.530196722247638e-05, Learning Rate: 0.000224\n",
      "Epoch 18236/40000, Loss: 5.550933929043822e-05, Learning Rate: 0.000224\n",
      "Epoch 18237/40000, Loss: 6.491279054898769e-05, Learning Rate: 0.000224\n",
      "Epoch 18238/40000, Loss: 1.4831224689260125e-05, Learning Rate: 0.000224\n",
      "Epoch 18239/40000, Loss: 5.6050335842883214e-05, Learning Rate: 0.000224\n",
      "Epoch 18240/40000, Loss: 6.430250505218282e-05, Learning Rate: 0.000224\n",
      "Epoch 18241/40000, Loss: 5.957064058748074e-05, Learning Rate: 0.000224\n",
      "Epoch 18242/40000, Loss: 6.499431765405461e-05, Learning Rate: 0.000224\n",
      "Epoch 18243/40000, Loss: 6.425940955523401e-05, Learning Rate: 0.000224\n",
      "Epoch 18244/40000, Loss: 6.298584776232019e-05, Learning Rate: 0.000224\n",
      "Epoch 18245/40000, Loss: 5.7032619224628434e-05, Learning Rate: 0.000224\n",
      "Epoch 18246/40000, Loss: 3.3534906833665445e-05, Learning Rate: 0.000224\n",
      "Epoch 18247/40000, Loss: 3.799211117438972e-05, Learning Rate: 0.000224\n",
      "Epoch 18248/40000, Loss: 5.745271482737735e-05, Learning Rate: 0.000224\n",
      "Epoch 18249/40000, Loss: 4.117753996979445e-05, Learning Rate: 0.000224\n",
      "Epoch 18250/40000, Loss: 4.3630316213238984e-05, Learning Rate: 0.000224\n",
      "Epoch 18251/40000, Loss: 1.705080285319127e-05, Learning Rate: 0.000224\n",
      "Epoch 18252/40000, Loss: 5.8523630286799744e-05, Learning Rate: 0.000224\n",
      "Epoch 18253/40000, Loss: 3.7983074435032904e-05, Learning Rate: 0.000224\n",
      "Epoch 18254/40000, Loss: 6.381333514582366e-05, Learning Rate: 0.000224\n",
      "Epoch 18255/40000, Loss: 5.843176040798426e-05, Learning Rate: 0.000224\n",
      "Epoch 18256/40000, Loss: 3.846513573080301e-05, Learning Rate: 0.000224\n",
      "Epoch 18257/40000, Loss: 6.388370820786804e-05, Learning Rate: 0.000224\n",
      "Epoch 18258/40000, Loss: 5.867172512807883e-05, Learning Rate: 0.000224\n",
      "Epoch 18259/40000, Loss: 3.8140588003443554e-05, Learning Rate: 0.000224\n",
      "Epoch 18260/40000, Loss: 5.6891116400947794e-05, Learning Rate: 0.000224\n",
      "Epoch 18261/40000, Loss: 5.596013943431899e-05, Learning Rate: 0.000224\n",
      "Epoch 18262/40000, Loss: 3.910118175554089e-05, Learning Rate: 0.000224\n",
      "Epoch 18263/40000, Loss: 5.74666446482297e-05, Learning Rate: 0.000223\n",
      "Epoch 18264/40000, Loss: 4.02150399168022e-05, Learning Rate: 0.000223\n",
      "Epoch 18265/40000, Loss: 4.3401563743827865e-05, Learning Rate: 0.000223\n",
      "Epoch 18266/40000, Loss: 6.091744580771774e-05, Learning Rate: 0.000223\n",
      "Epoch 18267/40000, Loss: 4.6282450057333335e-05, Learning Rate: 0.000223\n",
      "Epoch 18268/40000, Loss: 3.791986091528088e-05, Learning Rate: 0.000223\n",
      "Epoch 18269/40000, Loss: 4.599908061209135e-05, Learning Rate: 0.000223\n",
      "Epoch 18270/40000, Loss: 3.8546430005226284e-05, Learning Rate: 0.000223\n",
      "Epoch 18271/40000, Loss: 5.698201493942179e-05, Learning Rate: 0.000223\n",
      "Epoch 18272/40000, Loss: 4.026647729915567e-05, Learning Rate: 0.000223\n",
      "Epoch 18273/40000, Loss: 4.5564571337308735e-05, Learning Rate: 0.000223\n",
      "Epoch 18274/40000, Loss: 4.3297637603245676e-05, Learning Rate: 0.000223\n",
      "Epoch 18275/40000, Loss: 3.4926073567476124e-05, Learning Rate: 0.000223\n",
      "Epoch 18276/40000, Loss: 1.7827584088081494e-05, Learning Rate: 0.000223\n",
      "Epoch 18277/40000, Loss: 6.501381722046062e-05, Learning Rate: 0.000223\n",
      "Epoch 18278/40000, Loss: 3.3957050618482754e-05, Learning Rate: 0.000223\n",
      "Epoch 18279/40000, Loss: 4.134923074161634e-05, Learning Rate: 0.000223\n",
      "Epoch 18280/40000, Loss: 6.535018474096432e-05, Learning Rate: 0.000223\n",
      "Epoch 18281/40000, Loss: 1.75291806954192e-05, Learning Rate: 0.000223\n",
      "Epoch 18282/40000, Loss: 6.414039671653882e-05, Learning Rate: 0.000223\n",
      "Epoch 18283/40000, Loss: 1.5516383427893743e-05, Learning Rate: 0.000223\n",
      "Epoch 18284/40000, Loss: 6.439588469220325e-05, Learning Rate: 0.000223\n",
      "Epoch 18285/40000, Loss: 6.267177377594635e-05, Learning Rate: 0.000223\n",
      "Epoch 18286/40000, Loss: 3.768608803511597e-05, Learning Rate: 0.000223\n",
      "Epoch 18287/40000, Loss: 5.5421183787984774e-05, Learning Rate: 0.000223\n",
      "Epoch 18288/40000, Loss: 3.7179306673351675e-05, Learning Rate: 0.000223\n",
      "Epoch 18289/40000, Loss: 5.530626731342636e-05, Learning Rate: 0.000223\n",
      "Epoch 18290/40000, Loss: 3.805688174907118e-05, Learning Rate: 0.000223\n",
      "Epoch 18291/40000, Loss: 3.697056308737956e-05, Learning Rate: 0.000223\n",
      "Epoch 18292/40000, Loss: 3.254035982536152e-05, Learning Rate: 0.000223\n",
      "Epoch 18293/40000, Loss: 5.488945316756144e-05, Learning Rate: 0.000223\n",
      "Epoch 18294/40000, Loss: 6.195616879267618e-05, Learning Rate: 0.000223\n",
      "Epoch 18295/40000, Loss: 6.17802725173533e-05, Learning Rate: 0.000223\n",
      "Epoch 18296/40000, Loss: 4.1506835259497166e-05, Learning Rate: 0.000223\n",
      "Epoch 18297/40000, Loss: 6.199530616868287e-05, Learning Rate: 0.000223\n",
      "Epoch 18298/40000, Loss: 5.483886707224883e-05, Learning Rate: 0.000223\n",
      "Epoch 18299/40000, Loss: 6.188201950863004e-05, Learning Rate: 0.000223\n",
      "Epoch 18300/40000, Loss: 6.181310163810849e-05, Learning Rate: 0.000222\n",
      "Epoch 18301/40000, Loss: 3.239157740608789e-05, Learning Rate: 0.000222\n",
      "Epoch 18302/40000, Loss: 1.4441166968026664e-05, Learning Rate: 0.000222\n",
      "Epoch 18303/40000, Loss: 5.481851258082315e-05, Learning Rate: 0.000222\n",
      "Epoch 18304/40000, Loss: 6.169908010633662e-05, Learning Rate: 0.000222\n",
      "Epoch 18305/40000, Loss: 3.682867463794537e-05, Learning Rate: 0.000222\n",
      "Epoch 18306/40000, Loss: 3.679288784042001e-05, Learning Rate: 0.000222\n",
      "Epoch 18307/40000, Loss: 4.119813092984259e-05, Learning Rate: 0.000222\n",
      "Epoch 18308/40000, Loss: 3.6794830521102995e-05, Learning Rate: 0.000222\n",
      "Epoch 18309/40000, Loss: 4.161139440839179e-05, Learning Rate: 0.000222\n",
      "Epoch 18310/40000, Loss: 5.500668703461997e-05, Learning Rate: 0.000222\n",
      "Epoch 18311/40000, Loss: 3.240478690713644e-05, Learning Rate: 0.000222\n",
      "Epoch 18312/40000, Loss: 6.186301470734179e-05, Learning Rate: 0.000222\n",
      "Epoch 18313/40000, Loss: 3.675670814118348e-05, Learning Rate: 0.000222\n",
      "Epoch 18314/40000, Loss: 6.237265915842727e-05, Learning Rate: 0.000222\n",
      "Epoch 18315/40000, Loss: 3.70713860320393e-05, Learning Rate: 0.000222\n",
      "Epoch 18316/40000, Loss: 3.6769313737750053e-05, Learning Rate: 0.000222\n",
      "Epoch 18317/40000, Loss: 4.121832171222195e-05, Learning Rate: 0.000222\n",
      "Epoch 18318/40000, Loss: 5.502071508090012e-05, Learning Rate: 0.000222\n",
      "Epoch 18319/40000, Loss: 5.469025200000033e-05, Learning Rate: 0.000222\n",
      "Epoch 18320/40000, Loss: 4.1108662117039785e-05, Learning Rate: 0.000222\n",
      "Epoch 18321/40000, Loss: 4.1004335798788816e-05, Learning Rate: 0.000222\n",
      "Epoch 18322/40000, Loss: 3.228414061595686e-05, Learning Rate: 0.000222\n",
      "Epoch 18323/40000, Loss: 6.189298437675461e-05, Learning Rate: 0.000222\n",
      "Epoch 18324/40000, Loss: 3.672849197755568e-05, Learning Rate: 0.000222\n",
      "Epoch 18325/40000, Loss: 3.674537219922058e-05, Learning Rate: 0.000222\n",
      "Epoch 18326/40000, Loss: 3.235309122828767e-05, Learning Rate: 0.000222\n",
      "Epoch 18327/40000, Loss: 4.113014438189566e-05, Learning Rate: 0.000222\n",
      "Epoch 18328/40000, Loss: 5.501787381945178e-05, Learning Rate: 0.000222\n",
      "Epoch 18329/40000, Loss: 1.4603740055463277e-05, Learning Rate: 0.000222\n",
      "Epoch 18330/40000, Loss: 5.497015081346035e-05, Learning Rate: 0.000222\n",
      "Epoch 18331/40000, Loss: 4.1127612348645926e-05, Learning Rate: 0.000222\n",
      "Epoch 18332/40000, Loss: 4.136041025049053e-05, Learning Rate: 0.000222\n",
      "Epoch 18333/40000, Loss: 3.251216185162775e-05, Learning Rate: 0.000222\n",
      "Epoch 18334/40000, Loss: 6.196153844939545e-05, Learning Rate: 0.000222\n",
      "Epoch 18335/40000, Loss: 5.492337004397996e-05, Learning Rate: 0.000222\n",
      "Epoch 18336/40000, Loss: 1.4936520528863184e-05, Learning Rate: 0.000222\n",
      "Epoch 18337/40000, Loss: 6.202571239555255e-05, Learning Rate: 0.000222\n",
      "Epoch 18338/40000, Loss: 3.705790004460141e-05, Learning Rate: 0.000221\n",
      "Epoch 18339/40000, Loss: 3.2405117963207886e-05, Learning Rate: 0.000221\n",
      "Epoch 18340/40000, Loss: 3.6997713323216885e-05, Learning Rate: 0.000221\n",
      "Epoch 18341/40000, Loss: 5.4745745728723705e-05, Learning Rate: 0.000221\n",
      "Epoch 18342/40000, Loss: 3.695975101436488e-05, Learning Rate: 0.000221\n",
      "Epoch 18343/40000, Loss: 3.695919076562859e-05, Learning Rate: 0.000221\n",
      "Epoch 18344/40000, Loss: 3.6807938158744946e-05, Learning Rate: 0.000221\n",
      "Epoch 18345/40000, Loss: 1.52642787725199e-05, Learning Rate: 0.000221\n",
      "Epoch 18346/40000, Loss: 1.4643566828453913e-05, Learning Rate: 0.000221\n",
      "Epoch 18347/40000, Loss: 3.806001404882409e-05, Learning Rate: 0.000221\n",
      "Epoch 18348/40000, Loss: 3.7890495150350034e-05, Learning Rate: 0.000221\n",
      "Epoch 18349/40000, Loss: 4.2186125938314945e-05, Learning Rate: 0.000221\n",
      "Epoch 18350/40000, Loss: 6.255898915696889e-05, Learning Rate: 0.000221\n",
      "Epoch 18351/40000, Loss: 5.806317494716495e-05, Learning Rate: 0.000221\n",
      "Epoch 18352/40000, Loss: 3.331001425976865e-05, Learning Rate: 0.000221\n",
      "Epoch 18353/40000, Loss: 3.7704798160120845e-05, Learning Rate: 0.000221\n",
      "Epoch 18354/40000, Loss: 5.555909592658281e-05, Learning Rate: 0.000221\n",
      "Epoch 18355/40000, Loss: 3.264129918534309e-05, Learning Rate: 0.000221\n",
      "Epoch 18356/40000, Loss: 3.255382034694776e-05, Learning Rate: 0.000221\n",
      "Epoch 18357/40000, Loss: 6.304276030277833e-05, Learning Rate: 0.000221\n",
      "Epoch 18358/40000, Loss: 1.490345130150672e-05, Learning Rate: 0.000221\n",
      "Epoch 18359/40000, Loss: 7.058986375341192e-05, Learning Rate: 0.000221\n",
      "Epoch 18360/40000, Loss: 3.32093914039433e-05, Learning Rate: 0.000221\n",
      "Epoch 18361/40000, Loss: 1.5027086192276329e-05, Learning Rate: 0.000221\n",
      "Epoch 18362/40000, Loss: 1.5004086890257895e-05, Learning Rate: 0.000221\n",
      "Epoch 18363/40000, Loss: 5.5591328418813646e-05, Learning Rate: 0.000221\n",
      "Epoch 18364/40000, Loss: 4.170282409177162e-05, Learning Rate: 0.000221\n",
      "Epoch 18365/40000, Loss: 4.2005307477666065e-05, Learning Rate: 0.000221\n",
      "Epoch 18366/40000, Loss: 5.6449796829838306e-05, Learning Rate: 0.000221\n",
      "Epoch 18367/40000, Loss: 5.536673779715784e-05, Learning Rate: 0.000221\n",
      "Epoch 18368/40000, Loss: 6.234377360669896e-05, Learning Rate: 0.000221\n",
      "Epoch 18369/40000, Loss: 3.275700510130264e-05, Learning Rate: 0.000221\n",
      "Epoch 18370/40000, Loss: 3.710937380674295e-05, Learning Rate: 0.000221\n",
      "Epoch 18371/40000, Loss: 4.2949613998644054e-05, Learning Rate: 0.000221\n",
      "Epoch 18372/40000, Loss: 1.526746927993372e-05, Learning Rate: 0.000221\n",
      "Epoch 18373/40000, Loss: 3.2938940421445295e-05, Learning Rate: 0.000221\n",
      "Epoch 18374/40000, Loss: 3.271827881690115e-05, Learning Rate: 0.000221\n",
      "Epoch 18375/40000, Loss: 4.195149813313037e-05, Learning Rate: 0.000220\n",
      "Epoch 18376/40000, Loss: 6.318785744952038e-05, Learning Rate: 0.000220\n",
      "Epoch 18377/40000, Loss: 4.407540836837143e-05, Learning Rate: 0.000220\n",
      "Epoch 18378/40000, Loss: 6.271644087973982e-05, Learning Rate: 0.000220\n",
      "Epoch 18379/40000, Loss: 4.379218444228172e-05, Learning Rate: 0.000220\n",
      "Epoch 18380/40000, Loss: 3.7807072658324614e-05, Learning Rate: 0.000220\n",
      "Epoch 18381/40000, Loss: 1.5590592738590203e-05, Learning Rate: 0.000220\n",
      "Epoch 18382/40000, Loss: 6.399306585080922e-05, Learning Rate: 0.000220\n",
      "Epoch 18383/40000, Loss: 3.766849113162607e-05, Learning Rate: 0.000220\n",
      "Epoch 18384/40000, Loss: 6.531166582135484e-05, Learning Rate: 0.000220\n",
      "Epoch 18385/40000, Loss: 5.6036849855445325e-05, Learning Rate: 0.000220\n",
      "Epoch 18386/40000, Loss: 3.367631870787591e-05, Learning Rate: 0.000220\n",
      "Epoch 18387/40000, Loss: 6.291185127338395e-05, Learning Rate: 0.000220\n",
      "Epoch 18388/40000, Loss: 6.365374429151416e-05, Learning Rate: 0.000220\n",
      "Epoch 18389/40000, Loss: 6.346133159240708e-05, Learning Rate: 0.000220\n",
      "Epoch 18390/40000, Loss: 3.5973502235719934e-05, Learning Rate: 0.000220\n",
      "Epoch 18391/40000, Loss: 3.473547621979378e-05, Learning Rate: 0.000220\n",
      "Epoch 18392/40000, Loss: 6.641438085353002e-05, Learning Rate: 0.000220\n",
      "Epoch 18393/40000, Loss: 4.564281698549166e-05, Learning Rate: 0.000220\n",
      "Epoch 18394/40000, Loss: 4.3215015466557816e-05, Learning Rate: 0.000220\n",
      "Epoch 18395/40000, Loss: 4.2001942347269505e-05, Learning Rate: 0.000220\n",
      "Epoch 18396/40000, Loss: 4.764752520713955e-05, Learning Rate: 0.000220\n",
      "Epoch 18397/40000, Loss: 6.233139720279723e-05, Learning Rate: 0.000220\n",
      "Epoch 18398/40000, Loss: 2.1329007722670212e-05, Learning Rate: 0.000220\n",
      "Epoch 18399/40000, Loss: 6.568620301550254e-05, Learning Rate: 0.000220\n",
      "Epoch 18400/40000, Loss: 6.230045255506411e-05, Learning Rate: 0.000220\n",
      "Epoch 18401/40000, Loss: 6.265891715884209e-05, Learning Rate: 0.000220\n",
      "Epoch 18402/40000, Loss: 4.6070159442024305e-05, Learning Rate: 0.000220\n",
      "Epoch 18403/40000, Loss: 3.467443093541078e-05, Learning Rate: 0.000220\n",
      "Epoch 18404/40000, Loss: 1.7707889128359966e-05, Learning Rate: 0.000220\n",
      "Epoch 18405/40000, Loss: 4.634823199012317e-05, Learning Rate: 0.000220\n",
      "Epoch 18406/40000, Loss: 4.324537076172419e-05, Learning Rate: 0.000220\n",
      "Epoch 18407/40000, Loss: 4.30719337600749e-05, Learning Rate: 0.000220\n",
      "Epoch 18408/40000, Loss: 6.230076542124152e-05, Learning Rate: 0.000220\n",
      "Epoch 18409/40000, Loss: 3.6033281503478065e-05, Learning Rate: 0.000220\n",
      "Epoch 18410/40000, Loss: 6.439635762944818e-05, Learning Rate: 0.000220\n",
      "Epoch 18411/40000, Loss: 5.661052273353562e-05, Learning Rate: 0.000220\n",
      "Epoch 18412/40000, Loss: 5.681428956449963e-05, Learning Rate: 0.000220\n",
      "Epoch 18413/40000, Loss: 6.266255513764918e-05, Learning Rate: 0.000219\n",
      "Epoch 18414/40000, Loss: 1.4738034224137664e-05, Learning Rate: 0.000219\n",
      "Epoch 18415/40000, Loss: 6.330504402285442e-05, Learning Rate: 0.000219\n",
      "Epoch 18416/40000, Loss: 5.602044620900415e-05, Learning Rate: 0.000219\n",
      "Epoch 18417/40000, Loss: 6.221816875040531e-05, Learning Rate: 0.000219\n",
      "Epoch 18418/40000, Loss: 5.86487258260604e-05, Learning Rate: 0.000219\n",
      "Epoch 18419/40000, Loss: 1.4881706192682032e-05, Learning Rate: 0.000219\n",
      "Epoch 18420/40000, Loss: 3.699161243275739e-05, Learning Rate: 0.000219\n",
      "Epoch 18421/40000, Loss: 4.1410541598452255e-05, Learning Rate: 0.000219\n",
      "Epoch 18422/40000, Loss: 5.631197564071044e-05, Learning Rate: 0.000219\n",
      "Epoch 18423/40000, Loss: 3.258964716224e-05, Learning Rate: 0.000219\n",
      "Epoch 18424/40000, Loss: 3.719405867741443e-05, Learning Rate: 0.000219\n",
      "Epoch 18425/40000, Loss: 6.290125020314008e-05, Learning Rate: 0.000219\n",
      "Epoch 18426/40000, Loss: 5.710710320272483e-05, Learning Rate: 0.000219\n",
      "Epoch 18427/40000, Loss: 3.2866515539353713e-05, Learning Rate: 0.000219\n",
      "Epoch 18428/40000, Loss: 5.875479837413877e-05, Learning Rate: 0.000219\n",
      "Epoch 18429/40000, Loss: 1.5347626685979776e-05, Learning Rate: 0.000219\n",
      "Epoch 18430/40000, Loss: 1.538612923468463e-05, Learning Rate: 0.000219\n",
      "Epoch 18431/40000, Loss: 4.189432365819812e-05, Learning Rate: 0.000219\n",
      "Epoch 18432/40000, Loss: 3.811729038716294e-05, Learning Rate: 0.000219\n",
      "Epoch 18433/40000, Loss: 3.75642157450784e-05, Learning Rate: 0.000219\n",
      "Epoch 18434/40000, Loss: 6.235705950530246e-05, Learning Rate: 0.000219\n",
      "Epoch 18435/40000, Loss: 4.207733945804648e-05, Learning Rate: 0.000219\n",
      "Epoch 18436/40000, Loss: 1.5129644452827051e-05, Learning Rate: 0.000219\n",
      "Epoch 18437/40000, Loss: 3.265388295403682e-05, Learning Rate: 0.000219\n",
      "Epoch 18438/40000, Loss: 1.6696872989996336e-05, Learning Rate: 0.000219\n",
      "Epoch 18439/40000, Loss: 6.224780372576788e-05, Learning Rate: 0.000219\n",
      "Epoch 18440/40000, Loss: 4.265109237167053e-05, Learning Rate: 0.000219\n",
      "Epoch 18441/40000, Loss: 6.199903873493895e-05, Learning Rate: 0.000219\n",
      "Epoch 18442/40000, Loss: 3.697185456985608e-05, Learning Rate: 0.000219\n",
      "Epoch 18443/40000, Loss: 3.6964163882657886e-05, Learning Rate: 0.000219\n",
      "Epoch 18444/40000, Loss: 4.201263072900474e-05, Learning Rate: 0.000219\n",
      "Epoch 18445/40000, Loss: 5.500581028172746e-05, Learning Rate: 0.000219\n",
      "Epoch 18446/40000, Loss: 3.272431422374211e-05, Learning Rate: 0.000219\n",
      "Epoch 18447/40000, Loss: 4.189287210465409e-05, Learning Rate: 0.000219\n",
      "Epoch 18448/40000, Loss: 6.223901436896995e-05, Learning Rate: 0.000219\n",
      "Epoch 18449/40000, Loss: 1.5075121154950466e-05, Learning Rate: 0.000219\n",
      "Epoch 18450/40000, Loss: 3.695359191624448e-05, Learning Rate: 0.000219\n",
      "Epoch 18451/40000, Loss: 3.6894514778396115e-05, Learning Rate: 0.000218\n",
      "Epoch 18452/40000, Loss: 3.7127530958969146e-05, Learning Rate: 0.000218\n",
      "Epoch 18453/40000, Loss: 1.555279413878452e-05, Learning Rate: 0.000218\n",
      "Epoch 18454/40000, Loss: 4.194394205114804e-05, Learning Rate: 0.000218\n",
      "Epoch 18455/40000, Loss: 6.215715256985277e-05, Learning Rate: 0.000218\n",
      "Epoch 18456/40000, Loss: 5.50781624042429e-05, Learning Rate: 0.000218\n",
      "Epoch 18457/40000, Loss: 5.522701394511387e-05, Learning Rate: 0.000218\n",
      "Epoch 18458/40000, Loss: 3.240610385546461e-05, Learning Rate: 0.000218\n",
      "Epoch 18459/40000, Loss: 3.2870077120605856e-05, Learning Rate: 0.000218\n",
      "Epoch 18460/40000, Loss: 4.1650175262475386e-05, Learning Rate: 0.000218\n",
      "Epoch 18461/40000, Loss: 3.239511715946719e-05, Learning Rate: 0.000218\n",
      "Epoch 18462/40000, Loss: 4.1838011384243146e-05, Learning Rate: 0.000218\n",
      "Epoch 18463/40000, Loss: 1.5160228940658271e-05, Learning Rate: 0.000218\n",
      "Epoch 18464/40000, Loss: 5.548063927562907e-05, Learning Rate: 0.000218\n",
      "Epoch 18465/40000, Loss: 1.5684820027672686e-05, Learning Rate: 0.000218\n",
      "Epoch 18466/40000, Loss: 1.4705675312143285e-05, Learning Rate: 0.000218\n",
      "Epoch 18467/40000, Loss: 6.294382183114067e-05, Learning Rate: 0.000218\n",
      "Epoch 18468/40000, Loss: 4.351948155090213e-05, Learning Rate: 0.000218\n",
      "Epoch 18469/40000, Loss: 4.183037162874825e-05, Learning Rate: 0.000218\n",
      "Epoch 18470/40000, Loss: 1.4697273400088307e-05, Learning Rate: 0.000218\n",
      "Epoch 18471/40000, Loss: 3.255641786381602e-05, Learning Rate: 0.000218\n",
      "Epoch 18472/40000, Loss: 3.711843237397261e-05, Learning Rate: 0.000218\n",
      "Epoch 18473/40000, Loss: 6.166483944980428e-05, Learning Rate: 0.000218\n",
      "Epoch 18474/40000, Loss: 6.15419412497431e-05, Learning Rate: 0.000218\n",
      "Epoch 18475/40000, Loss: 3.246638516429812e-05, Learning Rate: 0.000218\n",
      "Epoch 18476/40000, Loss: 6.180755735840648e-05, Learning Rate: 0.000218\n",
      "Epoch 18477/40000, Loss: 6.165047670947388e-05, Learning Rate: 0.000218\n",
      "Epoch 18478/40000, Loss: 5.493421122082509e-05, Learning Rate: 0.000218\n",
      "Epoch 18479/40000, Loss: 3.6863984860247e-05, Learning Rate: 0.000218\n",
      "Epoch 18480/40000, Loss: 1.4535967238771264e-05, Learning Rate: 0.000218\n",
      "Epoch 18481/40000, Loss: 4.1304116166429594e-05, Learning Rate: 0.000218\n",
      "Epoch 18482/40000, Loss: 3.225938053219579e-05, Learning Rate: 0.000218\n",
      "Epoch 18483/40000, Loss: 3.661686787381768e-05, Learning Rate: 0.000218\n",
      "Epoch 18484/40000, Loss: 5.446488648885861e-05, Learning Rate: 0.000218\n",
      "Epoch 18485/40000, Loss: 6.162254430819303e-05, Learning Rate: 0.000218\n",
      "Epoch 18486/40000, Loss: 3.6639332392951474e-05, Learning Rate: 0.000218\n",
      "Epoch 18487/40000, Loss: 1.4504093996947631e-05, Learning Rate: 0.000218\n",
      "Epoch 18488/40000, Loss: 6.165408558445051e-05, Learning Rate: 0.000218\n",
      "Epoch 18489/40000, Loss: 3.2234107493422925e-05, Learning Rate: 0.000218\n",
      "Epoch 18490/40000, Loss: 5.4488671594299376e-05, Learning Rate: 0.000217\n",
      "Epoch 18491/40000, Loss: 1.4471470421995036e-05, Learning Rate: 0.000217\n",
      "Epoch 18492/40000, Loss: 1.4311122868093662e-05, Learning Rate: 0.000217\n",
      "Epoch 18493/40000, Loss: 1.434329169569537e-05, Learning Rate: 0.000217\n",
      "Epoch 18494/40000, Loss: 4.083662133780308e-05, Learning Rate: 0.000217\n",
      "Epoch 18495/40000, Loss: 6.143526843516156e-05, Learning Rate: 0.000217\n",
      "Epoch 18496/40000, Loss: 1.4424274922930636e-05, Learning Rate: 0.000217\n",
      "Epoch 18497/40000, Loss: 3.647894482128322e-05, Learning Rate: 0.000217\n",
      "Epoch 18498/40000, Loss: 3.6482753785094246e-05, Learning Rate: 0.000217\n",
      "Epoch 18499/40000, Loss: 5.455648715724237e-05, Learning Rate: 0.000217\n",
      "Epoch 18500/40000, Loss: 3.674383333418518e-05, Learning Rate: 0.000217\n",
      "Epoch 18501/40000, Loss: 3.2230182114290074e-05, Learning Rate: 0.000217\n",
      "Epoch 18502/40000, Loss: 3.2226027542492375e-05, Learning Rate: 0.000217\n",
      "Epoch 18503/40000, Loss: 1.4546480997523759e-05, Learning Rate: 0.000217\n",
      "Epoch 18504/40000, Loss: 6.167146057123318e-05, Learning Rate: 0.000217\n",
      "Epoch 18505/40000, Loss: 4.1326267819385976e-05, Learning Rate: 0.000217\n",
      "Epoch 18506/40000, Loss: 6.154728907858953e-05, Learning Rate: 0.000217\n",
      "Epoch 18507/40000, Loss: 5.4798671044409275e-05, Learning Rate: 0.000217\n",
      "Epoch 18508/40000, Loss: 1.4522190213028807e-05, Learning Rate: 0.000217\n",
      "Epoch 18509/40000, Loss: 4.087560955667868e-05, Learning Rate: 0.000217\n",
      "Epoch 18510/40000, Loss: 1.4441372513829265e-05, Learning Rate: 0.000217\n",
      "Epoch 18511/40000, Loss: 4.1213399526895955e-05, Learning Rate: 0.000217\n",
      "Epoch 18512/40000, Loss: 1.4356427527673077e-05, Learning Rate: 0.000217\n",
      "Epoch 18513/40000, Loss: 6.358593236654997e-05, Learning Rate: 0.000217\n",
      "Epoch 18514/40000, Loss: 3.655053660622798e-05, Learning Rate: 0.000217\n",
      "Epoch 18515/40000, Loss: 1.4526169252349064e-05, Learning Rate: 0.000217\n",
      "Epoch 18516/40000, Loss: 3.25738474202808e-05, Learning Rate: 0.000217\n",
      "Epoch 18517/40000, Loss: 3.6693218135042116e-05, Learning Rate: 0.000217\n",
      "Epoch 18518/40000, Loss: 4.15411195717752e-05, Learning Rate: 0.000217\n",
      "Epoch 18519/40000, Loss: 5.531880015041679e-05, Learning Rate: 0.000217\n",
      "Epoch 18520/40000, Loss: 3.682729948195629e-05, Learning Rate: 0.000217\n",
      "Epoch 18521/40000, Loss: 4.2429877794347703e-05, Learning Rate: 0.000217\n",
      "Epoch 18522/40000, Loss: 3.703898619278334e-05, Learning Rate: 0.000217\n",
      "Epoch 18523/40000, Loss: 1.4925990399206057e-05, Learning Rate: 0.000217\n",
      "Epoch 18524/40000, Loss: 4.19427415181417e-05, Learning Rate: 0.000217\n",
      "Epoch 18525/40000, Loss: 1.5418459952343255e-05, Learning Rate: 0.000217\n",
      "Epoch 18526/40000, Loss: 4.445799640961923e-05, Learning Rate: 0.000217\n",
      "Epoch 18527/40000, Loss: 3.2799740438349545e-05, Learning Rate: 0.000217\n",
      "Epoch 18528/40000, Loss: 3.2754058338468894e-05, Learning Rate: 0.000216\n",
      "Epoch 18529/40000, Loss: 5.6423792557325214e-05, Learning Rate: 0.000216\n",
      "Epoch 18530/40000, Loss: 6.383966683642939e-05, Learning Rate: 0.000216\n",
      "Epoch 18531/40000, Loss: 1.6023817806853913e-05, Learning Rate: 0.000216\n",
      "Epoch 18532/40000, Loss: 1.5652147340006195e-05, Learning Rate: 0.000216\n",
      "Epoch 18533/40000, Loss: 3.74824448954314e-05, Learning Rate: 0.000216\n",
      "Epoch 18534/40000, Loss: 6.307831790763885e-05, Learning Rate: 0.000216\n",
      "Epoch 18535/40000, Loss: 4.2039457184728235e-05, Learning Rate: 0.000216\n",
      "Epoch 18536/40000, Loss: 5.593867172137834e-05, Learning Rate: 0.000216\n",
      "Epoch 18537/40000, Loss: 1.549376975162886e-05, Learning Rate: 0.000216\n",
      "Epoch 18538/40000, Loss: 5.771773066953756e-05, Learning Rate: 0.000216\n",
      "Epoch 18539/40000, Loss: 5.683279596269131e-05, Learning Rate: 0.000216\n",
      "Epoch 18540/40000, Loss: 6.6038963268511e-05, Learning Rate: 0.000216\n",
      "Epoch 18541/40000, Loss: 4.317386992624961e-05, Learning Rate: 0.000216\n",
      "Epoch 18542/40000, Loss: 3.9222617488121614e-05, Learning Rate: 0.000216\n",
      "Epoch 18543/40000, Loss: 3.347081656102091e-05, Learning Rate: 0.000216\n",
      "Epoch 18544/40000, Loss: 4.259380148141645e-05, Learning Rate: 0.000216\n",
      "Epoch 18545/40000, Loss: 1.5928724678815342e-05, Learning Rate: 0.000216\n",
      "Epoch 18546/40000, Loss: 5.697139204130508e-05, Learning Rate: 0.000216\n",
      "Epoch 18547/40000, Loss: 5.560145291383378e-05, Learning Rate: 0.000216\n",
      "Epoch 18548/40000, Loss: 5.7364817621419206e-05, Learning Rate: 0.000216\n",
      "Epoch 18549/40000, Loss: 6.284497067099437e-05, Learning Rate: 0.000216\n",
      "Epoch 18550/40000, Loss: 6.291741010500118e-05, Learning Rate: 0.000216\n",
      "Epoch 18551/40000, Loss: 5.891838372917846e-05, Learning Rate: 0.000216\n",
      "Epoch 18552/40000, Loss: 4.272851219866425e-05, Learning Rate: 0.000216\n",
      "Epoch 18553/40000, Loss: 4.253157749189995e-05, Learning Rate: 0.000216\n",
      "Epoch 18554/40000, Loss: 3.322505654068664e-05, Learning Rate: 0.000216\n",
      "Epoch 18555/40000, Loss: 1.5404209989355877e-05, Learning Rate: 0.000216\n",
      "Epoch 18556/40000, Loss: 4.3501346226548776e-05, Learning Rate: 0.000216\n",
      "Epoch 18557/40000, Loss: 4.293439633329399e-05, Learning Rate: 0.000216\n",
      "Epoch 18558/40000, Loss: 4.2870731704169884e-05, Learning Rate: 0.000216\n",
      "Epoch 18559/40000, Loss: 6.307586590992287e-05, Learning Rate: 0.000216\n",
      "Epoch 18560/40000, Loss: 5.5903165048221126e-05, Learning Rate: 0.000216\n",
      "Epoch 18561/40000, Loss: 3.831272988463752e-05, Learning Rate: 0.000216\n",
      "Epoch 18562/40000, Loss: 6.45009713480249e-05, Learning Rate: 0.000216\n",
      "Epoch 18563/40000, Loss: 3.3462456485722214e-05, Learning Rate: 0.000216\n",
      "Epoch 18564/40000, Loss: 1.4972410099289846e-05, Learning Rate: 0.000216\n",
      "Epoch 18565/40000, Loss: 5.6014421716099605e-05, Learning Rate: 0.000216\n",
      "Epoch 18566/40000, Loss: 4.461586649995297e-05, Learning Rate: 0.000215\n",
      "Epoch 18567/40000, Loss: 3.455452679190785e-05, Learning Rate: 0.000215\n",
      "Epoch 18568/40000, Loss: 4.291816730983555e-05, Learning Rate: 0.000215\n",
      "Epoch 18569/40000, Loss: 3.8288293580990285e-05, Learning Rate: 0.000215\n",
      "Epoch 18570/40000, Loss: 3.351916166138835e-05, Learning Rate: 0.000215\n",
      "Epoch 18571/40000, Loss: 3.832317815977149e-05, Learning Rate: 0.000215\n",
      "Epoch 18572/40000, Loss: 1.6089790733531117e-05, Learning Rate: 0.000215\n",
      "Epoch 18573/40000, Loss: 1.5388972315122373e-05, Learning Rate: 0.000215\n",
      "Epoch 18574/40000, Loss: 4.228825855534524e-05, Learning Rate: 0.000215\n",
      "Epoch 18575/40000, Loss: 1.751415584294591e-05, Learning Rate: 0.000215\n",
      "Epoch 18576/40000, Loss: 5.611732922261581e-05, Learning Rate: 0.000215\n",
      "Epoch 18577/40000, Loss: 6.257750646909699e-05, Learning Rate: 0.000215\n",
      "Epoch 18578/40000, Loss: 4.185415673418902e-05, Learning Rate: 0.000215\n",
      "Epoch 18579/40000, Loss: 6.203508382895961e-05, Learning Rate: 0.000215\n",
      "Epoch 18580/40000, Loss: 6.208573176991194e-05, Learning Rate: 0.000215\n",
      "Epoch 18581/40000, Loss: 3.266270141466521e-05, Learning Rate: 0.000215\n",
      "Epoch 18582/40000, Loss: 3.265714258304797e-05, Learning Rate: 0.000215\n",
      "Epoch 18583/40000, Loss: 1.4754225958313327e-05, Learning Rate: 0.000215\n",
      "Epoch 18584/40000, Loss: 5.5075619457056746e-05, Learning Rate: 0.000215\n",
      "Epoch 18585/40000, Loss: 3.2442872907267883e-05, Learning Rate: 0.000215\n",
      "Epoch 18586/40000, Loss: 4.137395808356814e-05, Learning Rate: 0.000215\n",
      "Epoch 18587/40000, Loss: 3.3067910408135504e-05, Learning Rate: 0.000215\n",
      "Epoch 18588/40000, Loss: 1.5861782230786048e-05, Learning Rate: 0.000215\n",
      "Epoch 18589/40000, Loss: 5.661765317199752e-05, Learning Rate: 0.000215\n",
      "Epoch 18590/40000, Loss: 4.165936115896329e-05, Learning Rate: 0.000215\n",
      "Epoch 18591/40000, Loss: 3.356111119501293e-05, Learning Rate: 0.000215\n",
      "Epoch 18592/40000, Loss: 1.5248783711285796e-05, Learning Rate: 0.000215\n",
      "Epoch 18593/40000, Loss: 1.545397208246868e-05, Learning Rate: 0.000215\n",
      "Epoch 18594/40000, Loss: 6.178989860927686e-05, Learning Rate: 0.000215\n",
      "Epoch 18595/40000, Loss: 4.167771476204507e-05, Learning Rate: 0.000215\n",
      "Epoch 18596/40000, Loss: 4.237582834321074e-05, Learning Rate: 0.000215\n",
      "Epoch 18597/40000, Loss: 6.230347935343161e-05, Learning Rate: 0.000215\n",
      "Epoch 18598/40000, Loss: 6.165147351566702e-05, Learning Rate: 0.000215\n",
      "Epoch 18599/40000, Loss: 1.695778155408334e-05, Learning Rate: 0.000215\n",
      "Epoch 18600/40000, Loss: 3.6856024962617084e-05, Learning Rate: 0.000215\n",
      "Epoch 18601/40000, Loss: 5.527093162527308e-05, Learning Rate: 0.000215\n",
      "Epoch 18602/40000, Loss: 4.1914590838132426e-05, Learning Rate: 0.000215\n",
      "Epoch 18603/40000, Loss: 3.2485302654094994e-05, Learning Rate: 0.000215\n",
      "Epoch 18604/40000, Loss: 3.6824076232733205e-05, Learning Rate: 0.000215\n",
      "Epoch 18605/40000, Loss: 5.516324017662555e-05, Learning Rate: 0.000214\n",
      "Epoch 18606/40000, Loss: 3.7974590668454766e-05, Learning Rate: 0.000214\n",
      "Epoch 18607/40000, Loss: 6.216339534148574e-05, Learning Rate: 0.000214\n",
      "Epoch 18608/40000, Loss: 3.270269007771276e-05, Learning Rate: 0.000214\n",
      "Epoch 18609/40000, Loss: 5.486149893840775e-05, Learning Rate: 0.000214\n",
      "Epoch 18610/40000, Loss: 3.229182766517624e-05, Learning Rate: 0.000214\n",
      "Epoch 18611/40000, Loss: 3.218608253519051e-05, Learning Rate: 0.000214\n",
      "Epoch 18612/40000, Loss: 5.4994197853375226e-05, Learning Rate: 0.000214\n",
      "Epoch 18613/40000, Loss: 5.467966548167169e-05, Learning Rate: 0.000214\n",
      "Epoch 18614/40000, Loss: 4.127958891331218e-05, Learning Rate: 0.000214\n",
      "Epoch 18615/40000, Loss: 3.6691180866910145e-05, Learning Rate: 0.000214\n",
      "Epoch 18616/40000, Loss: 3.229946014471352e-05, Learning Rate: 0.000214\n",
      "Epoch 18617/40000, Loss: 5.512410280061886e-05, Learning Rate: 0.000214\n",
      "Epoch 18618/40000, Loss: 4.1198927647201344e-05, Learning Rate: 0.000214\n",
      "Epoch 18619/40000, Loss: 6.195226887939498e-05, Learning Rate: 0.000214\n",
      "Epoch 18620/40000, Loss: 3.741520777111873e-05, Learning Rate: 0.000214\n",
      "Epoch 18621/40000, Loss: 4.132332469453104e-05, Learning Rate: 0.000214\n",
      "Epoch 18622/40000, Loss: 3.68243308912497e-05, Learning Rate: 0.000214\n",
      "Epoch 18623/40000, Loss: 3.666631528176367e-05, Learning Rate: 0.000214\n",
      "Epoch 18624/40000, Loss: 1.4628830285801087e-05, Learning Rate: 0.000214\n",
      "Epoch 18625/40000, Loss: 3.665597250801511e-05, Learning Rate: 0.000214\n",
      "Epoch 18626/40000, Loss: 1.4620722140534781e-05, Learning Rate: 0.000214\n",
      "Epoch 18627/40000, Loss: 3.2179894333239645e-05, Learning Rate: 0.000214\n",
      "Epoch 18628/40000, Loss: 4.110567897441797e-05, Learning Rate: 0.000214\n",
      "Epoch 18629/40000, Loss: 5.470984251587652e-05, Learning Rate: 0.000214\n",
      "Epoch 18630/40000, Loss: 4.1203853470506147e-05, Learning Rate: 0.000214\n",
      "Epoch 18631/40000, Loss: 1.4538391042151488e-05, Learning Rate: 0.000214\n",
      "Epoch 18632/40000, Loss: 4.13585476053413e-05, Learning Rate: 0.000214\n",
      "Epoch 18633/40000, Loss: 5.477283411892131e-05, Learning Rate: 0.000214\n",
      "Epoch 18634/40000, Loss: 3.6492849176283926e-05, Learning Rate: 0.000214\n",
      "Epoch 18635/40000, Loss: 5.467572555062361e-05, Learning Rate: 0.000214\n",
      "Epoch 18636/40000, Loss: 4.095733311260119e-05, Learning Rate: 0.000214\n",
      "Epoch 18637/40000, Loss: 3.221672886866145e-05, Learning Rate: 0.000214\n",
      "Epoch 18638/40000, Loss: 3.208827547496185e-05, Learning Rate: 0.000214\n",
      "Epoch 18639/40000, Loss: 6.132652924861759e-05, Learning Rate: 0.000214\n",
      "Epoch 18640/40000, Loss: 4.104783511138521e-05, Learning Rate: 0.000214\n",
      "Epoch 18641/40000, Loss: 6.126910011516884e-05, Learning Rate: 0.000214\n",
      "Epoch 18642/40000, Loss: 3.228386412956752e-05, Learning Rate: 0.000214\n",
      "Epoch 18643/40000, Loss: 3.6415207432582974e-05, Learning Rate: 0.000214\n",
      "Epoch 18644/40000, Loss: 4.100927981198765e-05, Learning Rate: 0.000213\n",
      "Epoch 18645/40000, Loss: 4.100115256733261e-05, Learning Rate: 0.000213\n",
      "Epoch 18646/40000, Loss: 3.651205406640656e-05, Learning Rate: 0.000213\n",
      "Epoch 18647/40000, Loss: 1.4557528629666194e-05, Learning Rate: 0.000213\n",
      "Epoch 18648/40000, Loss: 3.680457666632719e-05, Learning Rate: 0.000213\n",
      "Epoch 18649/40000, Loss: 1.4719536011398304e-05, Learning Rate: 0.000213\n",
      "Epoch 18650/40000, Loss: 6.172499706735834e-05, Learning Rate: 0.000213\n",
      "Epoch 18651/40000, Loss: 5.501732448465191e-05, Learning Rate: 0.000213\n",
      "Epoch 18652/40000, Loss: 3.2474868930876255e-05, Learning Rate: 0.000213\n",
      "Epoch 18653/40000, Loss: 3.691154779517092e-05, Learning Rate: 0.000213\n",
      "Epoch 18654/40000, Loss: 4.219203765387647e-05, Learning Rate: 0.000213\n",
      "Epoch 18655/40000, Loss: 4.21230579377152e-05, Learning Rate: 0.000213\n",
      "Epoch 18656/40000, Loss: 3.279437078163028e-05, Learning Rate: 0.000213\n",
      "Epoch 18657/40000, Loss: 3.695767009048723e-05, Learning Rate: 0.000213\n",
      "Epoch 18658/40000, Loss: 1.4732657291460782e-05, Learning Rate: 0.000213\n",
      "Epoch 18659/40000, Loss: 3.317983282613568e-05, Learning Rate: 0.000213\n",
      "Epoch 18660/40000, Loss: 6.180060881888494e-05, Learning Rate: 0.000213\n",
      "Epoch 18661/40000, Loss: 4.145204366068356e-05, Learning Rate: 0.000213\n",
      "Epoch 18662/40000, Loss: 3.6785037082154304e-05, Learning Rate: 0.000213\n",
      "Epoch 18663/40000, Loss: 6.547950761159882e-05, Learning Rate: 0.000213\n",
      "Epoch 18664/40000, Loss: 3.254590046708472e-05, Learning Rate: 0.000213\n",
      "Epoch 18665/40000, Loss: 3.244625622755848e-05, Learning Rate: 0.000213\n",
      "Epoch 18666/40000, Loss: 3.232404196751304e-05, Learning Rate: 0.000213\n",
      "Epoch 18667/40000, Loss: 5.519841943169013e-05, Learning Rate: 0.000213\n",
      "Epoch 18668/40000, Loss: 4.162177719990723e-05, Learning Rate: 0.000213\n",
      "Epoch 18669/40000, Loss: 6.253755418583751e-05, Learning Rate: 0.000213\n",
      "Epoch 18670/40000, Loss: 3.672092134365812e-05, Learning Rate: 0.000213\n",
      "Epoch 18671/40000, Loss: 5.5778225942049176e-05, Learning Rate: 0.000213\n",
      "Epoch 18672/40000, Loss: 4.254037048667669e-05, Learning Rate: 0.000213\n",
      "Epoch 18673/40000, Loss: 6.300187669694424e-05, Learning Rate: 0.000213\n",
      "Epoch 18674/40000, Loss: 6.255492189666256e-05, Learning Rate: 0.000213\n",
      "Epoch 18675/40000, Loss: 6.228947313502431e-05, Learning Rate: 0.000213\n",
      "Epoch 18676/40000, Loss: 3.841915531666018e-05, Learning Rate: 0.000213\n",
      "Epoch 18677/40000, Loss: 4.192930646240711e-05, Learning Rate: 0.000213\n",
      "Epoch 18678/40000, Loss: 3.27316956827417e-05, Learning Rate: 0.000213\n",
      "Epoch 18679/40000, Loss: 5.560226782108657e-05, Learning Rate: 0.000213\n",
      "Epoch 18680/40000, Loss: 5.517683894140646e-05, Learning Rate: 0.000213\n",
      "Epoch 18681/40000, Loss: 3.228321293136105e-05, Learning Rate: 0.000213\n",
      "Epoch 18682/40000, Loss: 3.2392974389949813e-05, Learning Rate: 0.000213\n",
      "Epoch 18683/40000, Loss: 6.222636147867888e-05, Learning Rate: 0.000212\n",
      "Epoch 18684/40000, Loss: 3.70765665138606e-05, Learning Rate: 0.000212\n",
      "Epoch 18685/40000, Loss: 6.368594767991453e-05, Learning Rate: 0.000212\n",
      "Epoch 18686/40000, Loss: 3.2954303605947644e-05, Learning Rate: 0.000212\n",
      "Epoch 18687/40000, Loss: 1.548461295897141e-05, Learning Rate: 0.000212\n",
      "Epoch 18688/40000, Loss: 1.4888095392961986e-05, Learning Rate: 0.000212\n",
      "Epoch 18689/40000, Loss: 4.3939326133113354e-05, Learning Rate: 0.000212\n",
      "Epoch 18690/40000, Loss: 1.9137243725708686e-05, Learning Rate: 0.000212\n",
      "Epoch 18691/40000, Loss: 3.773277421714738e-05, Learning Rate: 0.000212\n",
      "Epoch 18692/40000, Loss: 3.778231257456355e-05, Learning Rate: 0.000212\n",
      "Epoch 18693/40000, Loss: 4.828010423807427e-05, Learning Rate: 0.000212\n",
      "Epoch 18694/40000, Loss: 6.076294812373817e-05, Learning Rate: 0.000212\n",
      "Epoch 18695/40000, Loss: 4.684367013396695e-05, Learning Rate: 0.000212\n",
      "Epoch 18696/40000, Loss: 4.397468001116067e-05, Learning Rate: 0.000212\n",
      "Epoch 18697/40000, Loss: 4.2758012568810955e-05, Learning Rate: 0.000212\n",
      "Epoch 18698/40000, Loss: 3.7950376281514764e-05, Learning Rate: 0.000212\n",
      "Epoch 18699/40000, Loss: 5.724765287595801e-05, Learning Rate: 0.000212\n",
      "Epoch 18700/40000, Loss: 5.680259710061364e-05, Learning Rate: 0.000212\n",
      "Epoch 18701/40000, Loss: 6.371177005348727e-05, Learning Rate: 0.000212\n",
      "Epoch 18702/40000, Loss: 5.9683312429115176e-05, Learning Rate: 0.000212\n",
      "Epoch 18703/40000, Loss: 1.625912409508601e-05, Learning Rate: 0.000212\n",
      "Epoch 18704/40000, Loss: 6.17151235928759e-05, Learning Rate: 0.000212\n",
      "Epoch 18705/40000, Loss: 6.622588261961937e-05, Learning Rate: 0.000212\n",
      "Epoch 18706/40000, Loss: 3.431299774092622e-05, Learning Rate: 0.000212\n",
      "Epoch 18707/40000, Loss: 6.421178113669157e-05, Learning Rate: 0.000212\n",
      "Epoch 18708/40000, Loss: 4.010310294688679e-05, Learning Rate: 0.000212\n",
      "Epoch 18709/40000, Loss: 3.350793122081086e-05, Learning Rate: 0.000212\n",
      "Epoch 18710/40000, Loss: 5.568130291067064e-05, Learning Rate: 0.000212\n",
      "Epoch 18711/40000, Loss: 6.323710113065317e-05, Learning Rate: 0.000212\n",
      "Epoch 18712/40000, Loss: 6.290760939009488e-05, Learning Rate: 0.000212\n",
      "Epoch 18713/40000, Loss: 3.722987821674906e-05, Learning Rate: 0.000212\n",
      "Epoch 18714/40000, Loss: 4.194741268293001e-05, Learning Rate: 0.000212\n",
      "Epoch 18715/40000, Loss: 3.407636904739775e-05, Learning Rate: 0.000212\n",
      "Epoch 18716/40000, Loss: 3.302787445136346e-05, Learning Rate: 0.000212\n",
      "Epoch 18717/40000, Loss: 4.4245502067497e-05, Learning Rate: 0.000212\n",
      "Epoch 18718/40000, Loss: 6.757400115020573e-05, Learning Rate: 0.000212\n",
      "Epoch 18719/40000, Loss: 3.877770359395072e-05, Learning Rate: 0.000212\n",
      "Epoch 18720/40000, Loss: 3.3119118597824126e-05, Learning Rate: 0.000212\n",
      "Epoch 18721/40000, Loss: 5.592353409156203e-05, Learning Rate: 0.000212\n",
      "Epoch 18722/40000, Loss: 3.2950036256806925e-05, Learning Rate: 0.000212\n",
      "Epoch 18723/40000, Loss: 3.6812256439588964e-05, Learning Rate: 0.000211\n",
      "Epoch 18724/40000, Loss: 5.776322723249905e-05, Learning Rate: 0.000211\n",
      "Epoch 18725/40000, Loss: 3.8247675547609106e-05, Learning Rate: 0.000211\n",
      "Epoch 18726/40000, Loss: 3.344921060488559e-05, Learning Rate: 0.000211\n",
      "Epoch 18727/40000, Loss: 4.587146395351738e-05, Learning Rate: 0.000211\n",
      "Epoch 18728/40000, Loss: 6.452067464124411e-05, Learning Rate: 0.000211\n",
      "Epoch 18729/40000, Loss: 5.773023440269753e-05, Learning Rate: 0.000211\n",
      "Epoch 18730/40000, Loss: 5.6010481785051525e-05, Learning Rate: 0.000211\n",
      "Epoch 18731/40000, Loss: 5.565269384533167e-05, Learning Rate: 0.000211\n",
      "Epoch 18732/40000, Loss: 5.5349653848679736e-05, Learning Rate: 0.000211\n",
      "Epoch 18733/40000, Loss: 3.740316242328845e-05, Learning Rate: 0.000211\n",
      "Epoch 18734/40000, Loss: 3.2996682421071455e-05, Learning Rate: 0.000211\n",
      "Epoch 18735/40000, Loss: 3.2920903322519735e-05, Learning Rate: 0.000211\n",
      "Epoch 18736/40000, Loss: 6.391208444256335e-05, Learning Rate: 0.000211\n",
      "Epoch 18737/40000, Loss: 3.778480822802521e-05, Learning Rate: 0.000211\n",
      "Epoch 18738/40000, Loss: 5.647091529681347e-05, Learning Rate: 0.000211\n",
      "Epoch 18739/40000, Loss: 3.276153438491747e-05, Learning Rate: 0.000211\n",
      "Epoch 18740/40000, Loss: 1.5830584743525833e-05, Learning Rate: 0.000211\n",
      "Epoch 18741/40000, Loss: 3.732454206328839e-05, Learning Rate: 0.000211\n",
      "Epoch 18742/40000, Loss: 1.5460165741387755e-05, Learning Rate: 0.000211\n",
      "Epoch 18743/40000, Loss: 1.6149404473253526e-05, Learning Rate: 0.000211\n",
      "Epoch 18744/40000, Loss: 6.171939457999542e-05, Learning Rate: 0.000211\n",
      "Epoch 18745/40000, Loss: 1.4978326362324879e-05, Learning Rate: 0.000211\n",
      "Epoch 18746/40000, Loss: 1.4478140656137839e-05, Learning Rate: 0.000211\n",
      "Epoch 18747/40000, Loss: 3.6600238672690466e-05, Learning Rate: 0.000211\n",
      "Epoch 18748/40000, Loss: 3.6561825254466385e-05, Learning Rate: 0.000211\n",
      "Epoch 18749/40000, Loss: 5.521286584553309e-05, Learning Rate: 0.000211\n",
      "Epoch 18750/40000, Loss: 6.117865268606693e-05, Learning Rate: 0.000211\n",
      "Epoch 18751/40000, Loss: 3.225337786716409e-05, Learning Rate: 0.000211\n",
      "Epoch 18752/40000, Loss: 4.089074718649499e-05, Learning Rate: 0.000211\n",
      "Epoch 18753/40000, Loss: 3.64530787919648e-05, Learning Rate: 0.000211\n",
      "Epoch 18754/40000, Loss: 6.11708965152502e-05, Learning Rate: 0.000211\n",
      "Epoch 18755/40000, Loss: 3.2202358852373436e-05, Learning Rate: 0.000211\n",
      "Epoch 18756/40000, Loss: 6.155457958811894e-05, Learning Rate: 0.000211\n",
      "Epoch 18757/40000, Loss: 4.1128350858343765e-05, Learning Rate: 0.000211\n",
      "Epoch 18758/40000, Loss: 1.4238195944926701e-05, Learning Rate: 0.000211\n",
      "Epoch 18759/40000, Loss: 3.6269677366362885e-05, Learning Rate: 0.000211\n",
      "Epoch 18760/40000, Loss: 3.617861511884257e-05, Learning Rate: 0.000211\n",
      "Epoch 18761/40000, Loss: 3.641379953478463e-05, Learning Rate: 0.000211\n",
      "Epoch 18762/40000, Loss: 5.460303873405792e-05, Learning Rate: 0.000210\n",
      "Epoch 18763/40000, Loss: 5.4429216106655076e-05, Learning Rate: 0.000210\n",
      "Epoch 18764/40000, Loss: 3.6354962503537536e-05, Learning Rate: 0.000210\n",
      "Epoch 18765/40000, Loss: 3.627223850344308e-05, Learning Rate: 0.000210\n",
      "Epoch 18766/40000, Loss: 5.435121784103103e-05, Learning Rate: 0.000210\n",
      "Epoch 18767/40000, Loss: 6.130077235866338e-05, Learning Rate: 0.000210\n",
      "Epoch 18768/40000, Loss: 3.2066174753708765e-05, Learning Rate: 0.000210\n",
      "Epoch 18769/40000, Loss: 5.4659696615999565e-05, Learning Rate: 0.000210\n",
      "Epoch 18770/40000, Loss: 3.203210872015916e-05, Learning Rate: 0.000210\n",
      "Epoch 18771/40000, Loss: 3.205663597327657e-05, Learning Rate: 0.000210\n",
      "Epoch 18772/40000, Loss: 6.1003087466815487e-05, Learning Rate: 0.000210\n",
      "Epoch 18773/40000, Loss: 4.079083009855822e-05, Learning Rate: 0.000210\n",
      "Epoch 18774/40000, Loss: 4.087881461600773e-05, Learning Rate: 0.000210\n",
      "Epoch 18775/40000, Loss: 3.209446731489152e-05, Learning Rate: 0.000210\n",
      "Epoch 18776/40000, Loss: 3.609048144426197e-05, Learning Rate: 0.000210\n",
      "Epoch 18777/40000, Loss: 3.620179631980136e-05, Learning Rate: 0.000210\n",
      "Epoch 18778/40000, Loss: 6.103278064983897e-05, Learning Rate: 0.000210\n",
      "Epoch 18779/40000, Loss: 3.638715134002268e-05, Learning Rate: 0.000210\n",
      "Epoch 18780/40000, Loss: 5.441784378490411e-05, Learning Rate: 0.000210\n",
      "Epoch 18781/40000, Loss: 4.1249375499319285e-05, Learning Rate: 0.000210\n",
      "Epoch 18782/40000, Loss: 3.2204661692958325e-05, Learning Rate: 0.000210\n",
      "Epoch 18783/40000, Loss: 6.139395554782823e-05, Learning Rate: 0.000210\n",
      "Epoch 18784/40000, Loss: 4.1381521441508085e-05, Learning Rate: 0.000210\n",
      "Epoch 18785/40000, Loss: 3.2362426281906664e-05, Learning Rate: 0.000210\n",
      "Epoch 18786/40000, Loss: 1.4605242540710606e-05, Learning Rate: 0.000210\n",
      "Epoch 18787/40000, Loss: 3.37799901899416e-05, Learning Rate: 0.000210\n",
      "Epoch 18788/40000, Loss: 3.675126936286688e-05, Learning Rate: 0.000210\n",
      "Epoch 18789/40000, Loss: 5.506659363163635e-05, Learning Rate: 0.000210\n",
      "Epoch 18790/40000, Loss: 5.459642125060782e-05, Learning Rate: 0.000210\n",
      "Epoch 18791/40000, Loss: 1.5016299585113302e-05, Learning Rate: 0.000210\n",
      "Epoch 18792/40000, Loss: 3.2406293030362576e-05, Learning Rate: 0.000210\n",
      "Epoch 18793/40000, Loss: 3.25417349813506e-05, Learning Rate: 0.000210\n",
      "Epoch 18794/40000, Loss: 5.532885552383959e-05, Learning Rate: 0.000210\n",
      "Epoch 18795/40000, Loss: 3.294776979601011e-05, Learning Rate: 0.000210\n",
      "Epoch 18796/40000, Loss: 5.503931242856197e-05, Learning Rate: 0.000210\n",
      "Epoch 18797/40000, Loss: 6.24434687779285e-05, Learning Rate: 0.000210\n",
      "Epoch 18798/40000, Loss: 5.64764195587486e-05, Learning Rate: 0.000210\n",
      "Epoch 18799/40000, Loss: 3.641585499281064e-05, Learning Rate: 0.000210\n",
      "Epoch 18800/40000, Loss: 3.2663105230312794e-05, Learning Rate: 0.000210\n",
      "Epoch 18801/40000, Loss: 3.7031037209089845e-05, Learning Rate: 0.000210\n",
      "Epoch 18802/40000, Loss: 6.174604641273618e-05, Learning Rate: 0.000209\n",
      "Epoch 18803/40000, Loss: 4.193087079329416e-05, Learning Rate: 0.000209\n",
      "Epoch 18804/40000, Loss: 4.146438368479721e-05, Learning Rate: 0.000209\n",
      "Epoch 18805/40000, Loss: 5.54645448573865e-05, Learning Rate: 0.000209\n",
      "Epoch 18806/40000, Loss: 3.277160067227669e-05, Learning Rate: 0.000209\n",
      "Epoch 18807/40000, Loss: 4.2837018554564565e-05, Learning Rate: 0.000209\n",
      "Epoch 18808/40000, Loss: 5.6133743782993406e-05, Learning Rate: 0.000209\n",
      "Epoch 18809/40000, Loss: 4.28424755227752e-05, Learning Rate: 0.000209\n",
      "Epoch 18810/40000, Loss: 4.306596747483127e-05, Learning Rate: 0.000209\n",
      "Epoch 18811/40000, Loss: 3.533144626999274e-05, Learning Rate: 0.000209\n",
      "Epoch 18812/40000, Loss: 3.768737587961368e-05, Learning Rate: 0.000209\n",
      "Epoch 18813/40000, Loss: 6.779093382647261e-05, Learning Rate: 0.000209\n",
      "Epoch 18814/40000, Loss: 6.840616697445512e-05, Learning Rate: 0.000209\n",
      "Epoch 18815/40000, Loss: 3.489777009235695e-05, Learning Rate: 0.000209\n",
      "Epoch 18816/40000, Loss: 2.0762805434060283e-05, Learning Rate: 0.000209\n",
      "Epoch 18817/40000, Loss: 4.845913281315006e-05, Learning Rate: 0.000209\n",
      "Epoch 18818/40000, Loss: 6.310782191576436e-05, Learning Rate: 0.000209\n",
      "Epoch 18819/40000, Loss: 4.0952738345367834e-05, Learning Rate: 0.000209\n",
      "Epoch 18820/40000, Loss: 6.983887578826398e-05, Learning Rate: 0.000209\n",
      "Epoch 18821/40000, Loss: 1.8161685147788376e-05, Learning Rate: 0.000209\n",
      "Epoch 18822/40000, Loss: 3.5551198379835114e-05, Learning Rate: 0.000209\n",
      "Epoch 18823/40000, Loss: 6.564028444699943e-05, Learning Rate: 0.000209\n",
      "Epoch 18824/40000, Loss: 3.775167715502903e-05, Learning Rate: 0.000209\n",
      "Epoch 18825/40000, Loss: 1.699317363090813e-05, Learning Rate: 0.000209\n",
      "Epoch 18826/40000, Loss: 6.168691470520571e-05, Learning Rate: 0.000209\n",
      "Epoch 18827/40000, Loss: 3.428881973377429e-05, Learning Rate: 0.000209\n",
      "Epoch 18828/40000, Loss: 6.291754107223824e-05, Learning Rate: 0.000209\n",
      "Epoch 18829/40000, Loss: 6.277873035287485e-05, Learning Rate: 0.000209\n",
      "Epoch 18830/40000, Loss: 5.629597944789566e-05, Learning Rate: 0.000209\n",
      "Epoch 18831/40000, Loss: 1.6802310710772872e-05, Learning Rate: 0.000209\n",
      "Epoch 18832/40000, Loss: 5.9217367379460484e-05, Learning Rate: 0.000209\n",
      "Epoch 18833/40000, Loss: 5.650539605994709e-05, Learning Rate: 0.000209\n",
      "Epoch 18834/40000, Loss: 4.281155634089373e-05, Learning Rate: 0.000209\n",
      "Epoch 18835/40000, Loss: 1.4854822438792326e-05, Learning Rate: 0.000209\n",
      "Epoch 18836/40000, Loss: 3.3353713661199436e-05, Learning Rate: 0.000209\n",
      "Epoch 18837/40000, Loss: 6.148728425614536e-05, Learning Rate: 0.000209\n",
      "Epoch 18838/40000, Loss: 3.6734825698658824e-05, Learning Rate: 0.000209\n",
      "Epoch 18839/40000, Loss: 6.200390635058284e-05, Learning Rate: 0.000209\n",
      "Epoch 18840/40000, Loss: 4.1876803152263165e-05, Learning Rate: 0.000209\n",
      "Epoch 18841/40000, Loss: 4.195353903924115e-05, Learning Rate: 0.000209\n",
      "Epoch 18842/40000, Loss: 6.111440598033369e-05, Learning Rate: 0.000208\n",
      "Epoch 18843/40000, Loss: 6.107862282078713e-05, Learning Rate: 0.000208\n",
      "Epoch 18844/40000, Loss: 1.4743153769813944e-05, Learning Rate: 0.000208\n",
      "Epoch 18845/40000, Loss: 3.229040157748386e-05, Learning Rate: 0.000208\n",
      "Epoch 18846/40000, Loss: 3.6200715840095654e-05, Learning Rate: 0.000208\n",
      "Epoch 18847/40000, Loss: 3.201908111805096e-05, Learning Rate: 0.000208\n",
      "Epoch 18848/40000, Loss: 4.105372499907389e-05, Learning Rate: 0.000208\n",
      "Epoch 18849/40000, Loss: 4.084028842044063e-05, Learning Rate: 0.000208\n",
      "Epoch 18850/40000, Loss: 1.4255308087740559e-05, Learning Rate: 0.000208\n",
      "Epoch 18851/40000, Loss: 1.4257097063818946e-05, Learning Rate: 0.000208\n",
      "Epoch 18852/40000, Loss: 6.080072853364982e-05, Learning Rate: 0.000208\n",
      "Epoch 18853/40000, Loss: 5.42033594683744e-05, Learning Rate: 0.000208\n",
      "Epoch 18854/40000, Loss: 3.610515341279097e-05, Learning Rate: 0.000208\n",
      "Epoch 18855/40000, Loss: 1.4156548786559142e-05, Learning Rate: 0.000208\n",
      "Epoch 18856/40000, Loss: 5.421240712166764e-05, Learning Rate: 0.000208\n",
      "Epoch 18857/40000, Loss: 4.066689143655822e-05, Learning Rate: 0.000208\n",
      "Epoch 18858/40000, Loss: 4.075841570738703e-05, Learning Rate: 0.000208\n",
      "Epoch 18859/40000, Loss: 4.070613795192912e-05, Learning Rate: 0.000208\n",
      "Epoch 18860/40000, Loss: 1.4252792425395455e-05, Learning Rate: 0.000208\n",
      "Epoch 18861/40000, Loss: 3.6054534575669095e-05, Learning Rate: 0.000208\n",
      "Epoch 18862/40000, Loss: 3.6119381547905505e-05, Learning Rate: 0.000208\n",
      "Epoch 18863/40000, Loss: 5.425058407126926e-05, Learning Rate: 0.000208\n",
      "Epoch 18864/40000, Loss: 4.069430724484846e-05, Learning Rate: 0.000208\n",
      "Epoch 18865/40000, Loss: 3.607483449741267e-05, Learning Rate: 0.000208\n",
      "Epoch 18866/40000, Loss: 3.18577658617869e-05, Learning Rate: 0.000208\n",
      "Epoch 18867/40000, Loss: 4.07915904361289e-05, Learning Rate: 0.000208\n",
      "Epoch 18868/40000, Loss: 6.098338417359628e-05, Learning Rate: 0.000208\n",
      "Epoch 18869/40000, Loss: 4.0786180761642754e-05, Learning Rate: 0.000208\n",
      "Epoch 18870/40000, Loss: 5.421147943707183e-05, Learning Rate: 0.000208\n",
      "Epoch 18871/40000, Loss: 5.427284486358985e-05, Learning Rate: 0.000208\n",
      "Epoch 18872/40000, Loss: 1.4227128303900827e-05, Learning Rate: 0.000208\n",
      "Epoch 18873/40000, Loss: 3.6063342122361064e-05, Learning Rate: 0.000208\n",
      "Epoch 18874/40000, Loss: 1.4380982975126244e-05, Learning Rate: 0.000208\n",
      "Epoch 18875/40000, Loss: 4.090878428542055e-05, Learning Rate: 0.000208\n",
      "Epoch 18876/40000, Loss: 4.0952261770144105e-05, Learning Rate: 0.000208\n",
      "Epoch 18877/40000, Loss: 4.090687070856802e-05, Learning Rate: 0.000208\n",
      "Epoch 18878/40000, Loss: 6.097240839153528e-05, Learning Rate: 0.000208\n",
      "Epoch 18879/40000, Loss: 3.609917257563211e-05, Learning Rate: 0.000208\n",
      "Epoch 18880/40000, Loss: 1.4441392522712704e-05, Learning Rate: 0.000208\n",
      "Epoch 18881/40000, Loss: 5.433675323729403e-05, Learning Rate: 0.000208\n",
      "Epoch 18882/40000, Loss: 4.0867675124900416e-05, Learning Rate: 0.000207\n",
      "Epoch 18883/40000, Loss: 3.2179883419303223e-05, Learning Rate: 0.000207\n",
      "Epoch 18884/40000, Loss: 5.4814019676996395e-05, Learning Rate: 0.000207\n",
      "Epoch 18885/40000, Loss: 6.303550617303699e-05, Learning Rate: 0.000207\n",
      "Epoch 18886/40000, Loss: 3.6428053135750815e-05, Learning Rate: 0.000207\n",
      "Epoch 18887/40000, Loss: 1.4635417755926028e-05, Learning Rate: 0.000207\n",
      "Epoch 18888/40000, Loss: 1.4641828784078825e-05, Learning Rate: 0.000207\n",
      "Epoch 18889/40000, Loss: 3.6906360037392005e-05, Learning Rate: 0.000207\n",
      "Epoch 18890/40000, Loss: 1.4894551895849872e-05, Learning Rate: 0.000207\n",
      "Epoch 18891/40000, Loss: 5.580012657446787e-05, Learning Rate: 0.000207\n",
      "Epoch 18892/40000, Loss: 4.217726018396206e-05, Learning Rate: 0.000207\n",
      "Epoch 18893/40000, Loss: 1.5870195056777447e-05, Learning Rate: 0.000207\n",
      "Epoch 18894/40000, Loss: 5.662788316840306e-05, Learning Rate: 0.000207\n",
      "Epoch 18895/40000, Loss: 4.4020860514137894e-05, Learning Rate: 0.000207\n",
      "Epoch 18896/40000, Loss: 4.3915882997680455e-05, Learning Rate: 0.000207\n",
      "Epoch 18897/40000, Loss: 6.276327621890232e-05, Learning Rate: 0.000207\n",
      "Epoch 18898/40000, Loss: 6.223967648111284e-05, Learning Rate: 0.000207\n",
      "Epoch 18899/40000, Loss: 3.6808116419706494e-05, Learning Rate: 0.000207\n",
      "Epoch 18900/40000, Loss: 4.259508568793535e-05, Learning Rate: 0.000207\n",
      "Epoch 18901/40000, Loss: 4.210670158499852e-05, Learning Rate: 0.000207\n",
      "Epoch 18902/40000, Loss: 1.5027768313302658e-05, Learning Rate: 0.000207\n",
      "Epoch 18903/40000, Loss: 1.5241566870827228e-05, Learning Rate: 0.000207\n",
      "Epoch 18904/40000, Loss: 6.177993782330304e-05, Learning Rate: 0.000207\n",
      "Epoch 18905/40000, Loss: 1.5104101294127759e-05, Learning Rate: 0.000207\n",
      "Epoch 18906/40000, Loss: 1.4730087968928274e-05, Learning Rate: 0.000207\n",
      "Epoch 18907/40000, Loss: 6.155945447972044e-05, Learning Rate: 0.000207\n",
      "Epoch 18908/40000, Loss: 4.122787504456937e-05, Learning Rate: 0.000207\n",
      "Epoch 18909/40000, Loss: 5.4611387895420194e-05, Learning Rate: 0.000207\n",
      "Epoch 18910/40000, Loss: 4.1121045796899125e-05, Learning Rate: 0.000207\n",
      "Epoch 18911/40000, Loss: 4.093179450137541e-05, Learning Rate: 0.000207\n",
      "Epoch 18912/40000, Loss: 1.4352960533869918e-05, Learning Rate: 0.000207\n",
      "Epoch 18913/40000, Loss: 6.094417767599225e-05, Learning Rate: 0.000207\n",
      "Epoch 18914/40000, Loss: 3.203830783604644e-05, Learning Rate: 0.000207\n",
      "Epoch 18915/40000, Loss: 6.128681707195938e-05, Learning Rate: 0.000207\n",
      "Epoch 18916/40000, Loss: 3.221537917852402e-05, Learning Rate: 0.000207\n",
      "Epoch 18917/40000, Loss: 4.0844846807885915e-05, Learning Rate: 0.000207\n",
      "Epoch 18918/40000, Loss: 5.4194886615732685e-05, Learning Rate: 0.000207\n",
      "Epoch 18919/40000, Loss: 4.071955117979087e-05, Learning Rate: 0.000207\n",
      "Epoch 18920/40000, Loss: 3.598212424549274e-05, Learning Rate: 0.000207\n",
      "Epoch 18921/40000, Loss: 1.442325083189644e-05, Learning Rate: 0.000207\n",
      "Epoch 18922/40000, Loss: 3.6183253541821614e-05, Learning Rate: 0.000206\n",
      "Epoch 18923/40000, Loss: 3.1898129236651585e-05, Learning Rate: 0.000206\n",
      "Epoch 18924/40000, Loss: 3.625995304901153e-05, Learning Rate: 0.000206\n",
      "Epoch 18925/40000, Loss: 4.0747589082457125e-05, Learning Rate: 0.000206\n",
      "Epoch 18926/40000, Loss: 6.108456727815792e-05, Learning Rate: 0.000206\n",
      "Epoch 18927/40000, Loss: 5.433225669548847e-05, Learning Rate: 0.000206\n",
      "Epoch 18928/40000, Loss: 4.086849730811082e-05, Learning Rate: 0.000206\n",
      "Epoch 18929/40000, Loss: 5.443626287160441e-05, Learning Rate: 0.000206\n",
      "Epoch 18930/40000, Loss: 1.4623477909481153e-05, Learning Rate: 0.000206\n",
      "Epoch 18931/40000, Loss: 3.194579767296091e-05, Learning Rate: 0.000206\n",
      "Epoch 18932/40000, Loss: 3.659436333691701e-05, Learning Rate: 0.000206\n",
      "Epoch 18933/40000, Loss: 6.135497096693143e-05, Learning Rate: 0.000206\n",
      "Epoch 18934/40000, Loss: 4.105699554202147e-05, Learning Rate: 0.000206\n",
      "Epoch 18935/40000, Loss: 4.093219467904419e-05, Learning Rate: 0.000206\n",
      "Epoch 18936/40000, Loss: 4.073546006111428e-05, Learning Rate: 0.000206\n",
      "Epoch 18937/40000, Loss: 5.4311894928105175e-05, Learning Rate: 0.000206\n",
      "Epoch 18938/40000, Loss: 3.619056224124506e-05, Learning Rate: 0.000206\n",
      "Epoch 18939/40000, Loss: 6.0910009779036045e-05, Learning Rate: 0.000206\n",
      "Epoch 18940/40000, Loss: 5.457417864818126e-05, Learning Rate: 0.000206\n",
      "Epoch 18941/40000, Loss: 1.4385452232090756e-05, Learning Rate: 0.000206\n",
      "Epoch 18942/40000, Loss: 4.076500408700667e-05, Learning Rate: 0.000206\n",
      "Epoch 18943/40000, Loss: 1.4440124687098432e-05, Learning Rate: 0.000206\n",
      "Epoch 18944/40000, Loss: 3.609681152738631e-05, Learning Rate: 0.000206\n",
      "Epoch 18945/40000, Loss: 3.612702857935801e-05, Learning Rate: 0.000206\n",
      "Epoch 18946/40000, Loss: 3.619252674980089e-05, Learning Rate: 0.000206\n",
      "Epoch 18947/40000, Loss: 6.101039252826013e-05, Learning Rate: 0.000206\n",
      "Epoch 18948/40000, Loss: 3.199593265890144e-05, Learning Rate: 0.000206\n",
      "Epoch 18949/40000, Loss: 4.15290669479873e-05, Learning Rate: 0.000206\n",
      "Epoch 18950/40000, Loss: 3.267334614065476e-05, Learning Rate: 0.000206\n",
      "Epoch 18951/40000, Loss: 6.210446736076847e-05, Learning Rate: 0.000206\n",
      "Epoch 18952/40000, Loss: 6.192480941535905e-05, Learning Rate: 0.000206\n",
      "Epoch 18953/40000, Loss: 1.5557761798845604e-05, Learning Rate: 0.000206\n",
      "Epoch 18954/40000, Loss: 4.2328003473812714e-05, Learning Rate: 0.000206\n",
      "Epoch 18955/40000, Loss: 5.5273514590226114e-05, Learning Rate: 0.000206\n",
      "Epoch 18956/40000, Loss: 5.508029425982386e-05, Learning Rate: 0.000206\n",
      "Epoch 18957/40000, Loss: 6.195211608428508e-05, Learning Rate: 0.000206\n",
      "Epoch 18958/40000, Loss: 6.188094266690314e-05, Learning Rate: 0.000206\n",
      "Epoch 18959/40000, Loss: 5.577066622208804e-05, Learning Rate: 0.000206\n",
      "Epoch 18960/40000, Loss: 5.530467751668766e-05, Learning Rate: 0.000206\n",
      "Epoch 18961/40000, Loss: 5.640523158945143e-05, Learning Rate: 0.000206\n",
      "Epoch 18962/40000, Loss: 4.243011790094897e-05, Learning Rate: 0.000205\n",
      "Epoch 18963/40000, Loss: 3.2975778594845906e-05, Learning Rate: 0.000205\n",
      "Epoch 18964/40000, Loss: 6.319692329270765e-05, Learning Rate: 0.000205\n",
      "Epoch 18965/40000, Loss: 6.255995685933158e-05, Learning Rate: 0.000205\n",
      "Epoch 18966/40000, Loss: 5.5778527894290164e-05, Learning Rate: 0.000205\n",
      "Epoch 18967/40000, Loss: 6.205563840921968e-05, Learning Rate: 0.000205\n",
      "Epoch 18968/40000, Loss: 3.281523459008895e-05, Learning Rate: 0.000205\n",
      "Epoch 18969/40000, Loss: 3.7912886909907684e-05, Learning Rate: 0.000205\n",
      "Epoch 18970/40000, Loss: 6.16764955339022e-05, Learning Rate: 0.000205\n",
      "Epoch 18971/40000, Loss: 1.547530700918287e-05, Learning Rate: 0.000205\n",
      "Epoch 18972/40000, Loss: 5.5483589676441625e-05, Learning Rate: 0.000205\n",
      "Epoch 18973/40000, Loss: 3.255530828027986e-05, Learning Rate: 0.000205\n",
      "Epoch 18974/40000, Loss: 4.1589832108002156e-05, Learning Rate: 0.000205\n",
      "Epoch 18975/40000, Loss: 6.175123417051509e-05, Learning Rate: 0.000205\n",
      "Epoch 18976/40000, Loss: 5.599382348009385e-05, Learning Rate: 0.000205\n",
      "Epoch 18977/40000, Loss: 3.26589542964939e-05, Learning Rate: 0.000205\n",
      "Epoch 18978/40000, Loss: 3.641110015450977e-05, Learning Rate: 0.000205\n",
      "Epoch 18979/40000, Loss: 3.377981920493767e-05, Learning Rate: 0.000205\n",
      "Epoch 18980/40000, Loss: 3.2538475352339447e-05, Learning Rate: 0.000205\n",
      "Epoch 18981/40000, Loss: 1.4595577340514865e-05, Learning Rate: 0.000205\n",
      "Epoch 18982/40000, Loss: 6.156996823847294e-05, Learning Rate: 0.000205\n",
      "Epoch 18983/40000, Loss: 3.6437872040551156e-05, Learning Rate: 0.000205\n",
      "Epoch 18984/40000, Loss: 5.598693678621203e-05, Learning Rate: 0.000205\n",
      "Epoch 18985/40000, Loss: 3.265386840212159e-05, Learning Rate: 0.000205\n",
      "Epoch 18986/40000, Loss: 4.199643444735557e-05, Learning Rate: 0.000205\n",
      "Epoch 18987/40000, Loss: 6.361945997923613e-05, Learning Rate: 0.000205\n",
      "Epoch 18988/40000, Loss: 3.2795887818792835e-05, Learning Rate: 0.000205\n",
      "Epoch 18989/40000, Loss: 3.280677992734127e-05, Learning Rate: 0.000205\n",
      "Epoch 18990/40000, Loss: 1.5844114386709407e-05, Learning Rate: 0.000205\n",
      "Epoch 18991/40000, Loss: 1.57788072101539e-05, Learning Rate: 0.000205\n",
      "Epoch 18992/40000, Loss: 4.34148169006221e-05, Learning Rate: 0.000205\n",
      "Epoch 18993/40000, Loss: 3.79014054487925e-05, Learning Rate: 0.000205\n",
      "Epoch 18994/40000, Loss: 1.5559055100311525e-05, Learning Rate: 0.000205\n",
      "Epoch 18995/40000, Loss: 3.314402056275867e-05, Learning Rate: 0.000205\n",
      "Epoch 18996/40000, Loss: 6.243491225177422e-05, Learning Rate: 0.000205\n",
      "Epoch 18997/40000, Loss: 6.1342230765149e-05, Learning Rate: 0.000205\n",
      "Epoch 18998/40000, Loss: 5.527609391720034e-05, Learning Rate: 0.000205\n",
      "Epoch 18999/40000, Loss: 3.6355515476316214e-05, Learning Rate: 0.000205\n",
      "Epoch 19000/40000, Loss: 4.23055280407425e-05, Learning Rate: 0.000205\n",
      "Epoch 19001/40000, Loss: 6.194985326146707e-05, Learning Rate: 0.000205\n",
      "Epoch 19002/40000, Loss: 4.2584837501635775e-05, Learning Rate: 0.000205\n",
      "Epoch 19003/40000, Loss: 4.2965788452420384e-05, Learning Rate: 0.000204\n",
      "Epoch 19004/40000, Loss: 3.3392265322618186e-05, Learning Rate: 0.000204\n",
      "Epoch 19005/40000, Loss: 1.5310130038415082e-05, Learning Rate: 0.000204\n",
      "Epoch 19006/40000, Loss: 6.4002801082097e-05, Learning Rate: 0.000204\n",
      "Epoch 19007/40000, Loss: 5.6202432460850105e-05, Learning Rate: 0.000204\n",
      "Epoch 19008/40000, Loss: 3.741269756574184e-05, Learning Rate: 0.000204\n",
      "Epoch 19009/40000, Loss: 6.604239752050489e-05, Learning Rate: 0.000204\n",
      "Epoch 19010/40000, Loss: 3.602769720600918e-05, Learning Rate: 0.000204\n",
      "Epoch 19011/40000, Loss: 3.4659642551559955e-05, Learning Rate: 0.000204\n",
      "Epoch 19012/40000, Loss: 6.782695709262043e-05, Learning Rate: 0.000204\n",
      "Epoch 19013/40000, Loss: 6.254840263864025e-05, Learning Rate: 0.000204\n",
      "Epoch 19014/40000, Loss: 6.329947063932195e-05, Learning Rate: 0.000204\n",
      "Epoch 19015/40000, Loss: 1.765560227795504e-05, Learning Rate: 0.000204\n",
      "Epoch 19016/40000, Loss: 3.7473142583621666e-05, Learning Rate: 0.000204\n",
      "Epoch 19017/40000, Loss: 2.4004750230233185e-05, Learning Rate: 0.000204\n",
      "Epoch 19018/40000, Loss: 4.659028854803182e-05, Learning Rate: 0.000204\n",
      "Epoch 19019/40000, Loss: 4.29150604759343e-05, Learning Rate: 0.000204\n",
      "Epoch 19020/40000, Loss: 5.706658703275025e-05, Learning Rate: 0.000204\n",
      "Epoch 19021/40000, Loss: 3.2902975362958387e-05, Learning Rate: 0.000204\n",
      "Epoch 19022/40000, Loss: 5.8780489780474454e-05, Learning Rate: 0.000204\n",
      "Epoch 19023/40000, Loss: 5.531203714781441e-05, Learning Rate: 0.000204\n",
      "Epoch 19024/40000, Loss: 3.671104059321806e-05, Learning Rate: 0.000204\n",
      "Epoch 19025/40000, Loss: 6.139848119346425e-05, Learning Rate: 0.000204\n",
      "Epoch 19026/40000, Loss: 3.660352376755327e-05, Learning Rate: 0.000204\n",
      "Epoch 19027/40000, Loss: 6.185351230669767e-05, Learning Rate: 0.000204\n",
      "Epoch 19028/40000, Loss: 3.221612860215828e-05, Learning Rate: 0.000204\n",
      "Epoch 19029/40000, Loss: 1.478521699027624e-05, Learning Rate: 0.000204\n",
      "Epoch 19030/40000, Loss: 1.4774250303162262e-05, Learning Rate: 0.000204\n",
      "Epoch 19031/40000, Loss: 3.217811172362417e-05, Learning Rate: 0.000204\n",
      "Epoch 19032/40000, Loss: 4.1284689359599724e-05, Learning Rate: 0.000204\n",
      "Epoch 19033/40000, Loss: 1.4540204574586824e-05, Learning Rate: 0.000204\n",
      "Epoch 19034/40000, Loss: 4.128192085772753e-05, Learning Rate: 0.000204\n",
      "Epoch 19035/40000, Loss: 3.1867031793808565e-05, Learning Rate: 0.000204\n",
      "Epoch 19036/40000, Loss: 5.430402961792424e-05, Learning Rate: 0.000204\n",
      "Epoch 19037/40000, Loss: 3.609380291891284e-05, Learning Rate: 0.000204\n",
      "Epoch 19038/40000, Loss: 6.075450801290572e-05, Learning Rate: 0.000204\n",
      "Epoch 19039/40000, Loss: 6.0690726968459785e-05, Learning Rate: 0.000204\n",
      "Epoch 19040/40000, Loss: 3.180864587193355e-05, Learning Rate: 0.000204\n",
      "Epoch 19041/40000, Loss: 5.426949792308733e-05, Learning Rate: 0.000204\n",
      "Epoch 19042/40000, Loss: 3.1920455512590706e-05, Learning Rate: 0.000204\n",
      "Epoch 19043/40000, Loss: 5.467921801027842e-05, Learning Rate: 0.000204\n",
      "Epoch 19044/40000, Loss: 3.603481309255585e-05, Learning Rate: 0.000203\n",
      "Epoch 19045/40000, Loss: 3.182998625561595e-05, Learning Rate: 0.000203\n",
      "Epoch 19046/40000, Loss: 5.4254214774118736e-05, Learning Rate: 0.000203\n",
      "Epoch 19047/40000, Loss: 4.088622881681658e-05, Learning Rate: 0.000203\n",
      "Epoch 19048/40000, Loss: 5.500298721017316e-05, Learning Rate: 0.000203\n",
      "Epoch 19049/40000, Loss: 3.180709609296173e-05, Learning Rate: 0.000203\n",
      "Epoch 19050/40000, Loss: 1.4508929780276958e-05, Learning Rate: 0.000203\n",
      "Epoch 19051/40000, Loss: 1.4308508070826065e-05, Learning Rate: 0.000203\n",
      "Epoch 19052/40000, Loss: 4.07521401939448e-05, Learning Rate: 0.000203\n",
      "Epoch 19053/40000, Loss: 3.177076723659411e-05, Learning Rate: 0.000203\n",
      "Epoch 19054/40000, Loss: 6.14059972576797e-05, Learning Rate: 0.000203\n",
      "Epoch 19055/40000, Loss: 1.4353922779264394e-05, Learning Rate: 0.000203\n",
      "Epoch 19056/40000, Loss: 6.448644126066938e-05, Learning Rate: 0.000203\n",
      "Epoch 19057/40000, Loss: 5.4652104154229164e-05, Learning Rate: 0.000203\n",
      "Epoch 19058/40000, Loss: 3.61784259439446e-05, Learning Rate: 0.000203\n",
      "Epoch 19059/40000, Loss: 3.1970193958841264e-05, Learning Rate: 0.000203\n",
      "Epoch 19060/40000, Loss: 4.100065416423604e-05, Learning Rate: 0.000203\n",
      "Epoch 19061/40000, Loss: 1.4344102964969352e-05, Learning Rate: 0.000203\n",
      "Epoch 19062/40000, Loss: 1.436748880223604e-05, Learning Rate: 0.000203\n",
      "Epoch 19063/40000, Loss: 6.11183641012758e-05, Learning Rate: 0.000203\n",
      "Epoch 19064/40000, Loss: 3.2061783713288605e-05, Learning Rate: 0.000203\n",
      "Epoch 19065/40000, Loss: 6.26170658506453e-05, Learning Rate: 0.000203\n",
      "Epoch 19066/40000, Loss: 3.19961181958206e-05, Learning Rate: 0.000203\n",
      "Epoch 19067/40000, Loss: 7.017970347078517e-05, Learning Rate: 0.000203\n",
      "Epoch 19068/40000, Loss: 3.809174449997954e-05, Learning Rate: 0.000203\n",
      "Epoch 19069/40000, Loss: 4.147879735683091e-05, Learning Rate: 0.000203\n",
      "Epoch 19070/40000, Loss: 6.236891204025596e-05, Learning Rate: 0.000203\n",
      "Epoch 19071/40000, Loss: 3.708011718117632e-05, Learning Rate: 0.000203\n",
      "Epoch 19072/40000, Loss: 1.4951823686715215e-05, Learning Rate: 0.000203\n",
      "Epoch 19073/40000, Loss: 1.4653427570010535e-05, Learning Rate: 0.000203\n",
      "Epoch 19074/40000, Loss: 5.447507282951847e-05, Learning Rate: 0.000203\n",
      "Epoch 19075/40000, Loss: 3.215464312233962e-05, Learning Rate: 0.000203\n",
      "Epoch 19076/40000, Loss: 1.4763090803171508e-05, Learning Rate: 0.000203\n",
      "Epoch 19077/40000, Loss: 3.6253728467272595e-05, Learning Rate: 0.000203\n",
      "Epoch 19078/40000, Loss: 6.135728472145274e-05, Learning Rate: 0.000203\n",
      "Epoch 19079/40000, Loss: 6.111993570812047e-05, Learning Rate: 0.000203\n",
      "Epoch 19080/40000, Loss: 5.478880848386325e-05, Learning Rate: 0.000203\n",
      "Epoch 19081/40000, Loss: 3.2087416911963373e-05, Learning Rate: 0.000203\n",
      "Epoch 19082/40000, Loss: 5.4485408327309415e-05, Learning Rate: 0.000203\n",
      "Epoch 19083/40000, Loss: 6.129549001343548e-05, Learning Rate: 0.000203\n",
      "Epoch 19084/40000, Loss: 3.204355016350746e-05, Learning Rate: 0.000203\n",
      "Epoch 19085/40000, Loss: 3.6331810406409204e-05, Learning Rate: 0.000202\n",
      "Epoch 19086/40000, Loss: 6.090835086070001e-05, Learning Rate: 0.000202\n",
      "Epoch 19087/40000, Loss: 1.445153793611098e-05, Learning Rate: 0.000202\n",
      "Epoch 19088/40000, Loss: 5.446091017802246e-05, Learning Rate: 0.000202\n",
      "Epoch 19089/40000, Loss: 4.092294329893775e-05, Learning Rate: 0.000202\n",
      "Epoch 19090/40000, Loss: 3.612601358327083e-05, Learning Rate: 0.000202\n",
      "Epoch 19091/40000, Loss: 3.599801129894331e-05, Learning Rate: 0.000202\n",
      "Epoch 19092/40000, Loss: 4.106767664779909e-05, Learning Rate: 0.000202\n",
      "Epoch 19093/40000, Loss: 5.421930836746469e-05, Learning Rate: 0.000202\n",
      "Epoch 19094/40000, Loss: 5.419059016276151e-05, Learning Rate: 0.000202\n",
      "Epoch 19095/40000, Loss: 6.0657101130345836e-05, Learning Rate: 0.000202\n",
      "Epoch 19096/40000, Loss: 5.4202806495595723e-05, Learning Rate: 0.000202\n",
      "Epoch 19097/40000, Loss: 1.4223416656022891e-05, Learning Rate: 0.000202\n",
      "Epoch 19098/40000, Loss: 3.178032420692034e-05, Learning Rate: 0.000202\n",
      "Epoch 19099/40000, Loss: 3.605184247135185e-05, Learning Rate: 0.000202\n",
      "Epoch 19100/40000, Loss: 1.4304519027064089e-05, Learning Rate: 0.000202\n",
      "Epoch 19101/40000, Loss: 6.092276453273371e-05, Learning Rate: 0.000202\n",
      "Epoch 19102/40000, Loss: 3.1912211852613837e-05, Learning Rate: 0.000202\n",
      "Epoch 19103/40000, Loss: 1.4327172721095849e-05, Learning Rate: 0.000202\n",
      "Epoch 19104/40000, Loss: 6.0810540162492543e-05, Learning Rate: 0.000202\n",
      "Epoch 19105/40000, Loss: 5.429525481304154e-05, Learning Rate: 0.000202\n",
      "Epoch 19106/40000, Loss: 1.4407811249839142e-05, Learning Rate: 0.000202\n",
      "Epoch 19107/40000, Loss: 5.473896089824848e-05, Learning Rate: 0.000202\n",
      "Epoch 19108/40000, Loss: 3.2100873795570806e-05, Learning Rate: 0.000202\n",
      "Epoch 19109/40000, Loss: 3.6257319152355194e-05, Learning Rate: 0.000202\n",
      "Epoch 19110/40000, Loss: 3.227983324904926e-05, Learning Rate: 0.000202\n",
      "Epoch 19111/40000, Loss: 4.126759813516401e-05, Learning Rate: 0.000202\n",
      "Epoch 19112/40000, Loss: 5.5160267947940156e-05, Learning Rate: 0.000202\n",
      "Epoch 19113/40000, Loss: 4.120592711842619e-05, Learning Rate: 0.000202\n",
      "Epoch 19114/40000, Loss: 4.092668314115144e-05, Learning Rate: 0.000202\n",
      "Epoch 19115/40000, Loss: 3.6196150176692754e-05, Learning Rate: 0.000202\n",
      "Epoch 19116/40000, Loss: 3.1994972232496366e-05, Learning Rate: 0.000202\n",
      "Epoch 19117/40000, Loss: 3.617311449488625e-05, Learning Rate: 0.000202\n",
      "Epoch 19118/40000, Loss: 3.609353370848112e-05, Learning Rate: 0.000202\n",
      "Epoch 19119/40000, Loss: 3.6145498597761616e-05, Learning Rate: 0.000202\n",
      "Epoch 19120/40000, Loss: 3.320275936857797e-05, Learning Rate: 0.000202\n",
      "Epoch 19121/40000, Loss: 4.2327326809754595e-05, Learning Rate: 0.000202\n",
      "Epoch 19122/40000, Loss: 1.5107459148566704e-05, Learning Rate: 0.000202\n",
      "Epoch 19123/40000, Loss: 3.6740762880072e-05, Learning Rate: 0.000202\n",
      "Epoch 19124/40000, Loss: 4.196061854599975e-05, Learning Rate: 0.000202\n",
      "Epoch 19125/40000, Loss: 4.0957827877718955e-05, Learning Rate: 0.000202\n",
      "Epoch 19126/40000, Loss: 5.6861357734305784e-05, Learning Rate: 0.000201\n",
      "Epoch 19127/40000, Loss: 5.5326265282928944e-05, Learning Rate: 0.000201\n",
      "Epoch 19128/40000, Loss: 5.639209848595783e-05, Learning Rate: 0.000201\n",
      "Epoch 19129/40000, Loss: 1.7929571185959503e-05, Learning Rate: 0.000201\n",
      "Epoch 19130/40000, Loss: 3.747509254026227e-05, Learning Rate: 0.000201\n",
      "Epoch 19131/40000, Loss: 3.66403728548903e-05, Learning Rate: 0.000201\n",
      "Epoch 19132/40000, Loss: 6.150463741505519e-05, Learning Rate: 0.000201\n",
      "Epoch 19133/40000, Loss: 3.2305670174537227e-05, Learning Rate: 0.000201\n",
      "Epoch 19134/40000, Loss: 3.2142153941094875e-05, Learning Rate: 0.000201\n",
      "Epoch 19135/40000, Loss: 3.203794403816573e-05, Learning Rate: 0.000201\n",
      "Epoch 19136/40000, Loss: 3.1924635550240055e-05, Learning Rate: 0.000201\n",
      "Epoch 19137/40000, Loss: 3.6320951039670035e-05, Learning Rate: 0.000201\n",
      "Epoch 19138/40000, Loss: 3.60713638656307e-05, Learning Rate: 0.000201\n",
      "Epoch 19139/40000, Loss: 4.12590270570945e-05, Learning Rate: 0.000201\n",
      "Epoch 19140/40000, Loss: 3.613876106101088e-05, Learning Rate: 0.000201\n",
      "Epoch 19141/40000, Loss: 5.458707892103121e-05, Learning Rate: 0.000201\n",
      "Epoch 19142/40000, Loss: 5.4741780331823975e-05, Learning Rate: 0.000201\n",
      "Epoch 19143/40000, Loss: 4.144413469475694e-05, Learning Rate: 0.000201\n",
      "Epoch 19144/40000, Loss: 5.6070901337079704e-05, Learning Rate: 0.000201\n",
      "Epoch 19145/40000, Loss: 3.190564166288823e-05, Learning Rate: 0.000201\n",
      "Epoch 19146/40000, Loss: 5.460351530928165e-05, Learning Rate: 0.000201\n",
      "Epoch 19147/40000, Loss: 3.218238998670131e-05, Learning Rate: 0.000201\n",
      "Epoch 19148/40000, Loss: 5.642648466164246e-05, Learning Rate: 0.000201\n",
      "Epoch 19149/40000, Loss: 6.193800072651356e-05, Learning Rate: 0.000201\n",
      "Epoch 19150/40000, Loss: 4.2406183638377115e-05, Learning Rate: 0.000201\n",
      "Epoch 19151/40000, Loss: 6.467377534136176e-05, Learning Rate: 0.000201\n",
      "Epoch 19152/40000, Loss: 5.578391574090347e-05, Learning Rate: 0.000201\n",
      "Epoch 19153/40000, Loss: 1.5242640984070022e-05, Learning Rate: 0.000201\n",
      "Epoch 19154/40000, Loss: 3.2967913284664974e-05, Learning Rate: 0.000201\n",
      "Epoch 19155/40000, Loss: 3.226198532502167e-05, Learning Rate: 0.000201\n",
      "Epoch 19156/40000, Loss: 4.3739404645748436e-05, Learning Rate: 0.000201\n",
      "Epoch 19157/40000, Loss: 5.8526144130155444e-05, Learning Rate: 0.000201\n",
      "Epoch 19158/40000, Loss: 1.5424620869453065e-05, Learning Rate: 0.000201\n",
      "Epoch 19159/40000, Loss: 1.5007248293841258e-05, Learning Rate: 0.000201\n",
      "Epoch 19160/40000, Loss: 3.6445351724978536e-05, Learning Rate: 0.000201\n",
      "Epoch 19161/40000, Loss: 4.269491910235956e-05, Learning Rate: 0.000201\n",
      "Epoch 19162/40000, Loss: 3.201296567567624e-05, Learning Rate: 0.000201\n",
      "Epoch 19163/40000, Loss: 6.0938105889363214e-05, Learning Rate: 0.000201\n",
      "Epoch 19164/40000, Loss: 3.213674062862992e-05, Learning Rate: 0.000201\n",
      "Epoch 19165/40000, Loss: 4.158761657890864e-05, Learning Rate: 0.000201\n",
      "Epoch 19166/40000, Loss: 4.1454673919361085e-05, Learning Rate: 0.000201\n",
      "Epoch 19167/40000, Loss: 6.135915464255959e-05, Learning Rate: 0.000201\n",
      "Epoch 19168/40000, Loss: 1.4436909623327665e-05, Learning Rate: 0.000200\n",
      "Epoch 19169/40000, Loss: 3.637543341028504e-05, Learning Rate: 0.000200\n",
      "Epoch 19170/40000, Loss: 6.089262024033815e-05, Learning Rate: 0.000200\n",
      "Epoch 19171/40000, Loss: 1.4611070582759567e-05, Learning Rate: 0.000200\n",
      "Epoch 19172/40000, Loss: 6.07893307460472e-05, Learning Rate: 0.000200\n",
      "Epoch 19173/40000, Loss: 3.22792366205249e-05, Learning Rate: 0.000200\n",
      "Epoch 19174/40000, Loss: 3.1923009373713285e-05, Learning Rate: 0.000200\n",
      "Epoch 19175/40000, Loss: 1.4435458069783635e-05, Learning Rate: 0.000200\n",
      "Epoch 19176/40000, Loss: 1.4384968380909413e-05, Learning Rate: 0.000200\n",
      "Epoch 19177/40000, Loss: 6.190256681293249e-05, Learning Rate: 0.000200\n",
      "Epoch 19178/40000, Loss: 1.4925964933354408e-05, Learning Rate: 0.000200\n",
      "Epoch 19179/40000, Loss: 4.148532752878964e-05, Learning Rate: 0.000200\n",
      "Epoch 19180/40000, Loss: 5.5369357141898945e-05, Learning Rate: 0.000200\n",
      "Epoch 19181/40000, Loss: 6.230627332115546e-05, Learning Rate: 0.000200\n",
      "Epoch 19182/40000, Loss: 6.117367593105882e-05, Learning Rate: 0.000200\n",
      "Epoch 19183/40000, Loss: 5.7261731853941455e-05, Learning Rate: 0.000200\n",
      "Epoch 19184/40000, Loss: 6.394362571882084e-05, Learning Rate: 0.000200\n",
      "Epoch 19185/40000, Loss: 6.234148168005049e-05, Learning Rate: 0.000200\n",
      "Epoch 19186/40000, Loss: 3.4517739550210536e-05, Learning Rate: 0.000200\n",
      "Epoch 19187/40000, Loss: 3.315943104098551e-05, Learning Rate: 0.000200\n",
      "Epoch 19188/40000, Loss: 1.6258338291663677e-05, Learning Rate: 0.000200\n",
      "Epoch 19189/40000, Loss: 1.5594501746818423e-05, Learning Rate: 0.000200\n",
      "Epoch 19190/40000, Loss: 1.5659543350921012e-05, Learning Rate: 0.000200\n",
      "Epoch 19191/40000, Loss: 5.6902572396211326e-05, Learning Rate: 0.000200\n",
      "Epoch 19192/40000, Loss: 5.55315418750979e-05, Learning Rate: 0.000200\n",
      "Epoch 19193/40000, Loss: 5.5699496442684904e-05, Learning Rate: 0.000200\n",
      "Epoch 19194/40000, Loss: 5.710972982342355e-05, Learning Rate: 0.000200\n",
      "Epoch 19195/40000, Loss: 3.7206766137387604e-05, Learning Rate: 0.000200\n",
      "Epoch 19196/40000, Loss: 1.5223748960124794e-05, Learning Rate: 0.000200\n",
      "Epoch 19197/40000, Loss: 4.378790981718339e-05, Learning Rate: 0.000200\n",
      "Epoch 19198/40000, Loss: 4.2909410694846883e-05, Learning Rate: 0.000200\n",
      "Epoch 19199/40000, Loss: 5.556154064834118e-05, Learning Rate: 0.000200\n",
      "Epoch 19200/40000, Loss: 1.5878087651799433e-05, Learning Rate: 0.000200\n",
      "Epoch 19201/40000, Loss: 5.6811506510712206e-05, Learning Rate: 0.000200\n",
      "Epoch 19202/40000, Loss: 3.3037875255104154e-05, Learning Rate: 0.000200\n",
      "Epoch 19203/40000, Loss: 6.493776163551956e-05, Learning Rate: 0.000200\n",
      "Epoch 19204/40000, Loss: 5.820241494802758e-05, Learning Rate: 0.000200\n",
      "Epoch 19205/40000, Loss: 7.710974023211747e-05, Learning Rate: 0.000200\n",
      "Epoch 19206/40000, Loss: 7.674789958400652e-05, Learning Rate: 0.000200\n",
      "Epoch 19207/40000, Loss: 4.552173049887642e-05, Learning Rate: 0.000200\n",
      "Epoch 19208/40000, Loss: 7.842089689802378e-05, Learning Rate: 0.000200\n",
      "Epoch 19209/40000, Loss: 5.2678969950648025e-05, Learning Rate: 0.000199\n",
      "Epoch 19210/40000, Loss: 8.300818444695324e-05, Learning Rate: 0.000199\n",
      "Epoch 19211/40000, Loss: 1.967278149095364e-05, Learning Rate: 0.000199\n",
      "Epoch 19212/40000, Loss: 1.8394106518826447e-05, Learning Rate: 0.000199\n",
      "Epoch 19213/40000, Loss: 1.8284341422258876e-05, Learning Rate: 0.000199\n",
      "Epoch 19214/40000, Loss: 4.7834353608777747e-05, Learning Rate: 0.000199\n",
      "Epoch 19215/40000, Loss: 3.895574627676979e-05, Learning Rate: 0.000199\n",
      "Epoch 19216/40000, Loss: 3.656669287011027e-05, Learning Rate: 0.000199\n",
      "Epoch 19217/40000, Loss: 4.5734981540590525e-05, Learning Rate: 0.000199\n",
      "Epoch 19218/40000, Loss: 3.795704833464697e-05, Learning Rate: 0.000199\n",
      "Epoch 19219/40000, Loss: 4.311544762458652e-05, Learning Rate: 0.000199\n",
      "Epoch 19220/40000, Loss: 5.4509637266164646e-05, Learning Rate: 0.000199\n",
      "Epoch 19221/40000, Loss: 6.1029684729874134e-05, Learning Rate: 0.000199\n",
      "Epoch 19222/40000, Loss: 5.543952283915132e-05, Learning Rate: 0.000199\n",
      "Epoch 19223/40000, Loss: 3.203329106327146e-05, Learning Rate: 0.000199\n",
      "Epoch 19224/40000, Loss: 1.5017364603409078e-05, Learning Rate: 0.000199\n",
      "Epoch 19225/40000, Loss: 5.603600948234089e-05, Learning Rate: 0.000199\n",
      "Epoch 19226/40000, Loss: 1.4617728993471246e-05, Learning Rate: 0.000199\n",
      "Epoch 19227/40000, Loss: 6.171526911202818e-05, Learning Rate: 0.000199\n",
      "Epoch 19228/40000, Loss: 3.2725063647376373e-05, Learning Rate: 0.000199\n",
      "Epoch 19229/40000, Loss: 3.1693933124188334e-05, Learning Rate: 0.000199\n",
      "Epoch 19230/40000, Loss: 5.48020689166151e-05, Learning Rate: 0.000199\n",
      "Epoch 19231/40000, Loss: 3.6852470657322556e-05, Learning Rate: 0.000199\n",
      "Epoch 19232/40000, Loss: 6.107206718297675e-05, Learning Rate: 0.000199\n",
      "Epoch 19233/40000, Loss: 4.1385061194887385e-05, Learning Rate: 0.000199\n",
      "Epoch 19234/40000, Loss: 4.117845310247503e-05, Learning Rate: 0.000199\n",
      "Epoch 19235/40000, Loss: 3.630348510341719e-05, Learning Rate: 0.000199\n",
      "Epoch 19236/40000, Loss: 6.13133524893783e-05, Learning Rate: 0.000199\n",
      "Epoch 19237/40000, Loss: 1.4654461665486451e-05, Learning Rate: 0.000199\n",
      "Epoch 19238/40000, Loss: 4.1651201172498986e-05, Learning Rate: 0.000199\n",
      "Epoch 19239/40000, Loss: 3.6220691981725395e-05, Learning Rate: 0.000199\n",
      "Epoch 19240/40000, Loss: 3.5961042158305645e-05, Learning Rate: 0.000199\n",
      "Epoch 19241/40000, Loss: 1.4608684978156816e-05, Learning Rate: 0.000199\n",
      "Epoch 19242/40000, Loss: 6.0771195421693847e-05, Learning Rate: 0.000199\n",
      "Epoch 19243/40000, Loss: 1.4464709238382056e-05, Learning Rate: 0.000199\n",
      "Epoch 19244/40000, Loss: 4.081911174580455e-05, Learning Rate: 0.000199\n",
      "Epoch 19245/40000, Loss: 3.5939501685788855e-05, Learning Rate: 0.000199\n",
      "Epoch 19246/40000, Loss: 3.5820692573906854e-05, Learning Rate: 0.000199\n",
      "Epoch 19247/40000, Loss: 6.043181201675907e-05, Learning Rate: 0.000199\n",
      "Epoch 19248/40000, Loss: 5.4073148930910975e-05, Learning Rate: 0.000199\n",
      "Epoch 19249/40000, Loss: 3.1721388950245455e-05, Learning Rate: 0.000199\n",
      "Epoch 19250/40000, Loss: 1.4270635801949538e-05, Learning Rate: 0.000199\n",
      "Epoch 19251/40000, Loss: 1.4287215890362859e-05, Learning Rate: 0.000198\n",
      "Epoch 19252/40000, Loss: 3.161914355587214e-05, Learning Rate: 0.000198\n",
      "Epoch 19253/40000, Loss: 3.158568506478332e-05, Learning Rate: 0.000198\n",
      "Epoch 19254/40000, Loss: 5.387875353335403e-05, Learning Rate: 0.000198\n",
      "Epoch 19255/40000, Loss: 4.063344749738462e-05, Learning Rate: 0.000198\n",
      "Epoch 19256/40000, Loss: 4.0534923755330965e-05, Learning Rate: 0.000198\n",
      "Epoch 19257/40000, Loss: 5.388226782088168e-05, Learning Rate: 0.000198\n",
      "Epoch 19258/40000, Loss: 5.388615682022646e-05, Learning Rate: 0.000198\n",
      "Epoch 19259/40000, Loss: 6.05278619332239e-05, Learning Rate: 0.000198\n",
      "Epoch 19260/40000, Loss: 3.15730030706618e-05, Learning Rate: 0.000198\n",
      "Epoch 19261/40000, Loss: 1.4189510693540797e-05, Learning Rate: 0.000198\n",
      "Epoch 19262/40000, Loss: 5.387097189668566e-05, Learning Rate: 0.000198\n",
      "Epoch 19263/40000, Loss: 3.157828177791089e-05, Learning Rate: 0.000198\n",
      "Epoch 19264/40000, Loss: 1.4051403923076577e-05, Learning Rate: 0.000198\n",
      "Epoch 19265/40000, Loss: 4.051014548167586e-05, Learning Rate: 0.000198\n",
      "Epoch 19266/40000, Loss: 3.572004061425105e-05, Learning Rate: 0.000198\n",
      "Epoch 19267/40000, Loss: 3.153527359245345e-05, Learning Rate: 0.000198\n",
      "Epoch 19268/40000, Loss: 3.585973900044337e-05, Learning Rate: 0.000198\n",
      "Epoch 19269/40000, Loss: 3.158545587211847e-05, Learning Rate: 0.000198\n",
      "Epoch 19270/40000, Loss: 4.054762757732533e-05, Learning Rate: 0.000198\n",
      "Epoch 19271/40000, Loss: 1.4284714779932983e-05, Learning Rate: 0.000198\n",
      "Epoch 19272/40000, Loss: 6.043392568244599e-05, Learning Rate: 0.000198\n",
      "Epoch 19273/40000, Loss: 3.610761268646456e-05, Learning Rate: 0.000198\n",
      "Epoch 19274/40000, Loss: 1.4199652468960267e-05, Learning Rate: 0.000198\n",
      "Epoch 19275/40000, Loss: 5.400956069934182e-05, Learning Rate: 0.000198\n",
      "Epoch 19276/40000, Loss: 3.615150126279332e-05, Learning Rate: 0.000198\n",
      "Epoch 19277/40000, Loss: 4.05576174671296e-05, Learning Rate: 0.000198\n",
      "Epoch 19278/40000, Loss: 3.151542114210315e-05, Learning Rate: 0.000198\n",
      "Epoch 19279/40000, Loss: 4.0615381294628605e-05, Learning Rate: 0.000198\n",
      "Epoch 19280/40000, Loss: 4.0524490032112226e-05, Learning Rate: 0.000198\n",
      "Epoch 19281/40000, Loss: 6.040182415745221e-05, Learning Rate: 0.000198\n",
      "Epoch 19282/40000, Loss: 4.058866034029052e-05, Learning Rate: 0.000198\n",
      "Epoch 19283/40000, Loss: 3.155435842927545e-05, Learning Rate: 0.000198\n",
      "Epoch 19284/40000, Loss: 3.153110446874052e-05, Learning Rate: 0.000198\n",
      "Epoch 19285/40000, Loss: 3.157391620334238e-05, Learning Rate: 0.000198\n",
      "Epoch 19286/40000, Loss: 3.1511499400949106e-05, Learning Rate: 0.000198\n",
      "Epoch 19287/40000, Loss: 6.032119199517183e-05, Learning Rate: 0.000198\n",
      "Epoch 19288/40000, Loss: 3.164332520100288e-05, Learning Rate: 0.000198\n",
      "Epoch 19289/40000, Loss: 3.570791523088701e-05, Learning Rate: 0.000198\n",
      "Epoch 19290/40000, Loss: 5.3982406825525686e-05, Learning Rate: 0.000198\n",
      "Epoch 19291/40000, Loss: 6.026842675055377e-05, Learning Rate: 0.000198\n",
      "Epoch 19292/40000, Loss: 5.394447725848295e-05, Learning Rate: 0.000198\n",
      "Epoch 19293/40000, Loss: 4.0506714867660776e-05, Learning Rate: 0.000197\n",
      "Epoch 19294/40000, Loss: 4.052897565998137e-05, Learning Rate: 0.000197\n",
      "Epoch 19295/40000, Loss: 3.158436084049754e-05, Learning Rate: 0.000197\n",
      "Epoch 19296/40000, Loss: 3.152676436002366e-05, Learning Rate: 0.000197\n",
      "Epoch 19297/40000, Loss: 3.1544874218525365e-05, Learning Rate: 0.000197\n",
      "Epoch 19298/40000, Loss: 1.4183574421622325e-05, Learning Rate: 0.000197\n",
      "Epoch 19299/40000, Loss: 6.027162817190401e-05, Learning Rate: 0.000197\n",
      "Epoch 19300/40000, Loss: 4.045762761961669e-05, Learning Rate: 0.000197\n",
      "Epoch 19301/40000, Loss: 6.034167017787695e-05, Learning Rate: 0.000197\n",
      "Epoch 19302/40000, Loss: 6.032527380739339e-05, Learning Rate: 0.000197\n",
      "Epoch 19303/40000, Loss: 4.062787047587335e-05, Learning Rate: 0.000197\n",
      "Epoch 19304/40000, Loss: 5.397851055022329e-05, Learning Rate: 0.000197\n",
      "Epoch 19305/40000, Loss: 4.0552047721575946e-05, Learning Rate: 0.000197\n",
      "Epoch 19306/40000, Loss: 3.156514503643848e-05, Learning Rate: 0.000197\n",
      "Epoch 19307/40000, Loss: 3.149452822981402e-05, Learning Rate: 0.000197\n",
      "Epoch 19308/40000, Loss: 6.0347156249918044e-05, Learning Rate: 0.000197\n",
      "Epoch 19309/40000, Loss: 3.1654391932534054e-05, Learning Rate: 0.000197\n",
      "Epoch 19310/40000, Loss: 3.163108340231702e-05, Learning Rate: 0.000197\n",
      "Epoch 19311/40000, Loss: 3.574218135327101e-05, Learning Rate: 0.000197\n",
      "Epoch 19312/40000, Loss: 3.5770040994975716e-05, Learning Rate: 0.000197\n",
      "Epoch 19313/40000, Loss: 4.064653330715373e-05, Learning Rate: 0.000197\n",
      "Epoch 19314/40000, Loss: 1.4308428035292309e-05, Learning Rate: 0.000197\n",
      "Epoch 19315/40000, Loss: 6.041003143764101e-05, Learning Rate: 0.000197\n",
      "Epoch 19316/40000, Loss: 4.098082354175858e-05, Learning Rate: 0.000197\n",
      "Epoch 19317/40000, Loss: 3.5831912100547925e-05, Learning Rate: 0.000197\n",
      "Epoch 19318/40000, Loss: 3.1709907489130273e-05, Learning Rate: 0.000197\n",
      "Epoch 19319/40000, Loss: 6.0590740758925676e-05, Learning Rate: 0.000197\n",
      "Epoch 19320/40000, Loss: 3.598438706831075e-05, Learning Rate: 0.000197\n",
      "Epoch 19321/40000, Loss: 4.117222488275729e-05, Learning Rate: 0.000197\n",
      "Epoch 19322/40000, Loss: 6.037281491444446e-05, Learning Rate: 0.000197\n",
      "Epoch 19323/40000, Loss: 6.033654062775895e-05, Learning Rate: 0.000197\n",
      "Epoch 19324/40000, Loss: 3.152763383695856e-05, Learning Rate: 0.000197\n",
      "Epoch 19325/40000, Loss: 6.050286174286157e-05, Learning Rate: 0.000197\n",
      "Epoch 19326/40000, Loss: 5.401444650487974e-05, Learning Rate: 0.000197\n",
      "Epoch 19327/40000, Loss: 5.411005258793011e-05, Learning Rate: 0.000197\n",
      "Epoch 19328/40000, Loss: 5.404773764894344e-05, Learning Rate: 0.000197\n",
      "Epoch 19329/40000, Loss: 6.0549013142008334e-05, Learning Rate: 0.000197\n",
      "Epoch 19330/40000, Loss: 3.6103658203501254e-05, Learning Rate: 0.000197\n",
      "Epoch 19331/40000, Loss: 3.1724826840218157e-05, Learning Rate: 0.000197\n",
      "Epoch 19332/40000, Loss: 6.0474445490399376e-05, Learning Rate: 0.000197\n",
      "Epoch 19333/40000, Loss: 1.4496698895527516e-05, Learning Rate: 0.000197\n",
      "Epoch 19334/40000, Loss: 5.431788304122165e-05, Learning Rate: 0.000197\n",
      "Epoch 19335/40000, Loss: 4.089381400262937e-05, Learning Rate: 0.000197\n",
      "Epoch 19336/40000, Loss: 3.1819898140383884e-05, Learning Rate: 0.000196\n",
      "Epoch 19337/40000, Loss: 4.221874041832052e-05, Learning Rate: 0.000196\n",
      "Epoch 19338/40000, Loss: 4.17363116866909e-05, Learning Rate: 0.000196\n",
      "Epoch 19339/40000, Loss: 3.173232835251838e-05, Learning Rate: 0.000196\n",
      "Epoch 19340/40000, Loss: 3.166650276398286e-05, Learning Rate: 0.000196\n",
      "Epoch 19341/40000, Loss: 5.433234400697984e-05, Learning Rate: 0.000196\n",
      "Epoch 19342/40000, Loss: 3.196510442649014e-05, Learning Rate: 0.000196\n",
      "Epoch 19343/40000, Loss: 3.6333123716758564e-05, Learning Rate: 0.000196\n",
      "Epoch 19344/40000, Loss: 6.06617504672613e-05, Learning Rate: 0.000196\n",
      "Epoch 19345/40000, Loss: 6.0789661802118644e-05, Learning Rate: 0.000196\n",
      "Epoch 19346/40000, Loss: 1.4892428225721233e-05, Learning Rate: 0.000196\n",
      "Epoch 19347/40000, Loss: 3.6503406590782106e-05, Learning Rate: 0.000196\n",
      "Epoch 19348/40000, Loss: 1.495403193985112e-05, Learning Rate: 0.000196\n",
      "Epoch 19349/40000, Loss: 5.489531031344086e-05, Learning Rate: 0.000196\n",
      "Epoch 19350/40000, Loss: 1.518715544079896e-05, Learning Rate: 0.000196\n",
      "Epoch 19351/40000, Loss: 5.6257922551594675e-05, Learning Rate: 0.000196\n",
      "Epoch 19352/40000, Loss: 6.432223744923249e-05, Learning Rate: 0.000196\n",
      "Epoch 19353/40000, Loss: 5.696448351955041e-05, Learning Rate: 0.000196\n",
      "Epoch 19354/40000, Loss: 6.653977470705286e-05, Learning Rate: 0.000196\n",
      "Epoch 19355/40000, Loss: 5.98245496803429e-05, Learning Rate: 0.000196\n",
      "Epoch 19356/40000, Loss: 3.869472129736096e-05, Learning Rate: 0.000196\n",
      "Epoch 19357/40000, Loss: 7.323883619392291e-05, Learning Rate: 0.000196\n",
      "Epoch 19358/40000, Loss: 6.318129453575239e-05, Learning Rate: 0.000196\n",
      "Epoch 19359/40000, Loss: 1.863306897575967e-05, Learning Rate: 0.000196\n",
      "Epoch 19360/40000, Loss: 3.9622078475076705e-05, Learning Rate: 0.000196\n",
      "Epoch 19361/40000, Loss: 6.860241410322487e-05, Learning Rate: 0.000196\n",
      "Epoch 19362/40000, Loss: 4.347566209617071e-05, Learning Rate: 0.000196\n",
      "Epoch 19363/40000, Loss: 3.7861722375964746e-05, Learning Rate: 0.000196\n",
      "Epoch 19364/40000, Loss: 4.296740371501073e-05, Learning Rate: 0.000196\n",
      "Epoch 19365/40000, Loss: 1.5058209100970998e-05, Learning Rate: 0.000196\n",
      "Epoch 19366/40000, Loss: 3.763534550671466e-05, Learning Rate: 0.000196\n",
      "Epoch 19367/40000, Loss: 5.647228317684494e-05, Learning Rate: 0.000196\n",
      "Epoch 19368/40000, Loss: 5.521350249182433e-05, Learning Rate: 0.000196\n",
      "Epoch 19369/40000, Loss: 3.6645222280640155e-05, Learning Rate: 0.000196\n",
      "Epoch 19370/40000, Loss: 6.150185072328895e-05, Learning Rate: 0.000196\n",
      "Epoch 19371/40000, Loss: 5.869450978934765e-05, Learning Rate: 0.000196\n",
      "Epoch 19372/40000, Loss: 4.344835542724468e-05, Learning Rate: 0.000196\n",
      "Epoch 19373/40000, Loss: 1.6774949472164735e-05, Learning Rate: 0.000196\n",
      "Epoch 19374/40000, Loss: 4.374553100205958e-05, Learning Rate: 0.000196\n",
      "Epoch 19375/40000, Loss: 6.385556480381638e-05, Learning Rate: 0.000196\n",
      "Epoch 19376/40000, Loss: 4.272377918823622e-05, Learning Rate: 0.000196\n",
      "Epoch 19377/40000, Loss: 4.176207585260272e-05, Learning Rate: 0.000196\n",
      "Epoch 19378/40000, Loss: 1.476690340496134e-05, Learning Rate: 0.000195\n",
      "Epoch 19379/40000, Loss: 6.0586651670746505e-05, Learning Rate: 0.000195\n",
      "Epoch 19380/40000, Loss: 3.6114015529165044e-05, Learning Rate: 0.000195\n",
      "Epoch 19381/40000, Loss: 1.4652276149718091e-05, Learning Rate: 0.000195\n",
      "Epoch 19382/40000, Loss: 6.28094348940067e-05, Learning Rate: 0.000195\n",
      "Epoch 19383/40000, Loss: 4.1125022107735276e-05, Learning Rate: 0.000195\n",
      "Epoch 19384/40000, Loss: 3.632506195572205e-05, Learning Rate: 0.000195\n",
      "Epoch 19385/40000, Loss: 6.089368253014982e-05, Learning Rate: 0.000195\n",
      "Epoch 19386/40000, Loss: 6.0484566347440705e-05, Learning Rate: 0.000195\n",
      "Epoch 19387/40000, Loss: 4.10488537454512e-05, Learning Rate: 0.000195\n",
      "Epoch 19388/40000, Loss: 5.441707980935462e-05, Learning Rate: 0.000195\n",
      "Epoch 19389/40000, Loss: 5.441220127977431e-05, Learning Rate: 0.000195\n",
      "Epoch 19390/40000, Loss: 4.0866776544135064e-05, Learning Rate: 0.000195\n",
      "Epoch 19391/40000, Loss: 6.0723676142515615e-05, Learning Rate: 0.000195\n",
      "Epoch 19392/40000, Loss: 5.428645454230718e-05, Learning Rate: 0.000195\n",
      "Epoch 19393/40000, Loss: 3.593342626118101e-05, Learning Rate: 0.000195\n",
      "Epoch 19394/40000, Loss: 4.151109169470146e-05, Learning Rate: 0.000195\n",
      "Epoch 19395/40000, Loss: 3.605076199164614e-05, Learning Rate: 0.000195\n",
      "Epoch 19396/40000, Loss: 3.617639231379144e-05, Learning Rate: 0.000195\n",
      "Epoch 19397/40000, Loss: 3.153597572236322e-05, Learning Rate: 0.000195\n",
      "Epoch 19398/40000, Loss: 1.4392663615581114e-05, Learning Rate: 0.000195\n",
      "Epoch 19399/40000, Loss: 3.234238101867959e-05, Learning Rate: 0.000195\n",
      "Epoch 19400/40000, Loss: 5.407475691754371e-05, Learning Rate: 0.000195\n",
      "Epoch 19401/40000, Loss: 1.4675818420073483e-05, Learning Rate: 0.000195\n",
      "Epoch 19402/40000, Loss: 3.220284270355478e-05, Learning Rate: 0.000195\n",
      "Epoch 19403/40000, Loss: 4.1298972064396366e-05, Learning Rate: 0.000195\n",
      "Epoch 19404/40000, Loss: 5.430518285720609e-05, Learning Rate: 0.000195\n",
      "Epoch 19405/40000, Loss: 4.3961830670014024e-05, Learning Rate: 0.000195\n",
      "Epoch 19406/40000, Loss: 6.091808245400898e-05, Learning Rate: 0.000195\n",
      "Epoch 19407/40000, Loss: 3.620246934588067e-05, Learning Rate: 0.000195\n",
      "Epoch 19408/40000, Loss: 4.166058715782128e-05, Learning Rate: 0.000195\n",
      "Epoch 19409/40000, Loss: 3.302583718323149e-05, Learning Rate: 0.000195\n",
      "Epoch 19410/40000, Loss: 1.4961089618736878e-05, Learning Rate: 0.000195\n",
      "Epoch 19411/40000, Loss: 3.1808300263946876e-05, Learning Rate: 0.000195\n",
      "Epoch 19412/40000, Loss: 3.588416802813299e-05, Learning Rate: 0.000195\n",
      "Epoch 19413/40000, Loss: 1.4988235307100695e-05, Learning Rate: 0.000195\n",
      "Epoch 19414/40000, Loss: 6.028441202943213e-05, Learning Rate: 0.000195\n",
      "Epoch 19415/40000, Loss: 3.5723882319871336e-05, Learning Rate: 0.000195\n",
      "Epoch 19416/40000, Loss: 5.4128300689626485e-05, Learning Rate: 0.000195\n",
      "Epoch 19417/40000, Loss: 5.4187781643122435e-05, Learning Rate: 0.000195\n",
      "Epoch 19418/40000, Loss: 4.094252290087752e-05, Learning Rate: 0.000195\n",
      "Epoch 19419/40000, Loss: 5.467636219691485e-05, Learning Rate: 0.000195\n",
      "Epoch 19420/40000, Loss: 3.578776158974506e-05, Learning Rate: 0.000195\n",
      "Epoch 19421/40000, Loss: 1.436209277017042e-05, Learning Rate: 0.000194\n",
      "Epoch 19422/40000, Loss: 3.1530715205008164e-05, Learning Rate: 0.000194\n",
      "Epoch 19423/40000, Loss: 1.459691338823177e-05, Learning Rate: 0.000194\n",
      "Epoch 19424/40000, Loss: 1.4377402294485364e-05, Learning Rate: 0.000194\n",
      "Epoch 19425/40000, Loss: 6.0848531575175e-05, Learning Rate: 0.000194\n",
      "Epoch 19426/40000, Loss: 3.588872277759947e-05, Learning Rate: 0.000194\n",
      "Epoch 19427/40000, Loss: 3.577605093596503e-05, Learning Rate: 0.000194\n",
      "Epoch 19428/40000, Loss: 5.406478885561228e-05, Learning Rate: 0.000194\n",
      "Epoch 19429/40000, Loss: 6.076946738176048e-05, Learning Rate: 0.000194\n",
      "Epoch 19430/40000, Loss: 3.579107942641713e-05, Learning Rate: 0.000194\n",
      "Epoch 19431/40000, Loss: 3.577307870727964e-05, Learning Rate: 0.000194\n",
      "Epoch 19432/40000, Loss: 6.079857121221721e-05, Learning Rate: 0.000194\n",
      "Epoch 19433/40000, Loss: 3.5761197068495676e-05, Learning Rate: 0.000194\n",
      "Epoch 19434/40000, Loss: 1.4602087503590155e-05, Learning Rate: 0.000194\n",
      "Epoch 19435/40000, Loss: 3.571005436242558e-05, Learning Rate: 0.000194\n",
      "Epoch 19436/40000, Loss: 3.5722270695259795e-05, Learning Rate: 0.000194\n",
      "Epoch 19437/40000, Loss: 3.5629178455565125e-05, Learning Rate: 0.000194\n",
      "Epoch 19438/40000, Loss: 3.558126627467573e-05, Learning Rate: 0.000194\n",
      "Epoch 19439/40000, Loss: 1.4291500519902911e-05, Learning Rate: 0.000194\n",
      "Epoch 19440/40000, Loss: 4.059447383042425e-05, Learning Rate: 0.000194\n",
      "Epoch 19441/40000, Loss: 3.142196510452777e-05, Learning Rate: 0.000194\n",
      "Epoch 19442/40000, Loss: 1.42412727655028e-05, Learning Rate: 0.000194\n",
      "Epoch 19443/40000, Loss: 3.568230022210628e-05, Learning Rate: 0.000194\n",
      "Epoch 19444/40000, Loss: 4.059807179146446e-05, Learning Rate: 0.000194\n",
      "Epoch 19445/40000, Loss: 5.3944117098581046e-05, Learning Rate: 0.000194\n",
      "Epoch 19446/40000, Loss: 4.055718454765156e-05, Learning Rate: 0.000194\n",
      "Epoch 19447/40000, Loss: 3.135459701297805e-05, Learning Rate: 0.000194\n",
      "Epoch 19448/40000, Loss: 1.41856135087437e-05, Learning Rate: 0.000194\n",
      "Epoch 19449/40000, Loss: 3.556397496140562e-05, Learning Rate: 0.000194\n",
      "Epoch 19450/40000, Loss: 1.4210804692993406e-05, Learning Rate: 0.000194\n",
      "Epoch 19451/40000, Loss: 6.02449472353328e-05, Learning Rate: 0.000194\n",
      "Epoch 19452/40000, Loss: 6.032237797626294e-05, Learning Rate: 0.000194\n",
      "Epoch 19453/40000, Loss: 1.4134348930383567e-05, Learning Rate: 0.000194\n",
      "Epoch 19454/40000, Loss: 3.14611716021318e-05, Learning Rate: 0.000194\n",
      "Epoch 19455/40000, Loss: 3.5566557926358655e-05, Learning Rate: 0.000194\n",
      "Epoch 19456/40000, Loss: 3.1625746487407014e-05, Learning Rate: 0.000194\n",
      "Epoch 19457/40000, Loss: 6.015029066475108e-05, Learning Rate: 0.000194\n",
      "Epoch 19458/40000, Loss: 5.3939715144224465e-05, Learning Rate: 0.000194\n",
      "Epoch 19459/40000, Loss: 1.4176126569509506e-05, Learning Rate: 0.000194\n",
      "Epoch 19460/40000, Loss: 5.382870222092606e-05, Learning Rate: 0.000194\n",
      "Epoch 19461/40000, Loss: 4.070082650287077e-05, Learning Rate: 0.000194\n",
      "Epoch 19462/40000, Loss: 1.4210078916221391e-05, Learning Rate: 0.000194\n",
      "Epoch 19463/40000, Loss: 3.5523975384421647e-05, Learning Rate: 0.000194\n",
      "Epoch 19464/40000, Loss: 1.4538981304212939e-05, Learning Rate: 0.000193\n",
      "Epoch 19465/40000, Loss: 4.054850433021784e-05, Learning Rate: 0.000193\n",
      "Epoch 19466/40000, Loss: 4.0668968722457066e-05, Learning Rate: 0.000193\n",
      "Epoch 19467/40000, Loss: 6.020228465786204e-05, Learning Rate: 0.000193\n",
      "Epoch 19468/40000, Loss: 6.025265247444622e-05, Learning Rate: 0.000193\n",
      "Epoch 19469/40000, Loss: 3.564753569662571e-05, Learning Rate: 0.000193\n",
      "Epoch 19470/40000, Loss: 6.114102143328637e-05, Learning Rate: 0.000193\n",
      "Epoch 19471/40000, Loss: 4.071280636708252e-05, Learning Rate: 0.000193\n",
      "Epoch 19472/40000, Loss: 4.071437433594838e-05, Learning Rate: 0.000193\n",
      "Epoch 19473/40000, Loss: 5.4227497457759455e-05, Learning Rate: 0.000193\n",
      "Epoch 19474/40000, Loss: 5.398983194027096e-05, Learning Rate: 0.000193\n",
      "Epoch 19475/40000, Loss: 3.150170232402161e-05, Learning Rate: 0.000193\n",
      "Epoch 19476/40000, Loss: 1.464873093937058e-05, Learning Rate: 0.000193\n",
      "Epoch 19477/40000, Loss: 3.156251841573976e-05, Learning Rate: 0.000193\n",
      "Epoch 19478/40000, Loss: 3.151645069010556e-05, Learning Rate: 0.000193\n",
      "Epoch 19479/40000, Loss: 3.167546310578473e-05, Learning Rate: 0.000193\n",
      "Epoch 19480/40000, Loss: 5.4014530178392306e-05, Learning Rate: 0.000193\n",
      "Epoch 19481/40000, Loss: 3.5708610084839165e-05, Learning Rate: 0.000193\n",
      "Epoch 19482/40000, Loss: 1.4314191503217444e-05, Learning Rate: 0.000193\n",
      "Epoch 19483/40000, Loss: 3.1525498343398795e-05, Learning Rate: 0.000193\n",
      "Epoch 19484/40000, Loss: 3.5614742955658585e-05, Learning Rate: 0.000193\n",
      "Epoch 19485/40000, Loss: 6.051085074432194e-05, Learning Rate: 0.000193\n",
      "Epoch 19486/40000, Loss: 4.083281601197086e-05, Learning Rate: 0.000193\n",
      "Epoch 19487/40000, Loss: 3.55323645635508e-05, Learning Rate: 0.000193\n",
      "Epoch 19488/40000, Loss: 4.075431570527144e-05, Learning Rate: 0.000193\n",
      "Epoch 19489/40000, Loss: 3.578399264370091e-05, Learning Rate: 0.000193\n",
      "Epoch 19490/40000, Loss: 4.0708750020712614e-05, Learning Rate: 0.000193\n",
      "Epoch 19491/40000, Loss: 3.6001634725835174e-05, Learning Rate: 0.000193\n",
      "Epoch 19492/40000, Loss: 3.5665343602886423e-05, Learning Rate: 0.000193\n",
      "Epoch 19493/40000, Loss: 3.565458609955385e-05, Learning Rate: 0.000193\n",
      "Epoch 19494/40000, Loss: 3.1472809496335685e-05, Learning Rate: 0.000193\n",
      "Epoch 19495/40000, Loss: 3.1403858884004876e-05, Learning Rate: 0.000193\n",
      "Epoch 19496/40000, Loss: 3.142983769066632e-05, Learning Rate: 0.000193\n",
      "Epoch 19497/40000, Loss: 4.068799171363935e-05, Learning Rate: 0.000193\n",
      "Epoch 19498/40000, Loss: 3.583445504773408e-05, Learning Rate: 0.000193\n",
      "Epoch 19499/40000, Loss: 3.586189995985478e-05, Learning Rate: 0.000193\n",
      "Epoch 19500/40000, Loss: 3.5821765777654946e-05, Learning Rate: 0.000193\n",
      "Epoch 19501/40000, Loss: 3.573502544895746e-05, Learning Rate: 0.000193\n",
      "Epoch 19502/40000, Loss: 3.188459959346801e-05, Learning Rate: 0.000193\n",
      "Epoch 19503/40000, Loss: 4.15094182244502e-05, Learning Rate: 0.000193\n",
      "Epoch 19504/40000, Loss: 3.27612942783162e-05, Learning Rate: 0.000193\n",
      "Epoch 19505/40000, Loss: 1.4967972674639896e-05, Learning Rate: 0.000193\n",
      "Epoch 19506/40000, Loss: 3.191663563484326e-05, Learning Rate: 0.000193\n",
      "Epoch 19507/40000, Loss: 3.18901329592336e-05, Learning Rate: 0.000192\n",
      "Epoch 19508/40000, Loss: 1.542338759463746e-05, Learning Rate: 0.000192\n",
      "Epoch 19509/40000, Loss: 3.2130272302310914e-05, Learning Rate: 0.000192\n",
      "Epoch 19510/40000, Loss: 3.633876622188836e-05, Learning Rate: 0.000192\n",
      "Epoch 19511/40000, Loss: 3.2270960218738765e-05, Learning Rate: 0.000192\n",
      "Epoch 19512/40000, Loss: 5.5720360251143575e-05, Learning Rate: 0.000192\n",
      "Epoch 19513/40000, Loss: 3.670898513519205e-05, Learning Rate: 0.000192\n",
      "Epoch 19514/40000, Loss: 3.6639328754972667e-05, Learning Rate: 0.000192\n",
      "Epoch 19515/40000, Loss: 5.7672477851156145e-05, Learning Rate: 0.000192\n",
      "Epoch 19516/40000, Loss: 5.597570270765573e-05, Learning Rate: 0.000192\n",
      "Epoch 19517/40000, Loss: 5.558359407586977e-05, Learning Rate: 0.000192\n",
      "Epoch 19518/40000, Loss: 5.610856169369072e-05, Learning Rate: 0.000192\n",
      "Epoch 19519/40000, Loss: 3.721355460584164e-05, Learning Rate: 0.000192\n",
      "Epoch 19520/40000, Loss: 4.1902621887857094e-05, Learning Rate: 0.000192\n",
      "Epoch 19521/40000, Loss: 6.179296906339005e-05, Learning Rate: 0.000192\n",
      "Epoch 19522/40000, Loss: 5.793148011434823e-05, Learning Rate: 0.000192\n",
      "Epoch 19523/40000, Loss: 3.8889953430043533e-05, Learning Rate: 0.000192\n",
      "Epoch 19524/40000, Loss: 4.334939512773417e-05, Learning Rate: 0.000192\n",
      "Epoch 19525/40000, Loss: 5.071551277069375e-05, Learning Rate: 0.000192\n",
      "Epoch 19526/40000, Loss: 1.781960963853635e-05, Learning Rate: 0.000192\n",
      "Epoch 19527/40000, Loss: 5.465128197101876e-05, Learning Rate: 0.000192\n",
      "Epoch 19528/40000, Loss: 6.441019650083035e-05, Learning Rate: 0.000192\n",
      "Epoch 19529/40000, Loss: 3.4614473406691104e-05, Learning Rate: 0.000192\n",
      "Epoch 19530/40000, Loss: 1.594385685166344e-05, Learning Rate: 0.000192\n",
      "Epoch 19531/40000, Loss: 3.2742598705226555e-05, Learning Rate: 0.000192\n",
      "Epoch 19532/40000, Loss: 4.372149123810232e-05, Learning Rate: 0.000192\n",
      "Epoch 19533/40000, Loss: 5.74609475734178e-05, Learning Rate: 0.000192\n",
      "Epoch 19534/40000, Loss: 1.5380066543002613e-05, Learning Rate: 0.000192\n",
      "Epoch 19535/40000, Loss: 6.215519533725455e-05, Learning Rate: 0.000192\n",
      "Epoch 19536/40000, Loss: 3.6266712413635105e-05, Learning Rate: 0.000192\n",
      "Epoch 19537/40000, Loss: 6.451368244597688e-05, Learning Rate: 0.000192\n",
      "Epoch 19538/40000, Loss: 1.6554149624425918e-05, Learning Rate: 0.000192\n",
      "Epoch 19539/40000, Loss: 3.225026375730522e-05, Learning Rate: 0.000192\n",
      "Epoch 19540/40000, Loss: 4.14679670939222e-05, Learning Rate: 0.000192\n",
      "Epoch 19541/40000, Loss: 3.601310891099274e-05, Learning Rate: 0.000192\n",
      "Epoch 19542/40000, Loss: 6.070551535231061e-05, Learning Rate: 0.000192\n",
      "Epoch 19543/40000, Loss: 3.1552990549243987e-05, Learning Rate: 0.000192\n",
      "Epoch 19544/40000, Loss: 3.130814366159029e-05, Learning Rate: 0.000192\n",
      "Epoch 19545/40000, Loss: 5.3804556955583394e-05, Learning Rate: 0.000192\n",
      "Epoch 19546/40000, Loss: 3.560115146683529e-05, Learning Rate: 0.000192\n",
      "Epoch 19547/40000, Loss: 4.073714080732316e-05, Learning Rate: 0.000192\n",
      "Epoch 19548/40000, Loss: 5.399995279731229e-05, Learning Rate: 0.000192\n",
      "Epoch 19549/40000, Loss: 3.556548472261056e-05, Learning Rate: 0.000192\n",
      "Epoch 19550/40000, Loss: 3.131820631097071e-05, Learning Rate: 0.000191\n",
      "Epoch 19551/40000, Loss: 4.064106906298548e-05, Learning Rate: 0.000191\n",
      "Epoch 19552/40000, Loss: 3.5633282095659524e-05, Learning Rate: 0.000191\n",
      "Epoch 19553/40000, Loss: 3.1478026357945055e-05, Learning Rate: 0.000191\n",
      "Epoch 19554/40000, Loss: 3.557675881893374e-05, Learning Rate: 0.000191\n",
      "Epoch 19555/40000, Loss: 5.4112988436827436e-05, Learning Rate: 0.000191\n",
      "Epoch 19556/40000, Loss: 3.6283123336033896e-05, Learning Rate: 0.000191\n",
      "Epoch 19557/40000, Loss: 3.5667042538989335e-05, Learning Rate: 0.000191\n",
      "Epoch 19558/40000, Loss: 5.429525117506273e-05, Learning Rate: 0.000191\n",
      "Epoch 19559/40000, Loss: 3.570188710000366e-05, Learning Rate: 0.000191\n",
      "Epoch 19560/40000, Loss: 3.5577289963839576e-05, Learning Rate: 0.000191\n",
      "Epoch 19561/40000, Loss: 4.102688762941398e-05, Learning Rate: 0.000191\n",
      "Epoch 19562/40000, Loss: 5.419592707767151e-05, Learning Rate: 0.000191\n",
      "Epoch 19563/40000, Loss: 4.0942330088000745e-05, Learning Rate: 0.000191\n",
      "Epoch 19564/40000, Loss: 5.470695759868249e-05, Learning Rate: 0.000191\n",
      "Epoch 19565/40000, Loss: 3.2085074053611606e-05, Learning Rate: 0.000191\n",
      "Epoch 19566/40000, Loss: 6.178469629958272e-05, Learning Rate: 0.000191\n",
      "Epoch 19567/40000, Loss: 6.282022513914853e-05, Learning Rate: 0.000191\n",
      "Epoch 19568/40000, Loss: 5.70949814573396e-05, Learning Rate: 0.000191\n",
      "Epoch 19569/40000, Loss: 3.372213177499361e-05, Learning Rate: 0.000191\n",
      "Epoch 19570/40000, Loss: 3.244157778681256e-05, Learning Rate: 0.000191\n",
      "Epoch 19571/40000, Loss: 3.640662180259824e-05, Learning Rate: 0.000191\n",
      "Epoch 19572/40000, Loss: 6.173309520818293e-05, Learning Rate: 0.000191\n",
      "Epoch 19573/40000, Loss: 3.3305655961157754e-05, Learning Rate: 0.000191\n",
      "Epoch 19574/40000, Loss: 3.6675442970590666e-05, Learning Rate: 0.000191\n",
      "Epoch 19575/40000, Loss: 6.204244709806517e-05, Learning Rate: 0.000191\n",
      "Epoch 19576/40000, Loss: 4.209308099234477e-05, Learning Rate: 0.000191\n",
      "Epoch 19577/40000, Loss: 1.5774437997606583e-05, Learning Rate: 0.000191\n",
      "Epoch 19578/40000, Loss: 5.7114302762784064e-05, Learning Rate: 0.000191\n",
      "Epoch 19579/40000, Loss: 4.5043132558930665e-05, Learning Rate: 0.000191\n",
      "Epoch 19580/40000, Loss: 3.424693932174705e-05, Learning Rate: 0.000191\n",
      "Epoch 19581/40000, Loss: 6.308118463493884e-05, Learning Rate: 0.000191\n",
      "Epoch 19582/40000, Loss: 4.056664329255e-05, Learning Rate: 0.000191\n",
      "Epoch 19583/40000, Loss: 4.5573440729640424e-05, Learning Rate: 0.000191\n",
      "Epoch 19584/40000, Loss: 4.3083033233415335e-05, Learning Rate: 0.000191\n",
      "Epoch 19585/40000, Loss: 6.399192352546379e-05, Learning Rate: 0.000191\n",
      "Epoch 19586/40000, Loss: 6.211523577803746e-05, Learning Rate: 0.000191\n",
      "Epoch 19587/40000, Loss: 6.0946236771997064e-05, Learning Rate: 0.000191\n",
      "Epoch 19588/40000, Loss: 4.166764847468585e-05, Learning Rate: 0.000191\n",
      "Epoch 19589/40000, Loss: 3.215983451809734e-05, Learning Rate: 0.000191\n",
      "Epoch 19590/40000, Loss: 3.1690582545707e-05, Learning Rate: 0.000191\n",
      "Epoch 19591/40000, Loss: 4.2107229091925547e-05, Learning Rate: 0.000191\n",
      "Epoch 19592/40000, Loss: 3.617049515014514e-05, Learning Rate: 0.000191\n",
      "Epoch 19593/40000, Loss: 1.4583931260858662e-05, Learning Rate: 0.000191\n",
      "Epoch 19594/40000, Loss: 5.435177081380971e-05, Learning Rate: 0.000190\n",
      "Epoch 19595/40000, Loss: 1.4443988220591564e-05, Learning Rate: 0.000190\n",
      "Epoch 19596/40000, Loss: 3.580997872631997e-05, Learning Rate: 0.000190\n",
      "Epoch 19597/40000, Loss: 3.557493255357258e-05, Learning Rate: 0.000190\n",
      "Epoch 19598/40000, Loss: 4.108438588446006e-05, Learning Rate: 0.000190\n",
      "Epoch 19599/40000, Loss: 3.1868745281826705e-05, Learning Rate: 0.000190\n",
      "Epoch 19600/40000, Loss: 6.056087295291945e-05, Learning Rate: 0.000190\n",
      "Epoch 19601/40000, Loss: 5.489521936397068e-05, Learning Rate: 0.000190\n",
      "Epoch 19602/40000, Loss: 4.1928691643988714e-05, Learning Rate: 0.000190\n",
      "Epoch 19603/40000, Loss: 6.170212873257697e-05, Learning Rate: 0.000190\n",
      "Epoch 19604/40000, Loss: 4.19470998167526e-05, Learning Rate: 0.000190\n",
      "Epoch 19605/40000, Loss: 3.6039087717654184e-05, Learning Rate: 0.000190\n",
      "Epoch 19606/40000, Loss: 5.5244017858058214e-05, Learning Rate: 0.000190\n",
      "Epoch 19607/40000, Loss: 4.189613173366524e-05, Learning Rate: 0.000190\n",
      "Epoch 19608/40000, Loss: 4.100093065062538e-05, Learning Rate: 0.000190\n",
      "Epoch 19609/40000, Loss: 3.569261389202438e-05, Learning Rate: 0.000190\n",
      "Epoch 19610/40000, Loss: 3.160771302646026e-05, Learning Rate: 0.000190\n",
      "Epoch 19611/40000, Loss: 1.4427194400923327e-05, Learning Rate: 0.000190\n",
      "Epoch 19612/40000, Loss: 3.559887409210205e-05, Learning Rate: 0.000190\n",
      "Epoch 19613/40000, Loss: 4.0511280531063676e-05, Learning Rate: 0.000190\n",
      "Epoch 19614/40000, Loss: 3.1357965781353414e-05, Learning Rate: 0.000190\n",
      "Epoch 19615/40000, Loss: 6.021402441547252e-05, Learning Rate: 0.000190\n",
      "Epoch 19616/40000, Loss: 6.016764382366091e-05, Learning Rate: 0.000190\n",
      "Epoch 19617/40000, Loss: 6.011914592818357e-05, Learning Rate: 0.000190\n",
      "Epoch 19618/40000, Loss: 5.3756044508190826e-05, Learning Rate: 0.000190\n",
      "Epoch 19619/40000, Loss: 3.5454493627185e-05, Learning Rate: 0.000190\n",
      "Epoch 19620/40000, Loss: 3.1322837457992136e-05, Learning Rate: 0.000190\n",
      "Epoch 19621/40000, Loss: 5.369972495827824e-05, Learning Rate: 0.000190\n",
      "Epoch 19622/40000, Loss: 6.003589078318328e-05, Learning Rate: 0.000190\n",
      "Epoch 19623/40000, Loss: 3.552262933226302e-05, Learning Rate: 0.000190\n",
      "Epoch 19624/40000, Loss: 3.5485187254380435e-05, Learning Rate: 0.000190\n",
      "Epoch 19625/40000, Loss: 1.4178250239638146e-05, Learning Rate: 0.000190\n",
      "Epoch 19626/40000, Loss: 4.0464808989781886e-05, Learning Rate: 0.000190\n",
      "Epoch 19627/40000, Loss: 1.4152928088151384e-05, Learning Rate: 0.000190\n",
      "Epoch 19628/40000, Loss: 4.040755447931588e-05, Learning Rate: 0.000190\n",
      "Epoch 19629/40000, Loss: 3.145565642626025e-05, Learning Rate: 0.000190\n",
      "Epoch 19630/40000, Loss: 6.0213609685888514e-05, Learning Rate: 0.000190\n",
      "Epoch 19631/40000, Loss: 1.4397821360034868e-05, Learning Rate: 0.000190\n",
      "Epoch 19632/40000, Loss: 5.3772531828144565e-05, Learning Rate: 0.000190\n",
      "Epoch 19633/40000, Loss: 5.3800791647518054e-05, Learning Rate: 0.000190\n",
      "Epoch 19634/40000, Loss: 6.015855251462199e-05, Learning Rate: 0.000190\n",
      "Epoch 19635/40000, Loss: 4.070972136105411e-05, Learning Rate: 0.000190\n",
      "Epoch 19636/40000, Loss: 1.434186560800299e-05, Learning Rate: 0.000190\n",
      "Epoch 19637/40000, Loss: 5.383414099924266e-05, Learning Rate: 0.000190\n",
      "Epoch 19638/40000, Loss: 5.378164496505633e-05, Learning Rate: 0.000189\n",
      "Epoch 19639/40000, Loss: 3.55452903022524e-05, Learning Rate: 0.000189\n",
      "Epoch 19640/40000, Loss: 3.5435514291748405e-05, Learning Rate: 0.000189\n",
      "Epoch 19641/40000, Loss: 3.12997872242704e-05, Learning Rate: 0.000189\n",
      "Epoch 19642/40000, Loss: 1.4168599591357633e-05, Learning Rate: 0.000189\n",
      "Epoch 19643/40000, Loss: 3.553530041244812e-05, Learning Rate: 0.000189\n",
      "Epoch 19644/40000, Loss: 3.552863563527353e-05, Learning Rate: 0.000189\n",
      "Epoch 19645/40000, Loss: 3.1377392588183284e-05, Learning Rate: 0.000189\n",
      "Epoch 19646/40000, Loss: 5.392852835939266e-05, Learning Rate: 0.000189\n",
      "Epoch 19647/40000, Loss: 4.0451890527037904e-05, Learning Rate: 0.000189\n",
      "Epoch 19648/40000, Loss: 3.5594701330410317e-05, Learning Rate: 0.000189\n",
      "Epoch 19649/40000, Loss: 6.01580977672711e-05, Learning Rate: 0.000189\n",
      "Epoch 19650/40000, Loss: 5.811953087686561e-05, Learning Rate: 0.000189\n",
      "Epoch 19651/40000, Loss: 6.0644142649834976e-05, Learning Rate: 0.000189\n",
      "Epoch 19652/40000, Loss: 3.5595403460320085e-05, Learning Rate: 0.000189\n",
      "Epoch 19653/40000, Loss: 3.553499118424952e-05, Learning Rate: 0.000189\n",
      "Epoch 19654/40000, Loss: 3.557105083018541e-05, Learning Rate: 0.000189\n",
      "Epoch 19655/40000, Loss: 4.085374530404806e-05, Learning Rate: 0.000189\n",
      "Epoch 19656/40000, Loss: 3.155173180857673e-05, Learning Rate: 0.000189\n",
      "Epoch 19657/40000, Loss: 3.152054705424234e-05, Learning Rate: 0.000189\n",
      "Epoch 19658/40000, Loss: 3.156211823807098e-05, Learning Rate: 0.000189\n",
      "Epoch 19659/40000, Loss: 3.572506830096245e-05, Learning Rate: 0.000189\n",
      "Epoch 19660/40000, Loss: 6.0746806411771104e-05, Learning Rate: 0.000189\n",
      "Epoch 19661/40000, Loss: 4.0774346416583285e-05, Learning Rate: 0.000189\n",
      "Epoch 19662/40000, Loss: 5.4216790886130184e-05, Learning Rate: 0.000189\n",
      "Epoch 19663/40000, Loss: 4.0843158785719424e-05, Learning Rate: 0.000189\n",
      "Epoch 19664/40000, Loss: 4.047896072734147e-05, Learning Rate: 0.000189\n",
      "Epoch 19665/40000, Loss: 1.4404002286028117e-05, Learning Rate: 0.000189\n",
      "Epoch 19666/40000, Loss: 3.5548444429878145e-05, Learning Rate: 0.000189\n",
      "Epoch 19667/40000, Loss: 5.550588684855029e-05, Learning Rate: 0.000189\n",
      "Epoch 19668/40000, Loss: 6.055850462871604e-05, Learning Rate: 0.000189\n",
      "Epoch 19669/40000, Loss: 5.8118923334404826e-05, Learning Rate: 0.000189\n",
      "Epoch 19670/40000, Loss: 5.492863056133501e-05, Learning Rate: 0.000189\n",
      "Epoch 19671/40000, Loss: 6.0840433434350416e-05, Learning Rate: 0.000189\n",
      "Epoch 19672/40000, Loss: 5.6193814089056104e-05, Learning Rate: 0.000189\n",
      "Epoch 19673/40000, Loss: 5.475540820043534e-05, Learning Rate: 0.000189\n",
      "Epoch 19674/40000, Loss: 1.5682111552450806e-05, Learning Rate: 0.000189\n",
      "Epoch 19675/40000, Loss: 1.4867734535073396e-05, Learning Rate: 0.000189\n",
      "Epoch 19676/40000, Loss: 5.837969365529716e-05, Learning Rate: 0.000189\n",
      "Epoch 19677/40000, Loss: 6.7036526161246e-05, Learning Rate: 0.000189\n",
      "Epoch 19678/40000, Loss: 5.916588634136133e-05, Learning Rate: 0.000189\n",
      "Epoch 19679/40000, Loss: 1.6279882402159274e-05, Learning Rate: 0.000189\n",
      "Epoch 19680/40000, Loss: 6.270607264013961e-05, Learning Rate: 0.000189\n",
      "Epoch 19681/40000, Loss: 3.83788938052021e-05, Learning Rate: 0.000189\n",
      "Epoch 19682/40000, Loss: 6.864091119496152e-05, Learning Rate: 0.000188\n",
      "Epoch 19683/40000, Loss: 2.296443926752545e-05, Learning Rate: 0.000188\n",
      "Epoch 19684/40000, Loss: 4.124181577935815e-05, Learning Rate: 0.000188\n",
      "Epoch 19685/40000, Loss: 6.806536111980677e-05, Learning Rate: 0.000188\n",
      "Epoch 19686/40000, Loss: 3.396737884031609e-05, Learning Rate: 0.000188\n",
      "Epoch 19687/40000, Loss: 6.606899114558473e-05, Learning Rate: 0.000188\n",
      "Epoch 19688/40000, Loss: 6.308570300461724e-05, Learning Rate: 0.000188\n",
      "Epoch 19689/40000, Loss: 5.793890522909351e-05, Learning Rate: 0.000188\n",
      "Epoch 19690/40000, Loss: 4.268353950465098e-05, Learning Rate: 0.000188\n",
      "Epoch 19691/40000, Loss: 3.649085192591883e-05, Learning Rate: 0.000188\n",
      "Epoch 19692/40000, Loss: 3.6118392017669976e-05, Learning Rate: 0.000188\n",
      "Epoch 19693/40000, Loss: 3.295067654107697e-05, Learning Rate: 0.000188\n",
      "Epoch 19694/40000, Loss: 4.134738628636114e-05, Learning Rate: 0.000188\n",
      "Epoch 19695/40000, Loss: 3.166890746797435e-05, Learning Rate: 0.000188\n",
      "Epoch 19696/40000, Loss: 1.4415938494494185e-05, Learning Rate: 0.000188\n",
      "Epoch 19697/40000, Loss: 3.1559422495774925e-05, Learning Rate: 0.000188\n",
      "Epoch 19698/40000, Loss: 1.4784794075239915e-05, Learning Rate: 0.000188\n",
      "Epoch 19699/40000, Loss: 6.134701106930152e-05, Learning Rate: 0.000188\n",
      "Epoch 19700/40000, Loss: 4.0964809159049764e-05, Learning Rate: 0.000188\n",
      "Epoch 19701/40000, Loss: 3.1511874112766236e-05, Learning Rate: 0.000188\n",
      "Epoch 19702/40000, Loss: 1.4465154890785925e-05, Learning Rate: 0.000188\n",
      "Epoch 19703/40000, Loss: 5.45441907888744e-05, Learning Rate: 0.000188\n",
      "Epoch 19704/40000, Loss: 4.1119481466012076e-05, Learning Rate: 0.000188\n",
      "Epoch 19705/40000, Loss: 5.5190557759488e-05, Learning Rate: 0.000188\n",
      "Epoch 19706/40000, Loss: 6.057045902707614e-05, Learning Rate: 0.000188\n",
      "Epoch 19707/40000, Loss: 5.7508335885358974e-05, Learning Rate: 0.000188\n",
      "Epoch 19708/40000, Loss: 3.1775125535205007e-05, Learning Rate: 0.000188\n",
      "Epoch 19709/40000, Loss: 6.332677003229037e-05, Learning Rate: 0.000188\n",
      "Epoch 19710/40000, Loss: 1.4759528312424663e-05, Learning Rate: 0.000188\n",
      "Epoch 19711/40000, Loss: 3.6245623050490394e-05, Learning Rate: 0.000188\n",
      "Epoch 19712/40000, Loss: 6.213242886587977e-05, Learning Rate: 0.000188\n",
      "Epoch 19713/40000, Loss: 6.48537024972029e-05, Learning Rate: 0.000188\n",
      "Epoch 19714/40000, Loss: 4.2134361137868837e-05, Learning Rate: 0.000188\n",
      "Epoch 19715/40000, Loss: 4.097054625162855e-05, Learning Rate: 0.000188\n",
      "Epoch 19716/40000, Loss: 6.05704881309066e-05, Learning Rate: 0.000188\n",
      "Epoch 19717/40000, Loss: 5.4171057854546234e-05, Learning Rate: 0.000188\n",
      "Epoch 19718/40000, Loss: 6.039163417881355e-05, Learning Rate: 0.000188\n",
      "Epoch 19719/40000, Loss: 3.194609234924428e-05, Learning Rate: 0.000188\n",
      "Epoch 19720/40000, Loss: 1.4417027159652207e-05, Learning Rate: 0.000188\n",
      "Epoch 19721/40000, Loss: 6.058944927644916e-05, Learning Rate: 0.000188\n",
      "Epoch 19722/40000, Loss: 4.068840644322336e-05, Learning Rate: 0.000188\n",
      "Epoch 19723/40000, Loss: 1.437991340935696e-05, Learning Rate: 0.000188\n",
      "Epoch 19724/40000, Loss: 5.372941086534411e-05, Learning Rate: 0.000188\n",
      "Epoch 19725/40000, Loss: 3.546770312823355e-05, Learning Rate: 0.000188\n",
      "Epoch 19726/40000, Loss: 1.4224218830349855e-05, Learning Rate: 0.000187\n",
      "Epoch 19727/40000, Loss: 3.541576734278351e-05, Learning Rate: 0.000187\n",
      "Epoch 19728/40000, Loss: 3.121378904324956e-05, Learning Rate: 0.000187\n",
      "Epoch 19729/40000, Loss: 3.5521450627129525e-05, Learning Rate: 0.000187\n",
      "Epoch 19730/40000, Loss: 4.0395083487965167e-05, Learning Rate: 0.000187\n",
      "Epoch 19731/40000, Loss: 5.982118454994634e-05, Learning Rate: 0.000187\n",
      "Epoch 19732/40000, Loss: 3.124183422187343e-05, Learning Rate: 0.000187\n",
      "Epoch 19733/40000, Loss: 1.4081146218813956e-05, Learning Rate: 0.000187\n",
      "Epoch 19734/40000, Loss: 4.031716161989607e-05, Learning Rate: 0.000187\n",
      "Epoch 19735/40000, Loss: 3.5369441320654005e-05, Learning Rate: 0.000187\n",
      "Epoch 19736/40000, Loss: 5.9836424043169245e-05, Learning Rate: 0.000187\n",
      "Epoch 19737/40000, Loss: 5.976766624371521e-05, Learning Rate: 0.000187\n",
      "Epoch 19738/40000, Loss: 3.127949821646325e-05, Learning Rate: 0.000187\n",
      "Epoch 19739/40000, Loss: 3.122379712294787e-05, Learning Rate: 0.000187\n",
      "Epoch 19740/40000, Loss: 4.037430335301906e-05, Learning Rate: 0.000187\n",
      "Epoch 19741/40000, Loss: 5.361871808418073e-05, Learning Rate: 0.000187\n",
      "Epoch 19742/40000, Loss: 5.99862469243817e-05, Learning Rate: 0.000187\n",
      "Epoch 19743/40000, Loss: 1.4157856639940292e-05, Learning Rate: 0.000187\n",
      "Epoch 19744/40000, Loss: 4.035060555906966e-05, Learning Rate: 0.000187\n",
      "Epoch 19745/40000, Loss: 3.538882447173819e-05, Learning Rate: 0.000187\n",
      "Epoch 19746/40000, Loss: 5.9882659115828574e-05, Learning Rate: 0.000187\n",
      "Epoch 19747/40000, Loss: 1.4144881788524799e-05, Learning Rate: 0.000187\n",
      "Epoch 19748/40000, Loss: 5.9911602875217795e-05, Learning Rate: 0.000187\n",
      "Epoch 19749/40000, Loss: 5.36557927262038e-05, Learning Rate: 0.000187\n",
      "Epoch 19750/40000, Loss: 4.0489088860340416e-05, Learning Rate: 0.000187\n",
      "Epoch 19751/40000, Loss: 6.009473145240918e-05, Learning Rate: 0.000187\n",
      "Epoch 19752/40000, Loss: 3.1228872103383765e-05, Learning Rate: 0.000187\n",
      "Epoch 19753/40000, Loss: 5.373682870413177e-05, Learning Rate: 0.000187\n",
      "Epoch 19754/40000, Loss: 3.1617430067854e-05, Learning Rate: 0.000187\n",
      "Epoch 19755/40000, Loss: 1.4287487829278689e-05, Learning Rate: 0.000187\n",
      "Epoch 19756/40000, Loss: 5.371933366404846e-05, Learning Rate: 0.000187\n",
      "Epoch 19757/40000, Loss: 4.054076998727396e-05, Learning Rate: 0.000187\n",
      "Epoch 19758/40000, Loss: 1.4377647858054843e-05, Learning Rate: 0.000187\n",
      "Epoch 19759/40000, Loss: 4.1195326048182324e-05, Learning Rate: 0.000187\n",
      "Epoch 19760/40000, Loss: 6.0599610151257366e-05, Learning Rate: 0.000187\n",
      "Epoch 19761/40000, Loss: 1.4475398529611994e-05, Learning Rate: 0.000187\n",
      "Epoch 19762/40000, Loss: 5.392678576754406e-05, Learning Rate: 0.000187\n",
      "Epoch 19763/40000, Loss: 4.0667695429874584e-05, Learning Rate: 0.000187\n",
      "Epoch 19764/40000, Loss: 6.0398317145882174e-05, Learning Rate: 0.000187\n",
      "Epoch 19765/40000, Loss: 3.129970718873665e-05, Learning Rate: 0.000187\n",
      "Epoch 19766/40000, Loss: 1.4313171050162055e-05, Learning Rate: 0.000187\n",
      "Epoch 19767/40000, Loss: 5.3749678045278415e-05, Learning Rate: 0.000187\n",
      "Epoch 19768/40000, Loss: 6.0359976487234235e-05, Learning Rate: 0.000187\n",
      "Epoch 19769/40000, Loss: 5.4152580560185015e-05, Learning Rate: 0.000187\n",
      "Epoch 19770/40000, Loss: 5.409158984548412e-05, Learning Rate: 0.000187\n",
      "Epoch 19771/40000, Loss: 6.330880569294095e-05, Learning Rate: 0.000186\n",
      "Epoch 19772/40000, Loss: 4.081727456650697e-05, Learning Rate: 0.000186\n",
      "Epoch 19773/40000, Loss: 3.148501491523348e-05, Learning Rate: 0.000186\n",
      "Epoch 19774/40000, Loss: 5.4001699027139693e-05, Learning Rate: 0.000186\n",
      "Epoch 19775/40000, Loss: 4.0743572753854096e-05, Learning Rate: 0.000186\n",
      "Epoch 19776/40000, Loss: 4.097245619050227e-05, Learning Rate: 0.000186\n",
      "Epoch 19777/40000, Loss: 5.414474799181335e-05, Learning Rate: 0.000186\n",
      "Epoch 19778/40000, Loss: 6.070686140446924e-05, Learning Rate: 0.000186\n",
      "Epoch 19779/40000, Loss: 3.565949009498581e-05, Learning Rate: 0.000186\n",
      "Epoch 19780/40000, Loss: 3.574482616386376e-05, Learning Rate: 0.000186\n",
      "Epoch 19781/40000, Loss: 4.109947258257307e-05, Learning Rate: 0.000186\n",
      "Epoch 19782/40000, Loss: 1.4884037227602676e-05, Learning Rate: 0.000186\n",
      "Epoch 19783/40000, Loss: 5.422098547569476e-05, Learning Rate: 0.000186\n",
      "Epoch 19784/40000, Loss: 4.150787208345719e-05, Learning Rate: 0.000186\n",
      "Epoch 19785/40000, Loss: 6.092197509133257e-05, Learning Rate: 0.000186\n",
      "Epoch 19786/40000, Loss: 6.0814316384494305e-05, Learning Rate: 0.000186\n",
      "Epoch 19787/40000, Loss: 1.5408579201903194e-05, Learning Rate: 0.000186\n",
      "Epoch 19788/40000, Loss: 1.5209376215352677e-05, Learning Rate: 0.000186\n",
      "Epoch 19789/40000, Loss: 3.626408215495758e-05, Learning Rate: 0.000186\n",
      "Epoch 19790/40000, Loss: 4.185036232229322e-05, Learning Rate: 0.000186\n",
      "Epoch 19791/40000, Loss: 4.180876931059174e-05, Learning Rate: 0.000186\n",
      "Epoch 19792/40000, Loss: 1.481244635215262e-05, Learning Rate: 0.000186\n",
      "Epoch 19793/40000, Loss: 6.112633127486333e-05, Learning Rate: 0.000186\n",
      "Epoch 19794/40000, Loss: 4.1257819248130545e-05, Learning Rate: 0.000186\n",
      "Epoch 19795/40000, Loss: 6.358504469972104e-05, Learning Rate: 0.000186\n",
      "Epoch 19796/40000, Loss: 3.6396868381416425e-05, Learning Rate: 0.000186\n",
      "Epoch 19797/40000, Loss: 3.616648609749973e-05, Learning Rate: 0.000186\n",
      "Epoch 19798/40000, Loss: 5.425403287517838e-05, Learning Rate: 0.000186\n",
      "Epoch 19799/40000, Loss: 4.12786757806316e-05, Learning Rate: 0.000186\n",
      "Epoch 19800/40000, Loss: 5.7120305427815765e-05, Learning Rate: 0.000186\n",
      "Epoch 19801/40000, Loss: 1.4965028640290257e-05, Learning Rate: 0.000186\n",
      "Epoch 19802/40000, Loss: 1.4615184227295686e-05, Learning Rate: 0.000186\n",
      "Epoch 19803/40000, Loss: 6.048740033293143e-05, Learning Rate: 0.000186\n",
      "Epoch 19804/40000, Loss: 6.016922270646319e-05, Learning Rate: 0.000186\n",
      "Epoch 19805/40000, Loss: 4.095155600225553e-05, Learning Rate: 0.000186\n",
      "Epoch 19806/40000, Loss: 4.130339220864698e-05, Learning Rate: 0.000186\n",
      "Epoch 19807/40000, Loss: 1.4437567187997047e-05, Learning Rate: 0.000186\n",
      "Epoch 19808/40000, Loss: 4.13167399528902e-05, Learning Rate: 0.000186\n",
      "Epoch 19809/40000, Loss: 5.506642992259003e-05, Learning Rate: 0.000186\n",
      "Epoch 19810/40000, Loss: 6.079204467823729e-05, Learning Rate: 0.000186\n",
      "Epoch 19811/40000, Loss: 3.2071253372123465e-05, Learning Rate: 0.000186\n",
      "Epoch 19812/40000, Loss: 6.175470480229706e-05, Learning Rate: 0.000186\n",
      "Epoch 19813/40000, Loss: 5.522213177755475e-05, Learning Rate: 0.000186\n",
      "Epoch 19814/40000, Loss: 5.408293509390205e-05, Learning Rate: 0.000186\n",
      "Epoch 19815/40000, Loss: 4.15270660596434e-05, Learning Rate: 0.000186\n",
      "Epoch 19816/40000, Loss: 3.257125717937015e-05, Learning Rate: 0.000185\n",
      "Epoch 19817/40000, Loss: 1.6301459254464135e-05, Learning Rate: 0.000185\n",
      "Epoch 19818/40000, Loss: 6.185509118949994e-05, Learning Rate: 0.000185\n",
      "Epoch 19819/40000, Loss: 3.227515480830334e-05, Learning Rate: 0.000185\n",
      "Epoch 19820/40000, Loss: 5.6858843890950084e-05, Learning Rate: 0.000185\n",
      "Epoch 19821/40000, Loss: 3.1884919735603034e-05, Learning Rate: 0.000185\n",
      "Epoch 19822/40000, Loss: 1.624661854293663e-05, Learning Rate: 0.000185\n",
      "Epoch 19823/40000, Loss: 4.998704753234051e-05, Learning Rate: 0.000185\n",
      "Epoch 19824/40000, Loss: 6.0570040659513324e-05, Learning Rate: 0.000185\n",
      "Epoch 19825/40000, Loss: 5.735345621360466e-05, Learning Rate: 0.000185\n",
      "Epoch 19826/40000, Loss: 3.7809248169651255e-05, Learning Rate: 0.000185\n",
      "Epoch 19827/40000, Loss: 1.6855554349604063e-05, Learning Rate: 0.000185\n",
      "Epoch 19828/40000, Loss: 3.8851823774166405e-05, Learning Rate: 0.000185\n",
      "Epoch 19829/40000, Loss: 3.66982385457959e-05, Learning Rate: 0.000185\n",
      "Epoch 19830/40000, Loss: 1.6361955204047263e-05, Learning Rate: 0.000185\n",
      "Epoch 19831/40000, Loss: 4.5017939555691555e-05, Learning Rate: 0.000185\n",
      "Epoch 19832/40000, Loss: 4.1035207686945796e-05, Learning Rate: 0.000185\n",
      "Epoch 19833/40000, Loss: 4.5889868488302454e-05, Learning Rate: 0.000185\n",
      "Epoch 19834/40000, Loss: 6.568612297996879e-05, Learning Rate: 0.000185\n",
      "Epoch 19835/40000, Loss: 6.371091876644641e-05, Learning Rate: 0.000185\n",
      "Epoch 19836/40000, Loss: 5.940103801549412e-05, Learning Rate: 0.000185\n",
      "Epoch 19837/40000, Loss: 3.24816137435846e-05, Learning Rate: 0.000185\n",
      "Epoch 19838/40000, Loss: 1.5385150618385524e-05, Learning Rate: 0.000185\n",
      "Epoch 19839/40000, Loss: 5.6975601182784885e-05, Learning Rate: 0.000185\n",
      "Epoch 19840/40000, Loss: 3.2800340704852715e-05, Learning Rate: 0.000185\n",
      "Epoch 19841/40000, Loss: 3.735535574378446e-05, Learning Rate: 0.000185\n",
      "Epoch 19842/40000, Loss: 4.1536281059961766e-05, Learning Rate: 0.000185\n",
      "Epoch 19843/40000, Loss: 3.597361865104176e-05, Learning Rate: 0.000185\n",
      "Epoch 19844/40000, Loss: 3.5599405237007886e-05, Learning Rate: 0.000185\n",
      "Epoch 19845/40000, Loss: 3.120128167211078e-05, Learning Rate: 0.000185\n",
      "Epoch 19846/40000, Loss: 6.033846875652671e-05, Learning Rate: 0.000185\n",
      "Epoch 19847/40000, Loss: 4.061843719682656e-05, Learning Rate: 0.000185\n",
      "Epoch 19848/40000, Loss: 3.544591163517907e-05, Learning Rate: 0.000185\n",
      "Epoch 19849/40000, Loss: 5.366596087696962e-05, Learning Rate: 0.000185\n",
      "Epoch 19850/40000, Loss: 6.019326610839926e-05, Learning Rate: 0.000185\n",
      "Epoch 19851/40000, Loss: 4.0617058402858675e-05, Learning Rate: 0.000185\n",
      "Epoch 19852/40000, Loss: 4.059724960825406e-05, Learning Rate: 0.000185\n",
      "Epoch 19853/40000, Loss: 4.04655838792678e-05, Learning Rate: 0.000185\n",
      "Epoch 19854/40000, Loss: 4.030854324810207e-05, Learning Rate: 0.000185\n",
      "Epoch 19855/40000, Loss: 3.113278580713086e-05, Learning Rate: 0.000185\n",
      "Epoch 19856/40000, Loss: 3.1107923859963194e-05, Learning Rate: 0.000185\n",
      "Epoch 19857/40000, Loss: 5.98256810917519e-05, Learning Rate: 0.000185\n",
      "Epoch 19858/40000, Loss: 3.529197056195699e-05, Learning Rate: 0.000185\n",
      "Epoch 19859/40000, Loss: 5.352158041205257e-05, Learning Rate: 0.000185\n",
      "Epoch 19860/40000, Loss: 1.4038470908417366e-05, Learning Rate: 0.000185\n",
      "Epoch 19861/40000, Loss: 3.54088879248593e-05, Learning Rate: 0.000184\n",
      "Epoch 19862/40000, Loss: 3.5335990105522797e-05, Learning Rate: 0.000184\n",
      "Epoch 19863/40000, Loss: 3.111195474048145e-05, Learning Rate: 0.000184\n",
      "Epoch 19864/40000, Loss: 3.1136467441683635e-05, Learning Rate: 0.000184\n",
      "Epoch 19865/40000, Loss: 5.36430197826121e-05, Learning Rate: 0.000184\n",
      "Epoch 19866/40000, Loss: 1.4153602023725398e-05, Learning Rate: 0.000184\n",
      "Epoch 19867/40000, Loss: 5.9817484725499526e-05, Learning Rate: 0.000184\n",
      "Epoch 19868/40000, Loss: 5.3606225264957175e-05, Learning Rate: 0.000184\n",
      "Epoch 19869/40000, Loss: 3.525122883729637e-05, Learning Rate: 0.000184\n",
      "Epoch 19870/40000, Loss: 5.9928588598268107e-05, Learning Rate: 0.000184\n",
      "Epoch 19871/40000, Loss: 3.535099676810205e-05, Learning Rate: 0.000184\n",
      "Epoch 19872/40000, Loss: 3.115704748779535e-05, Learning Rate: 0.000184\n",
      "Epoch 19873/40000, Loss: 4.028734838357195e-05, Learning Rate: 0.000184\n",
      "Epoch 19874/40000, Loss: 4.03312333219219e-05, Learning Rate: 0.000184\n",
      "Epoch 19875/40000, Loss: 4.026131864520721e-05, Learning Rate: 0.000184\n",
      "Epoch 19876/40000, Loss: 4.033905497635715e-05, Learning Rate: 0.000184\n",
      "Epoch 19877/40000, Loss: 1.4075509170652367e-05, Learning Rate: 0.000184\n",
      "Epoch 19878/40000, Loss: 4.030547643196769e-05, Learning Rate: 0.000184\n",
      "Epoch 19879/40000, Loss: 3.526252112351358e-05, Learning Rate: 0.000184\n",
      "Epoch 19880/40000, Loss: 4.0242724935524166e-05, Learning Rate: 0.000184\n",
      "Epoch 19881/40000, Loss: 1.4129408555163536e-05, Learning Rate: 0.000184\n",
      "Epoch 19882/40000, Loss: 3.106022268184461e-05, Learning Rate: 0.000184\n",
      "Epoch 19883/40000, Loss: 5.9848796809092164e-05, Learning Rate: 0.000184\n",
      "Epoch 19884/40000, Loss: 3.117992673651315e-05, Learning Rate: 0.000184\n",
      "Epoch 19885/40000, Loss: 5.358512862585485e-05, Learning Rate: 0.000184\n",
      "Epoch 19886/40000, Loss: 4.0324786823475733e-05, Learning Rate: 0.000184\n",
      "Epoch 19887/40000, Loss: 3.106048097833991e-05, Learning Rate: 0.000184\n",
      "Epoch 19888/40000, Loss: 3.5329630918568e-05, Learning Rate: 0.000184\n",
      "Epoch 19889/40000, Loss: 3.106288204435259e-05, Learning Rate: 0.000184\n",
      "Epoch 19890/40000, Loss: 3.536756048561074e-05, Learning Rate: 0.000184\n",
      "Epoch 19891/40000, Loss: 1.4198392818798311e-05, Learning Rate: 0.000184\n",
      "Epoch 19892/40000, Loss: 5.98276274104137e-05, Learning Rate: 0.000184\n",
      "Epoch 19893/40000, Loss: 5.988844350213185e-05, Learning Rate: 0.000184\n",
      "Epoch 19894/40000, Loss: 3.102491973550059e-05, Learning Rate: 0.000184\n",
      "Epoch 19895/40000, Loss: 5.9730446082539856e-05, Learning Rate: 0.000184\n",
      "Epoch 19896/40000, Loss: 1.4105723494139966e-05, Learning Rate: 0.000184\n",
      "Epoch 19897/40000, Loss: 3.1179912184597924e-05, Learning Rate: 0.000184\n",
      "Epoch 19898/40000, Loss: 3.113124330411665e-05, Learning Rate: 0.000184\n",
      "Epoch 19899/40000, Loss: 3.5290973755763844e-05, Learning Rate: 0.000184\n",
      "Epoch 19900/40000, Loss: 3.547885353327729e-05, Learning Rate: 0.000184\n",
      "Epoch 19901/40000, Loss: 3.5296914575155824e-05, Learning Rate: 0.000184\n",
      "Epoch 19902/40000, Loss: 6.036832928657532e-05, Learning Rate: 0.000184\n",
      "Epoch 19903/40000, Loss: 4.040934072691016e-05, Learning Rate: 0.000184\n",
      "Epoch 19904/40000, Loss: 3.111734622507356e-05, Learning Rate: 0.000184\n",
      "Epoch 19905/40000, Loss: 1.4307077435660176e-05, Learning Rate: 0.000184\n",
      "Epoch 19906/40000, Loss: 5.3716325055575e-05, Learning Rate: 0.000183\n",
      "Epoch 19907/40000, Loss: 3.533986455295235e-05, Learning Rate: 0.000183\n",
      "Epoch 19908/40000, Loss: 5.3911207942292094e-05, Learning Rate: 0.000183\n",
      "Epoch 19909/40000, Loss: 1.43292509164894e-05, Learning Rate: 0.000183\n",
      "Epoch 19910/40000, Loss: 4.0696628275327384e-05, Learning Rate: 0.000183\n",
      "Epoch 19911/40000, Loss: 3.570003536879085e-05, Learning Rate: 0.000183\n",
      "Epoch 19912/40000, Loss: 3.545269282767549e-05, Learning Rate: 0.000183\n",
      "Epoch 19913/40000, Loss: 6.005113391438499e-05, Learning Rate: 0.000183\n",
      "Epoch 19914/40000, Loss: 3.132443816866726e-05, Learning Rate: 0.000183\n",
      "Epoch 19915/40000, Loss: 5.384177347877994e-05, Learning Rate: 0.000183\n",
      "Epoch 19916/40000, Loss: 3.538685632520355e-05, Learning Rate: 0.000183\n",
      "Epoch 19917/40000, Loss: 5.9937032347079366e-05, Learning Rate: 0.000183\n",
      "Epoch 19918/40000, Loss: 5.98127517150715e-05, Learning Rate: 0.000183\n",
      "Epoch 19919/40000, Loss: 5.39524044143036e-05, Learning Rate: 0.000183\n",
      "Epoch 19920/40000, Loss: 3.529073001118377e-05, Learning Rate: 0.000183\n",
      "Epoch 19921/40000, Loss: 1.4216810996003915e-05, Learning Rate: 0.000183\n",
      "Epoch 19922/40000, Loss: 4.053804150316864e-05, Learning Rate: 0.000183\n",
      "Epoch 19923/40000, Loss: 5.358569978852756e-05, Learning Rate: 0.000183\n",
      "Epoch 19924/40000, Loss: 5.370266444515437e-05, Learning Rate: 0.000183\n",
      "Epoch 19925/40000, Loss: 3.1374289392260835e-05, Learning Rate: 0.000183\n",
      "Epoch 19926/40000, Loss: 3.556795127224177e-05, Learning Rate: 0.000183\n",
      "Epoch 19927/40000, Loss: 3.5540102544473484e-05, Learning Rate: 0.000183\n",
      "Epoch 19928/40000, Loss: 4.119773075217381e-05, Learning Rate: 0.000183\n",
      "Epoch 19929/40000, Loss: 3.594287409214303e-05, Learning Rate: 0.000183\n",
      "Epoch 19930/40000, Loss: 5.4720301704946905e-05, Learning Rate: 0.000183\n",
      "Epoch 19931/40000, Loss: 5.4828116844873875e-05, Learning Rate: 0.000183\n",
      "Epoch 19932/40000, Loss: 3.586864477256313e-05, Learning Rate: 0.000183\n",
      "Epoch 19933/40000, Loss: 4.3607262341538444e-05, Learning Rate: 0.000183\n",
      "Epoch 19934/40000, Loss: 5.6040127674350515e-05, Learning Rate: 0.000183\n",
      "Epoch 19935/40000, Loss: 4.705332685261965e-05, Learning Rate: 0.000183\n",
      "Epoch 19936/40000, Loss: 3.713262776727788e-05, Learning Rate: 0.000183\n",
      "Epoch 19937/40000, Loss: 4.2662755731726065e-05, Learning Rate: 0.000183\n",
      "Epoch 19938/40000, Loss: 1.5462910596397705e-05, Learning Rate: 0.000183\n",
      "Epoch 19939/40000, Loss: 6.162496720207855e-05, Learning Rate: 0.000183\n",
      "Epoch 19940/40000, Loss: 6.0988979385001585e-05, Learning Rate: 0.000183\n",
      "Epoch 19941/40000, Loss: 5.486337613547221e-05, Learning Rate: 0.000183\n",
      "Epoch 19942/40000, Loss: 4.138206350035034e-05, Learning Rate: 0.000183\n",
      "Epoch 19943/40000, Loss: 6.468988431151956e-05, Learning Rate: 0.000183\n",
      "Epoch 19944/40000, Loss: 3.615438617998734e-05, Learning Rate: 0.000183\n",
      "Epoch 19945/40000, Loss: 6.479015428340062e-05, Learning Rate: 0.000183\n",
      "Epoch 19946/40000, Loss: 1.4850907973595895e-05, Learning Rate: 0.000183\n",
      "Epoch 19947/40000, Loss: 1.4511718291032594e-05, Learning Rate: 0.000183\n",
      "Epoch 19948/40000, Loss: 1.4514766917272937e-05, Learning Rate: 0.000183\n",
      "Epoch 19949/40000, Loss: 8.165793406078592e-05, Learning Rate: 0.000183\n",
      "Epoch 19950/40000, Loss: 1.481116942159133e-05, Learning Rate: 0.000183\n",
      "Epoch 19951/40000, Loss: 4.155137139605358e-05, Learning Rate: 0.000183\n",
      "Epoch 19952/40000, Loss: 1.4575687600881793e-05, Learning Rate: 0.000182\n",
      "Epoch 19953/40000, Loss: 6.092215335229412e-05, Learning Rate: 0.000182\n",
      "Epoch 19954/40000, Loss: 3.141171328024939e-05, Learning Rate: 0.000182\n",
      "Epoch 19955/40000, Loss: 3.1147024856181815e-05, Learning Rate: 0.000182\n",
      "Epoch 19956/40000, Loss: 6.01466563239228e-05, Learning Rate: 0.000182\n",
      "Epoch 19957/40000, Loss: 3.556674346327782e-05, Learning Rate: 0.000182\n",
      "Epoch 19958/40000, Loss: 5.3992716857464984e-05, Learning Rate: 0.000182\n",
      "Epoch 19959/40000, Loss: 1.4264062883739825e-05, Learning Rate: 0.000182\n",
      "Epoch 19960/40000, Loss: 1.427955248800572e-05, Learning Rate: 0.000182\n",
      "Epoch 19961/40000, Loss: 5.3619496611645445e-05, Learning Rate: 0.000182\n",
      "Epoch 19962/40000, Loss: 5.994037928758189e-05, Learning Rate: 0.000182\n",
      "Epoch 19963/40000, Loss: 1.4450205526372883e-05, Learning Rate: 0.000182\n",
      "Epoch 19964/40000, Loss: 3.5493369068717584e-05, Learning Rate: 0.000182\n",
      "Epoch 19965/40000, Loss: 5.3902025683782995e-05, Learning Rate: 0.000182\n",
      "Epoch 19966/40000, Loss: 6.011281948303804e-05, Learning Rate: 0.000182\n",
      "Epoch 19967/40000, Loss: 3.5722721804631874e-05, Learning Rate: 0.000182\n",
      "Epoch 19968/40000, Loss: 1.46983475133311e-05, Learning Rate: 0.000182\n",
      "Epoch 19969/40000, Loss: 3.123905844404362e-05, Learning Rate: 0.000182\n",
      "Epoch 19970/40000, Loss: 3.120917608612217e-05, Learning Rate: 0.000182\n",
      "Epoch 19971/40000, Loss: 3.529660898493603e-05, Learning Rate: 0.000182\n",
      "Epoch 19972/40000, Loss: 5.404816329246387e-05, Learning Rate: 0.000182\n",
      "Epoch 19973/40000, Loss: 1.4862881471344735e-05, Learning Rate: 0.000182\n",
      "Epoch 19974/40000, Loss: 6.0948375903535634e-05, Learning Rate: 0.000182\n",
      "Epoch 19975/40000, Loss: 4.20118922193069e-05, Learning Rate: 0.000182\n",
      "Epoch 19976/40000, Loss: 6.241236405912787e-05, Learning Rate: 0.000182\n",
      "Epoch 19977/40000, Loss: 3.191220821463503e-05, Learning Rate: 0.000182\n",
      "Epoch 19978/40000, Loss: 1.5901799997664057e-05, Learning Rate: 0.000182\n",
      "Epoch 19979/40000, Loss: 3.342423588037491e-05, Learning Rate: 0.000182\n",
      "Epoch 19980/40000, Loss: 5.810109360027127e-05, Learning Rate: 0.000182\n",
      "Epoch 19981/40000, Loss: 4.292317680665292e-05, Learning Rate: 0.000182\n",
      "Epoch 19982/40000, Loss: 3.677439963212237e-05, Learning Rate: 0.000182\n",
      "Epoch 19983/40000, Loss: 1.537896787340287e-05, Learning Rate: 0.000182\n",
      "Epoch 19984/40000, Loss: 5.57369930902496e-05, Learning Rate: 0.000182\n",
      "Epoch 19985/40000, Loss: 3.2478070352226496e-05, Learning Rate: 0.000182\n",
      "Epoch 19986/40000, Loss: 6.06107605563011e-05, Learning Rate: 0.000182\n",
      "Epoch 19987/40000, Loss: 4.1341441828990355e-05, Learning Rate: 0.000182\n",
      "Epoch 19988/40000, Loss: 6.267325807129964e-05, Learning Rate: 0.000182\n",
      "Epoch 19989/40000, Loss: 1.4692465811094735e-05, Learning Rate: 0.000182\n",
      "Epoch 19990/40000, Loss: 4.143095065956004e-05, Learning Rate: 0.000182\n",
      "Epoch 19991/40000, Loss: 6.103270061430521e-05, Learning Rate: 0.000182\n",
      "Epoch 19992/40000, Loss: 6.084756023483351e-05, Learning Rate: 0.000182\n",
      "Epoch 19993/40000, Loss: 4.2384945118101314e-05, Learning Rate: 0.000182\n",
      "Epoch 19994/40000, Loss: 3.156380262225866e-05, Learning Rate: 0.000182\n",
      "Epoch 19995/40000, Loss: 3.585458034649491e-05, Learning Rate: 0.000182\n",
      "Epoch 19996/40000, Loss: 5.429033990367316e-05, Learning Rate: 0.000182\n",
      "Epoch 19997/40000, Loss: 3.132173151243478e-05, Learning Rate: 0.000181\n",
      "Epoch 19998/40000, Loss: 3.554782233550213e-05, Learning Rate: 0.000181\n",
      "Epoch 19999/40000, Loss: 3.1298572139348835e-05, Learning Rate: 0.000181\n",
      "Epoch 20000/40000, Loss: 4.0891693060984835e-05, Learning Rate: 0.000181\n",
      "Epoch 20001/40000, Loss: 5.998759297654033e-05, Learning Rate: 0.000181\n",
      "Epoch 20002/40000, Loss: 4.098828139831312e-05, Learning Rate: 0.000181\n",
      "Epoch 20003/40000, Loss: 3.123872011201456e-05, Learning Rate: 0.000181\n",
      "Epoch 20004/40000, Loss: 4.090810034540482e-05, Learning Rate: 0.000181\n",
      "Epoch 20005/40000, Loss: 1.4385672329808585e-05, Learning Rate: 0.000181\n",
      "Epoch 20006/40000, Loss: 3.549039684003219e-05, Learning Rate: 0.000181\n",
      "Epoch 20007/40000, Loss: 3.538413147907704e-05, Learning Rate: 0.000181\n",
      "Epoch 20008/40000, Loss: 5.3704508900409564e-05, Learning Rate: 0.000181\n",
      "Epoch 20009/40000, Loss: 3.1131588912103325e-05, Learning Rate: 0.000181\n",
      "Epoch 20010/40000, Loss: 5.406484706327319e-05, Learning Rate: 0.000181\n",
      "Epoch 20011/40000, Loss: 5.3645027946913615e-05, Learning Rate: 0.000181\n",
      "Epoch 20012/40000, Loss: 3.141656634397805e-05, Learning Rate: 0.000181\n",
      "Epoch 20013/40000, Loss: 4.071156945428811e-05, Learning Rate: 0.000181\n",
      "Epoch 20014/40000, Loss: 4.06393337470945e-05, Learning Rate: 0.000181\n",
      "Epoch 20015/40000, Loss: 3.151814598822966e-05, Learning Rate: 0.000181\n",
      "Epoch 20016/40000, Loss: 6.004343958920799e-05, Learning Rate: 0.000181\n",
      "Epoch 20017/40000, Loss: 1.4469632333202753e-05, Learning Rate: 0.000181\n",
      "Epoch 20018/40000, Loss: 3.557082891347818e-05, Learning Rate: 0.000181\n",
      "Epoch 20019/40000, Loss: 3.1067382224136963e-05, Learning Rate: 0.000181\n",
      "Epoch 20020/40000, Loss: 3.1233175832312554e-05, Learning Rate: 0.000181\n",
      "Epoch 20021/40000, Loss: 5.379685899242759e-05, Learning Rate: 0.000181\n",
      "Epoch 20022/40000, Loss: 5.3632869821740314e-05, Learning Rate: 0.000181\n",
      "Epoch 20023/40000, Loss: 4.0661489038029686e-05, Learning Rate: 0.000181\n",
      "Epoch 20024/40000, Loss: 4.0995553717948496e-05, Learning Rate: 0.000181\n",
      "Epoch 20025/40000, Loss: 1.4534420188283548e-05, Learning Rate: 0.000181\n",
      "Epoch 20026/40000, Loss: 5.3821619076188654e-05, Learning Rate: 0.000181\n",
      "Epoch 20027/40000, Loss: 4.0706385334488004e-05, Learning Rate: 0.000181\n",
      "Epoch 20028/40000, Loss: 3.1256116926670074e-05, Learning Rate: 0.000181\n",
      "Epoch 20029/40000, Loss: 4.071693911100738e-05, Learning Rate: 0.000181\n",
      "Epoch 20030/40000, Loss: 1.4377847037394531e-05, Learning Rate: 0.000181\n",
      "Epoch 20031/40000, Loss: 4.0635033656144515e-05, Learning Rate: 0.000181\n",
      "Epoch 20032/40000, Loss: 5.361381045076996e-05, Learning Rate: 0.000181\n",
      "Epoch 20033/40000, Loss: 6.0177771956659853e-05, Learning Rate: 0.000181\n",
      "Epoch 20034/40000, Loss: 5.977599721518345e-05, Learning Rate: 0.000181\n",
      "Epoch 20035/40000, Loss: 3.1023784686112776e-05, Learning Rate: 0.000181\n",
      "Epoch 20036/40000, Loss: 3.5223423765273765e-05, Learning Rate: 0.000181\n",
      "Epoch 20037/40000, Loss: 3.106378426309675e-05, Learning Rate: 0.000181\n",
      "Epoch 20038/40000, Loss: 5.350399442249909e-05, Learning Rate: 0.000181\n",
      "Epoch 20039/40000, Loss: 3.123977148788981e-05, Learning Rate: 0.000181\n",
      "Epoch 20040/40000, Loss: 1.4158447811496444e-05, Learning Rate: 0.000181\n",
      "Epoch 20041/40000, Loss: 3.10378527501598e-05, Learning Rate: 0.000181\n",
      "Epoch 20042/40000, Loss: 4.052291114930995e-05, Learning Rate: 0.000181\n",
      "Epoch 20043/40000, Loss: 6.014891914674081e-05, Learning Rate: 0.000180\n",
      "Epoch 20044/40000, Loss: 3.515676507959142e-05, Learning Rate: 0.000180\n",
      "Epoch 20045/40000, Loss: 1.404538761562435e-05, Learning Rate: 0.000180\n",
      "Epoch 20046/40000, Loss: 1.4108576579019427e-05, Learning Rate: 0.000180\n",
      "Epoch 20047/40000, Loss: 5.3462146752281114e-05, Learning Rate: 0.000180\n",
      "Epoch 20048/40000, Loss: 5.3412761189974844e-05, Learning Rate: 0.000180\n",
      "Epoch 20049/40000, Loss: 5.974207670078613e-05, Learning Rate: 0.000180\n",
      "Epoch 20050/40000, Loss: 4.0358034311793745e-05, Learning Rate: 0.000180\n",
      "Epoch 20051/40000, Loss: 5.969459380139597e-05, Learning Rate: 0.000180\n",
      "Epoch 20052/40000, Loss: 3.104624920524657e-05, Learning Rate: 0.000180\n",
      "Epoch 20053/40000, Loss: 1.4154712516756263e-05, Learning Rate: 0.000180\n",
      "Epoch 20054/40000, Loss: 4.030331547255628e-05, Learning Rate: 0.000180\n",
      "Epoch 20055/40000, Loss: 5.96355639572721e-05, Learning Rate: 0.000180\n",
      "Epoch 20056/40000, Loss: 3.517582081258297e-05, Learning Rate: 0.000180\n",
      "Epoch 20057/40000, Loss: 1.4212279893399682e-05, Learning Rate: 0.000180\n",
      "Epoch 20058/40000, Loss: 5.35494509676937e-05, Learning Rate: 0.000180\n",
      "Epoch 20059/40000, Loss: 5.9570797020569444e-05, Learning Rate: 0.000180\n",
      "Epoch 20060/40000, Loss: 5.344887176761404e-05, Learning Rate: 0.000180\n",
      "Epoch 20061/40000, Loss: 3.5182958527002484e-05, Learning Rate: 0.000180\n",
      "Epoch 20062/40000, Loss: 5.9667414461728185e-05, Learning Rate: 0.000180\n",
      "Epoch 20063/40000, Loss: 5.968823825241998e-05, Learning Rate: 0.000180\n",
      "Epoch 20064/40000, Loss: 3.1068557291291654e-05, Learning Rate: 0.000180\n",
      "Epoch 20065/40000, Loss: 4.051999349030666e-05, Learning Rate: 0.000180\n",
      "Epoch 20066/40000, Loss: 3.101279799011536e-05, Learning Rate: 0.000180\n",
      "Epoch 20067/40000, Loss: 5.982354196021333e-05, Learning Rate: 0.000180\n",
      "Epoch 20068/40000, Loss: 5.9767411585198715e-05, Learning Rate: 0.000180\n",
      "Epoch 20069/40000, Loss: 6.0057733207941055e-05, Learning Rate: 0.000180\n",
      "Epoch 20070/40000, Loss: 3.171467687934637e-05, Learning Rate: 0.000180\n",
      "Epoch 20071/40000, Loss: 3.5793822462437674e-05, Learning Rate: 0.000180\n",
      "Epoch 20072/40000, Loss: 1.6217176380450837e-05, Learning Rate: 0.000180\n",
      "Epoch 20073/40000, Loss: 5.499404142028652e-05, Learning Rate: 0.000180\n",
      "Epoch 20074/40000, Loss: 1.5323284969781525e-05, Learning Rate: 0.000180\n",
      "Epoch 20075/40000, Loss: 6.0290174587862566e-05, Learning Rate: 0.000180\n",
      "Epoch 20076/40000, Loss: 5.6799210142344236e-05, Learning Rate: 0.000180\n",
      "Epoch 20077/40000, Loss: 1.595759022166021e-05, Learning Rate: 0.000180\n",
      "Epoch 20078/40000, Loss: 4.372679177322425e-05, Learning Rate: 0.000180\n",
      "Epoch 20079/40000, Loss: 3.6216100852470845e-05, Learning Rate: 0.000180\n",
      "Epoch 20080/40000, Loss: 1.6890680853975937e-05, Learning Rate: 0.000180\n",
      "Epoch 20081/40000, Loss: 4.4901706132804975e-05, Learning Rate: 0.000180\n",
      "Epoch 20082/40000, Loss: 3.6774679756490514e-05, Learning Rate: 0.000180\n",
      "Epoch 20083/40000, Loss: 3.690720404847525e-05, Learning Rate: 0.000180\n",
      "Epoch 20084/40000, Loss: 6.172011489979923e-05, Learning Rate: 0.000180\n",
      "Epoch 20085/40000, Loss: 6.13783995504491e-05, Learning Rate: 0.000180\n",
      "Epoch 20086/40000, Loss: 4.537841959972866e-05, Learning Rate: 0.000180\n",
      "Epoch 20087/40000, Loss: 4.3700554670067504e-05, Learning Rate: 0.000180\n",
      "Epoch 20088/40000, Loss: 3.354154614498839e-05, Learning Rate: 0.000180\n",
      "Epoch 20089/40000, Loss: 3.975207800976932e-05, Learning Rate: 0.000180\n",
      "Epoch 20090/40000, Loss: 4.376597280497663e-05, Learning Rate: 0.000179\n",
      "Epoch 20091/40000, Loss: 6.425521860364825e-05, Learning Rate: 0.000179\n",
      "Epoch 20092/40000, Loss: 3.6607409128919244e-05, Learning Rate: 0.000179\n",
      "Epoch 20093/40000, Loss: 3.1754814699525014e-05, Learning Rate: 0.000179\n",
      "Epoch 20094/40000, Loss: 3.599686169764027e-05, Learning Rate: 0.000179\n",
      "Epoch 20095/40000, Loss: 6.1024344176985323e-05, Learning Rate: 0.000179\n",
      "Epoch 20096/40000, Loss: 3.225320324418135e-05, Learning Rate: 0.000179\n",
      "Epoch 20097/40000, Loss: 6.045885311323218e-05, Learning Rate: 0.000179\n",
      "Epoch 20098/40000, Loss: 1.4782724974793382e-05, Learning Rate: 0.000179\n",
      "Epoch 20099/40000, Loss: 5.50273762200959e-05, Learning Rate: 0.000179\n",
      "Epoch 20100/40000, Loss: 5.413126928033307e-05, Learning Rate: 0.000179\n",
      "Epoch 20101/40000, Loss: 5.384594987845048e-05, Learning Rate: 0.000179\n",
      "Epoch 20102/40000, Loss: 1.4575954082829412e-05, Learning Rate: 0.000179\n",
      "Epoch 20103/40000, Loss: 3.125984949292615e-05, Learning Rate: 0.000179\n",
      "Epoch 20104/40000, Loss: 1.5028143025119789e-05, Learning Rate: 0.000179\n",
      "Epoch 20105/40000, Loss: 4.079857899341732e-05, Learning Rate: 0.000179\n",
      "Epoch 20106/40000, Loss: 5.9783662436529994e-05, Learning Rate: 0.000179\n",
      "Epoch 20107/40000, Loss: 3.524204657878727e-05, Learning Rate: 0.000179\n",
      "Epoch 20108/40000, Loss: 4.064861423103139e-05, Learning Rate: 0.000179\n",
      "Epoch 20109/40000, Loss: 5.9804311604239047e-05, Learning Rate: 0.000179\n",
      "Epoch 20110/40000, Loss: 5.975453314022161e-05, Learning Rate: 0.000179\n",
      "Epoch 20111/40000, Loss: 4.101799277123064e-05, Learning Rate: 0.000179\n",
      "Epoch 20112/40000, Loss: 3.098602974205278e-05, Learning Rate: 0.000179\n",
      "Epoch 20113/40000, Loss: 3.52494498656597e-05, Learning Rate: 0.000179\n",
      "Epoch 20114/40000, Loss: 4.044449815410189e-05, Learning Rate: 0.000179\n",
      "Epoch 20115/40000, Loss: 3.0927978514228016e-05, Learning Rate: 0.000179\n",
      "Epoch 20116/40000, Loss: 5.344601595425047e-05, Learning Rate: 0.000179\n",
      "Epoch 20117/40000, Loss: 3.0995077395346016e-05, Learning Rate: 0.000179\n",
      "Epoch 20118/40000, Loss: 3.1004397897049785e-05, Learning Rate: 0.000179\n",
      "Epoch 20119/40000, Loss: 3.0876879463903606e-05, Learning Rate: 0.000179\n",
      "Epoch 20120/40000, Loss: 3.522808037814684e-05, Learning Rate: 0.000179\n",
      "Epoch 20121/40000, Loss: 1.4062620721233543e-05, Learning Rate: 0.000179\n",
      "Epoch 20122/40000, Loss: 4.037763210362755e-05, Learning Rate: 0.000179\n",
      "Epoch 20123/40000, Loss: 1.420340959157329e-05, Learning Rate: 0.000179\n",
      "Epoch 20124/40000, Loss: 5.354097083909437e-05, Learning Rate: 0.000179\n",
      "Epoch 20125/40000, Loss: 5.969696576357819e-05, Learning Rate: 0.000179\n",
      "Epoch 20126/40000, Loss: 3.528351953718811e-05, Learning Rate: 0.000179\n",
      "Epoch 20127/40000, Loss: 5.983866867609322e-05, Learning Rate: 0.000179\n",
      "Epoch 20128/40000, Loss: 5.409735604189336e-05, Learning Rate: 0.000179\n",
      "Epoch 20129/40000, Loss: 5.379008871386759e-05, Learning Rate: 0.000179\n",
      "Epoch 20130/40000, Loss: 6.283842958509922e-05, Learning Rate: 0.000179\n",
      "Epoch 20131/40000, Loss: 6.144834333099425e-05, Learning Rate: 0.000179\n",
      "Epoch 20132/40000, Loss: 3.539742101565935e-05, Learning Rate: 0.000179\n",
      "Epoch 20133/40000, Loss: 3.0927003535907716e-05, Learning Rate: 0.000179\n",
      "Epoch 20134/40000, Loss: 3.622769872890785e-05, Learning Rate: 0.000179\n",
      "Epoch 20135/40000, Loss: 5.471725671668537e-05, Learning Rate: 0.000179\n",
      "Epoch 20136/40000, Loss: 1.43935794767458e-05, Learning Rate: 0.000178\n",
      "Epoch 20137/40000, Loss: 1.4470666428678669e-05, Learning Rate: 0.000178\n",
      "Epoch 20138/40000, Loss: 3.558575190254487e-05, Learning Rate: 0.000178\n",
      "Epoch 20139/40000, Loss: 5.444246926344931e-05, Learning Rate: 0.000178\n",
      "Epoch 20140/40000, Loss: 3.664760151877999e-05, Learning Rate: 0.000178\n",
      "Epoch 20141/40000, Loss: 4.097567216376774e-05, Learning Rate: 0.000178\n",
      "Epoch 20142/40000, Loss: 4.095577969565056e-05, Learning Rate: 0.000178\n",
      "Epoch 20143/40000, Loss: 1.442617485736264e-05, Learning Rate: 0.000178\n",
      "Epoch 20144/40000, Loss: 4.097174314665608e-05, Learning Rate: 0.000178\n",
      "Epoch 20145/40000, Loss: 1.4841921256447677e-05, Learning Rate: 0.000178\n",
      "Epoch 20146/40000, Loss: 1.4446534805756528e-05, Learning Rate: 0.000178\n",
      "Epoch 20147/40000, Loss: 4.0791459468891844e-05, Learning Rate: 0.000178\n",
      "Epoch 20148/40000, Loss: 3.534659117576666e-05, Learning Rate: 0.000178\n",
      "Epoch 20149/40000, Loss: 1.432612680218881e-05, Learning Rate: 0.000178\n",
      "Epoch 20150/40000, Loss: 3.6165325582260266e-05, Learning Rate: 0.000178\n",
      "Epoch 20151/40000, Loss: 4.1247349145123735e-05, Learning Rate: 0.000178\n",
      "Epoch 20152/40000, Loss: 4.111765156267211e-05, Learning Rate: 0.000178\n",
      "Epoch 20153/40000, Loss: 6.103667692514136e-05, Learning Rate: 0.000178\n",
      "Epoch 20154/40000, Loss: 3.174503581249155e-05, Learning Rate: 0.000178\n",
      "Epoch 20155/40000, Loss: 6.130479596322402e-05, Learning Rate: 0.000178\n",
      "Epoch 20156/40000, Loss: 1.6175334167201072e-05, Learning Rate: 0.000178\n",
      "Epoch 20157/40000, Loss: 6.213458982529119e-05, Learning Rate: 0.000178\n",
      "Epoch 20158/40000, Loss: 3.172178185195662e-05, Learning Rate: 0.000178\n",
      "Epoch 20159/40000, Loss: 5.566665277001448e-05, Learning Rate: 0.000178\n",
      "Epoch 20160/40000, Loss: 5.640720701194368e-05, Learning Rate: 0.000178\n",
      "Epoch 20161/40000, Loss: 6.0735521401511505e-05, Learning Rate: 0.000178\n",
      "Epoch 20162/40000, Loss: 3.7462134059751406e-05, Learning Rate: 0.000178\n",
      "Epoch 20163/40000, Loss: 5.479049286805093e-05, Learning Rate: 0.000178\n",
      "Epoch 20164/40000, Loss: 6.095063145039603e-05, Learning Rate: 0.000178\n",
      "Epoch 20165/40000, Loss: 1.527143467683345e-05, Learning Rate: 0.000178\n",
      "Epoch 20166/40000, Loss: 3.284031481598504e-05, Learning Rate: 0.000178\n",
      "Epoch 20167/40000, Loss: 5.634776607621461e-05, Learning Rate: 0.000178\n",
      "Epoch 20168/40000, Loss: 3.599957926780917e-05, Learning Rate: 0.000178\n",
      "Epoch 20169/40000, Loss: 1.4741171071364079e-05, Learning Rate: 0.000178\n",
      "Epoch 20170/40000, Loss: 3.249891597079113e-05, Learning Rate: 0.000178\n",
      "Epoch 20171/40000, Loss: 6.242484232643619e-05, Learning Rate: 0.000178\n",
      "Epoch 20172/40000, Loss: 4.2504503653617576e-05, Learning Rate: 0.000178\n",
      "Epoch 20173/40000, Loss: 4.219626498525031e-05, Learning Rate: 0.000178\n",
      "Epoch 20174/40000, Loss: 1.5655168681405485e-05, Learning Rate: 0.000178\n",
      "Epoch 20175/40000, Loss: 6.0889433370903134e-05, Learning Rate: 0.000178\n",
      "Epoch 20176/40000, Loss: 4.41564989159815e-05, Learning Rate: 0.000178\n",
      "Epoch 20177/40000, Loss: 5.606482591247186e-05, Learning Rate: 0.000178\n",
      "Epoch 20178/40000, Loss: 3.7890033127041534e-05, Learning Rate: 0.000178\n",
      "Epoch 20179/40000, Loss: 2.1165056750760414e-05, Learning Rate: 0.000178\n",
      "Epoch 20180/40000, Loss: 6.137855962151662e-05, Learning Rate: 0.000178\n",
      "Epoch 20181/40000, Loss: 3.803271465585567e-05, Learning Rate: 0.000178\n",
      "Epoch 20182/40000, Loss: 1.4903212104400154e-05, Learning Rate: 0.000178\n",
      "Epoch 20183/40000, Loss: 1.4988059774623252e-05, Learning Rate: 0.000177\n",
      "Epoch 20184/40000, Loss: 4.1029194107977673e-05, Learning Rate: 0.000177\n",
      "Epoch 20185/40000, Loss: 4.106029155082069e-05, Learning Rate: 0.000177\n",
      "Epoch 20186/40000, Loss: 3.239016223233193e-05, Learning Rate: 0.000177\n",
      "Epoch 20187/40000, Loss: 4.164235360804014e-05, Learning Rate: 0.000177\n",
      "Epoch 20188/40000, Loss: 6.041690721758641e-05, Learning Rate: 0.000177\n",
      "Epoch 20189/40000, Loss: 5.503044303623028e-05, Learning Rate: 0.000177\n",
      "Epoch 20190/40000, Loss: 3.120469409623183e-05, Learning Rate: 0.000177\n",
      "Epoch 20191/40000, Loss: 1.595637331774924e-05, Learning Rate: 0.000177\n",
      "Epoch 20192/40000, Loss: 3.2757649023551494e-05, Learning Rate: 0.000177\n",
      "Epoch 20193/40000, Loss: 3.119541725027375e-05, Learning Rate: 0.000177\n",
      "Epoch 20194/40000, Loss: 6.029245923855342e-05, Learning Rate: 0.000177\n",
      "Epoch 20195/40000, Loss: 3.527728404151276e-05, Learning Rate: 0.000177\n",
      "Epoch 20196/40000, Loss: 3.100282629020512e-05, Learning Rate: 0.000177\n",
      "Epoch 20197/40000, Loss: 3.581887358450331e-05, Learning Rate: 0.000177\n",
      "Epoch 20198/40000, Loss: 6.003915405017324e-05, Learning Rate: 0.000177\n",
      "Epoch 20199/40000, Loss: 5.359068381949328e-05, Learning Rate: 0.000177\n",
      "Epoch 20200/40000, Loss: 3.109169119852595e-05, Learning Rate: 0.000177\n",
      "Epoch 20201/40000, Loss: 3.089042365900241e-05, Learning Rate: 0.000177\n",
      "Epoch 20202/40000, Loss: 1.4093396202952135e-05, Learning Rate: 0.000177\n",
      "Epoch 20203/40000, Loss: 5.991261059534736e-05, Learning Rate: 0.000177\n",
      "Epoch 20204/40000, Loss: 3.517187724355608e-05, Learning Rate: 0.000177\n",
      "Epoch 20205/40000, Loss: 1.4056717191124335e-05, Learning Rate: 0.000177\n",
      "Epoch 20206/40000, Loss: 5.9919257182627916e-05, Learning Rate: 0.000177\n",
      "Epoch 20207/40000, Loss: 1.4034426385478582e-05, Learning Rate: 0.000177\n",
      "Epoch 20208/40000, Loss: 3.078009467571974e-05, Learning Rate: 0.000177\n",
      "Epoch 20209/40000, Loss: 5.9676520322682336e-05, Learning Rate: 0.000177\n",
      "Epoch 20210/40000, Loss: 3.498133082757704e-05, Learning Rate: 0.000177\n",
      "Epoch 20211/40000, Loss: 1.4007146091898903e-05, Learning Rate: 0.000177\n",
      "Epoch 20212/40000, Loss: 3.079184898524545e-05, Learning Rate: 0.000177\n",
      "Epoch 20213/40000, Loss: 3.495692726573907e-05, Learning Rate: 0.000177\n",
      "Epoch 20214/40000, Loss: 5.327297549229115e-05, Learning Rate: 0.000177\n",
      "Epoch 20215/40000, Loss: 5.969516496406868e-05, Learning Rate: 0.000177\n",
      "Epoch 20216/40000, Loss: 5.3501211368711665e-05, Learning Rate: 0.000177\n",
      "Epoch 20217/40000, Loss: 6.0259753809077665e-05, Learning Rate: 0.000177\n",
      "Epoch 20218/40000, Loss: 4.035517122247256e-05, Learning Rate: 0.000177\n",
      "Epoch 20219/40000, Loss: 3.0875729862600565e-05, Learning Rate: 0.000177\n",
      "Epoch 20220/40000, Loss: 1.4312809071270749e-05, Learning Rate: 0.000177\n",
      "Epoch 20221/40000, Loss: 4.056343459524214e-05, Learning Rate: 0.000177\n",
      "Epoch 20222/40000, Loss: 3.193407246726565e-05, Learning Rate: 0.000177\n",
      "Epoch 20223/40000, Loss: 3.533443668857217e-05, Learning Rate: 0.000177\n",
      "Epoch 20224/40000, Loss: 1.4163725609250832e-05, Learning Rate: 0.000177\n",
      "Epoch 20225/40000, Loss: 5.3869051043875515e-05, Learning Rate: 0.000177\n",
      "Epoch 20226/40000, Loss: 3.528846718836576e-05, Learning Rate: 0.000177\n",
      "Epoch 20227/40000, Loss: 3.5252920497441664e-05, Learning Rate: 0.000177\n",
      "Epoch 20228/40000, Loss: 3.088906669290736e-05, Learning Rate: 0.000177\n",
      "Epoch 20229/40000, Loss: 3.075308268307708e-05, Learning Rate: 0.000177\n",
      "Epoch 20230/40000, Loss: 3.087360892095603e-05, Learning Rate: 0.000176\n",
      "Epoch 20231/40000, Loss: 5.367495759855956e-05, Learning Rate: 0.000176\n",
      "Epoch 20232/40000, Loss: 4.0681956306798384e-05, Learning Rate: 0.000176\n",
      "Epoch 20233/40000, Loss: 3.5118046071147546e-05, Learning Rate: 0.000176\n",
      "Epoch 20234/40000, Loss: 5.363217496778816e-05, Learning Rate: 0.000176\n",
      "Epoch 20235/40000, Loss: 3.4977594623342156e-05, Learning Rate: 0.000176\n",
      "Epoch 20236/40000, Loss: 1.4143922271614429e-05, Learning Rate: 0.000176\n",
      "Epoch 20237/40000, Loss: 3.510435271891765e-05, Learning Rate: 0.000176\n",
      "Epoch 20238/40000, Loss: 4.028222610941157e-05, Learning Rate: 0.000176\n",
      "Epoch 20239/40000, Loss: 4.033496225019917e-05, Learning Rate: 0.000176\n",
      "Epoch 20240/40000, Loss: 4.0396174881607294e-05, Learning Rate: 0.000176\n",
      "Epoch 20241/40000, Loss: 5.969985795672983e-05, Learning Rate: 0.000176\n",
      "Epoch 20242/40000, Loss: 5.9831232647411525e-05, Learning Rate: 0.000176\n",
      "Epoch 20243/40000, Loss: 5.360997965908609e-05, Learning Rate: 0.000176\n",
      "Epoch 20244/40000, Loss: 1.4126020687399432e-05, Learning Rate: 0.000176\n",
      "Epoch 20245/40000, Loss: 4.0643226384418085e-05, Learning Rate: 0.000176\n",
      "Epoch 20246/40000, Loss: 3.093241684837267e-05, Learning Rate: 0.000176\n",
      "Epoch 20247/40000, Loss: 6.038279752829112e-05, Learning Rate: 0.000176\n",
      "Epoch 20248/40000, Loss: 3.5342673072591424e-05, Learning Rate: 0.000176\n",
      "Epoch 20249/40000, Loss: 5.413878898252733e-05, Learning Rate: 0.000176\n",
      "Epoch 20250/40000, Loss: 3.2080984965432435e-05, Learning Rate: 0.000176\n",
      "Epoch 20251/40000, Loss: 3.115111030638218e-05, Learning Rate: 0.000176\n",
      "Epoch 20252/40000, Loss: 5.720648550777696e-05, Learning Rate: 0.000176\n",
      "Epoch 20253/40000, Loss: 1.4583984921046067e-05, Learning Rate: 0.000176\n",
      "Epoch 20254/40000, Loss: 3.582634963095188e-05, Learning Rate: 0.000176\n",
      "Epoch 20255/40000, Loss: 3.117796586593613e-05, Learning Rate: 0.000176\n",
      "Epoch 20256/40000, Loss: 4.32289598393254e-05, Learning Rate: 0.000176\n",
      "Epoch 20257/40000, Loss: 1.4720310900884215e-05, Learning Rate: 0.000176\n",
      "Epoch 20258/40000, Loss: 4.399881072458811e-05, Learning Rate: 0.000176\n",
      "Epoch 20259/40000, Loss: 1.693052581686061e-05, Learning Rate: 0.000176\n",
      "Epoch 20260/40000, Loss: 3.209275382687338e-05, Learning Rate: 0.000176\n",
      "Epoch 20261/40000, Loss: 5.763210720033385e-05, Learning Rate: 0.000176\n",
      "Epoch 20262/40000, Loss: 4.1909250285243616e-05, Learning Rate: 0.000176\n",
      "Epoch 20263/40000, Loss: 1.5535435522906482e-05, Learning Rate: 0.000176\n",
      "Epoch 20264/40000, Loss: 3.258985816501081e-05, Learning Rate: 0.000176\n",
      "Epoch 20265/40000, Loss: 5.926753510721028e-05, Learning Rate: 0.000176\n",
      "Epoch 20266/40000, Loss: 6.414804374799132e-05, Learning Rate: 0.000176\n",
      "Epoch 20267/40000, Loss: 6.14311356912367e-05, Learning Rate: 0.000176\n",
      "Epoch 20268/40000, Loss: 6.023280730005354e-05, Learning Rate: 0.000176\n",
      "Epoch 20269/40000, Loss: 3.646969344117679e-05, Learning Rate: 0.000176\n",
      "Epoch 20270/40000, Loss: 3.5540451790438965e-05, Learning Rate: 0.000176\n",
      "Epoch 20271/40000, Loss: 3.1275078072212636e-05, Learning Rate: 0.000176\n",
      "Epoch 20272/40000, Loss: 6.0183687310200185e-05, Learning Rate: 0.000176\n",
      "Epoch 20273/40000, Loss: 1.439715560991317e-05, Learning Rate: 0.000176\n",
      "Epoch 20274/40000, Loss: 5.4969459597487e-05, Learning Rate: 0.000176\n",
      "Epoch 20275/40000, Loss: 4.0948685636976734e-05, Learning Rate: 0.000176\n",
      "Epoch 20276/40000, Loss: 5.7792713050730526e-05, Learning Rate: 0.000176\n",
      "Epoch 20277/40000, Loss: 5.472365955938585e-05, Learning Rate: 0.000175\n",
      "Epoch 20278/40000, Loss: 5.985056486679241e-05, Learning Rate: 0.000175\n",
      "Epoch 20279/40000, Loss: 5.4446565627586097e-05, Learning Rate: 0.000175\n",
      "Epoch 20280/40000, Loss: 5.363586387829855e-05, Learning Rate: 0.000175\n",
      "Epoch 20281/40000, Loss: 4.06359504268039e-05, Learning Rate: 0.000175\n",
      "Epoch 20282/40000, Loss: 5.5018077546264976e-05, Learning Rate: 0.000175\n",
      "Epoch 20283/40000, Loss: 5.386349221225828e-05, Learning Rate: 0.000175\n",
      "Epoch 20284/40000, Loss: 5.990741556161083e-05, Learning Rate: 0.000175\n",
      "Epoch 20285/40000, Loss: 6.034591206116602e-05, Learning Rate: 0.000175\n",
      "Epoch 20286/40000, Loss: 5.394495383370668e-05, Learning Rate: 0.000175\n",
      "Epoch 20287/40000, Loss: 1.4356241081259213e-05, Learning Rate: 0.000175\n",
      "Epoch 20288/40000, Loss: 3.121031477348879e-05, Learning Rate: 0.000175\n",
      "Epoch 20289/40000, Loss: 1.430961856385693e-05, Learning Rate: 0.000175\n",
      "Epoch 20290/40000, Loss: 1.4314427062345203e-05, Learning Rate: 0.000175\n",
      "Epoch 20291/40000, Loss: 5.352099105948582e-05, Learning Rate: 0.000175\n",
      "Epoch 20292/40000, Loss: 3.5073953768005595e-05, Learning Rate: 0.000175\n",
      "Epoch 20293/40000, Loss: 5.955778033239767e-05, Learning Rate: 0.000175\n",
      "Epoch 20294/40000, Loss: 5.9668742323992774e-05, Learning Rate: 0.000175\n",
      "Epoch 20295/40000, Loss: 3.1108098482945934e-05, Learning Rate: 0.000175\n",
      "Epoch 20296/40000, Loss: 3.514256968628615e-05, Learning Rate: 0.000175\n",
      "Epoch 20297/40000, Loss: 4.037030157633126e-05, Learning Rate: 0.000175\n",
      "Epoch 20298/40000, Loss: 3.079733869526535e-05, Learning Rate: 0.000175\n",
      "Epoch 20299/40000, Loss: 5.3257936087902635e-05, Learning Rate: 0.000175\n",
      "Epoch 20300/40000, Loss: 3.495697455946356e-05, Learning Rate: 0.000175\n",
      "Epoch 20301/40000, Loss: 4.022611756226979e-05, Learning Rate: 0.000175\n",
      "Epoch 20302/40000, Loss: 1.402836187480716e-05, Learning Rate: 0.000175\n",
      "Epoch 20303/40000, Loss: 5.323265941115096e-05, Learning Rate: 0.000175\n",
      "Epoch 20304/40000, Loss: 5.952659557806328e-05, Learning Rate: 0.000175\n",
      "Epoch 20305/40000, Loss: 1.4080601431487594e-05, Learning Rate: 0.000175\n",
      "Epoch 20306/40000, Loss: 3.5030796425417066e-05, Learning Rate: 0.000175\n",
      "Epoch 20307/40000, Loss: 5.945003067608923e-05, Learning Rate: 0.000175\n",
      "Epoch 20308/40000, Loss: 5.334380693966523e-05, Learning Rate: 0.000175\n",
      "Epoch 20309/40000, Loss: 3.0720304494025186e-05, Learning Rate: 0.000175\n",
      "Epoch 20310/40000, Loss: 5.330310887075029e-05, Learning Rate: 0.000175\n",
      "Epoch 20311/40000, Loss: 1.4127638678473886e-05, Learning Rate: 0.000175\n",
      "Epoch 20312/40000, Loss: 4.04167985834647e-05, Learning Rate: 0.000175\n",
      "Epoch 20313/40000, Loss: 6.071586540201679e-05, Learning Rate: 0.000175\n",
      "Epoch 20314/40000, Loss: 6.00286184635479e-05, Learning Rate: 0.000175\n",
      "Epoch 20315/40000, Loss: 5.342180156731047e-05, Learning Rate: 0.000175\n",
      "Epoch 20316/40000, Loss: 4.052658550790511e-05, Learning Rate: 0.000175\n",
      "Epoch 20317/40000, Loss: 5.966002572677098e-05, Learning Rate: 0.000175\n",
      "Epoch 20318/40000, Loss: 3.5555749491322786e-05, Learning Rate: 0.000175\n",
      "Epoch 20319/40000, Loss: 4.042022555950098e-05, Learning Rate: 0.000175\n",
      "Epoch 20320/40000, Loss: 4.03552912757732e-05, Learning Rate: 0.000175\n",
      "Epoch 20321/40000, Loss: 3.5351429687580094e-05, Learning Rate: 0.000175\n",
      "Epoch 20322/40000, Loss: 5.368427446228452e-05, Learning Rate: 0.000175\n",
      "Epoch 20323/40000, Loss: 5.987998883938417e-05, Learning Rate: 0.000175\n",
      "Epoch 20324/40000, Loss: 3.0771465389989316e-05, Learning Rate: 0.000175\n",
      "Epoch 20325/40000, Loss: 3.5132969060214236e-05, Learning Rate: 0.000174\n",
      "Epoch 20326/40000, Loss: 5.9610636526485905e-05, Learning Rate: 0.000174\n",
      "Epoch 20327/40000, Loss: 5.943639189354144e-05, Learning Rate: 0.000174\n",
      "Epoch 20328/40000, Loss: 5.942262941971421e-05, Learning Rate: 0.000174\n",
      "Epoch 20329/40000, Loss: 3.4985703678103164e-05, Learning Rate: 0.000174\n",
      "Epoch 20330/40000, Loss: 1.424164747731993e-05, Learning Rate: 0.000174\n",
      "Epoch 20331/40000, Loss: 3.0792092729825526e-05, Learning Rate: 0.000174\n",
      "Epoch 20332/40000, Loss: 3.515038042678498e-05, Learning Rate: 0.000174\n",
      "Epoch 20333/40000, Loss: 4.035302481497638e-05, Learning Rate: 0.000174\n",
      "Epoch 20334/40000, Loss: 3.081999238929711e-05, Learning Rate: 0.000174\n",
      "Epoch 20335/40000, Loss: 3.080096939811483e-05, Learning Rate: 0.000174\n",
      "Epoch 20336/40000, Loss: 4.043102671857923e-05, Learning Rate: 0.000174\n",
      "Epoch 20337/40000, Loss: 1.4142848158371635e-05, Learning Rate: 0.000174\n",
      "Epoch 20338/40000, Loss: 4.0316765080206096e-05, Learning Rate: 0.000174\n",
      "Epoch 20339/40000, Loss: 4.0352679206989706e-05, Learning Rate: 0.000174\n",
      "Epoch 20340/40000, Loss: 3.0842282285448164e-05, Learning Rate: 0.000174\n",
      "Epoch 20341/40000, Loss: 5.989484634483233e-05, Learning Rate: 0.000174\n",
      "Epoch 20342/40000, Loss: 5.334917295840569e-05, Learning Rate: 0.000174\n",
      "Epoch 20343/40000, Loss: 1.4140899111225735e-05, Learning Rate: 0.000174\n",
      "Epoch 20344/40000, Loss: 4.035029633087106e-05, Learning Rate: 0.000174\n",
      "Epoch 20345/40000, Loss: 5.351440267986618e-05, Learning Rate: 0.000174\n",
      "Epoch 20346/40000, Loss: 3.076459688600153e-05, Learning Rate: 0.000174\n",
      "Epoch 20347/40000, Loss: 5.3488012781599537e-05, Learning Rate: 0.000174\n",
      "Epoch 20348/40000, Loss: 4.0293372876476496e-05, Learning Rate: 0.000174\n",
      "Epoch 20349/40000, Loss: 5.982835864415392e-05, Learning Rate: 0.000174\n",
      "Epoch 20350/40000, Loss: 3.5292247048346326e-05, Learning Rate: 0.000174\n",
      "Epoch 20351/40000, Loss: 5.954509833827615e-05, Learning Rate: 0.000174\n",
      "Epoch 20352/40000, Loss: 5.338613482308574e-05, Learning Rate: 0.000174\n",
      "Epoch 20353/40000, Loss: 5.99124250584282e-05, Learning Rate: 0.000174\n",
      "Epoch 20354/40000, Loss: 5.9876780142076313e-05, Learning Rate: 0.000174\n",
      "Epoch 20355/40000, Loss: 1.4216497220331803e-05, Learning Rate: 0.000174\n",
      "Epoch 20356/40000, Loss: 3.0801849788986146e-05, Learning Rate: 0.000174\n",
      "Epoch 20357/40000, Loss: 3.5109980672132224e-05, Learning Rate: 0.000174\n",
      "Epoch 20358/40000, Loss: 5.395129119278863e-05, Learning Rate: 0.000174\n",
      "Epoch 20359/40000, Loss: 4.050094503327273e-05, Learning Rate: 0.000174\n",
      "Epoch 20360/40000, Loss: 1.4165530956233852e-05, Learning Rate: 0.000174\n",
      "Epoch 20361/40000, Loss: 3.083992123720236e-05, Learning Rate: 0.000174\n",
      "Epoch 20362/40000, Loss: 3.5274399124318734e-05, Learning Rate: 0.000174\n",
      "Epoch 20363/40000, Loss: 3.516226206556894e-05, Learning Rate: 0.000174\n",
      "Epoch 20364/40000, Loss: 3.5366691008675843e-05, Learning Rate: 0.000174\n",
      "Epoch 20365/40000, Loss: 1.4246278624341357e-05, Learning Rate: 0.000174\n",
      "Epoch 20366/40000, Loss: 3.5956389183411375e-05, Learning Rate: 0.000174\n",
      "Epoch 20367/40000, Loss: 5.5128719395725057e-05, Learning Rate: 0.000174\n",
      "Epoch 20368/40000, Loss: 6.029169162502512e-05, Learning Rate: 0.000174\n",
      "Epoch 20369/40000, Loss: 1.4519573596771806e-05, Learning Rate: 0.000174\n",
      "Epoch 20370/40000, Loss: 3.15642828354612e-05, Learning Rate: 0.000174\n",
      "Epoch 20371/40000, Loss: 1.607728518138174e-05, Learning Rate: 0.000174\n",
      "Epoch 20372/40000, Loss: 4.198673923383467e-05, Learning Rate: 0.000174\n",
      "Epoch 20373/40000, Loss: 5.604535908787511e-05, Learning Rate: 0.000173\n",
      "Epoch 20374/40000, Loss: 4.257123146089725e-05, Learning Rate: 0.000173\n",
      "Epoch 20375/40000, Loss: 6.0876922361785546e-05, Learning Rate: 0.000173\n",
      "Epoch 20376/40000, Loss: 5.7099226978607476e-05, Learning Rate: 0.000173\n",
      "Epoch 20377/40000, Loss: 5.589571810560301e-05, Learning Rate: 0.000173\n",
      "Epoch 20378/40000, Loss: 3.795898373937234e-05, Learning Rate: 0.000173\n",
      "Epoch 20379/40000, Loss: 3.24386237480212e-05, Learning Rate: 0.000173\n",
      "Epoch 20380/40000, Loss: 3.213651507394388e-05, Learning Rate: 0.000173\n",
      "Epoch 20381/40000, Loss: 6.250811566133052e-05, Learning Rate: 0.000173\n",
      "Epoch 20382/40000, Loss: 4.171361797489226e-05, Learning Rate: 0.000173\n",
      "Epoch 20383/40000, Loss: 3.2012798328651115e-05, Learning Rate: 0.000173\n",
      "Epoch 20384/40000, Loss: 4.2256313463440165e-05, Learning Rate: 0.000173\n",
      "Epoch 20385/40000, Loss: 3.349063263158314e-05, Learning Rate: 0.000173\n",
      "Epoch 20386/40000, Loss: 3.281196404714137e-05, Learning Rate: 0.000173\n",
      "Epoch 20387/40000, Loss: 4.464071389520541e-05, Learning Rate: 0.000173\n",
      "Epoch 20388/40000, Loss: 6.200713687576354e-05, Learning Rate: 0.000173\n",
      "Epoch 20389/40000, Loss: 3.174416269757785e-05, Learning Rate: 0.000173\n",
      "Epoch 20390/40000, Loss: 4.2465613660169765e-05, Learning Rate: 0.000173\n",
      "Epoch 20391/40000, Loss: 3.576913513825275e-05, Learning Rate: 0.000173\n",
      "Epoch 20392/40000, Loss: 3.6568180803442374e-05, Learning Rate: 0.000173\n",
      "Epoch 20393/40000, Loss: 4.2240113543812186e-05, Learning Rate: 0.000173\n",
      "Epoch 20394/40000, Loss: 5.651921310345642e-05, Learning Rate: 0.000173\n",
      "Epoch 20395/40000, Loss: 3.717443178175017e-05, Learning Rate: 0.000173\n",
      "Epoch 20396/40000, Loss: 6.806432065786794e-05, Learning Rate: 0.000173\n",
      "Epoch 20397/40000, Loss: 4.2874969949480146e-05, Learning Rate: 0.000173\n",
      "Epoch 20398/40000, Loss: 1.7410020518582314e-05, Learning Rate: 0.000173\n",
      "Epoch 20399/40000, Loss: 1.61018888320541e-05, Learning Rate: 0.000173\n",
      "Epoch 20400/40000, Loss: 6.086795110604726e-05, Learning Rate: 0.000173\n",
      "Epoch 20401/40000, Loss: 6.026327901054174e-05, Learning Rate: 0.000173\n",
      "Epoch 20402/40000, Loss: 4.216788511257619e-05, Learning Rate: 0.000173\n",
      "Epoch 20403/40000, Loss: 6.085478526074439e-05, Learning Rate: 0.000173\n",
      "Epoch 20404/40000, Loss: 1.4746976376045495e-05, Learning Rate: 0.000173\n",
      "Epoch 20405/40000, Loss: 1.4268905033532064e-05, Learning Rate: 0.000173\n",
      "Epoch 20406/40000, Loss: 5.350472201826051e-05, Learning Rate: 0.000173\n",
      "Epoch 20407/40000, Loss: 1.4549445950251538e-05, Learning Rate: 0.000173\n",
      "Epoch 20408/40000, Loss: 4.057723708683625e-05, Learning Rate: 0.000173\n",
      "Epoch 20409/40000, Loss: 5.348704144125804e-05, Learning Rate: 0.000173\n",
      "Epoch 20410/40000, Loss: 1.4314844520413317e-05, Learning Rate: 0.000173\n",
      "Epoch 20411/40000, Loss: 4.069449641974643e-05, Learning Rate: 0.000173\n",
      "Epoch 20412/40000, Loss: 3.0872415663907304e-05, Learning Rate: 0.000173\n",
      "Epoch 20413/40000, Loss: 1.471376799599966e-05, Learning Rate: 0.000173\n",
      "Epoch 20414/40000, Loss: 4.151967368670739e-05, Learning Rate: 0.000173\n",
      "Epoch 20415/40000, Loss: 6.001175643177703e-05, Learning Rate: 0.000173\n",
      "Epoch 20416/40000, Loss: 5.955835149507038e-05, Learning Rate: 0.000173\n",
      "Epoch 20417/40000, Loss: 3.516430297167972e-05, Learning Rate: 0.000173\n",
      "Epoch 20418/40000, Loss: 6.000119537930004e-05, Learning Rate: 0.000173\n",
      "Epoch 20419/40000, Loss: 5.951213097432628e-05, Learning Rate: 0.000173\n",
      "Epoch 20420/40000, Loss: 4.0636045014252886e-05, Learning Rate: 0.000173\n",
      "Epoch 20421/40000, Loss: 1.4183078747009858e-05, Learning Rate: 0.000172\n",
      "Epoch 20422/40000, Loss: 5.335599053069018e-05, Learning Rate: 0.000172\n",
      "Epoch 20423/40000, Loss: 5.327930557541549e-05, Learning Rate: 0.000172\n",
      "Epoch 20424/40000, Loss: 1.4229770386009477e-05, Learning Rate: 0.000172\n",
      "Epoch 20425/40000, Loss: 1.4132952856016345e-05, Learning Rate: 0.000172\n",
      "Epoch 20426/40000, Loss: 5.935033914283849e-05, Learning Rate: 0.000172\n",
      "Epoch 20427/40000, Loss: 5.332618820830248e-05, Learning Rate: 0.000172\n",
      "Epoch 20428/40000, Loss: 4.063708183821291e-05, Learning Rate: 0.000172\n",
      "Epoch 20429/40000, Loss: 5.3679166740039364e-05, Learning Rate: 0.000172\n",
      "Epoch 20430/40000, Loss: 3.07019472529646e-05, Learning Rate: 0.000172\n",
      "Epoch 20431/40000, Loss: 1.4231555724109057e-05, Learning Rate: 0.000172\n",
      "Epoch 20432/40000, Loss: 3.5007218684768304e-05, Learning Rate: 0.000172\n",
      "Epoch 20433/40000, Loss: 4.039929262944497e-05, Learning Rate: 0.000172\n",
      "Epoch 20434/40000, Loss: 1.4143305634206627e-05, Learning Rate: 0.000172\n",
      "Epoch 20435/40000, Loss: 3.4991382563021034e-05, Learning Rate: 0.000172\n",
      "Epoch 20436/40000, Loss: 5.9408092056401074e-05, Learning Rate: 0.000172\n",
      "Epoch 20437/40000, Loss: 5.333006993168965e-05, Learning Rate: 0.000172\n",
      "Epoch 20438/40000, Loss: 4.024808731628582e-05, Learning Rate: 0.000172\n",
      "Epoch 20439/40000, Loss: 1.3970595318824053e-05, Learning Rate: 0.000172\n",
      "Epoch 20440/40000, Loss: 3.067097350140102e-05, Learning Rate: 0.000172\n",
      "Epoch 20441/40000, Loss: 4.018685649498366e-05, Learning Rate: 0.000172\n",
      "Epoch 20442/40000, Loss: 4.034509038319811e-05, Learning Rate: 0.000172\n",
      "Epoch 20443/40000, Loss: 5.333857188816182e-05, Learning Rate: 0.000172\n",
      "Epoch 20444/40000, Loss: 5.316749229677953e-05, Learning Rate: 0.000172\n",
      "Epoch 20445/40000, Loss: 5.9786325437016785e-05, Learning Rate: 0.000172\n",
      "Epoch 20446/40000, Loss: 5.3277788538252935e-05, Learning Rate: 0.000172\n",
      "Epoch 20447/40000, Loss: 4.020554115413688e-05, Learning Rate: 0.000172\n",
      "Epoch 20448/40000, Loss: 3.4891883842647076e-05, Learning Rate: 0.000172\n",
      "Epoch 20449/40000, Loss: 4.018096660729498e-05, Learning Rate: 0.000172\n",
      "Epoch 20450/40000, Loss: 5.321023490978405e-05, Learning Rate: 0.000172\n",
      "Epoch 20451/40000, Loss: 5.960249836789444e-05, Learning Rate: 0.000172\n",
      "Epoch 20452/40000, Loss: 1.4067561096453574e-05, Learning Rate: 0.000172\n",
      "Epoch 20453/40000, Loss: 3.49093388649635e-05, Learning Rate: 0.000172\n",
      "Epoch 20454/40000, Loss: 3.4872919059125707e-05, Learning Rate: 0.000172\n",
      "Epoch 20455/40000, Loss: 3.0614730349043384e-05, Learning Rate: 0.000172\n",
      "Epoch 20456/40000, Loss: 3.060108792851679e-05, Learning Rate: 0.000172\n",
      "Epoch 20457/40000, Loss: 5.331161810318008e-05, Learning Rate: 0.000172\n",
      "Epoch 20458/40000, Loss: 3.4932225389638916e-05, Learning Rate: 0.000172\n",
      "Epoch 20459/40000, Loss: 3.064807242481038e-05, Learning Rate: 0.000172\n",
      "Epoch 20460/40000, Loss: 5.325679376255721e-05, Learning Rate: 0.000172\n",
      "Epoch 20461/40000, Loss: 3.4917065931949764e-05, Learning Rate: 0.000172\n",
      "Epoch 20462/40000, Loss: 5.326548853190616e-05, Learning Rate: 0.000172\n",
      "Epoch 20463/40000, Loss: 5.95673882344272e-05, Learning Rate: 0.000172\n",
      "Epoch 20464/40000, Loss: 1.4129002011031844e-05, Learning Rate: 0.000172\n",
      "Epoch 20465/40000, Loss: 3.5000786738237366e-05, Learning Rate: 0.000172\n",
      "Epoch 20466/40000, Loss: 5.337609400157817e-05, Learning Rate: 0.000172\n",
      "Epoch 20467/40000, Loss: 5.9477191825862974e-05, Learning Rate: 0.000172\n",
      "Epoch 20468/40000, Loss: 4.045259629492648e-05, Learning Rate: 0.000172\n",
      "Epoch 20469/40000, Loss: 5.3608051530318335e-05, Learning Rate: 0.000172\n",
      "Epoch 20470/40000, Loss: 4.0616570913698524e-05, Learning Rate: 0.000171\n",
      "Epoch 20471/40000, Loss: 3.079195448663086e-05, Learning Rate: 0.000171\n",
      "Epoch 20472/40000, Loss: 5.976289321552031e-05, Learning Rate: 0.000171\n",
      "Epoch 20473/40000, Loss: 5.42637390026357e-05, Learning Rate: 0.000171\n",
      "Epoch 20474/40000, Loss: 5.413531471276656e-05, Learning Rate: 0.000171\n",
      "Epoch 20475/40000, Loss: 3.550307883415371e-05, Learning Rate: 0.000171\n",
      "Epoch 20476/40000, Loss: 3.1162962841335684e-05, Learning Rate: 0.000171\n",
      "Epoch 20477/40000, Loss: 6.167190440464765e-05, Learning Rate: 0.000171\n",
      "Epoch 20478/40000, Loss: 3.269616354373284e-05, Learning Rate: 0.000171\n",
      "Epoch 20479/40000, Loss: 1.5790377801749855e-05, Learning Rate: 0.000171\n",
      "Epoch 20480/40000, Loss: 1.5943514881655574e-05, Learning Rate: 0.000171\n",
      "Epoch 20481/40000, Loss: 1.5623820218024775e-05, Learning Rate: 0.000171\n",
      "Epoch 20482/40000, Loss: 3.2205585739575326e-05, Learning Rate: 0.000171\n",
      "Epoch 20483/40000, Loss: 3.664990435936488e-05, Learning Rate: 0.000171\n",
      "Epoch 20484/40000, Loss: 1.552219328004867e-05, Learning Rate: 0.000171\n",
      "Epoch 20485/40000, Loss: 4.1407627577427775e-05, Learning Rate: 0.000171\n",
      "Epoch 20486/40000, Loss: 3.466331327217631e-05, Learning Rate: 0.000171\n",
      "Epoch 20487/40000, Loss: 5.638835864374414e-05, Learning Rate: 0.000171\n",
      "Epoch 20488/40000, Loss: 3.639004717115313e-05, Learning Rate: 0.000171\n",
      "Epoch 20489/40000, Loss: 4.22696175519377e-05, Learning Rate: 0.000171\n",
      "Epoch 20490/40000, Loss: 1.4794296475884039e-05, Learning Rate: 0.000171\n",
      "Epoch 20491/40000, Loss: 3.735925565706566e-05, Learning Rate: 0.000171\n",
      "Epoch 20492/40000, Loss: 1.5927455024211667e-05, Learning Rate: 0.000171\n",
      "Epoch 20493/40000, Loss: 6.045848203939386e-05, Learning Rate: 0.000171\n",
      "Epoch 20494/40000, Loss: 1.6140445950441062e-05, Learning Rate: 0.000171\n",
      "Epoch 20495/40000, Loss: 5.4396579798776656e-05, Learning Rate: 0.000171\n",
      "Epoch 20496/40000, Loss: 1.942225208040327e-05, Learning Rate: 0.000171\n",
      "Epoch 20497/40000, Loss: 6.0449627198977396e-05, Learning Rate: 0.000171\n",
      "Epoch 20498/40000, Loss: 5.536531898542307e-05, Learning Rate: 0.000171\n",
      "Epoch 20499/40000, Loss: 6.167595711303875e-05, Learning Rate: 0.000171\n",
      "Epoch 20500/40000, Loss: 5.4049833124736324e-05, Learning Rate: 0.000171\n",
      "Epoch 20501/40000, Loss: 3.5271496017230675e-05, Learning Rate: 0.000171\n",
      "Epoch 20502/40000, Loss: 1.4387644114322029e-05, Learning Rate: 0.000171\n",
      "Epoch 20503/40000, Loss: 1.4233611182135064e-05, Learning Rate: 0.000171\n",
      "Epoch 20504/40000, Loss: 5.375448745326139e-05, Learning Rate: 0.000171\n",
      "Epoch 20505/40000, Loss: 3.525621650624089e-05, Learning Rate: 0.000171\n",
      "Epoch 20506/40000, Loss: 4.0573802834842354e-05, Learning Rate: 0.000171\n",
      "Epoch 20507/40000, Loss: 4.047947732033208e-05, Learning Rate: 0.000171\n",
      "Epoch 20508/40000, Loss: 3.092323458986357e-05, Learning Rate: 0.000171\n",
      "Epoch 20509/40000, Loss: 1.4463843399425969e-05, Learning Rate: 0.000171\n",
      "Epoch 20510/40000, Loss: 3.10935647576116e-05, Learning Rate: 0.000171\n",
      "Epoch 20511/40000, Loss: 1.4202871170709841e-05, Learning Rate: 0.000171\n",
      "Epoch 20512/40000, Loss: 4.048104165121913e-05, Learning Rate: 0.000171\n",
      "Epoch 20513/40000, Loss: 5.3384352213470265e-05, Learning Rate: 0.000171\n",
      "Epoch 20514/40000, Loss: 4.0879360312828794e-05, Learning Rate: 0.000171\n",
      "Epoch 20515/40000, Loss: 5.375553882913664e-05, Learning Rate: 0.000171\n",
      "Epoch 20516/40000, Loss: 5.345374665921554e-05, Learning Rate: 0.000171\n",
      "Epoch 20517/40000, Loss: 6.054389086784795e-05, Learning Rate: 0.000171\n",
      "Epoch 20518/40000, Loss: 6.005198156344704e-05, Learning Rate: 0.000170\n",
      "Epoch 20519/40000, Loss: 1.4146237845125142e-05, Learning Rate: 0.000170\n",
      "Epoch 20520/40000, Loss: 3.502438630675897e-05, Learning Rate: 0.000170\n",
      "Epoch 20521/40000, Loss: 3.0663552024634555e-05, Learning Rate: 0.000170\n",
      "Epoch 20522/40000, Loss: 1.4070164070290048e-05, Learning Rate: 0.000170\n",
      "Epoch 20523/40000, Loss: 4.016647289972752e-05, Learning Rate: 0.000170\n",
      "Epoch 20524/40000, Loss: 5.321510980138555e-05, Learning Rate: 0.000170\n",
      "Epoch 20525/40000, Loss: 1.392956528434297e-05, Learning Rate: 0.000170\n",
      "Epoch 20526/40000, Loss: 3.4952779969898984e-05, Learning Rate: 0.000170\n",
      "Epoch 20527/40000, Loss: 1.4035015738045331e-05, Learning Rate: 0.000170\n",
      "Epoch 20528/40000, Loss: 3.489190567051992e-05, Learning Rate: 0.000170\n",
      "Epoch 20529/40000, Loss: 5.3492494771489874e-05, Learning Rate: 0.000170\n",
      "Epoch 20530/40000, Loss: 4.030765558127314e-05, Learning Rate: 0.000170\n",
      "Epoch 20531/40000, Loss: 5.6487861002096906e-05, Learning Rate: 0.000170\n",
      "Epoch 20532/40000, Loss: 4.078801066498272e-05, Learning Rate: 0.000170\n",
      "Epoch 20533/40000, Loss: 1.4332204955280758e-05, Learning Rate: 0.000170\n",
      "Epoch 20534/40000, Loss: 1.4547898899763823e-05, Learning Rate: 0.000170\n",
      "Epoch 20535/40000, Loss: 5.3600622777594253e-05, Learning Rate: 0.000170\n",
      "Epoch 20536/40000, Loss: 4.0546030504629016e-05, Learning Rate: 0.000170\n",
      "Epoch 20537/40000, Loss: 4.053143129567616e-05, Learning Rate: 0.000170\n",
      "Epoch 20538/40000, Loss: 5.41737063031178e-05, Learning Rate: 0.000170\n",
      "Epoch 20539/40000, Loss: 5.3789019148098305e-05, Learning Rate: 0.000170\n",
      "Epoch 20540/40000, Loss: 4.4821212213719264e-05, Learning Rate: 0.000170\n",
      "Epoch 20541/40000, Loss: 5.473312921822071e-05, Learning Rate: 0.000170\n",
      "Epoch 20542/40000, Loss: 3.7944471841910854e-05, Learning Rate: 0.000170\n",
      "Epoch 20543/40000, Loss: 4.099117359146476e-05, Learning Rate: 0.000170\n",
      "Epoch 20544/40000, Loss: 4.111145972274244e-05, Learning Rate: 0.000170\n",
      "Epoch 20545/40000, Loss: 3.7894296838203445e-05, Learning Rate: 0.000170\n",
      "Epoch 20546/40000, Loss: 4.241120768710971e-05, Learning Rate: 0.000170\n",
      "Epoch 20547/40000, Loss: 4.3002251914003864e-05, Learning Rate: 0.000170\n",
      "Epoch 20548/40000, Loss: 4.4689430069411173e-05, Learning Rate: 0.000170\n",
      "Epoch 20549/40000, Loss: 3.6614575947169214e-05, Learning Rate: 0.000170\n",
      "Epoch 20550/40000, Loss: 6.053354081814177e-05, Learning Rate: 0.000170\n",
      "Epoch 20551/40000, Loss: 5.5172527936520055e-05, Learning Rate: 0.000170\n",
      "Epoch 20552/40000, Loss: 1.5822839486645535e-05, Learning Rate: 0.000170\n",
      "Epoch 20553/40000, Loss: 4.5640448661288247e-05, Learning Rate: 0.000170\n",
      "Epoch 20554/40000, Loss: 1.5881483705015853e-05, Learning Rate: 0.000170\n",
      "Epoch 20555/40000, Loss: 6.132744601927698e-05, Learning Rate: 0.000170\n",
      "Epoch 20556/40000, Loss: 3.1852672691456974e-05, Learning Rate: 0.000170\n",
      "Epoch 20557/40000, Loss: 3.837091207969934e-05, Learning Rate: 0.000170\n",
      "Epoch 20558/40000, Loss: 3.3944426832022145e-05, Learning Rate: 0.000170\n",
      "Epoch 20559/40000, Loss: 3.922402174794115e-05, Learning Rate: 0.000170\n",
      "Epoch 20560/40000, Loss: 3.6850320611847565e-05, Learning Rate: 0.000170\n",
      "Epoch 20561/40000, Loss: 1.5135379726416431e-05, Learning Rate: 0.000170\n",
      "Epoch 20562/40000, Loss: 3.9209982787724584e-05, Learning Rate: 0.000170\n",
      "Epoch 20563/40000, Loss: 3.23556741932407e-05, Learning Rate: 0.000170\n",
      "Epoch 20564/40000, Loss: 4.1423463699175045e-05, Learning Rate: 0.000170\n",
      "Epoch 20565/40000, Loss: 3.329176615807228e-05, Learning Rate: 0.000170\n",
      "Epoch 20566/40000, Loss: 6.090801252867095e-05, Learning Rate: 0.000170\n",
      "Epoch 20567/40000, Loss: 3.35093864123337e-05, Learning Rate: 0.000169\n",
      "Epoch 20568/40000, Loss: 4.220704431645572e-05, Learning Rate: 0.000169\n",
      "Epoch 20569/40000, Loss: 1.551760215079412e-05, Learning Rate: 0.000169\n",
      "Epoch 20570/40000, Loss: 4.5865301217418164e-05, Learning Rate: 0.000169\n",
      "Epoch 20571/40000, Loss: 1.6396868886658922e-05, Learning Rate: 0.000169\n",
      "Epoch 20572/40000, Loss: 3.114415449090302e-05, Learning Rate: 0.000169\n",
      "Epoch 20573/40000, Loss: 5.443161717266776e-05, Learning Rate: 0.000169\n",
      "Epoch 20574/40000, Loss: 4.191997504676692e-05, Learning Rate: 0.000169\n",
      "Epoch 20575/40000, Loss: 5.9793332184199244e-05, Learning Rate: 0.000169\n",
      "Epoch 20576/40000, Loss: 4.3835239921463653e-05, Learning Rate: 0.000169\n",
      "Epoch 20577/40000, Loss: 1.4769025256100576e-05, Learning Rate: 0.000169\n",
      "Epoch 20578/40000, Loss: 5.396535198087804e-05, Learning Rate: 0.000169\n",
      "Epoch 20579/40000, Loss: 3.614899469539523e-05, Learning Rate: 0.000169\n",
      "Epoch 20580/40000, Loss: 3.51853268512059e-05, Learning Rate: 0.000169\n",
      "Epoch 20581/40000, Loss: 3.076904977206141e-05, Learning Rate: 0.000169\n",
      "Epoch 20582/40000, Loss: 3.62863065674901e-05, Learning Rate: 0.000169\n",
      "Epoch 20583/40000, Loss: 1.4620780348195694e-05, Learning Rate: 0.000169\n",
      "Epoch 20584/40000, Loss: 5.342057556845248e-05, Learning Rate: 0.000169\n",
      "Epoch 20585/40000, Loss: 3.0591298127546906e-05, Learning Rate: 0.000169\n",
      "Epoch 20586/40000, Loss: 5.336679532774724e-05, Learning Rate: 0.000169\n",
      "Epoch 20587/40000, Loss: 3.5012635635212064e-05, Learning Rate: 0.000169\n",
      "Epoch 20588/40000, Loss: 1.4115689737081993e-05, Learning Rate: 0.000169\n",
      "Epoch 20589/40000, Loss: 5.322817014530301e-05, Learning Rate: 0.000169\n",
      "Epoch 20590/40000, Loss: 5.946752571617253e-05, Learning Rate: 0.000169\n",
      "Epoch 20591/40000, Loss: 4.049914059578441e-05, Learning Rate: 0.000169\n",
      "Epoch 20592/40000, Loss: 6.0582973674172536e-05, Learning Rate: 0.000169\n",
      "Epoch 20593/40000, Loss: 5.339194831321947e-05, Learning Rate: 0.000169\n",
      "Epoch 20594/40000, Loss: 6.089342787163332e-05, Learning Rate: 0.000169\n",
      "Epoch 20595/40000, Loss: 5.9815647546201944e-05, Learning Rate: 0.000169\n",
      "Epoch 20596/40000, Loss: 3.504590131342411e-05, Learning Rate: 0.000169\n",
      "Epoch 20597/40000, Loss: 3.059703522012569e-05, Learning Rate: 0.000169\n",
      "Epoch 20598/40000, Loss: 3.082538387388922e-05, Learning Rate: 0.000169\n",
      "Epoch 20599/40000, Loss: 4.052163421874866e-05, Learning Rate: 0.000169\n",
      "Epoch 20600/40000, Loss: 5.342581425793469e-05, Learning Rate: 0.000169\n",
      "Epoch 20601/40000, Loss: 3.5089789889752865e-05, Learning Rate: 0.000169\n",
      "Epoch 20602/40000, Loss: 4.036583413835615e-05, Learning Rate: 0.000169\n",
      "Epoch 20603/40000, Loss: 5.35102590220049e-05, Learning Rate: 0.000169\n",
      "Epoch 20604/40000, Loss: 4.037999678985216e-05, Learning Rate: 0.000169\n",
      "Epoch 20605/40000, Loss: 5.949272235739045e-05, Learning Rate: 0.000169\n",
      "Epoch 20606/40000, Loss: 3.0623734346590936e-05, Learning Rate: 0.000169\n",
      "Epoch 20607/40000, Loss: 3.0623708880739287e-05, Learning Rate: 0.000169\n",
      "Epoch 20608/40000, Loss: 4.0316452214028686e-05, Learning Rate: 0.000169\n",
      "Epoch 20609/40000, Loss: 3.0624662031186745e-05, Learning Rate: 0.000169\n",
      "Epoch 20610/40000, Loss: 1.409521973982919e-05, Learning Rate: 0.000169\n",
      "Epoch 20611/40000, Loss: 1.4009182450536173e-05, Learning Rate: 0.000169\n",
      "Epoch 20612/40000, Loss: 1.3991019841341767e-05, Learning Rate: 0.000169\n",
      "Epoch 20613/40000, Loss: 3.486867353785783e-05, Learning Rate: 0.000169\n",
      "Epoch 20614/40000, Loss: 1.4057922271604184e-05, Learning Rate: 0.000169\n",
      "Epoch 20615/40000, Loss: 4.022993380203843e-05, Learning Rate: 0.000169\n",
      "Epoch 20616/40000, Loss: 3.06352594634518e-05, Learning Rate: 0.000169\n",
      "Epoch 20617/40000, Loss: 5.9189056628383696e-05, Learning Rate: 0.000168\n",
      "Epoch 20618/40000, Loss: 5.3400239266920835e-05, Learning Rate: 0.000168\n",
      "Epoch 20619/40000, Loss: 5.9096240875078365e-05, Learning Rate: 0.000168\n",
      "Epoch 20620/40000, Loss: 4.018838080810383e-05, Learning Rate: 0.000168\n",
      "Epoch 20621/40000, Loss: 3.053756518056616e-05, Learning Rate: 0.000168\n",
      "Epoch 20622/40000, Loss: 1.4077609193918761e-05, Learning Rate: 0.000168\n",
      "Epoch 20623/40000, Loss: 3.473007745924406e-05, Learning Rate: 0.000168\n",
      "Epoch 20624/40000, Loss: 5.319781121215783e-05, Learning Rate: 0.000168\n",
      "Epoch 20625/40000, Loss: 3.4758813853841275e-05, Learning Rate: 0.000168\n",
      "Epoch 20626/40000, Loss: 1.397362575517036e-05, Learning Rate: 0.000168\n",
      "Epoch 20627/40000, Loss: 3.4723238059086725e-05, Learning Rate: 0.000168\n",
      "Epoch 20628/40000, Loss: 4.0271759644383565e-05, Learning Rate: 0.000168\n",
      "Epoch 20629/40000, Loss: 3.479977749520913e-05, Learning Rate: 0.000168\n",
      "Epoch 20630/40000, Loss: 3.4783381124725565e-05, Learning Rate: 0.000168\n",
      "Epoch 20631/40000, Loss: 1.4009853657626081e-05, Learning Rate: 0.000168\n",
      "Epoch 20632/40000, Loss: 5.933986176387407e-05, Learning Rate: 0.000168\n",
      "Epoch 20633/40000, Loss: 3.48410067090299e-05, Learning Rate: 0.000168\n",
      "Epoch 20634/40000, Loss: 4.028438706882298e-05, Learning Rate: 0.000168\n",
      "Epoch 20635/40000, Loss: 5.323333607520908e-05, Learning Rate: 0.000168\n",
      "Epoch 20636/40000, Loss: 5.319453339325264e-05, Learning Rate: 0.000168\n",
      "Epoch 20637/40000, Loss: 4.024723602924496e-05, Learning Rate: 0.000168\n",
      "Epoch 20638/40000, Loss: 5.928942118771374e-05, Learning Rate: 0.000168\n",
      "Epoch 20639/40000, Loss: 5.9199039242230356e-05, Learning Rate: 0.000168\n",
      "Epoch 20640/40000, Loss: 3.469078365014866e-05, Learning Rate: 0.000168\n",
      "Epoch 20641/40000, Loss: 5.3079871577210724e-05, Learning Rate: 0.000168\n",
      "Epoch 20642/40000, Loss: 5.298919859342277e-05, Learning Rate: 0.000168\n",
      "Epoch 20643/40000, Loss: 4.002128844149411e-05, Learning Rate: 0.000168\n",
      "Epoch 20644/40000, Loss: 3.464845576672815e-05, Learning Rate: 0.000168\n",
      "Epoch 20645/40000, Loss: 4.007400639238767e-05, Learning Rate: 0.000168\n",
      "Epoch 20646/40000, Loss: 5.3039893828099594e-05, Learning Rate: 0.000168\n",
      "Epoch 20647/40000, Loss: 5.9029880503658205e-05, Learning Rate: 0.000168\n",
      "Epoch 20648/40000, Loss: 3.058376751141623e-05, Learning Rate: 0.000168\n",
      "Epoch 20649/40000, Loss: 4.0122351492755115e-05, Learning Rate: 0.000168\n",
      "Epoch 20650/40000, Loss: 4.019582047476433e-05, Learning Rate: 0.000168\n",
      "Epoch 20651/40000, Loss: 5.3121249948162585e-05, Learning Rate: 0.000168\n",
      "Epoch 20652/40000, Loss: 3.4821234294213355e-05, Learning Rate: 0.000168\n",
      "Epoch 20653/40000, Loss: 3.472085518296808e-05, Learning Rate: 0.000168\n",
      "Epoch 20654/40000, Loss: 3.04105687973788e-05, Learning Rate: 0.000168\n",
      "Epoch 20655/40000, Loss: 3.4954806324094534e-05, Learning Rate: 0.000168\n",
      "Epoch 20656/40000, Loss: 5.307162427925505e-05, Learning Rate: 0.000168\n",
      "Epoch 20657/40000, Loss: 4.0332026401301846e-05, Learning Rate: 0.000168\n",
      "Epoch 20658/40000, Loss: 5.953043364570476e-05, Learning Rate: 0.000168\n",
      "Epoch 20659/40000, Loss: 4.0381160943070427e-05, Learning Rate: 0.000168\n",
      "Epoch 20660/40000, Loss: 3.062123869312927e-05, Learning Rate: 0.000168\n",
      "Epoch 20661/40000, Loss: 5.963717194390483e-05, Learning Rate: 0.000168\n",
      "Epoch 20662/40000, Loss: 3.657935303635895e-05, Learning Rate: 0.000168\n",
      "Epoch 20663/40000, Loss: 1.4598100278817583e-05, Learning Rate: 0.000168\n",
      "Epoch 20664/40000, Loss: 1.4431537238124292e-05, Learning Rate: 0.000168\n",
      "Epoch 20665/40000, Loss: 6.237394700292498e-05, Learning Rate: 0.000168\n",
      "Epoch 20666/40000, Loss: 5.4452582844533026e-05, Learning Rate: 0.000167\n",
      "Epoch 20667/40000, Loss: 1.4876511158945505e-05, Learning Rate: 0.000167\n",
      "Epoch 20668/40000, Loss: 5.4167092457646504e-05, Learning Rate: 0.000167\n",
      "Epoch 20669/40000, Loss: 3.092829865636304e-05, Learning Rate: 0.000167\n",
      "Epoch 20670/40000, Loss: 6.101710459915921e-05, Learning Rate: 0.000167\n",
      "Epoch 20671/40000, Loss: 3.239243233110756e-05, Learning Rate: 0.000167\n",
      "Epoch 20672/40000, Loss: 5.567653715843335e-05, Learning Rate: 0.000167\n",
      "Epoch 20673/40000, Loss: 1.5531470126006752e-05, Learning Rate: 0.000167\n",
      "Epoch 20674/40000, Loss: 3.631821891758591e-05, Learning Rate: 0.000167\n",
      "Epoch 20675/40000, Loss: 6.177193426992744e-05, Learning Rate: 0.000167\n",
      "Epoch 20676/40000, Loss: 6.012745507177897e-05, Learning Rate: 0.000167\n",
      "Epoch 20677/40000, Loss: 1.5112038454390131e-05, Learning Rate: 0.000167\n",
      "Epoch 20678/40000, Loss: 3.1207167921820655e-05, Learning Rate: 0.000167\n",
      "Epoch 20679/40000, Loss: 1.6386868082918227e-05, Learning Rate: 0.000167\n",
      "Epoch 20680/40000, Loss: 3.30784750985913e-05, Learning Rate: 0.000167\n",
      "Epoch 20681/40000, Loss: 3.093260602327064e-05, Learning Rate: 0.000167\n",
      "Epoch 20682/40000, Loss: 3.107033990090713e-05, Learning Rate: 0.000167\n",
      "Epoch 20683/40000, Loss: 5.3883235523244366e-05, Learning Rate: 0.000167\n",
      "Epoch 20684/40000, Loss: 5.3415304137161e-05, Learning Rate: 0.000167\n",
      "Epoch 20685/40000, Loss: 5.9903533838223666e-05, Learning Rate: 0.000167\n",
      "Epoch 20686/40000, Loss: 3.5416600439930335e-05, Learning Rate: 0.000167\n",
      "Epoch 20687/40000, Loss: 5.984530798741616e-05, Learning Rate: 0.000167\n",
      "Epoch 20688/40000, Loss: 5.4329906561179087e-05, Learning Rate: 0.000167\n",
      "Epoch 20689/40000, Loss: 6.16030883975327e-05, Learning Rate: 0.000167\n",
      "Epoch 20690/40000, Loss: 4.1219340346287936e-05, Learning Rate: 0.000167\n",
      "Epoch 20691/40000, Loss: 4.0669670852366835e-05, Learning Rate: 0.000167\n",
      "Epoch 20692/40000, Loss: 4.0723283746046945e-05, Learning Rate: 0.000167\n",
      "Epoch 20693/40000, Loss: 5.370147846406326e-05, Learning Rate: 0.000167\n",
      "Epoch 20694/40000, Loss: 5.3552499593934044e-05, Learning Rate: 0.000167\n",
      "Epoch 20695/40000, Loss: 1.4353789083543234e-05, Learning Rate: 0.000167\n",
      "Epoch 20696/40000, Loss: 5.9335725381970406e-05, Learning Rate: 0.000167\n",
      "Epoch 20697/40000, Loss: 3.497752550174482e-05, Learning Rate: 0.000167\n",
      "Epoch 20698/40000, Loss: 5.979254638077691e-05, Learning Rate: 0.000167\n",
      "Epoch 20699/40000, Loss: 4.066433757543564e-05, Learning Rate: 0.000167\n",
      "Epoch 20700/40000, Loss: 1.4214055227057543e-05, Learning Rate: 0.000167\n",
      "Epoch 20701/40000, Loss: 4.037685721414164e-05, Learning Rate: 0.000167\n",
      "Epoch 20702/40000, Loss: 5.366906407289207e-05, Learning Rate: 0.000167\n",
      "Epoch 20703/40000, Loss: 3.504808773868717e-05, Learning Rate: 0.000167\n",
      "Epoch 20704/40000, Loss: 5.556953328778036e-05, Learning Rate: 0.000167\n",
      "Epoch 20705/40000, Loss: 3.097274748142809e-05, Learning Rate: 0.000167\n",
      "Epoch 20706/40000, Loss: 5.944316217210144e-05, Learning Rate: 0.000167\n",
      "Epoch 20707/40000, Loss: 1.4291525076259859e-05, Learning Rate: 0.000167\n",
      "Epoch 20708/40000, Loss: 5.4095100495032966e-05, Learning Rate: 0.000167\n",
      "Epoch 20709/40000, Loss: 5.93308504903689e-05, Learning Rate: 0.000167\n",
      "Epoch 20710/40000, Loss: 5.6155397032853216e-05, Learning Rate: 0.000167\n",
      "Epoch 20711/40000, Loss: 4.184036151855253e-05, Learning Rate: 0.000167\n",
      "Epoch 20712/40000, Loss: 6.106319779064506e-05, Learning Rate: 0.000167\n",
      "Epoch 20713/40000, Loss: 6.012945596012287e-05, Learning Rate: 0.000167\n",
      "Epoch 20714/40000, Loss: 6.0155427490826696e-05, Learning Rate: 0.000167\n",
      "Epoch 20715/40000, Loss: 1.6090527424239554e-05, Learning Rate: 0.000167\n",
      "Epoch 20716/40000, Loss: 6.270501762628555e-05, Learning Rate: 0.000166\n",
      "Epoch 20717/40000, Loss: 3.116318112006411e-05, Learning Rate: 0.000166\n",
      "Epoch 20718/40000, Loss: 5.9840403991984203e-05, Learning Rate: 0.000166\n",
      "Epoch 20719/40000, Loss: 7.292422378668562e-05, Learning Rate: 0.000166\n",
      "Epoch 20720/40000, Loss: 5.9021949709858745e-05, Learning Rate: 0.000166\n",
      "Epoch 20721/40000, Loss: 3.638719135778956e-05, Learning Rate: 0.000166\n",
      "Epoch 20722/40000, Loss: 3.581191776902415e-05, Learning Rate: 0.000166\n",
      "Epoch 20723/40000, Loss: 6.788519385736436e-05, Learning Rate: 0.000166\n",
      "Epoch 20724/40000, Loss: 4.464833182282746e-05, Learning Rate: 0.000166\n",
      "Epoch 20725/40000, Loss: 1.7505832147435285e-05, Learning Rate: 0.000166\n",
      "Epoch 20726/40000, Loss: 4.671009082812816e-05, Learning Rate: 0.000166\n",
      "Epoch 20727/40000, Loss: 4.25676662416663e-05, Learning Rate: 0.000166\n",
      "Epoch 20728/40000, Loss: 3.218340498278849e-05, Learning Rate: 0.000166\n",
      "Epoch 20729/40000, Loss: 1.5259442079695873e-05, Learning Rate: 0.000166\n",
      "Epoch 20730/40000, Loss: 5.5593493016203865e-05, Learning Rate: 0.000166\n",
      "Epoch 20731/40000, Loss: 3.1077248422661796e-05, Learning Rate: 0.000166\n",
      "Epoch 20732/40000, Loss: 4.153976260568015e-05, Learning Rate: 0.000166\n",
      "Epoch 20733/40000, Loss: 1.4577412912331056e-05, Learning Rate: 0.000166\n",
      "Epoch 20734/40000, Loss: 1.4298908354248852e-05, Learning Rate: 0.000166\n",
      "Epoch 20735/40000, Loss: 3.054923581657931e-05, Learning Rate: 0.000166\n",
      "Epoch 20736/40000, Loss: 4.066358451382257e-05, Learning Rate: 0.000166\n",
      "Epoch 20737/40000, Loss: 3.4829463402274996e-05, Learning Rate: 0.000166\n",
      "Epoch 20738/40000, Loss: 5.935139415669255e-05, Learning Rate: 0.000166\n",
      "Epoch 20739/40000, Loss: 1.40891206683591e-05, Learning Rate: 0.000166\n",
      "Epoch 20740/40000, Loss: 5.332685395842418e-05, Learning Rate: 0.000166\n",
      "Epoch 20741/40000, Loss: 3.04303703160258e-05, Learning Rate: 0.000166\n",
      "Epoch 20742/40000, Loss: 1.4055725841899402e-05, Learning Rate: 0.000166\n",
      "Epoch 20743/40000, Loss: 3.473893229966052e-05, Learning Rate: 0.000166\n",
      "Epoch 20744/40000, Loss: 4.021630229544826e-05, Learning Rate: 0.000166\n",
      "Epoch 20745/40000, Loss: 5.295465962262824e-05, Learning Rate: 0.000166\n",
      "Epoch 20746/40000, Loss: 5.923222124692984e-05, Learning Rate: 0.000166\n",
      "Epoch 20747/40000, Loss: 3.0447628887486644e-05, Learning Rate: 0.000166\n",
      "Epoch 20748/40000, Loss: 1.406002866133349e-05, Learning Rate: 0.000166\n",
      "Epoch 20749/40000, Loss: 4.0069488022709265e-05, Learning Rate: 0.000166\n",
      "Epoch 20750/40000, Loss: 1.3963949641038198e-05, Learning Rate: 0.000166\n",
      "Epoch 20751/40000, Loss: 1.3983419194119051e-05, Learning Rate: 0.000166\n",
      "Epoch 20752/40000, Loss: 3.461869346210733e-05, Learning Rate: 0.000166\n",
      "Epoch 20753/40000, Loss: 4.0175553294830024e-05, Learning Rate: 0.000166\n",
      "Epoch 20754/40000, Loss: 1.3888559806218836e-05, Learning Rate: 0.000166\n",
      "Epoch 20755/40000, Loss: 5.9120069636264816e-05, Learning Rate: 0.000166\n",
      "Epoch 20756/40000, Loss: 3.999867840320803e-05, Learning Rate: 0.000166\n",
      "Epoch 20757/40000, Loss: 4.002881178166717e-05, Learning Rate: 0.000166\n",
      "Epoch 20758/40000, Loss: 4.005821392638609e-05, Learning Rate: 0.000166\n",
      "Epoch 20759/40000, Loss: 3.0372400942724198e-05, Learning Rate: 0.000166\n",
      "Epoch 20760/40000, Loss: 5.29742865182925e-05, Learning Rate: 0.000166\n",
      "Epoch 20761/40000, Loss: 1.3921651770942844e-05, Learning Rate: 0.000166\n",
      "Epoch 20762/40000, Loss: 5.30265097040683e-05, Learning Rate: 0.000166\n",
      "Epoch 20763/40000, Loss: 5.8957291912520304e-05, Learning Rate: 0.000166\n",
      "Epoch 20764/40000, Loss: 3.035652480321005e-05, Learning Rate: 0.000166\n",
      "Epoch 20765/40000, Loss: 3.464413748588413e-05, Learning Rate: 0.000166\n",
      "Epoch 20766/40000, Loss: 5.8917423302773386e-05, Learning Rate: 0.000165\n",
      "Epoch 20767/40000, Loss: 1.3857888916390948e-05, Learning Rate: 0.000165\n",
      "Epoch 20768/40000, Loss: 5.289887121762149e-05, Learning Rate: 0.000165\n",
      "Epoch 20769/40000, Loss: 5.8876794355455786e-05, Learning Rate: 0.000165\n",
      "Epoch 20770/40000, Loss: 3.994931830675341e-05, Learning Rate: 0.000165\n",
      "Epoch 20771/40000, Loss: 4.00451390305534e-05, Learning Rate: 0.000165\n",
      "Epoch 20772/40000, Loss: 5.295704249874689e-05, Learning Rate: 0.000165\n",
      "Epoch 20773/40000, Loss: 3.463457687757909e-05, Learning Rate: 0.000165\n",
      "Epoch 20774/40000, Loss: 3.036304042325355e-05, Learning Rate: 0.000165\n",
      "Epoch 20775/40000, Loss: 4.0092916606226936e-05, Learning Rate: 0.000165\n",
      "Epoch 20776/40000, Loss: 3.0495046303258277e-05, Learning Rate: 0.000165\n",
      "Epoch 20777/40000, Loss: 3.4648397559067234e-05, Learning Rate: 0.000165\n",
      "Epoch 20778/40000, Loss: 4.00147691834718e-05, Learning Rate: 0.000165\n",
      "Epoch 20779/40000, Loss: 5.309607877279632e-05, Learning Rate: 0.000165\n",
      "Epoch 20780/40000, Loss: 4.0177554183173925e-05, Learning Rate: 0.000165\n",
      "Epoch 20781/40000, Loss: 5.3594380005961284e-05, Learning Rate: 0.000165\n",
      "Epoch 20782/40000, Loss: 1.3943298654339742e-05, Learning Rate: 0.000165\n",
      "Epoch 20783/40000, Loss: 3.052959073102102e-05, Learning Rate: 0.000165\n",
      "Epoch 20784/40000, Loss: 1.4036689208296593e-05, Learning Rate: 0.000165\n",
      "Epoch 20785/40000, Loss: 1.3974378816783428e-05, Learning Rate: 0.000165\n",
      "Epoch 20786/40000, Loss: 5.9078480262542143e-05, Learning Rate: 0.000165\n",
      "Epoch 20787/40000, Loss: 1.390714714943897e-05, Learning Rate: 0.000165\n",
      "Epoch 20788/40000, Loss: 1.3910959751228802e-05, Learning Rate: 0.000165\n",
      "Epoch 20789/40000, Loss: 5.290840636007488e-05, Learning Rate: 0.000165\n",
      "Epoch 20790/40000, Loss: 5.295084702083841e-05, Learning Rate: 0.000165\n",
      "Epoch 20791/40000, Loss: 5.293032882036641e-05, Learning Rate: 0.000165\n",
      "Epoch 20792/40000, Loss: 4.009193435194902e-05, Learning Rate: 0.000165\n",
      "Epoch 20793/40000, Loss: 3.0323797545861453e-05, Learning Rate: 0.000165\n",
      "Epoch 20794/40000, Loss: 5.289659384288825e-05, Learning Rate: 0.000165\n",
      "Epoch 20795/40000, Loss: 1.392860031046439e-05, Learning Rate: 0.000165\n",
      "Epoch 20796/40000, Loss: 1.3947039406048134e-05, Learning Rate: 0.000165\n",
      "Epoch 20797/40000, Loss: 4.005177834187634e-05, Learning Rate: 0.000165\n",
      "Epoch 20798/40000, Loss: 3.450302756391466e-05, Learning Rate: 0.000165\n",
      "Epoch 20799/40000, Loss: 1.3858774764230475e-05, Learning Rate: 0.000165\n",
      "Epoch 20800/40000, Loss: 1.3988022146804724e-05, Learning Rate: 0.000165\n",
      "Epoch 20801/40000, Loss: 3.0233872166718356e-05, Learning Rate: 0.000165\n",
      "Epoch 20802/40000, Loss: 5.89197916269768e-05, Learning Rate: 0.000165\n",
      "Epoch 20803/40000, Loss: 3.4627442801138386e-05, Learning Rate: 0.000165\n",
      "Epoch 20804/40000, Loss: 5.3002677304903045e-05, Learning Rate: 0.000165\n",
      "Epoch 20805/40000, Loss: 1.4082888810662553e-05, Learning Rate: 0.000165\n",
      "Epoch 20806/40000, Loss: 5.908534149057232e-05, Learning Rate: 0.000165\n",
      "Epoch 20807/40000, Loss: 5.904283534619026e-05, Learning Rate: 0.000165\n",
      "Epoch 20808/40000, Loss: 3.0412587875616737e-05, Learning Rate: 0.000165\n",
      "Epoch 20809/40000, Loss: 5.317585601005703e-05, Learning Rate: 0.000165\n",
      "Epoch 20810/40000, Loss: 5.313967994879931e-05, Learning Rate: 0.000165\n",
      "Epoch 20811/40000, Loss: 5.897341179661453e-05, Learning Rate: 0.000165\n",
      "Epoch 20812/40000, Loss: 1.4097824532655068e-05, Learning Rate: 0.000165\n",
      "Epoch 20813/40000, Loss: 3.46621272910852e-05, Learning Rate: 0.000165\n",
      "Epoch 20814/40000, Loss: 5.3270894568413496e-05, Learning Rate: 0.000165\n",
      "Epoch 20815/40000, Loss: 3.4585962566779926e-05, Learning Rate: 0.000165\n",
      "Epoch 20816/40000, Loss: 5.3296065743779764e-05, Learning Rate: 0.000165\n",
      "Epoch 20817/40000, Loss: 3.459076106082648e-05, Learning Rate: 0.000164\n",
      "Epoch 20818/40000, Loss: 1.3974064131616615e-05, Learning Rate: 0.000164\n",
      "Epoch 20819/40000, Loss: 5.292348578223027e-05, Learning Rate: 0.000164\n",
      "Epoch 20820/40000, Loss: 3.454312536632642e-05, Learning Rate: 0.000164\n",
      "Epoch 20821/40000, Loss: 1.4048135199118406e-05, Learning Rate: 0.000164\n",
      "Epoch 20822/40000, Loss: 5.3325333283282816e-05, Learning Rate: 0.000164\n",
      "Epoch 20823/40000, Loss: 3.056431523873471e-05, Learning Rate: 0.000164\n",
      "Epoch 20824/40000, Loss: 5.9181751566939056e-05, Learning Rate: 0.000164\n",
      "Epoch 20825/40000, Loss: 3.497235229588114e-05, Learning Rate: 0.000164\n",
      "Epoch 20826/40000, Loss: 4.1312770918011665e-05, Learning Rate: 0.000164\n",
      "Epoch 20827/40000, Loss: 5.351257277652621e-05, Learning Rate: 0.000164\n",
      "Epoch 20828/40000, Loss: 5.9305690228939056e-05, Learning Rate: 0.000164\n",
      "Epoch 20829/40000, Loss: 5.9183126722928137e-05, Learning Rate: 0.000164\n",
      "Epoch 20830/40000, Loss: 5.342810254660435e-05, Learning Rate: 0.000164\n",
      "Epoch 20831/40000, Loss: 4.0727474697632715e-05, Learning Rate: 0.000164\n",
      "Epoch 20832/40000, Loss: 5.424956907518208e-05, Learning Rate: 0.000164\n",
      "Epoch 20833/40000, Loss: 5.352570951799862e-05, Learning Rate: 0.000164\n",
      "Epoch 20834/40000, Loss: 3.0774543120060116e-05, Learning Rate: 0.000164\n",
      "Epoch 20835/40000, Loss: 5.3744399338029325e-05, Learning Rate: 0.000164\n",
      "Epoch 20836/40000, Loss: 3.485374691081233e-05, Learning Rate: 0.000164\n",
      "Epoch 20837/40000, Loss: 1.4273398846853524e-05, Learning Rate: 0.000164\n",
      "Epoch 20838/40000, Loss: 4.0330374758923426e-05, Learning Rate: 0.000164\n",
      "Epoch 20839/40000, Loss: 4.0290811739396304e-05, Learning Rate: 0.000164\n",
      "Epoch 20840/40000, Loss: 6.103629129938781e-05, Learning Rate: 0.000164\n",
      "Epoch 20841/40000, Loss: 3.092892438871786e-05, Learning Rate: 0.000164\n",
      "Epoch 20842/40000, Loss: 5.388070348999463e-05, Learning Rate: 0.000164\n",
      "Epoch 20843/40000, Loss: 3.1046889489516616e-05, Learning Rate: 0.000164\n",
      "Epoch 20844/40000, Loss: 5.4075801017461345e-05, Learning Rate: 0.000164\n",
      "Epoch 20845/40000, Loss: 1.4600784197682515e-05, Learning Rate: 0.000164\n",
      "Epoch 20846/40000, Loss: 1.46722877616412e-05, Learning Rate: 0.000164\n",
      "Epoch 20847/40000, Loss: 3.065766577492468e-05, Learning Rate: 0.000164\n",
      "Epoch 20848/40000, Loss: 5.333985245670192e-05, Learning Rate: 0.000164\n",
      "Epoch 20849/40000, Loss: 1.4391188415174838e-05, Learning Rate: 0.000164\n",
      "Epoch 20850/40000, Loss: 4.0235168853541836e-05, Learning Rate: 0.000164\n",
      "Epoch 20851/40000, Loss: 3.48178728017956e-05, Learning Rate: 0.000164\n",
      "Epoch 20852/40000, Loss: 4.029207048006356e-05, Learning Rate: 0.000164\n",
      "Epoch 20853/40000, Loss: 1.4437224308494478e-05, Learning Rate: 0.000164\n",
      "Epoch 20854/40000, Loss: 3.51039161614608e-05, Learning Rate: 0.000164\n",
      "Epoch 20855/40000, Loss: 3.108575401711278e-05, Learning Rate: 0.000164\n",
      "Epoch 20856/40000, Loss: 3.091267353738658e-05, Learning Rate: 0.000164\n",
      "Epoch 20857/40000, Loss: 3.080029273405671e-05, Learning Rate: 0.000164\n",
      "Epoch 20858/40000, Loss: 3.093286068178713e-05, Learning Rate: 0.000164\n",
      "Epoch 20859/40000, Loss: 4.104264371562749e-05, Learning Rate: 0.000164\n",
      "Epoch 20860/40000, Loss: 5.438606604002416e-05, Learning Rate: 0.000164\n",
      "Epoch 20861/40000, Loss: 1.478905960539123e-05, Learning Rate: 0.000164\n",
      "Epoch 20862/40000, Loss: 6.03906883043237e-05, Learning Rate: 0.000164\n",
      "Epoch 20863/40000, Loss: 4.114879993721843e-05, Learning Rate: 0.000164\n",
      "Epoch 20864/40000, Loss: 6.084378037485294e-05, Learning Rate: 0.000164\n",
      "Epoch 20865/40000, Loss: 6.013781967340037e-05, Learning Rate: 0.000164\n",
      "Epoch 20866/40000, Loss: 4.071224611834623e-05, Learning Rate: 0.000164\n",
      "Epoch 20867/40000, Loss: 1.4500145880447235e-05, Learning Rate: 0.000164\n",
      "Epoch 20868/40000, Loss: 1.4485668543784413e-05, Learning Rate: 0.000163\n",
      "Epoch 20869/40000, Loss: 5.423820766736753e-05, Learning Rate: 0.000163\n",
      "Epoch 20870/40000, Loss: 1.5002887266746257e-05, Learning Rate: 0.000163\n",
      "Epoch 20871/40000, Loss: 3.068848673137836e-05, Learning Rate: 0.000163\n",
      "Epoch 20872/40000, Loss: 1.4811740584264044e-05, Learning Rate: 0.000163\n",
      "Epoch 20873/40000, Loss: 3.2275489502353594e-05, Learning Rate: 0.000163\n",
      "Epoch 20874/40000, Loss: 3.500276216072962e-05, Learning Rate: 0.000163\n",
      "Epoch 20875/40000, Loss: 5.949548358330503e-05, Learning Rate: 0.000163\n",
      "Epoch 20876/40000, Loss: 5.425456038210541e-05, Learning Rate: 0.000163\n",
      "Epoch 20877/40000, Loss: 5.419637818704359e-05, Learning Rate: 0.000163\n",
      "Epoch 20878/40000, Loss: 5.362693627830595e-05, Learning Rate: 0.000163\n",
      "Epoch 20879/40000, Loss: 5.37239684490487e-05, Learning Rate: 0.000163\n",
      "Epoch 20880/40000, Loss: 5.942302959738299e-05, Learning Rate: 0.000163\n",
      "Epoch 20881/40000, Loss: 3.054157423321158e-05, Learning Rate: 0.000163\n",
      "Epoch 20882/40000, Loss: 4.034290031995624e-05, Learning Rate: 0.000163\n",
      "Epoch 20883/40000, Loss: 5.933034481131472e-05, Learning Rate: 0.000163\n",
      "Epoch 20884/40000, Loss: 3.655087130027823e-05, Learning Rate: 0.000163\n",
      "Epoch 20885/40000, Loss: 4.033992445329204e-05, Learning Rate: 0.000163\n",
      "Epoch 20886/40000, Loss: 4.032647120766342e-05, Learning Rate: 0.000163\n",
      "Epoch 20887/40000, Loss: 6.045382906449959e-05, Learning Rate: 0.000163\n",
      "Epoch 20888/40000, Loss: 1.4166994333209004e-05, Learning Rate: 0.000163\n",
      "Epoch 20889/40000, Loss: 3.053158798138611e-05, Learning Rate: 0.000163\n",
      "Epoch 20890/40000, Loss: 5.375498949433677e-05, Learning Rate: 0.000163\n",
      "Epoch 20891/40000, Loss: 3.090392056037672e-05, Learning Rate: 0.000163\n",
      "Epoch 20892/40000, Loss: 4.038621409563348e-05, Learning Rate: 0.000163\n",
      "Epoch 20893/40000, Loss: 3.2797150197438896e-05, Learning Rate: 0.000163\n",
      "Epoch 20894/40000, Loss: 5.356154724722728e-05, Learning Rate: 0.000163\n",
      "Epoch 20895/40000, Loss: 3.922577525372617e-05, Learning Rate: 0.000163\n",
      "Epoch 20896/40000, Loss: 4.077463745488785e-05, Learning Rate: 0.000163\n",
      "Epoch 20897/40000, Loss: 4.0509930840926245e-05, Learning Rate: 0.000163\n",
      "Epoch 20898/40000, Loss: 1.4497606571239885e-05, Learning Rate: 0.000163\n",
      "Epoch 20899/40000, Loss: 6.0311605921015143e-05, Learning Rate: 0.000163\n",
      "Epoch 20900/40000, Loss: 4.042419823235832e-05, Learning Rate: 0.000163\n",
      "Epoch 20901/40000, Loss: 3.4906206565210596e-05, Learning Rate: 0.000163\n",
      "Epoch 20902/40000, Loss: 5.9329508076189086e-05, Learning Rate: 0.000163\n",
      "Epoch 20903/40000, Loss: 4.040998828713782e-05, Learning Rate: 0.000163\n",
      "Epoch 20904/40000, Loss: 3.081400791415945e-05, Learning Rate: 0.000163\n",
      "Epoch 20905/40000, Loss: 3.4691252949414775e-05, Learning Rate: 0.000163\n",
      "Epoch 20906/40000, Loss: 4.046149842906743e-05, Learning Rate: 0.000163\n",
      "Epoch 20907/40000, Loss: 1.422306741005741e-05, Learning Rate: 0.000163\n",
      "Epoch 20908/40000, Loss: 5.929977851337753e-05, Learning Rate: 0.000163\n",
      "Epoch 20909/40000, Loss: 3.0491273719235323e-05, Learning Rate: 0.000163\n",
      "Epoch 20910/40000, Loss: 4.0582483052276075e-05, Learning Rate: 0.000163\n",
      "Epoch 20911/40000, Loss: 4.024355439469218e-05, Learning Rate: 0.000163\n",
      "Epoch 20912/40000, Loss: 3.0443916330114007e-05, Learning Rate: 0.000163\n",
      "Epoch 20913/40000, Loss: 4.086401531822048e-05, Learning Rate: 0.000163\n",
      "Epoch 20914/40000, Loss: 1.4460768397839274e-05, Learning Rate: 0.000163\n",
      "Epoch 20915/40000, Loss: 3.0376966606127098e-05, Learning Rate: 0.000163\n",
      "Epoch 20916/40000, Loss: 3.0273557058535516e-05, Learning Rate: 0.000163\n",
      "Epoch 20917/40000, Loss: 4.011978307971731e-05, Learning Rate: 0.000163\n",
      "Epoch 20918/40000, Loss: 1.4180501239025034e-05, Learning Rate: 0.000163\n",
      "Epoch 20919/40000, Loss: 3.469612420303747e-05, Learning Rate: 0.000162\n",
      "Epoch 20920/40000, Loss: 4.0291750337928534e-05, Learning Rate: 0.000162\n",
      "Epoch 20921/40000, Loss: 4.02150399168022e-05, Learning Rate: 0.000162\n",
      "Epoch 20922/40000, Loss: 5.915960718994029e-05, Learning Rate: 0.000162\n",
      "Epoch 20923/40000, Loss: 3.473391188890673e-05, Learning Rate: 0.000162\n",
      "Epoch 20924/40000, Loss: 5.900282849324867e-05, Learning Rate: 0.000162\n",
      "Epoch 20925/40000, Loss: 1.4158719750412274e-05, Learning Rate: 0.000162\n",
      "Epoch 20926/40000, Loss: 3.478741200524382e-05, Learning Rate: 0.000162\n",
      "Epoch 20927/40000, Loss: 5.320698255673051e-05, Learning Rate: 0.000162\n",
      "Epoch 20928/40000, Loss: 3.056382774957456e-05, Learning Rate: 0.000162\n",
      "Epoch 20929/40000, Loss: 5.912905544391833e-05, Learning Rate: 0.000162\n",
      "Epoch 20930/40000, Loss: 1.4405422916752286e-05, Learning Rate: 0.000162\n",
      "Epoch 20931/40000, Loss: 4.0522463677916676e-05, Learning Rate: 0.000162\n",
      "Epoch 20932/40000, Loss: 1.4330172234622296e-05, Learning Rate: 0.000162\n",
      "Epoch 20933/40000, Loss: 3.46867018379271e-05, Learning Rate: 0.000162\n",
      "Epoch 20934/40000, Loss: 4.0647966670803726e-05, Learning Rate: 0.000162\n",
      "Epoch 20935/40000, Loss: 1.4427602764044423e-05, Learning Rate: 0.000162\n",
      "Epoch 20936/40000, Loss: 4.036575410282239e-05, Learning Rate: 0.000162\n",
      "Epoch 20937/40000, Loss: 1.4615000509365927e-05, Learning Rate: 0.000162\n",
      "Epoch 20938/40000, Loss: 1.4124198060017079e-05, Learning Rate: 0.000162\n",
      "Epoch 20939/40000, Loss: 5.9129575674887747e-05, Learning Rate: 0.000162\n",
      "Epoch 20940/40000, Loss: 1.4320276022772305e-05, Learning Rate: 0.000162\n",
      "Epoch 20941/40000, Loss: 3.0238235922297463e-05, Learning Rate: 0.000162\n",
      "Epoch 20942/40000, Loss: 5.9037112805526704e-05, Learning Rate: 0.000162\n",
      "Epoch 20943/40000, Loss: 3.456913691479713e-05, Learning Rate: 0.000162\n",
      "Epoch 20944/40000, Loss: 1.4093061508901883e-05, Learning Rate: 0.000162\n",
      "Epoch 20945/40000, Loss: 3.446411938057281e-05, Learning Rate: 0.000162\n",
      "Epoch 20946/40000, Loss: 4.015061495010741e-05, Learning Rate: 0.000162\n",
      "Epoch 20947/40000, Loss: 1.4064802599023096e-05, Learning Rate: 0.000162\n",
      "Epoch 20948/40000, Loss: 5.883834819542244e-05, Learning Rate: 0.000162\n",
      "Epoch 20949/40000, Loss: 3.020223812200129e-05, Learning Rate: 0.000162\n",
      "Epoch 20950/40000, Loss: 5.885087375645526e-05, Learning Rate: 0.000162\n",
      "Epoch 20951/40000, Loss: 5.287131716613658e-05, Learning Rate: 0.000162\n",
      "Epoch 20952/40000, Loss: 5.8822493883781135e-05, Learning Rate: 0.000162\n",
      "Epoch 20953/40000, Loss: 3.4675729693844914e-05, Learning Rate: 0.000162\n",
      "Epoch 20954/40000, Loss: 3.4486231015762314e-05, Learning Rate: 0.000162\n",
      "Epoch 20955/40000, Loss: 3.449979340075515e-05, Learning Rate: 0.000162\n",
      "Epoch 20956/40000, Loss: 5.9023626818088815e-05, Learning Rate: 0.000162\n",
      "Epoch 20957/40000, Loss: 3.020966141775716e-05, Learning Rate: 0.000162\n",
      "Epoch 20958/40000, Loss: 3.453492536209524e-05, Learning Rate: 0.000162\n",
      "Epoch 20959/40000, Loss: 4.002514833700843e-05, Learning Rate: 0.000162\n",
      "Epoch 20960/40000, Loss: 1.402907946612686e-05, Learning Rate: 0.000162\n",
      "Epoch 20961/40000, Loss: 1.4047115655557718e-05, Learning Rate: 0.000162\n",
      "Epoch 20962/40000, Loss: 3.028723585885018e-05, Learning Rate: 0.000162\n",
      "Epoch 20963/40000, Loss: 3.027413913514465e-05, Learning Rate: 0.000162\n",
      "Epoch 20964/40000, Loss: 3.022149030584842e-05, Learning Rate: 0.000162\n",
      "Epoch 20965/40000, Loss: 5.886841245228425e-05, Learning Rate: 0.000162\n",
      "Epoch 20966/40000, Loss: 1.4111998098087497e-05, Learning Rate: 0.000162\n",
      "Epoch 20967/40000, Loss: 3.467703209025785e-05, Learning Rate: 0.000162\n",
      "Epoch 20968/40000, Loss: 3.030316838703584e-05, Learning Rate: 0.000162\n",
      "Epoch 20969/40000, Loss: 4.03807898692321e-05, Learning Rate: 0.000162\n",
      "Epoch 20970/40000, Loss: 5.8941805036738515e-05, Learning Rate: 0.000161\n",
      "Epoch 20971/40000, Loss: 3.50401496689301e-05, Learning Rate: 0.000161\n",
      "Epoch 20972/40000, Loss: 5.341879659681581e-05, Learning Rate: 0.000161\n",
      "Epoch 20973/40000, Loss: 5.343160955817439e-05, Learning Rate: 0.000161\n",
      "Epoch 20974/40000, Loss: 3.491906318231486e-05, Learning Rate: 0.000161\n",
      "Epoch 20975/40000, Loss: 5.351163781597279e-05, Learning Rate: 0.000161\n",
      "Epoch 20976/40000, Loss: 3.068546357098967e-05, Learning Rate: 0.000161\n",
      "Epoch 20977/40000, Loss: 1.4279209608503152e-05, Learning Rate: 0.000161\n",
      "Epoch 20978/40000, Loss: 4.072042429470457e-05, Learning Rate: 0.000161\n",
      "Epoch 20979/40000, Loss: 5.997098924126476e-05, Learning Rate: 0.000161\n",
      "Epoch 20980/40000, Loss: 1.4331788406707346e-05, Learning Rate: 0.000161\n",
      "Epoch 20981/40000, Loss: 5.9663987485691905e-05, Learning Rate: 0.000161\n",
      "Epoch 20982/40000, Loss: 4.073871605214663e-05, Learning Rate: 0.000161\n",
      "Epoch 20983/40000, Loss: 5.356548717827536e-05, Learning Rate: 0.000161\n",
      "Epoch 20984/40000, Loss: 3.547941378201358e-05, Learning Rate: 0.000161\n",
      "Epoch 20985/40000, Loss: 6.000731445965357e-05, Learning Rate: 0.000161\n",
      "Epoch 20986/40000, Loss: 6.0145335737615824e-05, Learning Rate: 0.000161\n",
      "Epoch 20987/40000, Loss: 5.993635204504244e-05, Learning Rate: 0.000161\n",
      "Epoch 20988/40000, Loss: 5.845836494700052e-05, Learning Rate: 0.000161\n",
      "Epoch 20989/40000, Loss: 3.63233448297251e-05, Learning Rate: 0.000161\n",
      "Epoch 20990/40000, Loss: 6.226730329217389e-05, Learning Rate: 0.000161\n",
      "Epoch 20991/40000, Loss: 1.5432624422828667e-05, Learning Rate: 0.000161\n",
      "Epoch 20992/40000, Loss: 4.4214124500285834e-05, Learning Rate: 0.000161\n",
      "Epoch 20993/40000, Loss: 3.208795897080563e-05, Learning Rate: 0.000161\n",
      "Epoch 20994/40000, Loss: 5.622506796498783e-05, Learning Rate: 0.000161\n",
      "Epoch 20995/40000, Loss: 3.774532160605304e-05, Learning Rate: 0.000161\n",
      "Epoch 20996/40000, Loss: 6.436309922719374e-05, Learning Rate: 0.000161\n",
      "Epoch 20997/40000, Loss: 3.19120408676099e-05, Learning Rate: 0.000161\n",
      "Epoch 20998/40000, Loss: 3.1051680707605556e-05, Learning Rate: 0.000161\n",
      "Epoch 20999/40000, Loss: 3.714133345056325e-05, Learning Rate: 0.000161\n",
      "Epoch 21000/40000, Loss: 3.543473576428369e-05, Learning Rate: 0.000161\n",
      "Epoch 21001/40000, Loss: 6.065325942472555e-05, Learning Rate: 0.000161\n",
      "Epoch 21002/40000, Loss: 3.160797496093437e-05, Learning Rate: 0.000161\n",
      "Epoch 21003/40000, Loss: 6.268783181440085e-05, Learning Rate: 0.000161\n",
      "Epoch 21004/40000, Loss: 5.580483048106544e-05, Learning Rate: 0.000161\n",
      "Epoch 21005/40000, Loss: 7.281407306436449e-05, Learning Rate: 0.000161\n",
      "Epoch 21006/40000, Loss: 5.6867011153372005e-05, Learning Rate: 0.000161\n",
      "Epoch 21007/40000, Loss: 3.2530282624065876e-05, Learning Rate: 0.000161\n",
      "Epoch 21008/40000, Loss: 1.6188174413400702e-05, Learning Rate: 0.000161\n",
      "Epoch 21009/40000, Loss: 4.287698902771808e-05, Learning Rate: 0.000161\n",
      "Epoch 21010/40000, Loss: 5.548354238271713e-05, Learning Rate: 0.000161\n",
      "Epoch 21011/40000, Loss: 5.9880541812162846e-05, Learning Rate: 0.000161\n",
      "Epoch 21012/40000, Loss: 3.185480454703793e-05, Learning Rate: 0.000161\n",
      "Epoch 21013/40000, Loss: 4.739159703603946e-05, Learning Rate: 0.000161\n",
      "Epoch 21014/40000, Loss: 3.3036434615496546e-05, Learning Rate: 0.000161\n",
      "Epoch 21015/40000, Loss: 6.171721906866878e-05, Learning Rate: 0.000161\n",
      "Epoch 21016/40000, Loss: 6.016529368935153e-05, Learning Rate: 0.000161\n",
      "Epoch 21017/40000, Loss: 3.6104607715969905e-05, Learning Rate: 0.000161\n",
      "Epoch 21018/40000, Loss: 3.494137490633875e-05, Learning Rate: 0.000161\n",
      "Epoch 21019/40000, Loss: 5.35883846168872e-05, Learning Rate: 0.000161\n",
      "Epoch 21020/40000, Loss: 3.531797483447008e-05, Learning Rate: 0.000161\n",
      "Epoch 21021/40000, Loss: 4.063639426021837e-05, Learning Rate: 0.000161\n",
      "Epoch 21022/40000, Loss: 1.4697893675474916e-05, Learning Rate: 0.000160\n",
      "Epoch 21023/40000, Loss: 3.050471241294872e-05, Learning Rate: 0.000160\n",
      "Epoch 21024/40000, Loss: 4.027627073810436e-05, Learning Rate: 0.000160\n",
      "Epoch 21025/40000, Loss: 5.309004336595535e-05, Learning Rate: 0.000160\n",
      "Epoch 21026/40000, Loss: 3.0322033126140013e-05, Learning Rate: 0.000160\n",
      "Epoch 21027/40000, Loss: 3.0200300898286514e-05, Learning Rate: 0.000160\n",
      "Epoch 21028/40000, Loss: 4.037874168716371e-05, Learning Rate: 0.000160\n",
      "Epoch 21029/40000, Loss: 5.882386540179141e-05, Learning Rate: 0.000160\n",
      "Epoch 21030/40000, Loss: 1.3963615856482647e-05, Learning Rate: 0.000160\n",
      "Epoch 21031/40000, Loss: 3.44922264048364e-05, Learning Rate: 0.000160\n",
      "Epoch 21032/40000, Loss: 1.4090912372921593e-05, Learning Rate: 0.000160\n",
      "Epoch 21033/40000, Loss: 3.458182982285507e-05, Learning Rate: 0.000160\n",
      "Epoch 21034/40000, Loss: 3.4330903872614726e-05, Learning Rate: 0.000160\n",
      "Epoch 21035/40000, Loss: 4.01205979869701e-05, Learning Rate: 0.000160\n",
      "Epoch 21036/40000, Loss: 5.290347689879127e-05, Learning Rate: 0.000160\n",
      "Epoch 21037/40000, Loss: 4.002147397841327e-05, Learning Rate: 0.000160\n",
      "Epoch 21038/40000, Loss: 3.998194733867422e-05, Learning Rate: 0.000160\n",
      "Epoch 21039/40000, Loss: 3.0086141123319976e-05, Learning Rate: 0.000160\n",
      "Epoch 21040/40000, Loss: 3.997686872025952e-05, Learning Rate: 0.000160\n",
      "Epoch 21041/40000, Loss: 3.007090708706528e-05, Learning Rate: 0.000160\n",
      "Epoch 21042/40000, Loss: 5.878787487745285e-05, Learning Rate: 0.000160\n",
      "Epoch 21043/40000, Loss: 5.2806488383794203e-05, Learning Rate: 0.000160\n",
      "Epoch 21044/40000, Loss: 5.8726698625832796e-05, Learning Rate: 0.000160\n",
      "Epoch 21045/40000, Loss: 3.9932303479872644e-05, Learning Rate: 0.000160\n",
      "Epoch 21046/40000, Loss: 3.997445310233161e-05, Learning Rate: 0.000160\n",
      "Epoch 21047/40000, Loss: 5.87594295211602e-05, Learning Rate: 0.000160\n",
      "Epoch 21048/40000, Loss: 3.44474938174244e-05, Learning Rate: 0.000160\n",
      "Epoch 21049/40000, Loss: 1.3964215213491116e-05, Learning Rate: 0.000160\n",
      "Epoch 21050/40000, Loss: 3.0161954782670364e-05, Learning Rate: 0.000160\n",
      "Epoch 21051/40000, Loss: 5.8787067246157676e-05, Learning Rate: 0.000160\n",
      "Epoch 21052/40000, Loss: 1.3899233636038844e-05, Learning Rate: 0.000160\n",
      "Epoch 21053/40000, Loss: 1.398350104864221e-05, Learning Rate: 0.000160\n",
      "Epoch 21054/40000, Loss: 3.442279194132425e-05, Learning Rate: 0.000160\n",
      "Epoch 21055/40000, Loss: 4.007207462564111e-05, Learning Rate: 0.000160\n",
      "Epoch 21056/40000, Loss: 5.285641600494273e-05, Learning Rate: 0.000160\n",
      "Epoch 21057/40000, Loss: 3.0129616789054126e-05, Learning Rate: 0.000160\n",
      "Epoch 21058/40000, Loss: 5.289360706228763e-05, Learning Rate: 0.000160\n",
      "Epoch 21059/40000, Loss: 3.0113758839434013e-05, Learning Rate: 0.000160\n",
      "Epoch 21060/40000, Loss: 3.010056389030069e-05, Learning Rate: 0.000160\n",
      "Epoch 21061/40000, Loss: 1.3925118764746003e-05, Learning Rate: 0.000160\n",
      "Epoch 21062/40000, Loss: 1.388479086017469e-05, Learning Rate: 0.000160\n",
      "Epoch 21063/40000, Loss: 3.437266786932014e-05, Learning Rate: 0.000160\n",
      "Epoch 21064/40000, Loss: 3.434064637986012e-05, Learning Rate: 0.000160\n",
      "Epoch 21065/40000, Loss: 5.871452231076546e-05, Learning Rate: 0.000160\n",
      "Epoch 21066/40000, Loss: 5.284260623739101e-05, Learning Rate: 0.000160\n",
      "Epoch 21067/40000, Loss: 3.439564534346573e-05, Learning Rate: 0.000160\n",
      "Epoch 21068/40000, Loss: 3.437720442889258e-05, Learning Rate: 0.000160\n",
      "Epoch 21069/40000, Loss: 3.01459676848026e-05, Learning Rate: 0.000160\n",
      "Epoch 21070/40000, Loss: 1.3989228136779275e-05, Learning Rate: 0.000160\n",
      "Epoch 21071/40000, Loss: 3.028633727808483e-05, Learning Rate: 0.000160\n",
      "Epoch 21072/40000, Loss: 5.289498221827671e-05, Learning Rate: 0.000160\n",
      "Epoch 21073/40000, Loss: 4.011525015812367e-05, Learning Rate: 0.000160\n",
      "Epoch 21074/40000, Loss: 5.878821320948191e-05, Learning Rate: 0.000159\n",
      "Epoch 21075/40000, Loss: 3.447380004217848e-05, Learning Rate: 0.000159\n",
      "Epoch 21076/40000, Loss: 3.4459299058653414e-05, Learning Rate: 0.000159\n",
      "Epoch 21077/40000, Loss: 3.013539026142098e-05, Learning Rate: 0.000159\n",
      "Epoch 21078/40000, Loss: 4.0281280234921724e-05, Learning Rate: 0.000159\n",
      "Epoch 21079/40000, Loss: 5.297246389091015e-05, Learning Rate: 0.000159\n",
      "Epoch 21080/40000, Loss: 3.431156073929742e-05, Learning Rate: 0.000159\n",
      "Epoch 21081/40000, Loss: 5.9595437051029876e-05, Learning Rate: 0.000159\n",
      "Epoch 21082/40000, Loss: 1.4146552530291956e-05, Learning Rate: 0.000159\n",
      "Epoch 21083/40000, Loss: 5.3232917707646266e-05, Learning Rate: 0.000159\n",
      "Epoch 21084/40000, Loss: 3.036176713067107e-05, Learning Rate: 0.000159\n",
      "Epoch 21085/40000, Loss: 4.091068694833666e-05, Learning Rate: 0.000159\n",
      "Epoch 21086/40000, Loss: 3.1503695936407894e-05, Learning Rate: 0.000159\n",
      "Epoch 21087/40000, Loss: 3.0444352887570858e-05, Learning Rate: 0.000159\n",
      "Epoch 21088/40000, Loss: 3.043428478122223e-05, Learning Rate: 0.000159\n",
      "Epoch 21089/40000, Loss: 5.9032634453615174e-05, Learning Rate: 0.000159\n",
      "Epoch 21090/40000, Loss: 1.5012798940006178e-05, Learning Rate: 0.000159\n",
      "Epoch 21091/40000, Loss: 5.433593469206244e-05, Learning Rate: 0.000159\n",
      "Epoch 21092/40000, Loss: 5.965195668977685e-05, Learning Rate: 0.000159\n",
      "Epoch 21093/40000, Loss: 5.951266939518973e-05, Learning Rate: 0.000159\n",
      "Epoch 21094/40000, Loss: 4.088499554200098e-05, Learning Rate: 0.000159\n",
      "Epoch 21095/40000, Loss: 4.120852827327326e-05, Learning Rate: 0.000159\n",
      "Epoch 21096/40000, Loss: 4.0359023842029274e-05, Learning Rate: 0.000159\n",
      "Epoch 21097/40000, Loss: 4.1892624722095206e-05, Learning Rate: 0.000159\n",
      "Epoch 21098/40000, Loss: 4.048595656058751e-05, Learning Rate: 0.000159\n",
      "Epoch 21099/40000, Loss: 6.0070931795053184e-05, Learning Rate: 0.000159\n",
      "Epoch 21100/40000, Loss: 1.4678798834211193e-05, Learning Rate: 0.000159\n",
      "Epoch 21101/40000, Loss: 4.378413359518163e-05, Learning Rate: 0.000159\n",
      "Epoch 21102/40000, Loss: 4.171725231572054e-05, Learning Rate: 0.000159\n",
      "Epoch 21103/40000, Loss: 5.409207005868666e-05, Learning Rate: 0.000159\n",
      "Epoch 21104/40000, Loss: 5.416643398348242e-05, Learning Rate: 0.000159\n",
      "Epoch 21105/40000, Loss: 3.546907100826502e-05, Learning Rate: 0.000159\n",
      "Epoch 21106/40000, Loss: 3.543524144333787e-05, Learning Rate: 0.000159\n",
      "Epoch 21107/40000, Loss: 4.1159022657666355e-05, Learning Rate: 0.000159\n",
      "Epoch 21108/40000, Loss: 1.4870587619952857e-05, Learning Rate: 0.000159\n",
      "Epoch 21109/40000, Loss: 3.589516200008802e-05, Learning Rate: 0.000159\n",
      "Epoch 21110/40000, Loss: 1.6285657693515532e-05, Learning Rate: 0.000159\n",
      "Epoch 21111/40000, Loss: 6.632774602621794e-05, Learning Rate: 0.000159\n",
      "Epoch 21112/40000, Loss: 1.6745912944315933e-05, Learning Rate: 0.000159\n",
      "Epoch 21113/40000, Loss: 8.946425805334002e-05, Learning Rate: 0.000159\n",
      "Epoch 21114/40000, Loss: 6.575490260729566e-05, Learning Rate: 0.000159\n",
      "Epoch 21115/40000, Loss: 4.420524783199653e-05, Learning Rate: 0.000159\n",
      "Epoch 21116/40000, Loss: 1.5508567230426706e-05, Learning Rate: 0.000159\n",
      "Epoch 21117/40000, Loss: 6.595722516067326e-05, Learning Rate: 0.000159\n",
      "Epoch 21118/40000, Loss: 3.164276495226659e-05, Learning Rate: 0.000159\n",
      "Epoch 21119/40000, Loss: 3.067120269406587e-05, Learning Rate: 0.000159\n",
      "Epoch 21120/40000, Loss: 3.0579340091207996e-05, Learning Rate: 0.000159\n",
      "Epoch 21121/40000, Loss: 3.063129770453088e-05, Learning Rate: 0.000159\n",
      "Epoch 21122/40000, Loss: 3.498085789033212e-05, Learning Rate: 0.000159\n",
      "Epoch 21123/40000, Loss: 3.454590842011385e-05, Learning Rate: 0.000159\n",
      "Epoch 21124/40000, Loss: 4.016774983028881e-05, Learning Rate: 0.000159\n",
      "Epoch 21125/40000, Loss: 4.071600051247515e-05, Learning Rate: 0.000159\n",
      "Epoch 21126/40000, Loss: 4.0192302549257874e-05, Learning Rate: 0.000159\n",
      "Epoch 21127/40000, Loss: 3.0513339879689738e-05, Learning Rate: 0.000158\n",
      "Epoch 21128/40000, Loss: 3.057773210457526e-05, Learning Rate: 0.000158\n",
      "Epoch 21129/40000, Loss: 3.4560995118226856e-05, Learning Rate: 0.000158\n",
      "Epoch 21130/40000, Loss: 3.5551285691326484e-05, Learning Rate: 0.000158\n",
      "Epoch 21131/40000, Loss: 5.605783007922582e-05, Learning Rate: 0.000158\n",
      "Epoch 21132/40000, Loss: 1.437410719518084e-05, Learning Rate: 0.000158\n",
      "Epoch 21133/40000, Loss: 4.096774136996828e-05, Learning Rate: 0.000158\n",
      "Epoch 21134/40000, Loss: 5.987560871290043e-05, Learning Rate: 0.000158\n",
      "Epoch 21135/40000, Loss: 4.369879388832487e-05, Learning Rate: 0.000158\n",
      "Epoch 21136/40000, Loss: 4.1632338252384216e-05, Learning Rate: 0.000158\n",
      "Epoch 21137/40000, Loss: 1.4538110917783342e-05, Learning Rate: 0.000158\n",
      "Epoch 21138/40000, Loss: 5.988406337564811e-05, Learning Rate: 0.000158\n",
      "Epoch 21139/40000, Loss: 4.102000093553215e-05, Learning Rate: 0.000158\n",
      "Epoch 21140/40000, Loss: 3.463600660325028e-05, Learning Rate: 0.000158\n",
      "Epoch 21141/40000, Loss: 3.0771981982979923e-05, Learning Rate: 0.000158\n",
      "Epoch 21142/40000, Loss: 3.498039586702362e-05, Learning Rate: 0.000158\n",
      "Epoch 21143/40000, Loss: 3.2255749829346314e-05, Learning Rate: 0.000158\n",
      "Epoch 21144/40000, Loss: 5.4424264817498624e-05, Learning Rate: 0.000158\n",
      "Epoch 21145/40000, Loss: 1.4597305380448233e-05, Learning Rate: 0.000158\n",
      "Epoch 21146/40000, Loss: 5.424928167485632e-05, Learning Rate: 0.000158\n",
      "Epoch 21147/40000, Loss: 4.0915747376857325e-05, Learning Rate: 0.000158\n",
      "Epoch 21148/40000, Loss: 5.9419773606350645e-05, Learning Rate: 0.000158\n",
      "Epoch 21149/40000, Loss: 4.1060528019443154e-05, Learning Rate: 0.000158\n",
      "Epoch 21150/40000, Loss: 4.038245606352575e-05, Learning Rate: 0.000158\n",
      "Epoch 21151/40000, Loss: 4.015090598841198e-05, Learning Rate: 0.000158\n",
      "Epoch 21152/40000, Loss: 1.4257215298130177e-05, Learning Rate: 0.000158\n",
      "Epoch 21153/40000, Loss: 1.4229759472073056e-05, Learning Rate: 0.000158\n",
      "Epoch 21154/40000, Loss: 4.006623930763453e-05, Learning Rate: 0.000158\n",
      "Epoch 21155/40000, Loss: 3.4418029827065766e-05, Learning Rate: 0.000158\n",
      "Epoch 21156/40000, Loss: 5.2908897487213835e-05, Learning Rate: 0.000158\n",
      "Epoch 21157/40000, Loss: 5.894968853681348e-05, Learning Rate: 0.000158\n",
      "Epoch 21158/40000, Loss: 5.88793154747691e-05, Learning Rate: 0.000158\n",
      "Epoch 21159/40000, Loss: 5.8880446886178106e-05, Learning Rate: 0.000158\n",
      "Epoch 21160/40000, Loss: 1.4096703125687782e-05, Learning Rate: 0.000158\n",
      "Epoch 21161/40000, Loss: 5.30212237208616e-05, Learning Rate: 0.000158\n",
      "Epoch 21162/40000, Loss: 3.434556856518611e-05, Learning Rate: 0.000158\n",
      "Epoch 21163/40000, Loss: 5.336065441952087e-05, Learning Rate: 0.000158\n",
      "Epoch 21164/40000, Loss: 5.2917079301550984e-05, Learning Rate: 0.000158\n",
      "Epoch 21165/40000, Loss: 4.01048491767142e-05, Learning Rate: 0.000158\n",
      "Epoch 21166/40000, Loss: 5.899710959056392e-05, Learning Rate: 0.000158\n",
      "Epoch 21167/40000, Loss: 5.895207141293213e-05, Learning Rate: 0.000158\n",
      "Epoch 21168/40000, Loss: 1.4026342796569224e-05, Learning Rate: 0.000158\n",
      "Epoch 21169/40000, Loss: 1.401642475684639e-05, Learning Rate: 0.000158\n",
      "Epoch 21170/40000, Loss: 3.427913543418981e-05, Learning Rate: 0.000158\n",
      "Epoch 21171/40000, Loss: 1.3949630556453485e-05, Learning Rate: 0.000158\n",
      "Epoch 21172/40000, Loss: 1.395005892845802e-05, Learning Rate: 0.000158\n",
      "Epoch 21173/40000, Loss: 5.919668183196336e-05, Learning Rate: 0.000158\n",
      "Epoch 21174/40000, Loss: 3.447388735366985e-05, Learning Rate: 0.000158\n",
      "Epoch 21175/40000, Loss: 5.2801020501647145e-05, Learning Rate: 0.000158\n",
      "Epoch 21176/40000, Loss: 3.0034991141292267e-05, Learning Rate: 0.000158\n",
      "Epoch 21177/40000, Loss: 3.43991705449298e-05, Learning Rate: 0.000158\n",
      "Epoch 21178/40000, Loss: 5.284092912916094e-05, Learning Rate: 0.000158\n",
      "Epoch 21179/40000, Loss: 3.4355358366156e-05, Learning Rate: 0.000157\n",
      "Epoch 21180/40000, Loss: 5.869329834240489e-05, Learning Rate: 0.000157\n",
      "Epoch 21181/40000, Loss: 3.429088246775791e-05, Learning Rate: 0.000157\n",
      "Epoch 21182/40000, Loss: 3.428767740842886e-05, Learning Rate: 0.000157\n",
      "Epoch 21183/40000, Loss: 3.004761674674228e-05, Learning Rate: 0.000157\n",
      "Epoch 21184/40000, Loss: 1.4014412954566069e-05, Learning Rate: 0.000157\n",
      "Epoch 21185/40000, Loss: 5.277464515529573e-05, Learning Rate: 0.000157\n",
      "Epoch 21186/40000, Loss: 5.881797187612392e-05, Learning Rate: 0.000157\n",
      "Epoch 21187/40000, Loss: 3.007776649610605e-05, Learning Rate: 0.000157\n",
      "Epoch 21188/40000, Loss: 5.913029599469155e-05, Learning Rate: 0.000157\n",
      "Epoch 21189/40000, Loss: 3.9979109715204686e-05, Learning Rate: 0.000157\n",
      "Epoch 21190/40000, Loss: 5.8896635891869664e-05, Learning Rate: 0.000157\n",
      "Epoch 21191/40000, Loss: 3.4323486033827066e-05, Learning Rate: 0.000157\n",
      "Epoch 21192/40000, Loss: 2.998141098942142e-05, Learning Rate: 0.000157\n",
      "Epoch 21193/40000, Loss: 3.435924736550078e-05, Learning Rate: 0.000157\n",
      "Epoch 21194/40000, Loss: 3.0081344448262826e-05, Learning Rate: 0.000157\n",
      "Epoch 21195/40000, Loss: 1.3953027519164607e-05, Learning Rate: 0.000157\n",
      "Epoch 21196/40000, Loss: 4.0009432268561795e-05, Learning Rate: 0.000157\n",
      "Epoch 21197/40000, Loss: 1.3912965187046211e-05, Learning Rate: 0.000157\n",
      "Epoch 21198/40000, Loss: 4.005142909591086e-05, Learning Rate: 0.000157\n",
      "Epoch 21199/40000, Loss: 3.010882210219279e-05, Learning Rate: 0.000157\n",
      "Epoch 21200/40000, Loss: 3.0017621611477807e-05, Learning Rate: 0.000157\n",
      "Epoch 21201/40000, Loss: 1.4004064723849297e-05, Learning Rate: 0.000157\n",
      "Epoch 21202/40000, Loss: 1.4078518688620534e-05, Learning Rate: 0.000157\n",
      "Epoch 21203/40000, Loss: 4.010265547549352e-05, Learning Rate: 0.000157\n",
      "Epoch 21204/40000, Loss: 5.884967322344892e-05, Learning Rate: 0.000157\n",
      "Epoch 21205/40000, Loss: 1.3938723895989824e-05, Learning Rate: 0.000157\n",
      "Epoch 21206/40000, Loss: 5.8781402913155034e-05, Learning Rate: 0.000157\n",
      "Epoch 21207/40000, Loss: 3.010872023878619e-05, Learning Rate: 0.000157\n",
      "Epoch 21208/40000, Loss: 4.0224811527878046e-05, Learning Rate: 0.000157\n",
      "Epoch 21209/40000, Loss: 3.464163455646485e-05, Learning Rate: 0.000157\n",
      "Epoch 21210/40000, Loss: 3.0181587135302834e-05, Learning Rate: 0.000157\n",
      "Epoch 21211/40000, Loss: 5.296872404869646e-05, Learning Rate: 0.000157\n",
      "Epoch 21212/40000, Loss: 5.2830764616373926e-05, Learning Rate: 0.000157\n",
      "Epoch 21213/40000, Loss: 1.430609972885577e-05, Learning Rate: 0.000157\n",
      "Epoch 21214/40000, Loss: 3.439230204094201e-05, Learning Rate: 0.000157\n",
      "Epoch 21215/40000, Loss: 5.322314245859161e-05, Learning Rate: 0.000157\n",
      "Epoch 21216/40000, Loss: 1.4266805919760372e-05, Learning Rate: 0.000157\n",
      "Epoch 21217/40000, Loss: 3.024204124812968e-05, Learning Rate: 0.000157\n",
      "Epoch 21218/40000, Loss: 3.016282971657347e-05, Learning Rate: 0.000157\n",
      "Epoch 21219/40000, Loss: 5.917882299399935e-05, Learning Rate: 0.000157\n",
      "Epoch 21220/40000, Loss: 3.467485657893121e-05, Learning Rate: 0.000157\n",
      "Epoch 21221/40000, Loss: 3.469853618298657e-05, Learning Rate: 0.000157\n",
      "Epoch 21222/40000, Loss: 4.044847810291685e-05, Learning Rate: 0.000157\n",
      "Epoch 21223/40000, Loss: 1.460823023080593e-05, Learning Rate: 0.000157\n",
      "Epoch 21224/40000, Loss: 5.431891622720286e-05, Learning Rate: 0.000157\n",
      "Epoch 21225/40000, Loss: 3.4664597478695214e-05, Learning Rate: 0.000157\n",
      "Epoch 21226/40000, Loss: 3.4605942346388474e-05, Learning Rate: 0.000157\n",
      "Epoch 21227/40000, Loss: 5.4925010772421956e-05, Learning Rate: 0.000157\n",
      "Epoch 21228/40000, Loss: 5.371089719119482e-05, Learning Rate: 0.000157\n",
      "Epoch 21229/40000, Loss: 4.105372499907389e-05, Learning Rate: 0.000157\n",
      "Epoch 21230/40000, Loss: 1.4681169886898715e-05, Learning Rate: 0.000157\n",
      "Epoch 21231/40000, Loss: 3.0474202503683046e-05, Learning Rate: 0.000157\n",
      "Epoch 21232/40000, Loss: 4.1298997530248016e-05, Learning Rate: 0.000156\n",
      "Epoch 21233/40000, Loss: 5.408159631770104e-05, Learning Rate: 0.000156\n",
      "Epoch 21234/40000, Loss: 4.09291751566343e-05, Learning Rate: 0.000156\n",
      "Epoch 21235/40000, Loss: 3.533172275638208e-05, Learning Rate: 0.000156\n",
      "Epoch 21236/40000, Loss: 4.074117896379903e-05, Learning Rate: 0.000156\n",
      "Epoch 21237/40000, Loss: 1.4810977518209256e-05, Learning Rate: 0.000156\n",
      "Epoch 21238/40000, Loss: 5.536781100090593e-05, Learning Rate: 0.000156\n",
      "Epoch 21239/40000, Loss: 6.029827272868715e-05, Learning Rate: 0.000156\n",
      "Epoch 21240/40000, Loss: 3.549172106431797e-05, Learning Rate: 0.000156\n",
      "Epoch 21241/40000, Loss: 4.143749174545519e-05, Learning Rate: 0.000156\n",
      "Epoch 21242/40000, Loss: 4.109360452275723e-05, Learning Rate: 0.000156\n",
      "Epoch 21243/40000, Loss: 3.117327287327498e-05, Learning Rate: 0.000156\n",
      "Epoch 21244/40000, Loss: 5.465497451950796e-05, Learning Rate: 0.000156\n",
      "Epoch 21245/40000, Loss: 5.3403316996991634e-05, Learning Rate: 0.000156\n",
      "Epoch 21246/40000, Loss: 3.055888009839691e-05, Learning Rate: 0.000156\n",
      "Epoch 21247/40000, Loss: 5.435326966107823e-05, Learning Rate: 0.000156\n",
      "Epoch 21248/40000, Loss: 4.129677836317569e-05, Learning Rate: 0.000156\n",
      "Epoch 21249/40000, Loss: 1.4823363017058e-05, Learning Rate: 0.000156\n",
      "Epoch 21250/40000, Loss: 4.2031562770716846e-05, Learning Rate: 0.000156\n",
      "Epoch 21251/40000, Loss: 5.376707122195512e-05, Learning Rate: 0.000156\n",
      "Epoch 21252/40000, Loss: 5.3268682677298784e-05, Learning Rate: 0.000156\n",
      "Epoch 21253/40000, Loss: 5.902507109567523e-05, Learning Rate: 0.000156\n",
      "Epoch 21254/40000, Loss: 4.016416278318502e-05, Learning Rate: 0.000156\n",
      "Epoch 21255/40000, Loss: 3.4318098187213764e-05, Learning Rate: 0.000156\n",
      "Epoch 21256/40000, Loss: 3.4306351153645664e-05, Learning Rate: 0.000156\n",
      "Epoch 21257/40000, Loss: 3.429961725487374e-05, Learning Rate: 0.000156\n",
      "Epoch 21258/40000, Loss: 3.029436629731208e-05, Learning Rate: 0.000156\n",
      "Epoch 21259/40000, Loss: 4.007638926850632e-05, Learning Rate: 0.000156\n",
      "Epoch 21260/40000, Loss: 1.386245548928855e-05, Learning Rate: 0.000156\n",
      "Epoch 21261/40000, Loss: 3.000789183715824e-05, Learning Rate: 0.000156\n",
      "Epoch 21262/40000, Loss: 1.4118656508799177e-05, Learning Rate: 0.000156\n",
      "Epoch 21263/40000, Loss: 3.425459726713598e-05, Learning Rate: 0.000156\n",
      "Epoch 21264/40000, Loss: 5.274833529256284e-05, Learning Rate: 0.000156\n",
      "Epoch 21265/40000, Loss: 3.426302646403201e-05, Learning Rate: 0.000156\n",
      "Epoch 21266/40000, Loss: 5.857543146703392e-05, Learning Rate: 0.000156\n",
      "Epoch 21267/40000, Loss: 2.9901675588916987e-05, Learning Rate: 0.000156\n",
      "Epoch 21268/40000, Loss: 3.418877531657927e-05, Learning Rate: 0.000156\n",
      "Epoch 21269/40000, Loss: 5.8464211178943515e-05, Learning Rate: 0.000156\n",
      "Epoch 21270/40000, Loss: 2.984529419336468e-05, Learning Rate: 0.000156\n",
      "Epoch 21271/40000, Loss: 5.868817970622331e-05, Learning Rate: 0.000156\n",
      "Epoch 21272/40000, Loss: 1.3957924238638952e-05, Learning Rate: 0.000156\n",
      "Epoch 21273/40000, Loss: 5.26914227521047e-05, Learning Rate: 0.000156\n",
      "Epoch 21274/40000, Loss: 2.9986873414600268e-05, Learning Rate: 0.000156\n",
      "Epoch 21275/40000, Loss: 2.9961052860016935e-05, Learning Rate: 0.000156\n",
      "Epoch 21276/40000, Loss: 5.290546323521994e-05, Learning Rate: 0.000156\n",
      "Epoch 21277/40000, Loss: 1.3962812772660982e-05, Learning Rate: 0.000156\n",
      "Epoch 21278/40000, Loss: 1.4080729670240544e-05, Learning Rate: 0.000156\n",
      "Epoch 21279/40000, Loss: 4.0028760849963874e-05, Learning Rate: 0.000156\n",
      "Epoch 21280/40000, Loss: 5.2785882871830836e-05, Learning Rate: 0.000156\n",
      "Epoch 21281/40000, Loss: 5.2822437282884493e-05, Learning Rate: 0.000156\n",
      "Epoch 21282/40000, Loss: 4.036342215840705e-05, Learning Rate: 0.000156\n",
      "Epoch 21283/40000, Loss: 5.27915071870666e-05, Learning Rate: 0.000156\n",
      "Epoch 21284/40000, Loss: 5.280214827507734e-05, Learning Rate: 0.000156\n",
      "Epoch 21285/40000, Loss: 2.9996910598129034e-05, Learning Rate: 0.000156\n",
      "Epoch 21286/40000, Loss: 1.3986052181280684e-05, Learning Rate: 0.000155\n",
      "Epoch 21287/40000, Loss: 3.990326149505563e-05, Learning Rate: 0.000155\n",
      "Epoch 21288/40000, Loss: 2.9952148906886578e-05, Learning Rate: 0.000155\n",
      "Epoch 21289/40000, Loss: 3.429353455430828e-05, Learning Rate: 0.000155\n",
      "Epoch 21290/40000, Loss: 3.4220338420709595e-05, Learning Rate: 0.000155\n",
      "Epoch 21291/40000, Loss: 3.001870027219411e-05, Learning Rate: 0.000155\n",
      "Epoch 21292/40000, Loss: 3.4235385101055726e-05, Learning Rate: 0.000155\n",
      "Epoch 21293/40000, Loss: 1.3943737030785996e-05, Learning Rate: 0.000155\n",
      "Epoch 21294/40000, Loss: 2.9937815270386636e-05, Learning Rate: 0.000155\n",
      "Epoch 21295/40000, Loss: 3.430911237956025e-05, Learning Rate: 0.000155\n",
      "Epoch 21296/40000, Loss: 4.023162546218373e-05, Learning Rate: 0.000155\n",
      "Epoch 21297/40000, Loss: 1.437951868865639e-05, Learning Rate: 0.000155\n",
      "Epoch 21298/40000, Loss: 5.9297850384609774e-05, Learning Rate: 0.000155\n",
      "Epoch 21299/40000, Loss: 5.960208727628924e-05, Learning Rate: 0.000155\n",
      "Epoch 21300/40000, Loss: 1.4517994713969529e-05, Learning Rate: 0.000155\n",
      "Epoch 21301/40000, Loss: 5.388699355535209e-05, Learning Rate: 0.000155\n",
      "Epoch 21302/40000, Loss: 5.432853868114762e-05, Learning Rate: 0.000155\n",
      "Epoch 21303/40000, Loss: 1.4838986317045055e-05, Learning Rate: 0.000155\n",
      "Epoch 21304/40000, Loss: 6.349595787469298e-05, Learning Rate: 0.000155\n",
      "Epoch 21305/40000, Loss: 6.075076817069203e-05, Learning Rate: 0.000155\n",
      "Epoch 21306/40000, Loss: 3.533238486852497e-05, Learning Rate: 0.000155\n",
      "Epoch 21307/40000, Loss: 1.5687215636717156e-05, Learning Rate: 0.000155\n",
      "Epoch 21308/40000, Loss: 4.1440227505518124e-05, Learning Rate: 0.000155\n",
      "Epoch 21309/40000, Loss: 4.065770554007031e-05, Learning Rate: 0.000155\n",
      "Epoch 21310/40000, Loss: 3.5063181712757796e-05, Learning Rate: 0.000155\n",
      "Epoch 21311/40000, Loss: 5.444350972538814e-05, Learning Rate: 0.000155\n",
      "Epoch 21312/40000, Loss: 1.553090442030225e-05, Learning Rate: 0.000155\n",
      "Epoch 21313/40000, Loss: 3.554008435457945e-05, Learning Rate: 0.000155\n",
      "Epoch 21314/40000, Loss: 3.130716868326999e-05, Learning Rate: 0.000155\n",
      "Epoch 21315/40000, Loss: 5.801570659969002e-05, Learning Rate: 0.000155\n",
      "Epoch 21316/40000, Loss: 5.987365511828102e-05, Learning Rate: 0.000155\n",
      "Epoch 21317/40000, Loss: 3.1204002880258486e-05, Learning Rate: 0.000155\n",
      "Epoch 21318/40000, Loss: 3.08326052618213e-05, Learning Rate: 0.000155\n",
      "Epoch 21319/40000, Loss: 6.193686567712575e-05, Learning Rate: 0.000155\n",
      "Epoch 21320/40000, Loss: 4.2579948058119044e-05, Learning Rate: 0.000155\n",
      "Epoch 21321/40000, Loss: 1.9980901925009675e-05, Learning Rate: 0.000155\n",
      "Epoch 21322/40000, Loss: 4.618097227648832e-05, Learning Rate: 0.000155\n",
      "Epoch 21323/40000, Loss: 5.617029091808945e-05, Learning Rate: 0.000155\n",
      "Epoch 21324/40000, Loss: 3.760077015613206e-05, Learning Rate: 0.000155\n",
      "Epoch 21325/40000, Loss: 5.949447586317547e-05, Learning Rate: 0.000155\n",
      "Epoch 21326/40000, Loss: 3.8115555071271956e-05, Learning Rate: 0.000155\n",
      "Epoch 21327/40000, Loss: 3.078173176618293e-05, Learning Rate: 0.000155\n",
      "Epoch 21328/40000, Loss: 3.0418752430705354e-05, Learning Rate: 0.000155\n",
      "Epoch 21329/40000, Loss: 3.493513577268459e-05, Learning Rate: 0.000155\n",
      "Epoch 21330/40000, Loss: 5.932947533437982e-05, Learning Rate: 0.000155\n",
      "Epoch 21331/40000, Loss: 1.4414101315196604e-05, Learning Rate: 0.000155\n",
      "Epoch 21332/40000, Loss: 4.014736259705387e-05, Learning Rate: 0.000155\n",
      "Epoch 21333/40000, Loss: 5.277968739392236e-05, Learning Rate: 0.000155\n",
      "Epoch 21334/40000, Loss: 5.270494511933066e-05, Learning Rate: 0.000155\n",
      "Epoch 21335/40000, Loss: 1.3935657989350148e-05, Learning Rate: 0.000155\n",
      "Epoch 21336/40000, Loss: 5.2732597396243364e-05, Learning Rate: 0.000155\n",
      "Epoch 21337/40000, Loss: 5.856882853549905e-05, Learning Rate: 0.000155\n",
      "Epoch 21338/40000, Loss: 5.8451165386941284e-05, Learning Rate: 0.000155\n",
      "Epoch 21339/40000, Loss: 2.9969860406708904e-05, Learning Rate: 0.000155\n",
      "Epoch 21340/40000, Loss: 2.992295048898086e-05, Learning Rate: 0.000154\n",
      "Epoch 21341/40000, Loss: 5.274895374896005e-05, Learning Rate: 0.000154\n",
      "Epoch 21342/40000, Loss: 2.9958702725707553e-05, Learning Rate: 0.000154\n",
      "Epoch 21343/40000, Loss: 3.4194745239801705e-05, Learning Rate: 0.000154\n",
      "Epoch 21344/40000, Loss: 3.996842497144826e-05, Learning Rate: 0.000154\n",
      "Epoch 21345/40000, Loss: 2.9867187549825758e-05, Learning Rate: 0.000154\n",
      "Epoch 21346/40000, Loss: 4.009014446637593e-05, Learning Rate: 0.000154\n",
      "Epoch 21347/40000, Loss: 1.4177363482303917e-05, Learning Rate: 0.000154\n",
      "Epoch 21348/40000, Loss: 4.014889782411046e-05, Learning Rate: 0.000154\n",
      "Epoch 21349/40000, Loss: 3.997222665930167e-05, Learning Rate: 0.000154\n",
      "Epoch 21350/40000, Loss: 5.851792229805142e-05, Learning Rate: 0.000154\n",
      "Epoch 21351/40000, Loss: 5.846898420713842e-05, Learning Rate: 0.000154\n",
      "Epoch 21352/40000, Loss: 1.4004075637785718e-05, Learning Rate: 0.000154\n",
      "Epoch 21353/40000, Loss: 5.268313179840334e-05, Learning Rate: 0.000154\n",
      "Epoch 21354/40000, Loss: 3.998734609922394e-05, Learning Rate: 0.000154\n",
      "Epoch 21355/40000, Loss: 3.991816993220709e-05, Learning Rate: 0.000154\n",
      "Epoch 21356/40000, Loss: 5.8382225688546896e-05, Learning Rate: 0.000154\n",
      "Epoch 21357/40000, Loss: 1.3825860150973313e-05, Learning Rate: 0.000154\n",
      "Epoch 21358/40000, Loss: 3.979655957664363e-05, Learning Rate: 0.000154\n",
      "Epoch 21359/40000, Loss: 5.265790969133377e-05, Learning Rate: 0.000154\n",
      "Epoch 21360/40000, Loss: 2.9809896659571677e-05, Learning Rate: 0.000154\n",
      "Epoch 21361/40000, Loss: 5.265826257527806e-05, Learning Rate: 0.000154\n",
      "Epoch 21362/40000, Loss: 3.42294733854942e-05, Learning Rate: 0.000154\n",
      "Epoch 21363/40000, Loss: 2.980677345476579e-05, Learning Rate: 0.000154\n",
      "Epoch 21364/40000, Loss: 1.388466262142174e-05, Learning Rate: 0.000154\n",
      "Epoch 21365/40000, Loss: 3.991680932813324e-05, Learning Rate: 0.000154\n",
      "Epoch 21366/40000, Loss: 5.860031524207443e-05, Learning Rate: 0.000154\n",
      "Epoch 21367/40000, Loss: 1.3899800251238048e-05, Learning Rate: 0.000154\n",
      "Epoch 21368/40000, Loss: 5.867440268048085e-05, Learning Rate: 0.000154\n",
      "Epoch 21369/40000, Loss: 5.8605175581760705e-05, Learning Rate: 0.000154\n",
      "Epoch 21370/40000, Loss: 2.986535218951758e-05, Learning Rate: 0.000154\n",
      "Epoch 21371/40000, Loss: 2.980946192110423e-05, Learning Rate: 0.000154\n",
      "Epoch 21372/40000, Loss: 1.3829379895469174e-05, Learning Rate: 0.000154\n",
      "Epoch 21373/40000, Loss: 5.874233829672448e-05, Learning Rate: 0.000154\n",
      "Epoch 21374/40000, Loss: 2.983553713420406e-05, Learning Rate: 0.000154\n",
      "Epoch 21375/40000, Loss: 1.392129524901975e-05, Learning Rate: 0.000154\n",
      "Epoch 21376/40000, Loss: 3.9879287214716896e-05, Learning Rate: 0.000154\n",
      "Epoch 21377/40000, Loss: 1.3860417311661877e-05, Learning Rate: 0.000154\n",
      "Epoch 21378/40000, Loss: 3.984934664913453e-05, Learning Rate: 0.000154\n",
      "Epoch 21379/40000, Loss: 1.3961540389573202e-05, Learning Rate: 0.000154\n",
      "Epoch 21380/40000, Loss: 2.9799532057950273e-05, Learning Rate: 0.000154\n",
      "Epoch 21381/40000, Loss: 3.9891070628073066e-05, Learning Rate: 0.000154\n",
      "Epoch 21382/40000, Loss: 5.844974657520652e-05, Learning Rate: 0.000154\n",
      "Epoch 21383/40000, Loss: 2.98422783089336e-05, Learning Rate: 0.000154\n",
      "Epoch 21384/40000, Loss: 2.9737224394921213e-05, Learning Rate: 0.000154\n",
      "Epoch 21385/40000, Loss: 1.3877518540539313e-05, Learning Rate: 0.000154\n",
      "Epoch 21386/40000, Loss: 3.989881224697456e-05, Learning Rate: 0.000154\n",
      "Epoch 21387/40000, Loss: 1.3926795872976072e-05, Learning Rate: 0.000154\n",
      "Epoch 21388/40000, Loss: 5.853959373780526e-05, Learning Rate: 0.000154\n",
      "Epoch 21389/40000, Loss: 5.84525587328244e-05, Learning Rate: 0.000154\n",
      "Epoch 21390/40000, Loss: 5.838180732098408e-05, Learning Rate: 0.000154\n",
      "Epoch 21391/40000, Loss: 3.4266820875927806e-05, Learning Rate: 0.000154\n",
      "Epoch 21392/40000, Loss: 4.009350959677249e-05, Learning Rate: 0.000154\n",
      "Epoch 21393/40000, Loss: 3.432625817367807e-05, Learning Rate: 0.000154\n",
      "Epoch 21394/40000, Loss: 4.0117534808814526e-05, Learning Rate: 0.000153\n",
      "Epoch 21395/40000, Loss: 1.3862447303836234e-05, Learning Rate: 0.000153\n",
      "Epoch 21396/40000, Loss: 1.3874450814910233e-05, Learning Rate: 0.000153\n",
      "Epoch 21397/40000, Loss: 2.982569276355207e-05, Learning Rate: 0.000153\n",
      "Epoch 21398/40000, Loss: 3.994955841335468e-05, Learning Rate: 0.000153\n",
      "Epoch 21399/40000, Loss: 3.416435720282607e-05, Learning Rate: 0.000153\n",
      "Epoch 21400/40000, Loss: 1.3923661754233763e-05, Learning Rate: 0.000153\n",
      "Epoch 21401/40000, Loss: 3.4363565646344796e-05, Learning Rate: 0.000153\n",
      "Epoch 21402/40000, Loss: 5.875950591871515e-05, Learning Rate: 0.000153\n",
      "Epoch 21403/40000, Loss: 4.0131621062755585e-05, Learning Rate: 0.000153\n",
      "Epoch 21404/40000, Loss: 3.424936585361138e-05, Learning Rate: 0.000153\n",
      "Epoch 21405/40000, Loss: 3.0208899261197075e-05, Learning Rate: 0.000153\n",
      "Epoch 21406/40000, Loss: 3.018763891304843e-05, Learning Rate: 0.000153\n",
      "Epoch 21407/40000, Loss: 2.9858609195798635e-05, Learning Rate: 0.000153\n",
      "Epoch 21408/40000, Loss: 5.300019620335661e-05, Learning Rate: 0.000153\n",
      "Epoch 21409/40000, Loss: 5.314493319019675e-05, Learning Rate: 0.000153\n",
      "Epoch 21410/40000, Loss: 5.95493329456076e-05, Learning Rate: 0.000153\n",
      "Epoch 21411/40000, Loss: 5.4484167776536196e-05, Learning Rate: 0.000153\n",
      "Epoch 21412/40000, Loss: 1.4759772057004739e-05, Learning Rate: 0.000153\n",
      "Epoch 21413/40000, Loss: 3.446537448326126e-05, Learning Rate: 0.000153\n",
      "Epoch 21414/40000, Loss: 3.049006591027137e-05, Learning Rate: 0.000153\n",
      "Epoch 21415/40000, Loss: 6.0230478993617e-05, Learning Rate: 0.000153\n",
      "Epoch 21416/40000, Loss: 6.074984048609622e-05, Learning Rate: 0.000153\n",
      "Epoch 21417/40000, Loss: 1.4652107893198263e-05, Learning Rate: 0.000153\n",
      "Epoch 21418/40000, Loss: 3.59149526047986e-05, Learning Rate: 0.000153\n",
      "Epoch 21419/40000, Loss: 5.482756023411639e-05, Learning Rate: 0.000153\n",
      "Epoch 21420/40000, Loss: 1.4697196093038656e-05, Learning Rate: 0.000153\n",
      "Epoch 21421/40000, Loss: 5.99977865931578e-05, Learning Rate: 0.000153\n",
      "Epoch 21422/40000, Loss: 4.275266837794334e-05, Learning Rate: 0.000153\n",
      "Epoch 21423/40000, Loss: 6.311144534265622e-05, Learning Rate: 0.000153\n",
      "Epoch 21424/40000, Loss: 3.545837171259336e-05, Learning Rate: 0.000153\n",
      "Epoch 21425/40000, Loss: 1.4903346709616017e-05, Learning Rate: 0.000153\n",
      "Epoch 21426/40000, Loss: 6.575787119800225e-05, Learning Rate: 0.000153\n",
      "Epoch 21427/40000, Loss: 5.620133379125036e-05, Learning Rate: 0.000153\n",
      "Epoch 21428/40000, Loss: 1.9491119019221514e-05, Learning Rate: 0.000153\n",
      "Epoch 21429/40000, Loss: 6.323566049104556e-05, Learning Rate: 0.000153\n",
      "Epoch 21430/40000, Loss: 3.694191764225252e-05, Learning Rate: 0.000153\n",
      "Epoch 21431/40000, Loss: 3.562464553397149e-05, Learning Rate: 0.000153\n",
      "Epoch 21432/40000, Loss: 5.6515418691560626e-05, Learning Rate: 0.000153\n",
      "Epoch 21433/40000, Loss: 3.883293175022118e-05, Learning Rate: 0.000153\n",
      "Epoch 21434/40000, Loss: 6.007172487443313e-05, Learning Rate: 0.000153\n",
      "Epoch 21435/40000, Loss: 5.6454839068464935e-05, Learning Rate: 0.000153\n",
      "Epoch 21436/40000, Loss: 4.353066469775513e-05, Learning Rate: 0.000153\n",
      "Epoch 21437/40000, Loss: 6.004765964462422e-05, Learning Rate: 0.000153\n",
      "Epoch 21438/40000, Loss: 5.377756679081358e-05, Learning Rate: 0.000153\n",
      "Epoch 21439/40000, Loss: 1.4957117855374236e-05, Learning Rate: 0.000153\n",
      "Epoch 21440/40000, Loss: 5.3903600928606465e-05, Learning Rate: 0.000153\n",
      "Epoch 21441/40000, Loss: 5.9088437410537153e-05, Learning Rate: 0.000153\n",
      "Epoch 21442/40000, Loss: 1.4748065041203517e-05, Learning Rate: 0.000153\n",
      "Epoch 21443/40000, Loss: 5.322295328369364e-05, Learning Rate: 0.000153\n",
      "Epoch 21444/40000, Loss: 3.0434177460847422e-05, Learning Rate: 0.000153\n",
      "Epoch 21445/40000, Loss: 4.10713764722459e-05, Learning Rate: 0.000153\n",
      "Epoch 21446/40000, Loss: 3.444471803959459e-05, Learning Rate: 0.000153\n",
      "Epoch 21447/40000, Loss: 5.317987597663887e-05, Learning Rate: 0.000153\n",
      "Epoch 21448/40000, Loss: 3.432367884670384e-05, Learning Rate: 0.000152\n",
      "Epoch 21449/40000, Loss: 4.016104503534734e-05, Learning Rate: 0.000152\n",
      "Epoch 21450/40000, Loss: 1.421528031642083e-05, Learning Rate: 0.000152\n",
      "Epoch 21451/40000, Loss: 2.9972263291710988e-05, Learning Rate: 0.000152\n",
      "Epoch 21452/40000, Loss: 2.9919157896074466e-05, Learning Rate: 0.000152\n",
      "Epoch 21453/40000, Loss: 4.08998821512796e-05, Learning Rate: 0.000152\n",
      "Epoch 21454/40000, Loss: 1.421050910721533e-05, Learning Rate: 0.000152\n",
      "Epoch 21455/40000, Loss: 1.401292593072867e-05, Learning Rate: 0.000152\n",
      "Epoch 21456/40000, Loss: 2.994577880599536e-05, Learning Rate: 0.000152\n",
      "Epoch 21457/40000, Loss: 5.277025047689676e-05, Learning Rate: 0.000152\n",
      "Epoch 21458/40000, Loss: 5.2750598115380853e-05, Learning Rate: 0.000152\n",
      "Epoch 21459/40000, Loss: 3.4714044886641204e-05, Learning Rate: 0.000152\n",
      "Epoch 21460/40000, Loss: 3.4214983315905556e-05, Learning Rate: 0.000152\n",
      "Epoch 21461/40000, Loss: 4.0038390579866245e-05, Learning Rate: 0.000152\n",
      "Epoch 21462/40000, Loss: 5.8399517001817e-05, Learning Rate: 0.000152\n",
      "Epoch 21463/40000, Loss: 3.4210905141662806e-05, Learning Rate: 0.000152\n",
      "Epoch 21464/40000, Loss: 2.9736826036241837e-05, Learning Rate: 0.000152\n",
      "Epoch 21465/40000, Loss: 3.410183126106858e-05, Learning Rate: 0.000152\n",
      "Epoch 21466/40000, Loss: 2.9846290999557823e-05, Learning Rate: 0.000152\n",
      "Epoch 21467/40000, Loss: 3.407967233215459e-05, Learning Rate: 0.000152\n",
      "Epoch 21468/40000, Loss: 1.3883666724723298e-05, Learning Rate: 0.000152\n",
      "Epoch 21469/40000, Loss: 3.41371851391159e-05, Learning Rate: 0.000152\n",
      "Epoch 21470/40000, Loss: 1.391875139233889e-05, Learning Rate: 0.000152\n",
      "Epoch 21471/40000, Loss: 1.390813213220099e-05, Learning Rate: 0.000152\n",
      "Epoch 21472/40000, Loss: 2.9766484658466652e-05, Learning Rate: 0.000152\n",
      "Epoch 21473/40000, Loss: 3.4122153010685e-05, Learning Rate: 0.000152\n",
      "Epoch 21474/40000, Loss: 5.263794446364045e-05, Learning Rate: 0.000152\n",
      "Epoch 21475/40000, Loss: 5.261898331809789e-05, Learning Rate: 0.000152\n",
      "Epoch 21476/40000, Loss: 2.9727256332989782e-05, Learning Rate: 0.000152\n",
      "Epoch 21477/40000, Loss: 1.3913807379140053e-05, Learning Rate: 0.000152\n",
      "Epoch 21478/40000, Loss: 3.98536903958302e-05, Learning Rate: 0.000152\n",
      "Epoch 21479/40000, Loss: 5.828157372889109e-05, Learning Rate: 0.000152\n",
      "Epoch 21480/40000, Loss: 5.264488936518319e-05, Learning Rate: 0.000152\n",
      "Epoch 21481/40000, Loss: 5.267492451821454e-05, Learning Rate: 0.000152\n",
      "Epoch 21482/40000, Loss: 2.985534410981927e-05, Learning Rate: 0.000152\n",
      "Epoch 21483/40000, Loss: 5.838343713548966e-05, Learning Rate: 0.000152\n",
      "Epoch 21484/40000, Loss: 3.412480145925656e-05, Learning Rate: 0.000152\n",
      "Epoch 21485/40000, Loss: 2.9856648325221613e-05, Learning Rate: 0.000152\n",
      "Epoch 21486/40000, Loss: 5.840605081175454e-05, Learning Rate: 0.000152\n",
      "Epoch 21487/40000, Loss: 1.3939073141955305e-05, Learning Rate: 0.000152\n",
      "Epoch 21488/40000, Loss: 3.0008843168616295e-05, Learning Rate: 0.000152\n",
      "Epoch 21489/40000, Loss: 1.388279088132549e-05, Learning Rate: 0.000152\n",
      "Epoch 21490/40000, Loss: 3.016548726009205e-05, Learning Rate: 0.000152\n",
      "Epoch 21491/40000, Loss: 1.3912455870013218e-05, Learning Rate: 0.000152\n",
      "Epoch 21492/40000, Loss: 4.0112179704010487e-05, Learning Rate: 0.000152\n",
      "Epoch 21493/40000, Loss: 3.4166638215538114e-05, Learning Rate: 0.000152\n",
      "Epoch 21494/40000, Loss: 5.276359661365859e-05, Learning Rate: 0.000152\n",
      "Epoch 21495/40000, Loss: 2.9778469979646616e-05, Learning Rate: 0.000152\n",
      "Epoch 21496/40000, Loss: 2.981617217301391e-05, Learning Rate: 0.000152\n",
      "Epoch 21497/40000, Loss: 4.000348781119101e-05, Learning Rate: 0.000152\n",
      "Epoch 21498/40000, Loss: 3.0033907023607753e-05, Learning Rate: 0.000152\n",
      "Epoch 21499/40000, Loss: 5.851466266904026e-05, Learning Rate: 0.000152\n",
      "Epoch 21500/40000, Loss: 5.842092650709674e-05, Learning Rate: 0.000152\n",
      "Epoch 21501/40000, Loss: 1.383918151987018e-05, Learning Rate: 0.000152\n",
      "Epoch 21502/40000, Loss: 3.4112134017050266e-05, Learning Rate: 0.000152\n",
      "Epoch 21503/40000, Loss: 3.9879134419607e-05, Learning Rate: 0.000151\n",
      "Epoch 21504/40000, Loss: 3.4283671993762255e-05, Learning Rate: 0.000151\n",
      "Epoch 21505/40000, Loss: 2.980967656185385e-05, Learning Rate: 0.000151\n",
      "Epoch 21506/40000, Loss: 2.9749377063126303e-05, Learning Rate: 0.000151\n",
      "Epoch 21507/40000, Loss: 5.833610339323059e-05, Learning Rate: 0.000151\n",
      "Epoch 21508/40000, Loss: 5.833463364979252e-05, Learning Rate: 0.000151\n",
      "Epoch 21509/40000, Loss: 1.3859870705346111e-05, Learning Rate: 0.000151\n",
      "Epoch 21510/40000, Loss: 1.394895116391126e-05, Learning Rate: 0.000151\n",
      "Epoch 21511/40000, Loss: 3.9914040826261044e-05, Learning Rate: 0.000151\n",
      "Epoch 21512/40000, Loss: 2.980119279527571e-05, Learning Rate: 0.000151\n",
      "Epoch 21513/40000, Loss: 5.845913983648643e-05, Learning Rate: 0.000151\n",
      "Epoch 21514/40000, Loss: 3.991905759903602e-05, Learning Rate: 0.000151\n",
      "Epoch 21515/40000, Loss: 2.9759403332718648e-05, Learning Rate: 0.000151\n",
      "Epoch 21516/40000, Loss: 3.412065780139528e-05, Learning Rate: 0.000151\n",
      "Epoch 21517/40000, Loss: 3.407590338611044e-05, Learning Rate: 0.000151\n",
      "Epoch 21518/40000, Loss: 5.831525777466595e-05, Learning Rate: 0.000151\n",
      "Epoch 21519/40000, Loss: 5.275642251945101e-05, Learning Rate: 0.000151\n",
      "Epoch 21520/40000, Loss: 1.3979352843307424e-05, Learning Rate: 0.000151\n",
      "Epoch 21521/40000, Loss: 1.3922124708187766e-05, Learning Rate: 0.000151\n",
      "Epoch 21522/40000, Loss: 1.38870018417947e-05, Learning Rate: 0.000151\n",
      "Epoch 21523/40000, Loss: 5.294052243698388e-05, Learning Rate: 0.000151\n",
      "Epoch 21524/40000, Loss: 5.2716044592671096e-05, Learning Rate: 0.000151\n",
      "Epoch 21525/40000, Loss: 5.264412175165489e-05, Learning Rate: 0.000151\n",
      "Epoch 21526/40000, Loss: 2.976621181005612e-05, Learning Rate: 0.000151\n",
      "Epoch 21527/40000, Loss: 3.981816189480014e-05, Learning Rate: 0.000151\n",
      "Epoch 21528/40000, Loss: 5.8447785704629496e-05, Learning Rate: 0.000151\n",
      "Epoch 21529/40000, Loss: 3.0060931749176234e-05, Learning Rate: 0.000151\n",
      "Epoch 21530/40000, Loss: 5.339590643416159e-05, Learning Rate: 0.000151\n",
      "Epoch 21531/40000, Loss: 3.0301218430395238e-05, Learning Rate: 0.000151\n",
      "Epoch 21532/40000, Loss: 2.996392322529573e-05, Learning Rate: 0.000151\n",
      "Epoch 21533/40000, Loss: 5.866205901838839e-05, Learning Rate: 0.000151\n",
      "Epoch 21534/40000, Loss: 5.8584235375747085e-05, Learning Rate: 0.000151\n",
      "Epoch 21535/40000, Loss: 4.023732981295325e-05, Learning Rate: 0.000151\n",
      "Epoch 21536/40000, Loss: 4.023974543088116e-05, Learning Rate: 0.000151\n",
      "Epoch 21537/40000, Loss: 5.3352428949438035e-05, Learning Rate: 0.000151\n",
      "Epoch 21538/40000, Loss: 3.4571905416669324e-05, Learning Rate: 0.000151\n",
      "Epoch 21539/40000, Loss: 5.935251101618633e-05, Learning Rate: 0.000151\n",
      "Epoch 21540/40000, Loss: 5.409533332567662e-05, Learning Rate: 0.000151\n",
      "Epoch 21541/40000, Loss: 3.516523429425433e-05, Learning Rate: 0.000151\n",
      "Epoch 21542/40000, Loss: 6.013748134137131e-05, Learning Rate: 0.000151\n",
      "Epoch 21543/40000, Loss: 1.4744823602086399e-05, Learning Rate: 0.000151\n",
      "Epoch 21544/40000, Loss: 1.5271181837306358e-05, Learning Rate: 0.000151\n",
      "Epoch 21545/40000, Loss: 4.1462481021881104e-05, Learning Rate: 0.000151\n",
      "Epoch 21546/40000, Loss: 4.1214756492991e-05, Learning Rate: 0.000151\n",
      "Epoch 21547/40000, Loss: 5.963616058579646e-05, Learning Rate: 0.000151\n",
      "Epoch 21548/40000, Loss: 4.083654494024813e-05, Learning Rate: 0.000151\n",
      "Epoch 21549/40000, Loss: 4.062983498442918e-05, Learning Rate: 0.000151\n",
      "Epoch 21550/40000, Loss: 3.472613389021717e-05, Learning Rate: 0.000151\n",
      "Epoch 21551/40000, Loss: 3.1033832783577964e-05, Learning Rate: 0.000151\n",
      "Epoch 21552/40000, Loss: 5.800005965284072e-05, Learning Rate: 0.000151\n",
      "Epoch 21553/40000, Loss: 5.913870336371474e-05, Learning Rate: 0.000151\n",
      "Epoch 21554/40000, Loss: 5.426356437965296e-05, Learning Rate: 0.000151\n",
      "Epoch 21555/40000, Loss: 3.4384418540867046e-05, Learning Rate: 0.000151\n",
      "Epoch 21556/40000, Loss: 3.0014291041879915e-05, Learning Rate: 0.000151\n",
      "Epoch 21557/40000, Loss: 4.029051706311293e-05, Learning Rate: 0.000151\n",
      "Epoch 21558/40000, Loss: 5.299561598803848e-05, Learning Rate: 0.000150\n",
      "Epoch 21559/40000, Loss: 5.2872721425956115e-05, Learning Rate: 0.000150\n",
      "Epoch 21560/40000, Loss: 5.268715904094279e-05, Learning Rate: 0.000150\n",
      "Epoch 21561/40000, Loss: 5.2568364480976015e-05, Learning Rate: 0.000150\n",
      "Epoch 21562/40000, Loss: 1.40088886837475e-05, Learning Rate: 0.000150\n",
      "Epoch 21563/40000, Loss: 4.009956319350749e-05, Learning Rate: 0.000150\n",
      "Epoch 21564/40000, Loss: 1.4116450984147377e-05, Learning Rate: 0.000150\n",
      "Epoch 21565/40000, Loss: 3.40888618666213e-05, Learning Rate: 0.000150\n",
      "Epoch 21566/40000, Loss: 3.988012394984253e-05, Learning Rate: 0.000150\n",
      "Epoch 21567/40000, Loss: 2.9798353352816775e-05, Learning Rate: 0.000150\n",
      "Epoch 21568/40000, Loss: 3.40816804964561e-05, Learning Rate: 0.000150\n",
      "Epoch 21569/40000, Loss: 2.9785245715174824e-05, Learning Rate: 0.000150\n",
      "Epoch 21570/40000, Loss: 5.834879266330972e-05, Learning Rate: 0.000150\n",
      "Epoch 21571/40000, Loss: 5.254922871245071e-05, Learning Rate: 0.000150\n",
      "Epoch 21572/40000, Loss: 4.0088965761242434e-05, Learning Rate: 0.000150\n",
      "Epoch 21573/40000, Loss: 1.3846276488038711e-05, Learning Rate: 0.000150\n",
      "Epoch 21574/40000, Loss: 5.880635944777168e-05, Learning Rate: 0.000150\n",
      "Epoch 21575/40000, Loss: 3.9818769437260926e-05, Learning Rate: 0.000150\n",
      "Epoch 21576/40000, Loss: 5.256575968815014e-05, Learning Rate: 0.000150\n",
      "Epoch 21577/40000, Loss: 1.388396049151197e-05, Learning Rate: 0.000150\n",
      "Epoch 21578/40000, Loss: 5.833258182974532e-05, Learning Rate: 0.000150\n",
      "Epoch 21579/40000, Loss: 5.8338297094451264e-05, Learning Rate: 0.000150\n",
      "Epoch 21580/40000, Loss: 5.2528899686876684e-05, Learning Rate: 0.000150\n",
      "Epoch 21581/40000, Loss: 2.9757678930764087e-05, Learning Rate: 0.000150\n",
      "Epoch 21582/40000, Loss: 2.9687013011425734e-05, Learning Rate: 0.000150\n",
      "Epoch 21583/40000, Loss: 5.2606203098548576e-05, Learning Rate: 0.000150\n",
      "Epoch 21584/40000, Loss: 5.267561937216669e-05, Learning Rate: 0.000150\n",
      "Epoch 21585/40000, Loss: 2.9657434424734674e-05, Learning Rate: 0.000150\n",
      "Epoch 21586/40000, Loss: 2.9761293262708932e-05, Learning Rate: 0.000150\n",
      "Epoch 21587/40000, Loss: 5.2922117902198806e-05, Learning Rate: 0.000150\n",
      "Epoch 21588/40000, Loss: 5.298611358739436e-05, Learning Rate: 0.000150\n",
      "Epoch 21589/40000, Loss: 5.277908712741919e-05, Learning Rate: 0.000150\n",
      "Epoch 21590/40000, Loss: 4.006464951089583e-05, Learning Rate: 0.000150\n",
      "Epoch 21591/40000, Loss: 4.011828059446998e-05, Learning Rate: 0.000150\n",
      "Epoch 21592/40000, Loss: 4.017369792563841e-05, Learning Rate: 0.000150\n",
      "Epoch 21593/40000, Loss: 5.2672327001346275e-05, Learning Rate: 0.000150\n",
      "Epoch 21594/40000, Loss: 5.2667437557829544e-05, Learning Rate: 0.000150\n",
      "Epoch 21595/40000, Loss: 3.4127908293157816e-05, Learning Rate: 0.000150\n",
      "Epoch 21596/40000, Loss: 4.035415986436419e-05, Learning Rate: 0.000150\n",
      "Epoch 21597/40000, Loss: 5.294024958857335e-05, Learning Rate: 0.000150\n",
      "Epoch 21598/40000, Loss: 2.9708917281823233e-05, Learning Rate: 0.000150\n",
      "Epoch 21599/40000, Loss: 5.278622847981751e-05, Learning Rate: 0.000150\n",
      "Epoch 21600/40000, Loss: 2.9967157388455234e-05, Learning Rate: 0.000150\n",
      "Epoch 21601/40000, Loss: 1.4194400137057528e-05, Learning Rate: 0.000150\n",
      "Epoch 21602/40000, Loss: 1.4100178304943256e-05, Learning Rate: 0.000150\n",
      "Epoch 21603/40000, Loss: 3.0302746381494217e-05, Learning Rate: 0.000150\n",
      "Epoch 21604/40000, Loss: 2.9988343158038333e-05, Learning Rate: 0.000150\n",
      "Epoch 21605/40000, Loss: 5.871013490832411e-05, Learning Rate: 0.000150\n",
      "Epoch 21606/40000, Loss: 5.3163937991485e-05, Learning Rate: 0.000150\n",
      "Epoch 21607/40000, Loss: 5.291977504384704e-05, Learning Rate: 0.000150\n",
      "Epoch 21608/40000, Loss: 2.9800994525430724e-05, Learning Rate: 0.000150\n",
      "Epoch 21609/40000, Loss: 3.001060031238012e-05, Learning Rate: 0.000150\n",
      "Epoch 21610/40000, Loss: 5.871909888810478e-05, Learning Rate: 0.000150\n",
      "Epoch 21611/40000, Loss: 1.3981019947095774e-05, Learning Rate: 0.000150\n",
      "Epoch 21612/40000, Loss: 3.448390634730458e-05, Learning Rate: 0.000150\n",
      "Epoch 21613/40000, Loss: 5.9121335652889684e-05, Learning Rate: 0.000150\n",
      "Epoch 21614/40000, Loss: 5.898326708120294e-05, Learning Rate: 0.000149\n",
      "Epoch 21615/40000, Loss: 4.059835191583261e-05, Learning Rate: 0.000149\n",
      "Epoch 21616/40000, Loss: 2.992408371937927e-05, Learning Rate: 0.000149\n",
      "Epoch 21617/40000, Loss: 3.474537152214907e-05, Learning Rate: 0.000149\n",
      "Epoch 21618/40000, Loss: 5.318203693605028e-05, Learning Rate: 0.000149\n",
      "Epoch 21619/40000, Loss: 5.293261710903607e-05, Learning Rate: 0.000149\n",
      "Epoch 21620/40000, Loss: 5.937727109994739e-05, Learning Rate: 0.000149\n",
      "Epoch 21621/40000, Loss: 3.458801802480593e-05, Learning Rate: 0.000149\n",
      "Epoch 21622/40000, Loss: 3.0120845622150227e-05, Learning Rate: 0.000149\n",
      "Epoch 21623/40000, Loss: 5.4291092965286225e-05, Learning Rate: 0.000149\n",
      "Epoch 21624/40000, Loss: 3.5069762816419825e-05, Learning Rate: 0.000149\n",
      "Epoch 21625/40000, Loss: 5.995544051984325e-05, Learning Rate: 0.000149\n",
      "Epoch 21626/40000, Loss: 4.204845390631817e-05, Learning Rate: 0.000149\n",
      "Epoch 21627/40000, Loss: 3.699135413626209e-05, Learning Rate: 0.000149\n",
      "Epoch 21628/40000, Loss: 3.5934608604293317e-05, Learning Rate: 0.000149\n",
      "Epoch 21629/40000, Loss: 4.303784589865245e-05, Learning Rate: 0.000149\n",
      "Epoch 21630/40000, Loss: 4.225421434966847e-05, Learning Rate: 0.000149\n",
      "Epoch 21631/40000, Loss: 1.6393840269302018e-05, Learning Rate: 0.000149\n",
      "Epoch 21632/40000, Loss: 3.505713539198041e-05, Learning Rate: 0.000149\n",
      "Epoch 21633/40000, Loss: 3.0436107408604585e-05, Learning Rate: 0.000149\n",
      "Epoch 21634/40000, Loss: 1.838121170294471e-05, Learning Rate: 0.000149\n",
      "Epoch 21635/40000, Loss: 4.421957419253886e-05, Learning Rate: 0.000149\n",
      "Epoch 21636/40000, Loss: 6.15453245700337e-05, Learning Rate: 0.000149\n",
      "Epoch 21637/40000, Loss: 4.277921834727749e-05, Learning Rate: 0.000149\n",
      "Epoch 21638/40000, Loss: 6.0563823353732005e-05, Learning Rate: 0.000149\n",
      "Epoch 21639/40000, Loss: 5.9435347793623805e-05, Learning Rate: 0.000149\n",
      "Epoch 21640/40000, Loss: 4.1193299693986773e-05, Learning Rate: 0.000149\n",
      "Epoch 21641/40000, Loss: 4.269484634278342e-05, Learning Rate: 0.000149\n",
      "Epoch 21642/40000, Loss: 3.5036540793953463e-05, Learning Rate: 0.000149\n",
      "Epoch 21643/40000, Loss: 1.5340941899921745e-05, Learning Rate: 0.000149\n",
      "Epoch 21644/40000, Loss: 5.365483957575634e-05, Learning Rate: 0.000149\n",
      "Epoch 21645/40000, Loss: 5.382880408433266e-05, Learning Rate: 0.000149\n",
      "Epoch 21646/40000, Loss: 1.4792783076700289e-05, Learning Rate: 0.000149\n",
      "Epoch 21647/40000, Loss: 4.099255602341145e-05, Learning Rate: 0.000149\n",
      "Epoch 21648/40000, Loss: 1.4442484825849533e-05, Learning Rate: 0.000149\n",
      "Epoch 21649/40000, Loss: 1.4377541447174735e-05, Learning Rate: 0.000149\n",
      "Epoch 21650/40000, Loss: 5.313826841302216e-05, Learning Rate: 0.000149\n",
      "Epoch 21651/40000, Loss: 5.3022278734715655e-05, Learning Rate: 0.000149\n",
      "Epoch 21652/40000, Loss: 5.2755600336240605e-05, Learning Rate: 0.000149\n",
      "Epoch 21653/40000, Loss: 2.998041054524947e-05, Learning Rate: 0.000149\n",
      "Epoch 21654/40000, Loss: 5.285524093778804e-05, Learning Rate: 0.000149\n",
      "Epoch 21655/40000, Loss: 1.4052275219000876e-05, Learning Rate: 0.000149\n",
      "Epoch 21656/40000, Loss: 1.4004711374582257e-05, Learning Rate: 0.000149\n",
      "Epoch 21657/40000, Loss: 5.263875573291443e-05, Learning Rate: 0.000149\n",
      "Epoch 21658/40000, Loss: 3.402189031476155e-05, Learning Rate: 0.000149\n",
      "Epoch 21659/40000, Loss: 2.9685676054214127e-05, Learning Rate: 0.000149\n",
      "Epoch 21660/40000, Loss: 5.827807399327867e-05, Learning Rate: 0.000149\n",
      "Epoch 21661/40000, Loss: 2.967841828649398e-05, Learning Rate: 0.000149\n",
      "Epoch 21662/40000, Loss: 5.2780531405005604e-05, Learning Rate: 0.000149\n",
      "Epoch 21663/40000, Loss: 5.84333211008925e-05, Learning Rate: 0.000149\n",
      "Epoch 21664/40000, Loss: 3.399961133254692e-05, Learning Rate: 0.000149\n",
      "Epoch 21665/40000, Loss: 5.262731065158732e-05, Learning Rate: 0.000149\n",
      "Epoch 21666/40000, Loss: 2.9618227927130647e-05, Learning Rate: 0.000149\n",
      "Epoch 21667/40000, Loss: 3.398641274543479e-05, Learning Rate: 0.000149\n",
      "Epoch 21668/40000, Loss: 3.4013588447123766e-05, Learning Rate: 0.000149\n",
      "Epoch 21669/40000, Loss: 3.4031603718176484e-05, Learning Rate: 0.000149\n",
      "Epoch 21670/40000, Loss: 5.8333367633167654e-05, Learning Rate: 0.000148\n",
      "Epoch 21671/40000, Loss: 2.9710407034144737e-05, Learning Rate: 0.000148\n",
      "Epoch 21672/40000, Loss: 3.391694190213457e-05, Learning Rate: 0.000148\n",
      "Epoch 21673/40000, Loss: 3.40036021952983e-05, Learning Rate: 0.000148\n",
      "Epoch 21674/40000, Loss: 3.97909025195986e-05, Learning Rate: 0.000148\n",
      "Epoch 21675/40000, Loss: 5.257233715383336e-05, Learning Rate: 0.000148\n",
      "Epoch 21676/40000, Loss: 2.961450081784278e-05, Learning Rate: 0.000148\n",
      "Epoch 21677/40000, Loss: 3.396513420739211e-05, Learning Rate: 0.000148\n",
      "Epoch 21678/40000, Loss: 3.985454895882867e-05, Learning Rate: 0.000148\n",
      "Epoch 21679/40000, Loss: 1.3867625057173427e-05, Learning Rate: 0.000148\n",
      "Epoch 21680/40000, Loss: 5.255363794276491e-05, Learning Rate: 0.000148\n",
      "Epoch 21681/40000, Loss: 5.263111961539835e-05, Learning Rate: 0.000148\n",
      "Epoch 21682/40000, Loss: 5.838473589392379e-05, Learning Rate: 0.000148\n",
      "Epoch 21683/40000, Loss: 5.283638893160969e-05, Learning Rate: 0.000148\n",
      "Epoch 21684/40000, Loss: 3.413737431401387e-05, Learning Rate: 0.000148\n",
      "Epoch 21685/40000, Loss: 4.0159589843824506e-05, Learning Rate: 0.000148\n",
      "Epoch 21686/40000, Loss: 5.839955701958388e-05, Learning Rate: 0.000148\n",
      "Epoch 21687/40000, Loss: 3.404778180993162e-05, Learning Rate: 0.000148\n",
      "Epoch 21688/40000, Loss: 1.3994706932862755e-05, Learning Rate: 0.000148\n",
      "Epoch 21689/40000, Loss: 3.991953417425975e-05, Learning Rate: 0.000148\n",
      "Epoch 21690/40000, Loss: 2.976459654746577e-05, Learning Rate: 0.000148\n",
      "Epoch 21691/40000, Loss: 4.040426938445307e-05, Learning Rate: 0.000148\n",
      "Epoch 21692/40000, Loss: 2.964516716019716e-05, Learning Rate: 0.000148\n",
      "Epoch 21693/40000, Loss: 2.9670158255612478e-05, Learning Rate: 0.000148\n",
      "Epoch 21694/40000, Loss: 3.996718078269623e-05, Learning Rate: 0.000148\n",
      "Epoch 21695/40000, Loss: 2.9875578547944315e-05, Learning Rate: 0.000148\n",
      "Epoch 21696/40000, Loss: 1.3940329154138453e-05, Learning Rate: 0.000148\n",
      "Epoch 21697/40000, Loss: 5.274183058645576e-05, Learning Rate: 0.000148\n",
      "Epoch 21698/40000, Loss: 5.265345680527389e-05, Learning Rate: 0.000148\n",
      "Epoch 21699/40000, Loss: 2.9690467272303067e-05, Learning Rate: 0.000148\n",
      "Epoch 21700/40000, Loss: 1.3898292309022509e-05, Learning Rate: 0.000148\n",
      "Epoch 21701/40000, Loss: 5.2722974942298606e-05, Learning Rate: 0.000148\n",
      "Epoch 21702/40000, Loss: 5.917580347158946e-05, Learning Rate: 0.000148\n",
      "Epoch 21703/40000, Loss: 3.985053990618326e-05, Learning Rate: 0.000148\n",
      "Epoch 21704/40000, Loss: 3.399920751689933e-05, Learning Rate: 0.000148\n",
      "Epoch 21705/40000, Loss: 5.268430322757922e-05, Learning Rate: 0.000148\n",
      "Epoch 21706/40000, Loss: 5.2569666877388954e-05, Learning Rate: 0.000148\n",
      "Epoch 21707/40000, Loss: 2.9553866625064984e-05, Learning Rate: 0.000148\n",
      "Epoch 21708/40000, Loss: 3.982782436651178e-05, Learning Rate: 0.000148\n",
      "Epoch 21709/40000, Loss: 3.98171141569037e-05, Learning Rate: 0.000148\n",
      "Epoch 21710/40000, Loss: 5.8366917073726654e-05, Learning Rate: 0.000148\n",
      "Epoch 21711/40000, Loss: 1.3844193745171651e-05, Learning Rate: 0.000148\n",
      "Epoch 21712/40000, Loss: 1.393000638927333e-05, Learning Rate: 0.000148\n",
      "Epoch 21713/40000, Loss: 1.3871191185899079e-05, Learning Rate: 0.000148\n",
      "Epoch 21714/40000, Loss: 2.9671815354959108e-05, Learning Rate: 0.000148\n",
      "Epoch 21715/40000, Loss: 3.388934419490397e-05, Learning Rate: 0.000148\n",
      "Epoch 21716/40000, Loss: 5.830073496326804e-05, Learning Rate: 0.000148\n",
      "Epoch 21717/40000, Loss: 1.3873896023142152e-05, Learning Rate: 0.000148\n",
      "Epoch 21718/40000, Loss: 5.831105590914376e-05, Learning Rate: 0.000148\n",
      "Epoch 21719/40000, Loss: 3.396613465156406e-05, Learning Rate: 0.000148\n",
      "Epoch 21720/40000, Loss: 2.968736589537002e-05, Learning Rate: 0.000148\n",
      "Epoch 21721/40000, Loss: 2.965815110655967e-05, Learning Rate: 0.000148\n",
      "Epoch 21722/40000, Loss: 1.3982422387925908e-05, Learning Rate: 0.000148\n",
      "Epoch 21723/40000, Loss: 5.274315117276274e-05, Learning Rate: 0.000148\n",
      "Epoch 21724/40000, Loss: 2.9754150091321208e-05, Learning Rate: 0.000148\n",
      "Epoch 21725/40000, Loss: 5.337025868357159e-05, Learning Rate: 0.000148\n",
      "Epoch 21726/40000, Loss: 5.2796218369621783e-05, Learning Rate: 0.000147\n",
      "Epoch 21727/40000, Loss: 3.405989264138043e-05, Learning Rate: 0.000147\n",
      "Epoch 21728/40000, Loss: 5.3730280342279e-05, Learning Rate: 0.000147\n",
      "Epoch 21729/40000, Loss: 3.410863791941665e-05, Learning Rate: 0.000147\n",
      "Epoch 21730/40000, Loss: 2.9821827411069535e-05, Learning Rate: 0.000147\n",
      "Epoch 21731/40000, Loss: 1.3993516404298134e-05, Learning Rate: 0.000147\n",
      "Epoch 21732/40000, Loss: 5.2950967074139044e-05, Learning Rate: 0.000147\n",
      "Epoch 21733/40000, Loss: 5.305767263052985e-05, Learning Rate: 0.000147\n",
      "Epoch 21734/40000, Loss: 1.4326899872685317e-05, Learning Rate: 0.000147\n",
      "Epoch 21735/40000, Loss: 5.974531450192444e-05, Learning Rate: 0.000147\n",
      "Epoch 21736/40000, Loss: 5.3648855100618675e-05, Learning Rate: 0.000147\n",
      "Epoch 21737/40000, Loss: 1.481034814787563e-05, Learning Rate: 0.000147\n",
      "Epoch 21738/40000, Loss: 4.108233770239167e-05, Learning Rate: 0.000147\n",
      "Epoch 21739/40000, Loss: 6.229182326933369e-05, Learning Rate: 0.000147\n",
      "Epoch 21740/40000, Loss: 3.500698585412465e-05, Learning Rate: 0.000147\n",
      "Epoch 21741/40000, Loss: 4.1346520447405055e-05, Learning Rate: 0.000147\n",
      "Epoch 21742/40000, Loss: 4.079254358657636e-05, Learning Rate: 0.000147\n",
      "Epoch 21743/40000, Loss: 1.4693652701680548e-05, Learning Rate: 0.000147\n",
      "Epoch 21744/40000, Loss: 1.4688486771774478e-05, Learning Rate: 0.000147\n",
      "Epoch 21745/40000, Loss: 3.002645098604262e-05, Learning Rate: 0.000147\n",
      "Epoch 21746/40000, Loss: 5.3350278903963044e-05, Learning Rate: 0.000147\n",
      "Epoch 21747/40000, Loss: 4.047913898830302e-05, Learning Rate: 0.000147\n",
      "Epoch 21748/40000, Loss: 2.9832821383024566e-05, Learning Rate: 0.000147\n",
      "Epoch 21749/40000, Loss: 1.410783897881629e-05, Learning Rate: 0.000147\n",
      "Epoch 21750/40000, Loss: 1.4313993233372457e-05, Learning Rate: 0.000147\n",
      "Epoch 21751/40000, Loss: 4.072535375598818e-05, Learning Rate: 0.000147\n",
      "Epoch 21752/40000, Loss: 5.33868333150167e-05, Learning Rate: 0.000147\n",
      "Epoch 21753/40000, Loss: 4.103546962141991e-05, Learning Rate: 0.000147\n",
      "Epoch 21754/40000, Loss: 4.02975820179563e-05, Learning Rate: 0.000147\n",
      "Epoch 21755/40000, Loss: 4.0962695493362844e-05, Learning Rate: 0.000147\n",
      "Epoch 21756/40000, Loss: 4.024790905532427e-05, Learning Rate: 0.000147\n",
      "Epoch 21757/40000, Loss: 3.451741940807551e-05, Learning Rate: 0.000147\n",
      "Epoch 21758/40000, Loss: 4.0969487599795684e-05, Learning Rate: 0.000147\n",
      "Epoch 21759/40000, Loss: 3.436303450143896e-05, Learning Rate: 0.000147\n",
      "Epoch 21760/40000, Loss: 5.8957404689863324e-05, Learning Rate: 0.000147\n",
      "Epoch 21761/40000, Loss: 5.3204799769446254e-05, Learning Rate: 0.000147\n",
      "Epoch 21762/40000, Loss: 1.4165068932925351e-05, Learning Rate: 0.000147\n",
      "Epoch 21763/40000, Loss: 5.8919227740261704e-05, Learning Rate: 0.000147\n",
      "Epoch 21764/40000, Loss: 4.039820487378165e-05, Learning Rate: 0.000147\n",
      "Epoch 21765/40000, Loss: 5.335227251634933e-05, Learning Rate: 0.000147\n",
      "Epoch 21766/40000, Loss: 5.330970088834874e-05, Learning Rate: 0.000147\n",
      "Epoch 21767/40000, Loss: 5.901608892600052e-05, Learning Rate: 0.000147\n",
      "Epoch 21768/40000, Loss: 5.8813988289330155e-05, Learning Rate: 0.000147\n",
      "Epoch 21769/40000, Loss: 2.98912691505393e-05, Learning Rate: 0.000147\n",
      "Epoch 21770/40000, Loss: 5.944169606664218e-05, Learning Rate: 0.000147\n",
      "Epoch 21771/40000, Loss: 3.019548239535652e-05, Learning Rate: 0.000147\n",
      "Epoch 21772/40000, Loss: 2.993711495946627e-05, Learning Rate: 0.000147\n",
      "Epoch 21773/40000, Loss: 3.453851240919903e-05, Learning Rate: 0.000147\n",
      "Epoch 21774/40000, Loss: 5.3687821491621435e-05, Learning Rate: 0.000147\n",
      "Epoch 21775/40000, Loss: 5.407601565821096e-05, Learning Rate: 0.000147\n",
      "Epoch 21776/40000, Loss: 3.465554982540198e-05, Learning Rate: 0.000147\n",
      "Epoch 21777/40000, Loss: 4.1216240788344294e-05, Learning Rate: 0.000147\n",
      "Epoch 21778/40000, Loss: 4.0749629988567904e-05, Learning Rate: 0.000147\n",
      "Epoch 21779/40000, Loss: 1.4712776646774728e-05, Learning Rate: 0.000147\n",
      "Epoch 21780/40000, Loss: 4.1331561078550294e-05, Learning Rate: 0.000147\n",
      "Epoch 21781/40000, Loss: 5.647433499689214e-05, Learning Rate: 0.000147\n",
      "Epoch 21782/40000, Loss: 4.068901398568414e-05, Learning Rate: 0.000147\n",
      "Epoch 21783/40000, Loss: 1.557656105433125e-05, Learning Rate: 0.000146\n",
      "Epoch 21784/40000, Loss: 5.9691730712074786e-05, Learning Rate: 0.000146\n",
      "Epoch 21785/40000, Loss: 3.5362547350814566e-05, Learning Rate: 0.000146\n",
      "Epoch 21786/40000, Loss: 3.054806438740343e-05, Learning Rate: 0.000146\n",
      "Epoch 21787/40000, Loss: 4.2095336539205164e-05, Learning Rate: 0.000146\n",
      "Epoch 21788/40000, Loss: 3.103438575635664e-05, Learning Rate: 0.000146\n",
      "Epoch 21789/40000, Loss: 3.4911685361294076e-05, Learning Rate: 0.000146\n",
      "Epoch 21790/40000, Loss: 5.473966302815825e-05, Learning Rate: 0.000146\n",
      "Epoch 21791/40000, Loss: 2.9957236620248295e-05, Learning Rate: 0.000146\n",
      "Epoch 21792/40000, Loss: 5.915854126214981e-05, Learning Rate: 0.000146\n",
      "Epoch 21793/40000, Loss: 5.89737574046012e-05, Learning Rate: 0.000146\n",
      "Epoch 21794/40000, Loss: 5.3257073886925355e-05, Learning Rate: 0.000146\n",
      "Epoch 21795/40000, Loss: 5.9691181377274916e-05, Learning Rate: 0.000146\n",
      "Epoch 21796/40000, Loss: 5.562568185268901e-05, Learning Rate: 0.000146\n",
      "Epoch 21797/40000, Loss: 5.331766442395747e-05, Learning Rate: 0.000146\n",
      "Epoch 21798/40000, Loss: 3.0119394068606198e-05, Learning Rate: 0.000146\n",
      "Epoch 21799/40000, Loss: 5.86366186325904e-05, Learning Rate: 0.000146\n",
      "Epoch 21800/40000, Loss: 4.1173821955453604e-05, Learning Rate: 0.000146\n",
      "Epoch 21801/40000, Loss: 3.0094308385741897e-05, Learning Rate: 0.000146\n",
      "Epoch 21802/40000, Loss: 1.4259423551266082e-05, Learning Rate: 0.000146\n",
      "Epoch 21803/40000, Loss: 3.125769944745116e-05, Learning Rate: 0.000146\n",
      "Epoch 21804/40000, Loss: 3.566804662114009e-05, Learning Rate: 0.000146\n",
      "Epoch 21805/40000, Loss: 4.126865678699687e-05, Learning Rate: 0.000146\n",
      "Epoch 21806/40000, Loss: 4.0453989640809596e-05, Learning Rate: 0.000146\n",
      "Epoch 21807/40000, Loss: 5.316241367836483e-05, Learning Rate: 0.000146\n",
      "Epoch 21808/40000, Loss: 4.051819632877596e-05, Learning Rate: 0.000146\n",
      "Epoch 21809/40000, Loss: 5.8584260841598734e-05, Learning Rate: 0.000146\n",
      "Epoch 21810/40000, Loss: 2.9869777790736407e-05, Learning Rate: 0.000146\n",
      "Epoch 21811/40000, Loss: 2.965892053907737e-05, Learning Rate: 0.000146\n",
      "Epoch 21812/40000, Loss: 3.393227598280646e-05, Learning Rate: 0.000146\n",
      "Epoch 21813/40000, Loss: 5.829837391502224e-05, Learning Rate: 0.000146\n",
      "Epoch 21814/40000, Loss: 2.9667946364497766e-05, Learning Rate: 0.000146\n",
      "Epoch 21815/40000, Loss: 5.293577123666182e-05, Learning Rate: 0.000146\n",
      "Epoch 21816/40000, Loss: 1.4040923815628048e-05, Learning Rate: 0.000146\n",
      "Epoch 21817/40000, Loss: 3.985586226917803e-05, Learning Rate: 0.000146\n",
      "Epoch 21818/40000, Loss: 5.2556268201442435e-05, Learning Rate: 0.000146\n",
      "Epoch 21819/40000, Loss: 3.389290941413492e-05, Learning Rate: 0.000146\n",
      "Epoch 21820/40000, Loss: 3.9854381611803547e-05, Learning Rate: 0.000146\n",
      "Epoch 21821/40000, Loss: 2.9564471333287656e-05, Learning Rate: 0.000146\n",
      "Epoch 21822/40000, Loss: 2.951218993985094e-05, Learning Rate: 0.000146\n",
      "Epoch 21823/40000, Loss: 1.3866463632439263e-05, Learning Rate: 0.000146\n",
      "Epoch 21824/40000, Loss: 3.98015727114398e-05, Learning Rate: 0.000146\n",
      "Epoch 21825/40000, Loss: 5.2433366363402456e-05, Learning Rate: 0.000146\n",
      "Epoch 21826/40000, Loss: 2.95063200610457e-05, Learning Rate: 0.000146\n",
      "Epoch 21827/40000, Loss: 3.978905806434341e-05, Learning Rate: 0.000146\n",
      "Epoch 21828/40000, Loss: 5.820253136334941e-05, Learning Rate: 0.000146\n",
      "Epoch 21829/40000, Loss: 3.995226143160835e-05, Learning Rate: 0.000146\n",
      "Epoch 21830/40000, Loss: 1.398395215801429e-05, Learning Rate: 0.000146\n",
      "Epoch 21831/40000, Loss: 1.393126331095118e-05, Learning Rate: 0.000146\n",
      "Epoch 21832/40000, Loss: 2.9464872568496503e-05, Learning Rate: 0.000146\n",
      "Epoch 21833/40000, Loss: 5.2664596296381205e-05, Learning Rate: 0.000146\n",
      "Epoch 21834/40000, Loss: 1.4145232853479683e-05, Learning Rate: 0.000146\n",
      "Epoch 21835/40000, Loss: 2.9556611480074935e-05, Learning Rate: 0.000146\n",
      "Epoch 21836/40000, Loss: 5.837680510012433e-05, Learning Rate: 0.000146\n",
      "Epoch 21837/40000, Loss: 3.399608613108285e-05, Learning Rate: 0.000146\n",
      "Epoch 21838/40000, Loss: 5.9323778259567916e-05, Learning Rate: 0.000146\n",
      "Epoch 21839/40000, Loss: 3.4179571230197325e-05, Learning Rate: 0.000146\n",
      "Epoch 21840/40000, Loss: 5.309210246196017e-05, Learning Rate: 0.000145\n",
      "Epoch 21841/40000, Loss: 5.861043609911576e-05, Learning Rate: 0.000145\n",
      "Epoch 21842/40000, Loss: 5.367311678128317e-05, Learning Rate: 0.000145\n",
      "Epoch 21843/40000, Loss: 1.4313431165646762e-05, Learning Rate: 0.000145\n",
      "Epoch 21844/40000, Loss: 2.9701332096010447e-05, Learning Rate: 0.000145\n",
      "Epoch 21845/40000, Loss: 4.050663847010583e-05, Learning Rate: 0.000145\n",
      "Epoch 21846/40000, Loss: 2.9943210392957553e-05, Learning Rate: 0.000145\n",
      "Epoch 21847/40000, Loss: 1.4148167792882305e-05, Learning Rate: 0.000145\n",
      "Epoch 21848/40000, Loss: 3.442823435761966e-05, Learning Rate: 0.000145\n",
      "Epoch 21849/40000, Loss: 3.0425890145124868e-05, Learning Rate: 0.000145\n",
      "Epoch 21850/40000, Loss: 4.033616278320551e-05, Learning Rate: 0.000145\n",
      "Epoch 21851/40000, Loss: 3.4236512874485925e-05, Learning Rate: 0.000145\n",
      "Epoch 21852/40000, Loss: 5.8620698837330565e-05, Learning Rate: 0.000145\n",
      "Epoch 21853/40000, Loss: 5.844920815434307e-05, Learning Rate: 0.000145\n",
      "Epoch 21854/40000, Loss: 5.3653522627428174e-05, Learning Rate: 0.000145\n",
      "Epoch 21855/40000, Loss: 1.4016163731866982e-05, Learning Rate: 0.000145\n",
      "Epoch 21856/40000, Loss: 5.3486350225284696e-05, Learning Rate: 0.000145\n",
      "Epoch 21857/40000, Loss: 5.46296541870106e-05, Learning Rate: 0.000145\n",
      "Epoch 21858/40000, Loss: 5.979278648737818e-05, Learning Rate: 0.000145\n",
      "Epoch 21859/40000, Loss: 5.817167038912885e-05, Learning Rate: 0.000145\n",
      "Epoch 21860/40000, Loss: 5.9480567870195955e-05, Learning Rate: 0.000145\n",
      "Epoch 21861/40000, Loss: 3.541210026014596e-05, Learning Rate: 0.000145\n",
      "Epoch 21862/40000, Loss: 3.4797005355358124e-05, Learning Rate: 0.000145\n",
      "Epoch 21863/40000, Loss: 3.413664671825245e-05, Learning Rate: 0.000145\n",
      "Epoch 21864/40000, Loss: 5.9767447964986786e-05, Learning Rate: 0.000145\n",
      "Epoch 21865/40000, Loss: 4.245342643116601e-05, Learning Rate: 0.000145\n",
      "Epoch 21866/40000, Loss: 3.4925422369269654e-05, Learning Rate: 0.000145\n",
      "Epoch 21867/40000, Loss: 1.4857037967885844e-05, Learning Rate: 0.000145\n",
      "Epoch 21868/40000, Loss: 3.54875810444355e-05, Learning Rate: 0.000145\n",
      "Epoch 21869/40000, Loss: 3.616538015194237e-05, Learning Rate: 0.000145\n",
      "Epoch 21870/40000, Loss: 3.002165431098547e-05, Learning Rate: 0.000145\n",
      "Epoch 21871/40000, Loss: 5.9207795857219025e-05, Learning Rate: 0.000145\n",
      "Epoch 21872/40000, Loss: 1.486969631514512e-05, Learning Rate: 0.000145\n",
      "Epoch 21873/40000, Loss: 4.0935025026556104e-05, Learning Rate: 0.000145\n",
      "Epoch 21874/40000, Loss: 1.4024517440702766e-05, Learning Rate: 0.000145\n",
      "Epoch 21875/40000, Loss: 5.32983940502163e-05, Learning Rate: 0.000145\n",
      "Epoch 21876/40000, Loss: 3.449519135756418e-05, Learning Rate: 0.000145\n",
      "Epoch 21877/40000, Loss: 1.4070274119148962e-05, Learning Rate: 0.000145\n",
      "Epoch 21878/40000, Loss: 2.9601833375636488e-05, Learning Rate: 0.000145\n",
      "Epoch 21879/40000, Loss: 5.276229421724565e-05, Learning Rate: 0.000145\n",
      "Epoch 21880/40000, Loss: 1.3878458958060946e-05, Learning Rate: 0.000145\n",
      "Epoch 21881/40000, Loss: 5.8976278523914516e-05, Learning Rate: 0.000145\n",
      "Epoch 21882/40000, Loss: 5.8951674873242155e-05, Learning Rate: 0.000145\n",
      "Epoch 21883/40000, Loss: 5.9027792303822935e-05, Learning Rate: 0.000145\n",
      "Epoch 21884/40000, Loss: 1.4044579074834473e-05, Learning Rate: 0.000145\n",
      "Epoch 21885/40000, Loss: 2.980531644425355e-05, Learning Rate: 0.000145\n",
      "Epoch 21886/40000, Loss: 2.9600196285173297e-05, Learning Rate: 0.000145\n",
      "Epoch 21887/40000, Loss: 3.409634518902749e-05, Learning Rate: 0.000145\n",
      "Epoch 21888/40000, Loss: 5.4675507271895185e-05, Learning Rate: 0.000145\n",
      "Epoch 21889/40000, Loss: 1.4047514923731796e-05, Learning Rate: 0.000145\n",
      "Epoch 21890/40000, Loss: 2.952169961645268e-05, Learning Rate: 0.000145\n",
      "Epoch 21891/40000, Loss: 2.9555067158071324e-05, Learning Rate: 0.000145\n",
      "Epoch 21892/40000, Loss: 3.401324283913709e-05, Learning Rate: 0.000145\n",
      "Epoch 21893/40000, Loss: 2.9640674256370403e-05, Learning Rate: 0.000145\n",
      "Epoch 21894/40000, Loss: 3.990835102740675e-05, Learning Rate: 0.000145\n",
      "Epoch 21895/40000, Loss: 3.391740028746426e-05, Learning Rate: 0.000145\n",
      "Epoch 21896/40000, Loss: 4.00749449909199e-05, Learning Rate: 0.000145\n",
      "Epoch 21897/40000, Loss: 3.989755714428611e-05, Learning Rate: 0.000144\n",
      "Epoch 21898/40000, Loss: 1.3827760085405316e-05, Learning Rate: 0.000144\n",
      "Epoch 21899/40000, Loss: 5.814262476633303e-05, Learning Rate: 0.000144\n",
      "Epoch 21900/40000, Loss: 1.3918068361817859e-05, Learning Rate: 0.000144\n",
      "Epoch 21901/40000, Loss: 3.015552465512883e-05, Learning Rate: 0.000144\n",
      "Epoch 21902/40000, Loss: 5.279404285829514e-05, Learning Rate: 0.000144\n",
      "Epoch 21903/40000, Loss: 2.9696899218834005e-05, Learning Rate: 0.000144\n",
      "Epoch 21904/40000, Loss: 3.378433393663727e-05, Learning Rate: 0.000144\n",
      "Epoch 21905/40000, Loss: 2.9640521461260505e-05, Learning Rate: 0.000144\n",
      "Epoch 21906/40000, Loss: 4.077211633557454e-05, Learning Rate: 0.000144\n",
      "Epoch 21907/40000, Loss: 3.4262418921571225e-05, Learning Rate: 0.000144\n",
      "Epoch 21908/40000, Loss: 4.3932432163273916e-05, Learning Rate: 0.000144\n",
      "Epoch 21909/40000, Loss: 1.7367248801747337e-05, Learning Rate: 0.000144\n",
      "Epoch 21910/40000, Loss: 3.1605330150341615e-05, Learning Rate: 0.000144\n",
      "Epoch 21911/40000, Loss: 2.9858265406801365e-05, Learning Rate: 0.000144\n",
      "Epoch 21912/40000, Loss: 3.7096258893143386e-05, Learning Rate: 0.000144\n",
      "Epoch 21913/40000, Loss: 3.1927836971590295e-05, Learning Rate: 0.000144\n",
      "Epoch 21914/40000, Loss: 4.951709706801921e-05, Learning Rate: 0.000144\n",
      "Epoch 21915/40000, Loss: 5.71856799069792e-05, Learning Rate: 0.000144\n",
      "Epoch 21916/40000, Loss: 3.1399369618156925e-05, Learning Rate: 0.000144\n",
      "Epoch 21917/40000, Loss: 1.611308653082233e-05, Learning Rate: 0.000144\n",
      "Epoch 21918/40000, Loss: 3.550313340383582e-05, Learning Rate: 0.000144\n",
      "Epoch 21919/40000, Loss: 3.4405333281029016e-05, Learning Rate: 0.000144\n",
      "Epoch 21920/40000, Loss: 4.0956201701192185e-05, Learning Rate: 0.000144\n",
      "Epoch 21921/40000, Loss: 5.8720183005789295e-05, Learning Rate: 0.000144\n",
      "Epoch 21922/40000, Loss: 5.867391882929951e-05, Learning Rate: 0.000144\n",
      "Epoch 21923/40000, Loss: 4.0229402657132596e-05, Learning Rate: 0.000144\n",
      "Epoch 21924/40000, Loss: 6.095835487940349e-05, Learning Rate: 0.000144\n",
      "Epoch 21925/40000, Loss: 3.444319008849561e-05, Learning Rate: 0.000144\n",
      "Epoch 21926/40000, Loss: 4.129167427890934e-05, Learning Rate: 0.000144\n",
      "Epoch 21927/40000, Loss: 4.076030745636672e-05, Learning Rate: 0.000144\n",
      "Epoch 21928/40000, Loss: 5.39778484380804e-05, Learning Rate: 0.000144\n",
      "Epoch 21929/40000, Loss: 3.455688420217484e-05, Learning Rate: 0.000144\n",
      "Epoch 21930/40000, Loss: 3.408743941690773e-05, Learning Rate: 0.000144\n",
      "Epoch 21931/40000, Loss: 1.4134520824882202e-05, Learning Rate: 0.000144\n",
      "Epoch 21932/40000, Loss: 5.270036854199134e-05, Learning Rate: 0.000144\n",
      "Epoch 21933/40000, Loss: 3.425856266403571e-05, Learning Rate: 0.000144\n",
      "Epoch 21934/40000, Loss: 5.308306936058216e-05, Learning Rate: 0.000144\n",
      "Epoch 21935/40000, Loss: 1.397946380166104e-05, Learning Rate: 0.000144\n",
      "Epoch 21936/40000, Loss: 3.986110459663905e-05, Learning Rate: 0.000144\n",
      "Epoch 21937/40000, Loss: 1.3970760846859775e-05, Learning Rate: 0.000144\n",
      "Epoch 21938/40000, Loss: 5.827225686516613e-05, Learning Rate: 0.000144\n",
      "Epoch 21939/40000, Loss: 3.387799733900465e-05, Learning Rate: 0.000144\n",
      "Epoch 21940/40000, Loss: 1.3820285857946146e-05, Learning Rate: 0.000144\n",
      "Epoch 21941/40000, Loss: 1.3810982636641711e-05, Learning Rate: 0.000144\n",
      "Epoch 21942/40000, Loss: 5.271256668493152e-05, Learning Rate: 0.000144\n",
      "Epoch 21943/40000, Loss: 1.385742325510364e-05, Learning Rate: 0.000144\n",
      "Epoch 21944/40000, Loss: 3.9895661757327616e-05, Learning Rate: 0.000144\n",
      "Epoch 21945/40000, Loss: 2.9423981686704792e-05, Learning Rate: 0.000144\n",
      "Epoch 21946/40000, Loss: 5.301109922584146e-05, Learning Rate: 0.000144\n",
      "Epoch 21947/40000, Loss: 5.848353976034559e-05, Learning Rate: 0.000144\n",
      "Epoch 21948/40000, Loss: 5.551011417992413e-05, Learning Rate: 0.000144\n",
      "Epoch 21949/40000, Loss: 1.397425694449339e-05, Learning Rate: 0.000144\n",
      "Epoch 21950/40000, Loss: 5.821515878778882e-05, Learning Rate: 0.000144\n",
      "Epoch 21951/40000, Loss: 2.9953513148939237e-05, Learning Rate: 0.000144\n",
      "Epoch 21952/40000, Loss: 3.375372034497559e-05, Learning Rate: 0.000144\n",
      "Epoch 21953/40000, Loss: 3.3784148399718106e-05, Learning Rate: 0.000144\n",
      "Epoch 21954/40000, Loss: 5.263252751319669e-05, Learning Rate: 0.000144\n",
      "Epoch 21955/40000, Loss: 5.8370704209664837e-05, Learning Rate: 0.000143\n",
      "Epoch 21956/40000, Loss: 5.253579729469493e-05, Learning Rate: 0.000143\n",
      "Epoch 21957/40000, Loss: 3.37477758876048e-05, Learning Rate: 0.000143\n",
      "Epoch 21958/40000, Loss: 1.3826442227582447e-05, Learning Rate: 0.000143\n",
      "Epoch 21959/40000, Loss: 5.801632869406603e-05, Learning Rate: 0.000143\n",
      "Epoch 21960/40000, Loss: 2.9455291951308027e-05, Learning Rate: 0.000143\n",
      "Epoch 21961/40000, Loss: 2.937788849521894e-05, Learning Rate: 0.000143\n",
      "Epoch 21962/40000, Loss: 3.980260225944221e-05, Learning Rate: 0.000143\n",
      "Epoch 21963/40000, Loss: 2.940802187367808e-05, Learning Rate: 0.000143\n",
      "Epoch 21964/40000, Loss: 1.3774693798040971e-05, Learning Rate: 0.000143\n",
      "Epoch 21965/40000, Loss: 3.968762030126527e-05, Learning Rate: 0.000143\n",
      "Epoch 21966/40000, Loss: 2.9321068723220378e-05, Learning Rate: 0.000143\n",
      "Epoch 21967/40000, Loss: 1.3731127182836644e-05, Learning Rate: 0.000143\n",
      "Epoch 21968/40000, Loss: 3.382116119610146e-05, Learning Rate: 0.000143\n",
      "Epoch 21969/40000, Loss: 3.3720727515174076e-05, Learning Rate: 0.000143\n",
      "Epoch 21970/40000, Loss: 3.372415085323155e-05, Learning Rate: 0.000143\n",
      "Epoch 21971/40000, Loss: 5.23660855833441e-05, Learning Rate: 0.000143\n",
      "Epoch 21972/40000, Loss: 1.3798745385429356e-05, Learning Rate: 0.000143\n",
      "Epoch 21973/40000, Loss: 2.9377784812822938e-05, Learning Rate: 0.000143\n",
      "Epoch 21974/40000, Loss: 3.97651492676232e-05, Learning Rate: 0.000143\n",
      "Epoch 21975/40000, Loss: 3.9765232941135764e-05, Learning Rate: 0.000143\n",
      "Epoch 21976/40000, Loss: 2.929418405983597e-05, Learning Rate: 0.000143\n",
      "Epoch 21977/40000, Loss: 3.372501305420883e-05, Learning Rate: 0.000143\n",
      "Epoch 21978/40000, Loss: 3.373449362698011e-05, Learning Rate: 0.000143\n",
      "Epoch 21979/40000, Loss: 3.370898048160598e-05, Learning Rate: 0.000143\n",
      "Epoch 21980/40000, Loss: 5.820423393743113e-05, Learning Rate: 0.000143\n",
      "Epoch 21981/40000, Loss: 2.9378705221461132e-05, Learning Rate: 0.000143\n",
      "Epoch 21982/40000, Loss: 5.8120749599765986e-05, Learning Rate: 0.000143\n",
      "Epoch 21983/40000, Loss: 2.9377182727330364e-05, Learning Rate: 0.000143\n",
      "Epoch 21984/40000, Loss: 3.9711743738735095e-05, Learning Rate: 0.000143\n",
      "Epoch 21985/40000, Loss: 3.967797601944767e-05, Learning Rate: 0.000143\n",
      "Epoch 21986/40000, Loss: 5.801571751362644e-05, Learning Rate: 0.000143\n",
      "Epoch 21987/40000, Loss: 1.3715405657421798e-05, Learning Rate: 0.000143\n",
      "Epoch 21988/40000, Loss: 5.8030822401633486e-05, Learning Rate: 0.000143\n",
      "Epoch 21989/40000, Loss: 1.3782769201498013e-05, Learning Rate: 0.000143\n",
      "Epoch 21990/40000, Loss: 1.3800828128296416e-05, Learning Rate: 0.000143\n",
      "Epoch 21991/40000, Loss: 3.375557571416721e-05, Learning Rate: 0.000143\n",
      "Epoch 21992/40000, Loss: 3.984847353422083e-05, Learning Rate: 0.000143\n",
      "Epoch 21993/40000, Loss: 2.9401278879959136e-05, Learning Rate: 0.000143\n",
      "Epoch 21994/40000, Loss: 3.9784496038919315e-05, Learning Rate: 0.000143\n",
      "Epoch 21995/40000, Loss: 3.3738437196007e-05, Learning Rate: 0.000143\n",
      "Epoch 21996/40000, Loss: 2.944396146631334e-05, Learning Rate: 0.000143\n",
      "Epoch 21997/40000, Loss: 3.968854434788227e-05, Learning Rate: 0.000143\n",
      "Epoch 21998/40000, Loss: 1.377153330395231e-05, Learning Rate: 0.000143\n",
      "Epoch 21999/40000, Loss: 3.978149106842466e-05, Learning Rate: 0.000143\n",
      "Epoch 22000/40000, Loss: 5.237383447820321e-05, Learning Rate: 0.000143\n",
      "Epoch 22001/40000, Loss: 3.373744766577147e-05, Learning Rate: 0.000143\n",
      "Epoch 22002/40000, Loss: 3.3760930818971246e-05, Learning Rate: 0.000143\n",
      "Epoch 22003/40000, Loss: 1.3845310604665428e-05, Learning Rate: 0.000143\n",
      "Epoch 22004/40000, Loss: 3.3884920412674546e-05, Learning Rate: 0.000143\n",
      "Epoch 22005/40000, Loss: 3.980310430051759e-05, Learning Rate: 0.000143\n",
      "Epoch 22006/40000, Loss: 3.982509588240646e-05, Learning Rate: 0.000143\n",
      "Epoch 22007/40000, Loss: 2.9448032364598475e-05, Learning Rate: 0.000143\n",
      "Epoch 22008/40000, Loss: 3.3836426155176014e-05, Learning Rate: 0.000143\n",
      "Epoch 22009/40000, Loss: 5.8127228840021417e-05, Learning Rate: 0.000143\n",
      "Epoch 22010/40000, Loss: 5.287925523589365e-05, Learning Rate: 0.000143\n",
      "Epoch 22011/40000, Loss: 1.3933838999946602e-05, Learning Rate: 0.000143\n",
      "Epoch 22012/40000, Loss: 4.0149057895177975e-05, Learning Rate: 0.000143\n",
      "Epoch 22013/40000, Loss: 4.019009429612197e-05, Learning Rate: 0.000142\n",
      "Epoch 22014/40000, Loss: 5.2873336244374514e-05, Learning Rate: 0.000142\n",
      "Epoch 22015/40000, Loss: 1.3898609722673427e-05, Learning Rate: 0.000142\n",
      "Epoch 22016/40000, Loss: 3.395335443201475e-05, Learning Rate: 0.000142\n",
      "Epoch 22017/40000, Loss: 5.3554060286842287e-05, Learning Rate: 0.000142\n",
      "Epoch 22018/40000, Loss: 3.43986066582147e-05, Learning Rate: 0.000142\n",
      "Epoch 22019/40000, Loss: 5.898990639252588e-05, Learning Rate: 0.000142\n",
      "Epoch 22020/40000, Loss: 1.4146059584163595e-05, Learning Rate: 0.000142\n",
      "Epoch 22021/40000, Loss: 5.335833702702075e-05, Learning Rate: 0.000142\n",
      "Epoch 22022/40000, Loss: 4.019726475235075e-05, Learning Rate: 0.000142\n",
      "Epoch 22023/40000, Loss: 5.405606862041168e-05, Learning Rate: 0.000142\n",
      "Epoch 22024/40000, Loss: 5.304074511514045e-05, Learning Rate: 0.000142\n",
      "Epoch 22025/40000, Loss: 1.3934664821135812e-05, Learning Rate: 0.000142\n",
      "Epoch 22026/40000, Loss: 3.450689109740779e-05, Learning Rate: 0.000142\n",
      "Epoch 22027/40000, Loss: 5.3065774409333244e-05, Learning Rate: 0.000142\n",
      "Epoch 22028/40000, Loss: 2.955033050966449e-05, Learning Rate: 0.000142\n",
      "Epoch 22029/40000, Loss: 1.413365316693671e-05, Learning Rate: 0.000142\n",
      "Epoch 22030/40000, Loss: 5.8561956393532455e-05, Learning Rate: 0.000142\n",
      "Epoch 22031/40000, Loss: 4.00750468543265e-05, Learning Rate: 0.000142\n",
      "Epoch 22032/40000, Loss: 5.363112722989172e-05, Learning Rate: 0.000142\n",
      "Epoch 22033/40000, Loss: 3.412231308175251e-05, Learning Rate: 0.000142\n",
      "Epoch 22034/40000, Loss: 2.956351454486139e-05, Learning Rate: 0.000142\n",
      "Epoch 22035/40000, Loss: 3.564073995221406e-05, Learning Rate: 0.000142\n",
      "Epoch 22036/40000, Loss: 3.408846168895252e-05, Learning Rate: 0.000142\n",
      "Epoch 22037/40000, Loss: 5.896835136809386e-05, Learning Rate: 0.000142\n",
      "Epoch 22038/40000, Loss: 2.962409234896768e-05, Learning Rate: 0.000142\n",
      "Epoch 22039/40000, Loss: 1.4090575859881938e-05, Learning Rate: 0.000142\n",
      "Epoch 22040/40000, Loss: 5.9118832723470405e-05, Learning Rate: 0.000142\n",
      "Epoch 22041/40000, Loss: 5.2880503062624484e-05, Learning Rate: 0.000142\n",
      "Epoch 22042/40000, Loss: 5.905047510168515e-05, Learning Rate: 0.000142\n",
      "Epoch 22043/40000, Loss: 5.5464777688030154e-05, Learning Rate: 0.000142\n",
      "Epoch 22044/40000, Loss: 1.4388802810572088e-05, Learning Rate: 0.000142\n",
      "Epoch 22045/40000, Loss: 3.43682877428364e-05, Learning Rate: 0.000142\n",
      "Epoch 22046/40000, Loss: 4.12687732023187e-05, Learning Rate: 0.000142\n",
      "Epoch 22047/40000, Loss: 5.426808638731018e-05, Learning Rate: 0.000142\n",
      "Epoch 22048/40000, Loss: 5.331761349225417e-05, Learning Rate: 0.000142\n",
      "Epoch 22049/40000, Loss: 1.4426719644689001e-05, Learning Rate: 0.000142\n",
      "Epoch 22050/40000, Loss: 2.9625058232340962e-05, Learning Rate: 0.000142\n",
      "Epoch 22051/40000, Loss: 2.9529062885558233e-05, Learning Rate: 0.000142\n",
      "Epoch 22052/40000, Loss: 3.416123581700958e-05, Learning Rate: 0.000142\n",
      "Epoch 22053/40000, Loss: 3.3943015296244994e-05, Learning Rate: 0.000142\n",
      "Epoch 22054/40000, Loss: 3.446333357715048e-05, Learning Rate: 0.000142\n",
      "Epoch 22055/40000, Loss: 4.153602276346646e-05, Learning Rate: 0.000142\n",
      "Epoch 22056/40000, Loss: 1.4555354937328957e-05, Learning Rate: 0.000142\n",
      "Epoch 22057/40000, Loss: 3.5605884477263317e-05, Learning Rate: 0.000142\n",
      "Epoch 22058/40000, Loss: 5.961166607448831e-05, Learning Rate: 0.000142\n",
      "Epoch 22059/40000, Loss: 4.7798115701880306e-05, Learning Rate: 0.000142\n",
      "Epoch 22060/40000, Loss: 3.0226736271288246e-05, Learning Rate: 0.000142\n",
      "Epoch 22061/40000, Loss: 2.9820615964126773e-05, Learning Rate: 0.000142\n",
      "Epoch 22062/40000, Loss: 3.078586814808659e-05, Learning Rate: 0.000142\n",
      "Epoch 22063/40000, Loss: 1.5276898921001703e-05, Learning Rate: 0.000142\n",
      "Epoch 22064/40000, Loss: 3.0026956665096804e-05, Learning Rate: 0.000142\n",
      "Epoch 22065/40000, Loss: 4.079991049366072e-05, Learning Rate: 0.000142\n",
      "Epoch 22066/40000, Loss: 3.4983699151780456e-05, Learning Rate: 0.000142\n",
      "Epoch 22067/40000, Loss: 4.093120151082985e-05, Learning Rate: 0.000142\n",
      "Epoch 22068/40000, Loss: 3.4621702070580795e-05, Learning Rate: 0.000142\n",
      "Epoch 22069/40000, Loss: 1.4716966688865796e-05, Learning Rate: 0.000142\n",
      "Epoch 22070/40000, Loss: 3.7057579902466387e-05, Learning Rate: 0.000142\n",
      "Epoch 22071/40000, Loss: 4.030370109830983e-05, Learning Rate: 0.000142\n",
      "Epoch 22072/40000, Loss: 3.485765410005115e-05, Learning Rate: 0.000141\n",
      "Epoch 22073/40000, Loss: 3.0052195143071003e-05, Learning Rate: 0.000141\n",
      "Epoch 22074/40000, Loss: 5.317735121934675e-05, Learning Rate: 0.000141\n",
      "Epoch 22075/40000, Loss: 5.2824674639850855e-05, Learning Rate: 0.000141\n",
      "Epoch 22076/40000, Loss: 4.106354390387423e-05, Learning Rate: 0.000141\n",
      "Epoch 22077/40000, Loss: 2.974874951178208e-05, Learning Rate: 0.000141\n",
      "Epoch 22078/40000, Loss: 3.386230309843086e-05, Learning Rate: 0.000141\n",
      "Epoch 22079/40000, Loss: 3.397430191398598e-05, Learning Rate: 0.000141\n",
      "Epoch 22080/40000, Loss: 5.918401075177826e-05, Learning Rate: 0.000141\n",
      "Epoch 22081/40000, Loss: 1.5075859664648306e-05, Learning Rate: 0.000141\n",
      "Epoch 22082/40000, Loss: 5.4620493756374344e-05, Learning Rate: 0.000141\n",
      "Epoch 22083/40000, Loss: 3.056838977499865e-05, Learning Rate: 0.000141\n",
      "Epoch 22084/40000, Loss: 4.3317679228493944e-05, Learning Rate: 0.000141\n",
      "Epoch 22085/40000, Loss: 6.0414029576350003e-05, Learning Rate: 0.000141\n",
      "Epoch 22086/40000, Loss: 3.133157588308677e-05, Learning Rate: 0.000141\n",
      "Epoch 22087/40000, Loss: 3.486245623207651e-05, Learning Rate: 0.000141\n",
      "Epoch 22088/40000, Loss: 1.4644007023889571e-05, Learning Rate: 0.000141\n",
      "Epoch 22089/40000, Loss: 5.375106411520392e-05, Learning Rate: 0.000141\n",
      "Epoch 22090/40000, Loss: 5.3629366448149085e-05, Learning Rate: 0.000141\n",
      "Epoch 22091/40000, Loss: 5.2940031309844926e-05, Learning Rate: 0.000141\n",
      "Epoch 22092/40000, Loss: 3.386948083061725e-05, Learning Rate: 0.000141\n",
      "Epoch 22093/40000, Loss: 4.009453550679609e-05, Learning Rate: 0.000141\n",
      "Epoch 22094/40000, Loss: 5.2614297601394355e-05, Learning Rate: 0.000141\n",
      "Epoch 22095/40000, Loss: 5.852219692314975e-05, Learning Rate: 0.000141\n",
      "Epoch 22096/40000, Loss: 5.811987284687348e-05, Learning Rate: 0.000141\n",
      "Epoch 22097/40000, Loss: 3.3744472602847964e-05, Learning Rate: 0.000141\n",
      "Epoch 22098/40000, Loss: 5.826914275530726e-05, Learning Rate: 0.000141\n",
      "Epoch 22099/40000, Loss: 3.3775591873563826e-05, Learning Rate: 0.000141\n",
      "Epoch 22100/40000, Loss: 3.3735763281583786e-05, Learning Rate: 0.000141\n",
      "Epoch 22101/40000, Loss: 5.242834231466986e-05, Learning Rate: 0.000141\n",
      "Epoch 22102/40000, Loss: 5.804029933642596e-05, Learning Rate: 0.000141\n",
      "Epoch 22103/40000, Loss: 2.9404211090877652e-05, Learning Rate: 0.000141\n",
      "Epoch 22104/40000, Loss: 3.370179911144078e-05, Learning Rate: 0.000141\n",
      "Epoch 22105/40000, Loss: 2.9398350307019427e-05, Learning Rate: 0.000141\n",
      "Epoch 22106/40000, Loss: 5.232857074588537e-05, Learning Rate: 0.000141\n",
      "Epoch 22107/40000, Loss: 1.3777094864053652e-05, Learning Rate: 0.000141\n",
      "Epoch 22108/40000, Loss: 5.229865564615466e-05, Learning Rate: 0.000141\n",
      "Epoch 22109/40000, Loss: 5.230289025348611e-05, Learning Rate: 0.000141\n",
      "Epoch 22110/40000, Loss: 5.226365101407282e-05, Learning Rate: 0.000141\n",
      "Epoch 22111/40000, Loss: 3.361907147336751e-05, Learning Rate: 0.000141\n",
      "Epoch 22112/40000, Loss: 1.377119497192325e-05, Learning Rate: 0.000141\n",
      "Epoch 22113/40000, Loss: 2.929302536358591e-05, Learning Rate: 0.000141\n",
      "Epoch 22114/40000, Loss: 3.9733859011903405e-05, Learning Rate: 0.000141\n",
      "Epoch 22115/40000, Loss: 1.375912870571483e-05, Learning Rate: 0.000141\n",
      "Epoch 22116/40000, Loss: 2.9281853130669333e-05, Learning Rate: 0.000141\n",
      "Epoch 22117/40000, Loss: 3.97702788177412e-05, Learning Rate: 0.000141\n",
      "Epoch 22118/40000, Loss: 3.361683411640115e-05, Learning Rate: 0.000141\n",
      "Epoch 22119/40000, Loss: 3.9710907003609464e-05, Learning Rate: 0.000141\n",
      "Epoch 22120/40000, Loss: 5.790645445813425e-05, Learning Rate: 0.000141\n",
      "Epoch 22121/40000, Loss: 5.792186493636109e-05, Learning Rate: 0.000141\n",
      "Epoch 22122/40000, Loss: 3.966575241065584e-05, Learning Rate: 0.000141\n",
      "Epoch 22123/40000, Loss: 5.798944766866043e-05, Learning Rate: 0.000141\n",
      "Epoch 22124/40000, Loss: 2.9323016860871576e-05, Learning Rate: 0.000141\n",
      "Epoch 22125/40000, Loss: 2.925808985310141e-05, Learning Rate: 0.000141\n",
      "Epoch 22126/40000, Loss: 5.795364995719865e-05, Learning Rate: 0.000141\n",
      "Epoch 22127/40000, Loss: 3.359394759172574e-05, Learning Rate: 0.000141\n",
      "Epoch 22128/40000, Loss: 5.7931803894462064e-05, Learning Rate: 0.000141\n",
      "Epoch 22129/40000, Loss: 3.373666186234914e-05, Learning Rate: 0.000141\n",
      "Epoch 22130/40000, Loss: 2.9267243007780053e-05, Learning Rate: 0.000141\n",
      "Epoch 22131/40000, Loss: 3.96769646613393e-05, Learning Rate: 0.000140\n",
      "Epoch 22132/40000, Loss: 5.794599201180972e-05, Learning Rate: 0.000140\n",
      "Epoch 22133/40000, Loss: 2.9340146284084767e-05, Learning Rate: 0.000140\n",
      "Epoch 22134/40000, Loss: 5.231750037637539e-05, Learning Rate: 0.000140\n",
      "Epoch 22135/40000, Loss: 2.9674643883481622e-05, Learning Rate: 0.000140\n",
      "Epoch 22136/40000, Loss: 2.9293136321939528e-05, Learning Rate: 0.000140\n",
      "Epoch 22137/40000, Loss: 5.7932942581828684e-05, Learning Rate: 0.000140\n",
      "Epoch 22138/40000, Loss: 1.3693079381482676e-05, Learning Rate: 0.000140\n",
      "Epoch 22139/40000, Loss: 1.3694498193217441e-05, Learning Rate: 0.000140\n",
      "Epoch 22140/40000, Loss: 3.9699159970041364e-05, Learning Rate: 0.000140\n",
      "Epoch 22141/40000, Loss: 5.807647903566249e-05, Learning Rate: 0.000140\n",
      "Epoch 22142/40000, Loss: 3.971785554313101e-05, Learning Rate: 0.000140\n",
      "Epoch 22143/40000, Loss: 2.9157556127756834e-05, Learning Rate: 0.000140\n",
      "Epoch 22144/40000, Loss: 2.9138016543583944e-05, Learning Rate: 0.000140\n",
      "Epoch 22145/40000, Loss: 2.9221495424280874e-05, Learning Rate: 0.000140\n",
      "Epoch 22146/40000, Loss: 5.2216801122995093e-05, Learning Rate: 0.000140\n",
      "Epoch 22147/40000, Loss: 1.3809256415697746e-05, Learning Rate: 0.000140\n",
      "Epoch 22148/40000, Loss: 3.9634851418668404e-05, Learning Rate: 0.000140\n",
      "Epoch 22149/40000, Loss: 1.3687158570974134e-05, Learning Rate: 0.000140\n",
      "Epoch 22150/40000, Loss: 2.917917663580738e-05, Learning Rate: 0.000140\n",
      "Epoch 22151/40000, Loss: 3.9648708479944617e-05, Learning Rate: 0.000140\n",
      "Epoch 22152/40000, Loss: 1.3712244253838435e-05, Learning Rate: 0.000140\n",
      "Epoch 22153/40000, Loss: 1.3696173482458107e-05, Learning Rate: 0.000140\n",
      "Epoch 22154/40000, Loss: 3.354852378834039e-05, Learning Rate: 0.000140\n",
      "Epoch 22155/40000, Loss: 5.2253551984904334e-05, Learning Rate: 0.000140\n",
      "Epoch 22156/40000, Loss: 5.225135828368366e-05, Learning Rate: 0.000140\n",
      "Epoch 22157/40000, Loss: 5.800487633678131e-05, Learning Rate: 0.000140\n",
      "Epoch 22158/40000, Loss: 3.3673881262075156e-05, Learning Rate: 0.000140\n",
      "Epoch 22159/40000, Loss: 3.991669655079022e-05, Learning Rate: 0.000140\n",
      "Epoch 22160/40000, Loss: 5.834202966070734e-05, Learning Rate: 0.000140\n",
      "Epoch 22161/40000, Loss: 3.375515007064678e-05, Learning Rate: 0.000140\n",
      "Epoch 22162/40000, Loss: 5.262070772005245e-05, Learning Rate: 0.000140\n",
      "Epoch 22163/40000, Loss: 5.232346666161902e-05, Learning Rate: 0.000140\n",
      "Epoch 22164/40000, Loss: 3.9858823583927006e-05, Learning Rate: 0.000140\n",
      "Epoch 22165/40000, Loss: 5.8001765864901245e-05, Learning Rate: 0.000140\n",
      "Epoch 22166/40000, Loss: 1.3822117580275517e-05, Learning Rate: 0.000140\n",
      "Epoch 22167/40000, Loss: 5.2419869462028146e-05, Learning Rate: 0.000140\n",
      "Epoch 22168/40000, Loss: 3.994926737505011e-05, Learning Rate: 0.000140\n",
      "Epoch 22169/40000, Loss: 5.343399243429303e-05, Learning Rate: 0.000140\n",
      "Epoch 22170/40000, Loss: 2.9451814043568447e-05, Learning Rate: 0.000140\n",
      "Epoch 22171/40000, Loss: 1.383107610308798e-05, Learning Rate: 0.000140\n",
      "Epoch 22172/40000, Loss: 5.7977522374130785e-05, Learning Rate: 0.000140\n",
      "Epoch 22173/40000, Loss: 3.37003220920451e-05, Learning Rate: 0.000140\n",
      "Epoch 22174/40000, Loss: 2.9420680220937356e-05, Learning Rate: 0.000140\n",
      "Epoch 22175/40000, Loss: 3.3740154322003946e-05, Learning Rate: 0.000140\n",
      "Epoch 22176/40000, Loss: 2.943468643934466e-05, Learning Rate: 0.000140\n",
      "Epoch 22177/40000, Loss: 3.984019349445589e-05, Learning Rate: 0.000140\n",
      "Epoch 22178/40000, Loss: 5.813716052216478e-05, Learning Rate: 0.000140\n",
      "Epoch 22179/40000, Loss: 5.788643829873763e-05, Learning Rate: 0.000140\n",
      "Epoch 22180/40000, Loss: 3.994670259999111e-05, Learning Rate: 0.000140\n",
      "Epoch 22181/40000, Loss: 5.249113019090146e-05, Learning Rate: 0.000140\n",
      "Epoch 22182/40000, Loss: 4.088514106115326e-05, Learning Rate: 0.000140\n",
      "Epoch 22183/40000, Loss: 4.000187982455827e-05, Learning Rate: 0.000140\n",
      "Epoch 22184/40000, Loss: 3.401854701223783e-05, Learning Rate: 0.000140\n",
      "Epoch 22185/40000, Loss: 2.935855445684865e-05, Learning Rate: 0.000140\n",
      "Epoch 22186/40000, Loss: 5.8317855291534215e-05, Learning Rate: 0.000140\n",
      "Epoch 22187/40000, Loss: 1.3952205335954204e-05, Learning Rate: 0.000140\n",
      "Epoch 22188/40000, Loss: 3.380618363735266e-05, Learning Rate: 0.000140\n",
      "Epoch 22189/40000, Loss: 5.281147969071753e-05, Learning Rate: 0.000140\n",
      "Epoch 22190/40000, Loss: 5.804039028589614e-05, Learning Rate: 0.000140\n",
      "Epoch 22191/40000, Loss: 2.9335358703974634e-05, Learning Rate: 0.000139\n",
      "Epoch 22192/40000, Loss: 1.3802483408653643e-05, Learning Rate: 0.000139\n",
      "Epoch 22193/40000, Loss: 3.9869330066721886e-05, Learning Rate: 0.000139\n",
      "Epoch 22194/40000, Loss: 3.3595537388464436e-05, Learning Rate: 0.000139\n",
      "Epoch 22195/40000, Loss: 5.2393264923011884e-05, Learning Rate: 0.000139\n",
      "Epoch 22196/40000, Loss: 5.806599438074045e-05, Learning Rate: 0.000139\n",
      "Epoch 22197/40000, Loss: 4.0065748180495575e-05, Learning Rate: 0.000139\n",
      "Epoch 22198/40000, Loss: 5.810729999211617e-05, Learning Rate: 0.000139\n",
      "Epoch 22199/40000, Loss: 4.010322300018743e-05, Learning Rate: 0.000139\n",
      "Epoch 22200/40000, Loss: 5.883240737603046e-05, Learning Rate: 0.000139\n",
      "Epoch 22201/40000, Loss: 5.836530544911511e-05, Learning Rate: 0.000139\n",
      "Epoch 22202/40000, Loss: 4.0061553590931e-05, Learning Rate: 0.000139\n",
      "Epoch 22203/40000, Loss: 3.3892676583491266e-05, Learning Rate: 0.000139\n",
      "Epoch 22204/40000, Loss: 5.2916566346539184e-05, Learning Rate: 0.000139\n",
      "Epoch 22205/40000, Loss: 2.956424759759102e-05, Learning Rate: 0.000139\n",
      "Epoch 22206/40000, Loss: 4.0392158553004265e-05, Learning Rate: 0.000139\n",
      "Epoch 22207/40000, Loss: 3.069445301662199e-05, Learning Rate: 0.000139\n",
      "Epoch 22208/40000, Loss: 1.4542571079800837e-05, Learning Rate: 0.000139\n",
      "Epoch 22209/40000, Loss: 3.633677988545969e-05, Learning Rate: 0.000139\n",
      "Epoch 22210/40000, Loss: 1.6465857697767206e-05, Learning Rate: 0.000139\n",
      "Epoch 22211/40000, Loss: 3.873743480653502e-05, Learning Rate: 0.000139\n",
      "Epoch 22212/40000, Loss: 6.030849908711389e-05, Learning Rate: 0.000139\n",
      "Epoch 22213/40000, Loss: 6.0276543081272393e-05, Learning Rate: 0.000139\n",
      "Epoch 22214/40000, Loss: 4.173648267169483e-05, Learning Rate: 0.000139\n",
      "Epoch 22215/40000, Loss: 5.34536229679361e-05, Learning Rate: 0.000139\n",
      "Epoch 22216/40000, Loss: 5.323753794073127e-05, Learning Rate: 0.000139\n",
      "Epoch 22217/40000, Loss: 2.986250001413282e-05, Learning Rate: 0.000139\n",
      "Epoch 22218/40000, Loss: 4.067957343067974e-05, Learning Rate: 0.000139\n",
      "Epoch 22219/40000, Loss: 3.0270186471170746e-05, Learning Rate: 0.000139\n",
      "Epoch 22220/40000, Loss: 2.964285886264406e-05, Learning Rate: 0.000139\n",
      "Epoch 22221/40000, Loss: 3.371736602275632e-05, Learning Rate: 0.000139\n",
      "Epoch 22222/40000, Loss: 4.0122125938069075e-05, Learning Rate: 0.000139\n",
      "Epoch 22223/40000, Loss: 3.4041586332023144e-05, Learning Rate: 0.000139\n",
      "Epoch 22224/40000, Loss: 5.330467320163734e-05, Learning Rate: 0.000139\n",
      "Epoch 22225/40000, Loss: 1.4160878890834283e-05, Learning Rate: 0.000139\n",
      "Epoch 22226/40000, Loss: 5.3151383326621726e-05, Learning Rate: 0.000139\n",
      "Epoch 22227/40000, Loss: 2.9275955967023037e-05, Learning Rate: 0.000139\n",
      "Epoch 22228/40000, Loss: 5.591287845163606e-05, Learning Rate: 0.000139\n",
      "Epoch 22229/40000, Loss: 4.1142320696963e-05, Learning Rate: 0.000139\n",
      "Epoch 22230/40000, Loss: 6.659096834482625e-05, Learning Rate: 0.000139\n",
      "Epoch 22231/40000, Loss: 3.522900806274265e-05, Learning Rate: 0.000139\n",
      "Epoch 22232/40000, Loss: 3.4046450309688225e-05, Learning Rate: 0.000139\n",
      "Epoch 22233/40000, Loss: 3.0693787266500294e-05, Learning Rate: 0.000139\n",
      "Epoch 22234/40000, Loss: 1.5649997294531204e-05, Learning Rate: 0.000139\n",
      "Epoch 22235/40000, Loss: 3.423254383960739e-05, Learning Rate: 0.000139\n",
      "Epoch 22236/40000, Loss: 3.4861201129388064e-05, Learning Rate: 0.000139\n",
      "Epoch 22237/40000, Loss: 1.6798538126749918e-05, Learning Rate: 0.000139\n",
      "Epoch 22238/40000, Loss: 3.5409470001468435e-05, Learning Rate: 0.000139\n",
      "Epoch 22239/40000, Loss: 1.449852243240457e-05, Learning Rate: 0.000139\n",
      "Epoch 22240/40000, Loss: 3.0881757993483916e-05, Learning Rate: 0.000139\n",
      "Epoch 22241/40000, Loss: 6.12214716966264e-05, Learning Rate: 0.000139\n",
      "Epoch 22242/40000, Loss: 4.2056795791722834e-05, Learning Rate: 0.000139\n",
      "Epoch 22243/40000, Loss: 5.526622771867551e-05, Learning Rate: 0.000139\n",
      "Epoch 22244/40000, Loss: 4.075143806403503e-05, Learning Rate: 0.000139\n",
      "Epoch 22245/40000, Loss: 5.958220936008729e-05, Learning Rate: 0.000139\n",
      "Epoch 22246/40000, Loss: 1.4071425539441407e-05, Learning Rate: 0.000139\n",
      "Epoch 22247/40000, Loss: 5.7706201914697886e-05, Learning Rate: 0.000139\n",
      "Epoch 22248/40000, Loss: 5.8573252317728475e-05, Learning Rate: 0.000139\n",
      "Epoch 22249/40000, Loss: 4.059963976033032e-05, Learning Rate: 0.000139\n",
      "Epoch 22250/40000, Loss: 5.872304973308928e-05, Learning Rate: 0.000139\n",
      "Epoch 22251/40000, Loss: 1.4496117728413083e-05, Learning Rate: 0.000138\n",
      "Epoch 22252/40000, Loss: 3.005051621585153e-05, Learning Rate: 0.000138\n",
      "Epoch 22253/40000, Loss: 5.351209983928129e-05, Learning Rate: 0.000138\n",
      "Epoch 22254/40000, Loss: 4.106828055228107e-05, Learning Rate: 0.000138\n",
      "Epoch 22255/40000, Loss: 3.0015280572115444e-05, Learning Rate: 0.000138\n",
      "Epoch 22256/40000, Loss: 4.062904918100685e-05, Learning Rate: 0.000138\n",
      "Epoch 22257/40000, Loss: 3.421784640522674e-05, Learning Rate: 0.000138\n",
      "Epoch 22258/40000, Loss: 3.379545159987174e-05, Learning Rate: 0.000138\n",
      "Epoch 22259/40000, Loss: 5.257023076410405e-05, Learning Rate: 0.000138\n",
      "Epoch 22260/40000, Loss: 1.3974588000564836e-05, Learning Rate: 0.000138\n",
      "Epoch 22261/40000, Loss: 5.2774161304114386e-05, Learning Rate: 0.000138\n",
      "Epoch 22262/40000, Loss: 2.9521635951823555e-05, Learning Rate: 0.000138\n",
      "Epoch 22263/40000, Loss: 1.3903814760851674e-05, Learning Rate: 0.000138\n",
      "Epoch 22264/40000, Loss: 3.0466431780951098e-05, Learning Rate: 0.000138\n",
      "Epoch 22265/40000, Loss: 2.938730176538229e-05, Learning Rate: 0.000138\n",
      "Epoch 22266/40000, Loss: 5.232898911344819e-05, Learning Rate: 0.000138\n",
      "Epoch 22267/40000, Loss: 3.347928941366263e-05, Learning Rate: 0.000138\n",
      "Epoch 22268/40000, Loss: 3.97386284021195e-05, Learning Rate: 0.000138\n",
      "Epoch 22269/40000, Loss: 3.960598405683413e-05, Learning Rate: 0.000138\n",
      "Epoch 22270/40000, Loss: 3.343066418892704e-05, Learning Rate: 0.000138\n",
      "Epoch 22271/40000, Loss: 3.34350552293472e-05, Learning Rate: 0.000138\n",
      "Epoch 22272/40000, Loss: 2.9109549359418452e-05, Learning Rate: 0.000138\n",
      "Epoch 22273/40000, Loss: 5.838541983393952e-05, Learning Rate: 0.000138\n",
      "Epoch 22274/40000, Loss: 5.216517092776485e-05, Learning Rate: 0.000138\n",
      "Epoch 22275/40000, Loss: 3.9739650674164295e-05, Learning Rate: 0.000138\n",
      "Epoch 22276/40000, Loss: 1.369403071294073e-05, Learning Rate: 0.000138\n",
      "Epoch 22277/40000, Loss: 2.9137374440324493e-05, Learning Rate: 0.000138\n",
      "Epoch 22278/40000, Loss: 1.3765512449026573e-05, Learning Rate: 0.000138\n",
      "Epoch 22279/40000, Loss: 3.96456744056195e-05, Learning Rate: 0.000138\n",
      "Epoch 22280/40000, Loss: 2.9100881874910556e-05, Learning Rate: 0.000138\n",
      "Epoch 22281/40000, Loss: 5.8182369684800506e-05, Learning Rate: 0.000138\n",
      "Epoch 22282/40000, Loss: 2.9159646146581508e-05, Learning Rate: 0.000138\n",
      "Epoch 22283/40000, Loss: 5.832638271385804e-05, Learning Rate: 0.000138\n",
      "Epoch 22284/40000, Loss: 3.3523072488605976e-05, Learning Rate: 0.000138\n",
      "Epoch 22285/40000, Loss: 2.9224805984995328e-05, Learning Rate: 0.000138\n",
      "Epoch 22286/40000, Loss: 3.968145392718725e-05, Learning Rate: 0.000138\n",
      "Epoch 22287/40000, Loss: 5.218324440647848e-05, Learning Rate: 0.000138\n",
      "Epoch 22288/40000, Loss: 3.3369324228260666e-05, Learning Rate: 0.000138\n",
      "Epoch 22289/40000, Loss: 2.920931547123473e-05, Learning Rate: 0.000138\n",
      "Epoch 22290/40000, Loss: 5.2190782298566774e-05, Learning Rate: 0.000138\n",
      "Epoch 22291/40000, Loss: 1.3754910469288006e-05, Learning Rate: 0.000138\n",
      "Epoch 22292/40000, Loss: 3.964961797464639e-05, Learning Rate: 0.000138\n",
      "Epoch 22293/40000, Loss: 3.337087036925368e-05, Learning Rate: 0.000138\n",
      "Epoch 22294/40000, Loss: 2.915239929279778e-05, Learning Rate: 0.000138\n",
      "Epoch 22295/40000, Loss: 5.211220195633359e-05, Learning Rate: 0.000138\n",
      "Epoch 22296/40000, Loss: 1.3806058632326312e-05, Learning Rate: 0.000138\n",
      "Epoch 22297/40000, Loss: 5.2219314966350794e-05, Learning Rate: 0.000138\n",
      "Epoch 22298/40000, Loss: 5.2120405598543584e-05, Learning Rate: 0.000138\n",
      "Epoch 22299/40000, Loss: 1.3701745047001168e-05, Learning Rate: 0.000138\n",
      "Epoch 22300/40000, Loss: 1.3639717508340254e-05, Learning Rate: 0.000138\n",
      "Epoch 22301/40000, Loss: 3.340955663588829e-05, Learning Rate: 0.000138\n",
      "Epoch 22302/40000, Loss: 5.775882164016366e-05, Learning Rate: 0.000138\n",
      "Epoch 22303/40000, Loss: 5.769138806499541e-05, Learning Rate: 0.000138\n",
      "Epoch 22304/40000, Loss: 3.9582060708198696e-05, Learning Rate: 0.000138\n",
      "Epoch 22305/40000, Loss: 3.9600574382347986e-05, Learning Rate: 0.000138\n",
      "Epoch 22306/40000, Loss: 5.763325680163689e-05, Learning Rate: 0.000138\n",
      "Epoch 22307/40000, Loss: 2.908550413849298e-05, Learning Rate: 0.000138\n",
      "Epoch 22308/40000, Loss: 3.96439791074954e-05, Learning Rate: 0.000138\n",
      "Epoch 22309/40000, Loss: 3.9612408727407455e-05, Learning Rate: 0.000138\n",
      "Epoch 22310/40000, Loss: 3.961589027312584e-05, Learning Rate: 0.000138\n",
      "Epoch 22311/40000, Loss: 3.3358930522808805e-05, Learning Rate: 0.000137\n",
      "Epoch 22312/40000, Loss: 3.957744047511369e-05, Learning Rate: 0.000137\n",
      "Epoch 22313/40000, Loss: 5.211539246374741e-05, Learning Rate: 0.000137\n",
      "Epoch 22314/40000, Loss: 5.7914588978746906e-05, Learning Rate: 0.000137\n",
      "Epoch 22315/40000, Loss: 5.7932251365855336e-05, Learning Rate: 0.000137\n",
      "Epoch 22316/40000, Loss: 2.9066120987408794e-05, Learning Rate: 0.000137\n",
      "Epoch 22317/40000, Loss: 5.811296432511881e-05, Learning Rate: 0.000137\n",
      "Epoch 22318/40000, Loss: 3.344857395859435e-05, Learning Rate: 0.000137\n",
      "Epoch 22319/40000, Loss: 1.3831206160830334e-05, Learning Rate: 0.000137\n",
      "Epoch 22320/40000, Loss: 3.394500527065247e-05, Learning Rate: 0.000137\n",
      "Epoch 22321/40000, Loss: 2.9193486625445075e-05, Learning Rate: 0.000137\n",
      "Epoch 22322/40000, Loss: 3.362453935551457e-05, Learning Rate: 0.000137\n",
      "Epoch 22323/40000, Loss: 2.9268125217640772e-05, Learning Rate: 0.000137\n",
      "Epoch 22324/40000, Loss: 4.011288547189906e-05, Learning Rate: 0.000137\n",
      "Epoch 22325/40000, Loss: 2.9349621399887837e-05, Learning Rate: 0.000137\n",
      "Epoch 22326/40000, Loss: 5.955813321634196e-05, Learning Rate: 0.000137\n",
      "Epoch 22327/40000, Loss: 1.3791174751531798e-05, Learning Rate: 0.000137\n",
      "Epoch 22328/40000, Loss: 2.929320908151567e-05, Learning Rate: 0.000137\n",
      "Epoch 22329/40000, Loss: 1.4047112927073613e-05, Learning Rate: 0.000137\n",
      "Epoch 22330/40000, Loss: 3.987295713159256e-05, Learning Rate: 0.000137\n",
      "Epoch 22331/40000, Loss: 3.361588824191131e-05, Learning Rate: 0.000137\n",
      "Epoch 22332/40000, Loss: 2.9322032787604257e-05, Learning Rate: 0.000137\n",
      "Epoch 22333/40000, Loss: 3.4003482141997665e-05, Learning Rate: 0.000137\n",
      "Epoch 22334/40000, Loss: 5.236611104919575e-05, Learning Rate: 0.000137\n",
      "Epoch 22335/40000, Loss: 1.3923469850851689e-05, Learning Rate: 0.000137\n",
      "Epoch 22336/40000, Loss: 2.9147877285140567e-05, Learning Rate: 0.000137\n",
      "Epoch 22337/40000, Loss: 5.2406172471819445e-05, Learning Rate: 0.000137\n",
      "Epoch 22338/40000, Loss: 5.223555854172446e-05, Learning Rate: 0.000137\n",
      "Epoch 22339/40000, Loss: 3.992845449829474e-05, Learning Rate: 0.000137\n",
      "Epoch 22340/40000, Loss: 3.9986480260267854e-05, Learning Rate: 0.000137\n",
      "Epoch 22341/40000, Loss: 5.248573870630935e-05, Learning Rate: 0.000137\n",
      "Epoch 22342/40000, Loss: 2.9259215807542205e-05, Learning Rate: 0.000137\n",
      "Epoch 22343/40000, Loss: 3.350360202603042e-05, Learning Rate: 0.000137\n",
      "Epoch 22344/40000, Loss: 5.802707528346218e-05, Learning Rate: 0.000137\n",
      "Epoch 22345/40000, Loss: 3.358567846589722e-05, Learning Rate: 0.000137\n",
      "Epoch 22346/40000, Loss: 3.338691749377176e-05, Learning Rate: 0.000137\n",
      "Epoch 22347/40000, Loss: 2.921259874710813e-05, Learning Rate: 0.000137\n",
      "Epoch 22348/40000, Loss: 1.4234048649086617e-05, Learning Rate: 0.000137\n",
      "Epoch 22349/40000, Loss: 5.804950706078671e-05, Learning Rate: 0.000137\n",
      "Epoch 22350/40000, Loss: 3.98642587242648e-05, Learning Rate: 0.000137\n",
      "Epoch 22351/40000, Loss: 3.347630627104081e-05, Learning Rate: 0.000137\n",
      "Epoch 22352/40000, Loss: 5.7900168030755594e-05, Learning Rate: 0.000137\n",
      "Epoch 22353/40000, Loss: 2.926331217167899e-05, Learning Rate: 0.000137\n",
      "Epoch 22354/40000, Loss: 1.3855778888682835e-05, Learning Rate: 0.000137\n",
      "Epoch 22355/40000, Loss: 5.248638626653701e-05, Learning Rate: 0.000137\n",
      "Epoch 22356/40000, Loss: 1.4014954103913624e-05, Learning Rate: 0.000137\n",
      "Epoch 22357/40000, Loss: 5.293418507790193e-05, Learning Rate: 0.000137\n",
      "Epoch 22358/40000, Loss: 1.4306539014796726e-05, Learning Rate: 0.000137\n",
      "Epoch 22359/40000, Loss: 1.398370568495011e-05, Learning Rate: 0.000137\n",
      "Epoch 22360/40000, Loss: 5.7897992519428954e-05, Learning Rate: 0.000137\n",
      "Epoch 22361/40000, Loss: 3.354170257807709e-05, Learning Rate: 0.000137\n",
      "Epoch 22362/40000, Loss: 5.785203757113777e-05, Learning Rate: 0.000137\n",
      "Epoch 22363/40000, Loss: 5.2261857490520924e-05, Learning Rate: 0.000137\n",
      "Epoch 22364/40000, Loss: 3.987027957919054e-05, Learning Rate: 0.000137\n",
      "Epoch 22365/40000, Loss: 3.349369580973871e-05, Learning Rate: 0.000137\n",
      "Epoch 22366/40000, Loss: 4.003309732070193e-05, Learning Rate: 0.000137\n",
      "Epoch 22367/40000, Loss: 2.9144726795493625e-05, Learning Rate: 0.000137\n",
      "Epoch 22368/40000, Loss: 1.3678638424607925e-05, Learning Rate: 0.000137\n",
      "Epoch 22369/40000, Loss: 5.780451465398073e-05, Learning Rate: 0.000137\n",
      "Epoch 22370/40000, Loss: 3.987860327470116e-05, Learning Rate: 0.000137\n",
      "Epoch 22371/40000, Loss: 5.781231084256433e-05, Learning Rate: 0.000137\n",
      "Epoch 22372/40000, Loss: 5.778701597591862e-05, Learning Rate: 0.000136\n",
      "Epoch 22373/40000, Loss: 5.7840887166094035e-05, Learning Rate: 0.000136\n",
      "Epoch 22374/40000, Loss: 5.2129660616628826e-05, Learning Rate: 0.000136\n",
      "Epoch 22375/40000, Loss: 1.3831409887643531e-05, Learning Rate: 0.000136\n",
      "Epoch 22376/40000, Loss: 5.7820885558612645e-05, Learning Rate: 0.000136\n",
      "Epoch 22377/40000, Loss: 1.3735876564169303e-05, Learning Rate: 0.000136\n",
      "Epoch 22378/40000, Loss: 3.339091927045956e-05, Learning Rate: 0.000136\n",
      "Epoch 22379/40000, Loss: 1.3804552509100176e-05, Learning Rate: 0.000136\n",
      "Epoch 22380/40000, Loss: 5.2213548769941553e-05, Learning Rate: 0.000136\n",
      "Epoch 22381/40000, Loss: 5.792705269414e-05, Learning Rate: 0.000136\n",
      "Epoch 22382/40000, Loss: 5.822374805575237e-05, Learning Rate: 0.000136\n",
      "Epoch 22383/40000, Loss: 5.242113184067421e-05, Learning Rate: 0.000136\n",
      "Epoch 22384/40000, Loss: 3.9892423956189305e-05, Learning Rate: 0.000136\n",
      "Epoch 22385/40000, Loss: 3.356693923706189e-05, Learning Rate: 0.000136\n",
      "Epoch 22386/40000, Loss: 3.360083792358637e-05, Learning Rate: 0.000136\n",
      "Epoch 22387/40000, Loss: 5.795187098556198e-05, Learning Rate: 0.000136\n",
      "Epoch 22388/40000, Loss: 2.9211170840426348e-05, Learning Rate: 0.000136\n",
      "Epoch 22389/40000, Loss: 1.3862149899068754e-05, Learning Rate: 0.000136\n",
      "Epoch 22390/40000, Loss: 1.383396920573432e-05, Learning Rate: 0.000136\n",
      "Epoch 22391/40000, Loss: 1.3792213394481223e-05, Learning Rate: 0.000136\n",
      "Epoch 22392/40000, Loss: 3.98889860662166e-05, Learning Rate: 0.000136\n",
      "Epoch 22393/40000, Loss: 5.2210270951036364e-05, Learning Rate: 0.000136\n",
      "Epoch 22394/40000, Loss: 5.793657692265697e-05, Learning Rate: 0.000136\n",
      "Epoch 22395/40000, Loss: 3.98224510718137e-05, Learning Rate: 0.000136\n",
      "Epoch 22396/40000, Loss: 5.2317445806693286e-05, Learning Rate: 0.000136\n",
      "Epoch 22397/40000, Loss: 2.911048795795068e-05, Learning Rate: 0.000136\n",
      "Epoch 22398/40000, Loss: 3.991107223555446e-05, Learning Rate: 0.000136\n",
      "Epoch 22399/40000, Loss: 2.953517650894355e-05, Learning Rate: 0.000136\n",
      "Epoch 22400/40000, Loss: 1.3858190868631937e-05, Learning Rate: 0.000136\n",
      "Epoch 22401/40000, Loss: 2.9174851079005748e-05, Learning Rate: 0.000136\n",
      "Epoch 22402/40000, Loss: 5.252257687970996e-05, Learning Rate: 0.000136\n",
      "Epoch 22403/40000, Loss: 5.791541843791492e-05, Learning Rate: 0.000136\n",
      "Epoch 22404/40000, Loss: 5.327226608642377e-05, Learning Rate: 0.000136\n",
      "Epoch 22405/40000, Loss: 5.874252019566484e-05, Learning Rate: 0.000136\n",
      "Epoch 22406/40000, Loss: 3.402546644792892e-05, Learning Rate: 0.000136\n",
      "Epoch 22407/40000, Loss: 5.314060763339512e-05, Learning Rate: 0.000136\n",
      "Epoch 22408/40000, Loss: 1.4395547623280436e-05, Learning Rate: 0.000136\n",
      "Epoch 22409/40000, Loss: 6.325313006527722e-05, Learning Rate: 0.000136\n",
      "Epoch 22410/40000, Loss: 1.579207309987396e-05, Learning Rate: 0.000136\n",
      "Epoch 22411/40000, Loss: 3.0317087293951772e-05, Learning Rate: 0.000136\n",
      "Epoch 22412/40000, Loss: 5.502147178049199e-05, Learning Rate: 0.000136\n",
      "Epoch 22413/40000, Loss: 5.870666791452095e-05, Learning Rate: 0.000136\n",
      "Epoch 22414/40000, Loss: 4.213605643599294e-05, Learning Rate: 0.000136\n",
      "Epoch 22415/40000, Loss: 4.4524858822114766e-05, Learning Rate: 0.000136\n",
      "Epoch 22416/40000, Loss: 1.5673656889703125e-05, Learning Rate: 0.000136\n",
      "Epoch 22417/40000, Loss: 4.6053624828346074e-05, Learning Rate: 0.000136\n",
      "Epoch 22418/40000, Loss: 3.1232197216013446e-05, Learning Rate: 0.000136\n",
      "Epoch 22419/40000, Loss: 5.5265569244511425e-05, Learning Rate: 0.000136\n",
      "Epoch 22420/40000, Loss: 4.129428634769283e-05, Learning Rate: 0.000136\n",
      "Epoch 22421/40000, Loss: 4.1546238207956776e-05, Learning Rate: 0.000136\n",
      "Epoch 22422/40000, Loss: 3.385029413038865e-05, Learning Rate: 0.000136\n",
      "Epoch 22423/40000, Loss: 4.032329161418602e-05, Learning Rate: 0.000136\n",
      "Epoch 22424/40000, Loss: 1.5066768355609383e-05, Learning Rate: 0.000136\n",
      "Epoch 22425/40000, Loss: 5.901308759348467e-05, Learning Rate: 0.000136\n",
      "Epoch 22426/40000, Loss: 5.863690239493735e-05, Learning Rate: 0.000136\n",
      "Epoch 22427/40000, Loss: 5.353594315238297e-05, Learning Rate: 0.000136\n",
      "Epoch 22428/40000, Loss: 2.952082286356017e-05, Learning Rate: 0.000136\n",
      "Epoch 22429/40000, Loss: 4.136221468797885e-05, Learning Rate: 0.000136\n",
      "Epoch 22430/40000, Loss: 5.321452408679761e-05, Learning Rate: 0.000136\n",
      "Epoch 22431/40000, Loss: 5.833243631059304e-05, Learning Rate: 0.000136\n",
      "Epoch 22432/40000, Loss: 5.2339037210913375e-05, Learning Rate: 0.000136\n",
      "Epoch 22433/40000, Loss: 4.014682781416923e-05, Learning Rate: 0.000135\n",
      "Epoch 22434/40000, Loss: 2.9357699531828985e-05, Learning Rate: 0.000135\n",
      "Epoch 22435/40000, Loss: 5.796309415018186e-05, Learning Rate: 0.000135\n",
      "Epoch 22436/40000, Loss: 3.975822983193211e-05, Learning Rate: 0.000135\n",
      "Epoch 22437/40000, Loss: 3.982853013440035e-05, Learning Rate: 0.000135\n",
      "Epoch 22438/40000, Loss: 2.91632331936853e-05, Learning Rate: 0.000135\n",
      "Epoch 22439/40000, Loss: 5.811530718347058e-05, Learning Rate: 0.000135\n",
      "Epoch 22440/40000, Loss: 4.006711969850585e-05, Learning Rate: 0.000135\n",
      "Epoch 22441/40000, Loss: 4.003007052233443e-05, Learning Rate: 0.000135\n",
      "Epoch 22442/40000, Loss: 3.346320227137767e-05, Learning Rate: 0.000135\n",
      "Epoch 22443/40000, Loss: 4.144369449932128e-05, Learning Rate: 0.000135\n",
      "Epoch 22444/40000, Loss: 5.8037192502524704e-05, Learning Rate: 0.000135\n",
      "Epoch 22445/40000, Loss: 2.9304508643690497e-05, Learning Rate: 0.000135\n",
      "Epoch 22446/40000, Loss: 5.8836340031120926e-05, Learning Rate: 0.000135\n",
      "Epoch 22447/40000, Loss: 3.403931259526871e-05, Learning Rate: 0.000135\n",
      "Epoch 22448/40000, Loss: 5.2532664994942024e-05, Learning Rate: 0.000135\n",
      "Epoch 22449/40000, Loss: 3.4923468774650246e-05, Learning Rate: 0.000135\n",
      "Epoch 22450/40000, Loss: 1.4228293366613798e-05, Learning Rate: 0.000135\n",
      "Epoch 22451/40000, Loss: 5.818506542709656e-05, Learning Rate: 0.000135\n",
      "Epoch 22452/40000, Loss: 5.2477138524409384e-05, Learning Rate: 0.000135\n",
      "Epoch 22453/40000, Loss: 4.079670907231048e-05, Learning Rate: 0.000135\n",
      "Epoch 22454/40000, Loss: 5.31337755091954e-05, Learning Rate: 0.000135\n",
      "Epoch 22455/40000, Loss: 2.9333516067708842e-05, Learning Rate: 0.000135\n",
      "Epoch 22456/40000, Loss: 5.424246046459302e-05, Learning Rate: 0.000135\n",
      "Epoch 22457/40000, Loss: 4.085855471203104e-05, Learning Rate: 0.000135\n",
      "Epoch 22458/40000, Loss: 5.802359009976499e-05, Learning Rate: 0.000135\n",
      "Epoch 22459/40000, Loss: 1.418997908331221e-05, Learning Rate: 0.000135\n",
      "Epoch 22460/40000, Loss: 1.3892580682295375e-05, Learning Rate: 0.000135\n",
      "Epoch 22461/40000, Loss: 3.340297917020507e-05, Learning Rate: 0.000135\n",
      "Epoch 22462/40000, Loss: 5.210021117818542e-05, Learning Rate: 0.000135\n",
      "Epoch 22463/40000, Loss: 5.820036676595919e-05, Learning Rate: 0.000135\n",
      "Epoch 22464/40000, Loss: 5.804632746730931e-05, Learning Rate: 0.000135\n",
      "Epoch 22465/40000, Loss: 5.2209332352504134e-05, Learning Rate: 0.000135\n",
      "Epoch 22466/40000, Loss: 5.797758785774931e-05, Learning Rate: 0.000135\n",
      "Epoch 22467/40000, Loss: 5.207091453485191e-05, Learning Rate: 0.000135\n",
      "Epoch 22468/40000, Loss: 5.802695159218274e-05, Learning Rate: 0.000135\n",
      "Epoch 22469/40000, Loss: 2.8953751098015346e-05, Learning Rate: 0.000135\n",
      "Epoch 22470/40000, Loss: 3.961943002650514e-05, Learning Rate: 0.000135\n",
      "Epoch 22471/40000, Loss: 5.7847868447424844e-05, Learning Rate: 0.000135\n",
      "Epoch 22472/40000, Loss: 1.3763113202003296e-05, Learning Rate: 0.000135\n",
      "Epoch 22473/40000, Loss: 3.9670387195656076e-05, Learning Rate: 0.000135\n",
      "Epoch 22474/40000, Loss: 3.9479800761910155e-05, Learning Rate: 0.000135\n",
      "Epoch 22475/40000, Loss: 5.779734055977315e-05, Learning Rate: 0.000135\n",
      "Epoch 22476/40000, Loss: 5.773751763626933e-05, Learning Rate: 0.000135\n",
      "Epoch 22477/40000, Loss: 3.962344635510817e-05, Learning Rate: 0.000135\n",
      "Epoch 22478/40000, Loss: 5.1956198149127886e-05, Learning Rate: 0.000135\n",
      "Epoch 22479/40000, Loss: 2.896243313443847e-05, Learning Rate: 0.000135\n",
      "Epoch 22480/40000, Loss: 2.8928583560627885e-05, Learning Rate: 0.000135\n",
      "Epoch 22481/40000, Loss: 3.324947101646103e-05, Learning Rate: 0.000135\n",
      "Epoch 22482/40000, Loss: 1.3765824405709282e-05, Learning Rate: 0.000135\n",
      "Epoch 22483/40000, Loss: 1.3682988537766505e-05, Learning Rate: 0.000135\n",
      "Epoch 22484/40000, Loss: 5.197780774324201e-05, Learning Rate: 0.000135\n",
      "Epoch 22485/40000, Loss: 5.2022307500010356e-05, Learning Rate: 0.000135\n",
      "Epoch 22486/40000, Loss: 5.763748777098954e-05, Learning Rate: 0.000135\n",
      "Epoch 22487/40000, Loss: 5.768200571765192e-05, Learning Rate: 0.000135\n",
      "Epoch 22488/40000, Loss: 5.765310925198719e-05, Learning Rate: 0.000135\n",
      "Epoch 22489/40000, Loss: 3.327130252728239e-05, Learning Rate: 0.000135\n",
      "Epoch 22490/40000, Loss: 5.200128362048417e-05, Learning Rate: 0.000135\n",
      "Epoch 22491/40000, Loss: 2.8961507268832065e-05, Learning Rate: 0.000135\n",
      "Epoch 22492/40000, Loss: 3.9562459278386086e-05, Learning Rate: 0.000135\n",
      "Epoch 22493/40000, Loss: 2.905241126427427e-05, Learning Rate: 0.000135\n",
      "Epoch 22494/40000, Loss: 3.32299096044153e-05, Learning Rate: 0.000135\n",
      "Epoch 22495/40000, Loss: 3.310935426270589e-05, Learning Rate: 0.000134\n",
      "Epoch 22496/40000, Loss: 3.949719393858686e-05, Learning Rate: 0.000134\n",
      "Epoch 22497/40000, Loss: 5.197721839067526e-05, Learning Rate: 0.000134\n",
      "Epoch 22498/40000, Loss: 3.9575981645612046e-05, Learning Rate: 0.000134\n",
      "Epoch 22499/40000, Loss: 2.8955884772585705e-05, Learning Rate: 0.000134\n",
      "Epoch 22500/40000, Loss: 5.773296288680285e-05, Learning Rate: 0.000134\n",
      "Epoch 22501/40000, Loss: 5.1968694606330246e-05, Learning Rate: 0.000134\n",
      "Epoch 22502/40000, Loss: 5.772418444394134e-05, Learning Rate: 0.000134\n",
      "Epoch 22503/40000, Loss: 1.3749682693742216e-05, Learning Rate: 0.000134\n",
      "Epoch 22504/40000, Loss: 3.319215102237649e-05, Learning Rate: 0.000134\n",
      "Epoch 22505/40000, Loss: 3.9596234273631126e-05, Learning Rate: 0.000134\n",
      "Epoch 22506/40000, Loss: 5.200026134843938e-05, Learning Rate: 0.000134\n",
      "Epoch 22507/40000, Loss: 5.777661863248795e-05, Learning Rate: 0.000134\n",
      "Epoch 22508/40000, Loss: 3.327652666484937e-05, Learning Rate: 0.000134\n",
      "Epoch 22509/40000, Loss: 3.9729842683300376e-05, Learning Rate: 0.000134\n",
      "Epoch 22510/40000, Loss: 3.9526647015009075e-05, Learning Rate: 0.000134\n",
      "Epoch 22511/40000, Loss: 2.8911725166835822e-05, Learning Rate: 0.000134\n",
      "Epoch 22512/40000, Loss: 1.3773172213404905e-05, Learning Rate: 0.000134\n",
      "Epoch 22513/40000, Loss: 3.332436608616263e-05, Learning Rate: 0.000134\n",
      "Epoch 22514/40000, Loss: 3.969911631429568e-05, Learning Rate: 0.000134\n",
      "Epoch 22515/40000, Loss: 5.2085502829868346e-05, Learning Rate: 0.000134\n",
      "Epoch 22516/40000, Loss: 4.003784488304518e-05, Learning Rate: 0.000134\n",
      "Epoch 22517/40000, Loss: 3.32800809701439e-05, Learning Rate: 0.000134\n",
      "Epoch 22518/40000, Loss: 2.904433131334372e-05, Learning Rate: 0.000134\n",
      "Epoch 22519/40000, Loss: 5.7648547226563096e-05, Learning Rate: 0.000134\n",
      "Epoch 22520/40000, Loss: 1.382451500830939e-05, Learning Rate: 0.000134\n",
      "Epoch 22521/40000, Loss: 1.3696719179279171e-05, Learning Rate: 0.000134\n",
      "Epoch 22522/40000, Loss: 5.21567344549112e-05, Learning Rate: 0.000134\n",
      "Epoch 22523/40000, Loss: 5.201537351240404e-05, Learning Rate: 0.000134\n",
      "Epoch 22524/40000, Loss: 3.317499067634344e-05, Learning Rate: 0.000134\n",
      "Epoch 22525/40000, Loss: 5.219555532676168e-05, Learning Rate: 0.000134\n",
      "Epoch 22526/40000, Loss: 3.342931449878961e-05, Learning Rate: 0.000134\n",
      "Epoch 22527/40000, Loss: 5.2300598326837644e-05, Learning Rate: 0.000134\n",
      "Epoch 22528/40000, Loss: 2.9014610845479183e-05, Learning Rate: 0.000134\n",
      "Epoch 22529/40000, Loss: 5.210467497818172e-05, Learning Rate: 0.000134\n",
      "Epoch 22530/40000, Loss: 1.3718738955503795e-05, Learning Rate: 0.000134\n",
      "Epoch 22531/40000, Loss: 5.780654100817628e-05, Learning Rate: 0.000134\n",
      "Epoch 22532/40000, Loss: 3.977117012254894e-05, Learning Rate: 0.000134\n",
      "Epoch 22533/40000, Loss: 3.326724618091248e-05, Learning Rate: 0.000134\n",
      "Epoch 22534/40000, Loss: 5.79655788897071e-05, Learning Rate: 0.000134\n",
      "Epoch 22535/40000, Loss: 3.335527071612887e-05, Learning Rate: 0.000134\n",
      "Epoch 22536/40000, Loss: 2.902788583014626e-05, Learning Rate: 0.000134\n",
      "Epoch 22537/40000, Loss: 3.97380099457223e-05, Learning Rate: 0.000134\n",
      "Epoch 22538/40000, Loss: 5.783748929388821e-05, Learning Rate: 0.000134\n",
      "Epoch 22539/40000, Loss: 1.3794121514365543e-05, Learning Rate: 0.000134\n",
      "Epoch 22540/40000, Loss: 1.370288464386249e-05, Learning Rate: 0.000134\n",
      "Epoch 22541/40000, Loss: 5.21314432262443e-05, Learning Rate: 0.000134\n",
      "Epoch 22542/40000, Loss: 3.323717464809306e-05, Learning Rate: 0.000134\n",
      "Epoch 22543/40000, Loss: 1.368348330288427e-05, Learning Rate: 0.000134\n",
      "Epoch 22544/40000, Loss: 3.986219235230237e-05, Learning Rate: 0.000134\n",
      "Epoch 22545/40000, Loss: 5.7972432841779664e-05, Learning Rate: 0.000134\n",
      "Epoch 22546/40000, Loss: 3.332386040710844e-05, Learning Rate: 0.000134\n",
      "Epoch 22547/40000, Loss: 4.008969335700385e-05, Learning Rate: 0.000134\n",
      "Epoch 22548/40000, Loss: 5.216769932303578e-05, Learning Rate: 0.000134\n",
      "Epoch 22549/40000, Loss: 3.338287569931708e-05, Learning Rate: 0.000134\n",
      "Epoch 22550/40000, Loss: 5.7839850342134014e-05, Learning Rate: 0.000134\n",
      "Epoch 22551/40000, Loss: 1.3759197827312164e-05, Learning Rate: 0.000134\n",
      "Epoch 22552/40000, Loss: 3.965779615100473e-05, Learning Rate: 0.000134\n",
      "Epoch 22553/40000, Loss: 1.3779676919511985e-05, Learning Rate: 0.000134\n",
      "Epoch 22554/40000, Loss: 3.979849861934781e-05, Learning Rate: 0.000134\n",
      "Epoch 22555/40000, Loss: 1.4621392438129988e-05, Learning Rate: 0.000134\n",
      "Epoch 22556/40000, Loss: 5.23847047588788e-05, Learning Rate: 0.000134\n",
      "Epoch 22557/40000, Loss: 5.252601840766147e-05, Learning Rate: 0.000133\n",
      "Epoch 22558/40000, Loss: 2.9049628210486844e-05, Learning Rate: 0.000133\n",
      "Epoch 22559/40000, Loss: 3.988161188317463e-05, Learning Rate: 0.000133\n",
      "Epoch 22560/40000, Loss: 5.7921370171243325e-05, Learning Rate: 0.000133\n",
      "Epoch 22561/40000, Loss: 1.4760948943148833e-05, Learning Rate: 0.000133\n",
      "Epoch 22562/40000, Loss: 3.3941774745471776e-05, Learning Rate: 0.000133\n",
      "Epoch 22563/40000, Loss: 1.7551667042425834e-05, Learning Rate: 0.000133\n",
      "Epoch 22564/40000, Loss: 5.2727806178154424e-05, Learning Rate: 0.000133\n",
      "Epoch 22565/40000, Loss: 5.802384112030268e-05, Learning Rate: 0.000133\n",
      "Epoch 22566/40000, Loss: 1.440603045921307e-05, Learning Rate: 0.000133\n",
      "Epoch 22567/40000, Loss: 3.349446342326701e-05, Learning Rate: 0.000133\n",
      "Epoch 22568/40000, Loss: 3.990890400018543e-05, Learning Rate: 0.000133\n",
      "Epoch 22569/40000, Loss: 5.8319808886153623e-05, Learning Rate: 0.000133\n",
      "Epoch 22570/40000, Loss: 2.9267182981129736e-05, Learning Rate: 0.000133\n",
      "Epoch 22571/40000, Loss: 5.2302573749329895e-05, Learning Rate: 0.000133\n",
      "Epoch 22572/40000, Loss: 5.25970317539759e-05, Learning Rate: 0.000133\n",
      "Epoch 22573/40000, Loss: 4.0125614759745076e-05, Learning Rate: 0.000133\n",
      "Epoch 22574/40000, Loss: 1.3949482308817096e-05, Learning Rate: 0.000133\n",
      "Epoch 22575/40000, Loss: 5.847951251780614e-05, Learning Rate: 0.000133\n",
      "Epoch 22576/40000, Loss: 4.0055088902590796e-05, Learning Rate: 0.000133\n",
      "Epoch 22577/40000, Loss: 3.995965016656555e-05, Learning Rate: 0.000133\n",
      "Epoch 22578/40000, Loss: 3.994220242020674e-05, Learning Rate: 0.000133\n",
      "Epoch 22579/40000, Loss: 1.4167736480885651e-05, Learning Rate: 0.000133\n",
      "Epoch 22580/40000, Loss: 5.244412750471383e-05, Learning Rate: 0.000133\n",
      "Epoch 22581/40000, Loss: 1.4102490240475163e-05, Learning Rate: 0.000133\n",
      "Epoch 22582/40000, Loss: 1.3788336218567565e-05, Learning Rate: 0.000133\n",
      "Epoch 22583/40000, Loss: 3.365464726812206e-05, Learning Rate: 0.000133\n",
      "Epoch 22584/40000, Loss: 4.050838833791204e-05, Learning Rate: 0.000133\n",
      "Epoch 22585/40000, Loss: 1.4148019545245916e-05, Learning Rate: 0.000133\n",
      "Epoch 22586/40000, Loss: 2.9215974791441113e-05, Learning Rate: 0.000133\n",
      "Epoch 22587/40000, Loss: 3.3743093808880076e-05, Learning Rate: 0.000133\n",
      "Epoch 22588/40000, Loss: 5.9890098782489076e-05, Learning Rate: 0.000133\n",
      "Epoch 22589/40000, Loss: 4.108247230760753e-05, Learning Rate: 0.000133\n",
      "Epoch 22590/40000, Loss: 3.474325058050454e-05, Learning Rate: 0.000133\n",
      "Epoch 22591/40000, Loss: 3.11026960844174e-05, Learning Rate: 0.000133\n",
      "Epoch 22592/40000, Loss: 4.176933362032287e-05, Learning Rate: 0.000133\n",
      "Epoch 22593/40000, Loss: 3.1094525184016675e-05, Learning Rate: 0.000133\n",
      "Epoch 22594/40000, Loss: 4.698813063441776e-05, Learning Rate: 0.000133\n",
      "Epoch 22595/40000, Loss: 4.170592728769407e-05, Learning Rate: 0.000133\n",
      "Epoch 22596/40000, Loss: 3.036312773474492e-05, Learning Rate: 0.000133\n",
      "Epoch 22597/40000, Loss: 2.964431041618809e-05, Learning Rate: 0.000133\n",
      "Epoch 22598/40000, Loss: 1.44453915709164e-05, Learning Rate: 0.000133\n",
      "Epoch 22599/40000, Loss: 4.024332884000614e-05, Learning Rate: 0.000133\n",
      "Epoch 22600/40000, Loss: 5.300904012983665e-05, Learning Rate: 0.000133\n",
      "Epoch 22601/40000, Loss: 3.3519121643621475e-05, Learning Rate: 0.000133\n",
      "Epoch 22602/40000, Loss: 5.805314867757261e-05, Learning Rate: 0.000133\n",
      "Epoch 22603/40000, Loss: 1.3803413821733557e-05, Learning Rate: 0.000133\n",
      "Epoch 22604/40000, Loss: 1.38339592012926e-05, Learning Rate: 0.000133\n",
      "Epoch 22605/40000, Loss: 5.804590182378888e-05, Learning Rate: 0.000133\n",
      "Epoch 22606/40000, Loss: 5.224140113568865e-05, Learning Rate: 0.000133\n",
      "Epoch 22607/40000, Loss: 1.4015216947882436e-05, Learning Rate: 0.000133\n",
      "Epoch 22608/40000, Loss: 1.3812149518344086e-05, Learning Rate: 0.000133\n",
      "Epoch 22609/40000, Loss: 2.8921769626322202e-05, Learning Rate: 0.000133\n",
      "Epoch 22610/40000, Loss: 1.4035934327694122e-05, Learning Rate: 0.000133\n",
      "Epoch 22611/40000, Loss: 5.223139305599034e-05, Learning Rate: 0.000133\n",
      "Epoch 22612/40000, Loss: 3.32297568093054e-05, Learning Rate: 0.000133\n",
      "Epoch 22613/40000, Loss: 5.28209529875312e-05, Learning Rate: 0.000133\n",
      "Epoch 22614/40000, Loss: 3.332239793962799e-05, Learning Rate: 0.000133\n",
      "Epoch 22615/40000, Loss: 2.8851844035671093e-05, Learning Rate: 0.000133\n",
      "Epoch 22616/40000, Loss: 1.3866541848983616e-05, Learning Rate: 0.000133\n",
      "Epoch 22617/40000, Loss: 2.88474875560496e-05, Learning Rate: 0.000133\n",
      "Epoch 22618/40000, Loss: 3.313032357254997e-05, Learning Rate: 0.000133\n",
      "Epoch 22619/40000, Loss: 3.9639777241973206e-05, Learning Rate: 0.000133\n",
      "Epoch 22620/40000, Loss: 3.95819079130888e-05, Learning Rate: 0.000132\n",
      "Epoch 22621/40000, Loss: 2.8906411898788065e-05, Learning Rate: 0.000132\n",
      "Epoch 22622/40000, Loss: 5.786975452792831e-05, Learning Rate: 0.000132\n",
      "Epoch 22623/40000, Loss: 5.2040883019799367e-05, Learning Rate: 0.000132\n",
      "Epoch 22624/40000, Loss: 1.3686006241186988e-05, Learning Rate: 0.000132\n",
      "Epoch 22625/40000, Loss: 5.769821291323751e-05, Learning Rate: 0.000132\n",
      "Epoch 22626/40000, Loss: 3.952904080506414e-05, Learning Rate: 0.000132\n",
      "Epoch 22627/40000, Loss: 1.3613823284686077e-05, Learning Rate: 0.000132\n",
      "Epoch 22628/40000, Loss: 1.3696263522433583e-05, Learning Rate: 0.000132\n",
      "Epoch 22629/40000, Loss: 5.7938534155255184e-05, Learning Rate: 0.000132\n",
      "Epoch 22630/40000, Loss: 1.3700344425160438e-05, Learning Rate: 0.000132\n",
      "Epoch 22631/40000, Loss: 5.198159487918019e-05, Learning Rate: 0.000132\n",
      "Epoch 22632/40000, Loss: 3.962036862503737e-05, Learning Rate: 0.000132\n",
      "Epoch 22633/40000, Loss: 2.882161788875237e-05, Learning Rate: 0.000132\n",
      "Epoch 22634/40000, Loss: 3.9611495594726875e-05, Learning Rate: 0.000132\n",
      "Epoch 22635/40000, Loss: 1.3712451618630439e-05, Learning Rate: 0.000132\n",
      "Epoch 22636/40000, Loss: 3.309876046841964e-05, Learning Rate: 0.000132\n",
      "Epoch 22637/40000, Loss: 1.373469058307819e-05, Learning Rate: 0.000132\n",
      "Epoch 22638/40000, Loss: 1.3675418813363649e-05, Learning Rate: 0.000132\n",
      "Epoch 22639/40000, Loss: 3.956634827773087e-05, Learning Rate: 0.000132\n",
      "Epoch 22640/40000, Loss: 3.309991006972268e-05, Learning Rate: 0.000132\n",
      "Epoch 22641/40000, Loss: 5.1884446293115616e-05, Learning Rate: 0.000132\n",
      "Epoch 22642/40000, Loss: 2.8864165869890712e-05, Learning Rate: 0.000132\n",
      "Epoch 22643/40000, Loss: 2.882581793528516e-05, Learning Rate: 0.000132\n",
      "Epoch 22644/40000, Loss: 1.3689709703612607e-05, Learning Rate: 0.000132\n",
      "Epoch 22645/40000, Loss: 5.180637162993662e-05, Learning Rate: 0.000132\n",
      "Epoch 22646/40000, Loss: 3.3029507903847843e-05, Learning Rate: 0.000132\n",
      "Epoch 22647/40000, Loss: 2.8827667847508565e-05, Learning Rate: 0.000132\n",
      "Epoch 22648/40000, Loss: 5.1884147978853434e-05, Learning Rate: 0.000132\n",
      "Epoch 22649/40000, Loss: 5.1930732297478244e-05, Learning Rate: 0.000132\n",
      "Epoch 22650/40000, Loss: 3.3202766644535586e-05, Learning Rate: 0.000132\n",
      "Epoch 22651/40000, Loss: 2.8859800295322202e-05, Learning Rate: 0.000132\n",
      "Epoch 22652/40000, Loss: 2.8899075914523564e-05, Learning Rate: 0.000132\n",
      "Epoch 22653/40000, Loss: 5.2129147661617026e-05, Learning Rate: 0.000132\n",
      "Epoch 22654/40000, Loss: 1.4004381228005514e-05, Learning Rate: 0.000132\n",
      "Epoch 22655/40000, Loss: 5.7969642512034625e-05, Learning Rate: 0.000132\n",
      "Epoch 22656/40000, Loss: 3.998858664999716e-05, Learning Rate: 0.000132\n",
      "Epoch 22657/40000, Loss: 1.3808395124215167e-05, Learning Rate: 0.000132\n",
      "Epoch 22658/40000, Loss: 4.047832408105023e-05, Learning Rate: 0.000132\n",
      "Epoch 22659/40000, Loss: 1.4244838894228451e-05, Learning Rate: 0.000132\n",
      "Epoch 22660/40000, Loss: 1.3840024621458724e-05, Learning Rate: 0.000132\n",
      "Epoch 22661/40000, Loss: 5.238967787590809e-05, Learning Rate: 0.000132\n",
      "Epoch 22662/40000, Loss: 4.022953362436965e-05, Learning Rate: 0.000132\n",
      "Epoch 22663/40000, Loss: 5.2353647333802655e-05, Learning Rate: 0.000132\n",
      "Epoch 22664/40000, Loss: 1.397919186274521e-05, Learning Rate: 0.000132\n",
      "Epoch 22665/40000, Loss: 2.961390418931842e-05, Learning Rate: 0.000132\n",
      "Epoch 22666/40000, Loss: 4.037048347527161e-05, Learning Rate: 0.000132\n",
      "Epoch 22667/40000, Loss: 5.9325557231204584e-05, Learning Rate: 0.000132\n",
      "Epoch 22668/40000, Loss: 4.183320561423898e-05, Learning Rate: 0.000132\n",
      "Epoch 22669/40000, Loss: 5.338631308404729e-05, Learning Rate: 0.000132\n",
      "Epoch 22670/40000, Loss: 2.995781869685743e-05, Learning Rate: 0.000132\n",
      "Epoch 22671/40000, Loss: 3.3867960155475885e-05, Learning Rate: 0.000132\n",
      "Epoch 22672/40000, Loss: 5.31657351530157e-05, Learning Rate: 0.000132\n",
      "Epoch 22673/40000, Loss: 3.4346405300311744e-05, Learning Rate: 0.000132\n",
      "Epoch 22674/40000, Loss: 3.0144055926939473e-05, Learning Rate: 0.000132\n",
      "Epoch 22675/40000, Loss: 6.032460441929288e-05, Learning Rate: 0.000132\n",
      "Epoch 22676/40000, Loss: 3.074462074437179e-05, Learning Rate: 0.000132\n",
      "Epoch 22677/40000, Loss: 1.473104566684924e-05, Learning Rate: 0.000132\n",
      "Epoch 22678/40000, Loss: 2.9572673156508245e-05, Learning Rate: 0.000132\n",
      "Epoch 22679/40000, Loss: 2.9142165658413433e-05, Learning Rate: 0.000132\n",
      "Epoch 22680/40000, Loss: 2.948320070572663e-05, Learning Rate: 0.000132\n",
      "Epoch 22681/40000, Loss: 1.4367305993800983e-05, Learning Rate: 0.000132\n",
      "Epoch 22682/40000, Loss: 1.4346109310281463e-05, Learning Rate: 0.000132\n",
      "Epoch 22683/40000, Loss: 5.370895451051183e-05, Learning Rate: 0.000131\n",
      "Epoch 22684/40000, Loss: 3.372699211467989e-05, Learning Rate: 0.000131\n",
      "Epoch 22685/40000, Loss: 5.5800606787670404e-05, Learning Rate: 0.000131\n",
      "Epoch 22686/40000, Loss: 4.1328483348479494e-05, Learning Rate: 0.000131\n",
      "Epoch 22687/40000, Loss: 5.411781603470445e-05, Learning Rate: 0.000131\n",
      "Epoch 22688/40000, Loss: 5.839237564941868e-05, Learning Rate: 0.000131\n",
      "Epoch 22689/40000, Loss: 3.352604835527018e-05, Learning Rate: 0.000131\n",
      "Epoch 22690/40000, Loss: 5.290131957735866e-05, Learning Rate: 0.000131\n",
      "Epoch 22691/40000, Loss: 3.3246862585656345e-05, Learning Rate: 0.000131\n",
      "Epoch 22692/40000, Loss: 3.313873821753077e-05, Learning Rate: 0.000131\n",
      "Epoch 22693/40000, Loss: 5.305410013534129e-05, Learning Rate: 0.000131\n",
      "Epoch 22694/40000, Loss: 3.330842446302995e-05, Learning Rate: 0.000131\n",
      "Epoch 22695/40000, Loss: 2.8991491490160115e-05, Learning Rate: 0.000131\n",
      "Epoch 22696/40000, Loss: 2.883289744204376e-05, Learning Rate: 0.000131\n",
      "Epoch 22697/40000, Loss: 2.8808288334403187e-05, Learning Rate: 0.000131\n",
      "Epoch 22698/40000, Loss: 3.302262848592363e-05, Learning Rate: 0.000131\n",
      "Epoch 22699/40000, Loss: 3.306434882688336e-05, Learning Rate: 0.000131\n",
      "Epoch 22700/40000, Loss: 1.3745962860411964e-05, Learning Rate: 0.000131\n",
      "Epoch 22701/40000, Loss: 2.8904738428536803e-05, Learning Rate: 0.000131\n",
      "Epoch 22702/40000, Loss: 3.305148857180029e-05, Learning Rate: 0.000131\n",
      "Epoch 22703/40000, Loss: 3.307290171505883e-05, Learning Rate: 0.000131\n",
      "Epoch 22704/40000, Loss: 1.3658276657224633e-05, Learning Rate: 0.000131\n",
      "Epoch 22705/40000, Loss: 2.8839949663961306e-05, Learning Rate: 0.000131\n",
      "Epoch 22706/40000, Loss: 1.3740928807237651e-05, Learning Rate: 0.000131\n",
      "Epoch 22707/40000, Loss: 3.301643664599396e-05, Learning Rate: 0.000131\n",
      "Epoch 22708/40000, Loss: 2.8803229724871926e-05, Learning Rate: 0.000131\n",
      "Epoch 22709/40000, Loss: 4.015865852124989e-05, Learning Rate: 0.000131\n",
      "Epoch 22710/40000, Loss: 5.768322080257349e-05, Learning Rate: 0.000131\n",
      "Epoch 22711/40000, Loss: 5.204016997595318e-05, Learning Rate: 0.000131\n",
      "Epoch 22712/40000, Loss: 5.757673716288991e-05, Learning Rate: 0.000131\n",
      "Epoch 22713/40000, Loss: 2.893276177928783e-05, Learning Rate: 0.000131\n",
      "Epoch 22714/40000, Loss: 1.3816852515446953e-05, Learning Rate: 0.000131\n",
      "Epoch 22715/40000, Loss: 5.795540346298367e-05, Learning Rate: 0.000131\n",
      "Epoch 22716/40000, Loss: 5.1940798584837466e-05, Learning Rate: 0.000131\n",
      "Epoch 22717/40000, Loss: 3.988638491136953e-05, Learning Rate: 0.000131\n",
      "Epoch 22718/40000, Loss: 5.20045286975801e-05, Learning Rate: 0.000131\n",
      "Epoch 22719/40000, Loss: 3.971172554884106e-05, Learning Rate: 0.000131\n",
      "Epoch 22720/40000, Loss: 5.246774526312947e-05, Learning Rate: 0.000131\n",
      "Epoch 22721/40000, Loss: 5.2006897021783516e-05, Learning Rate: 0.000131\n",
      "Epoch 22722/40000, Loss: 5.7770710554905236e-05, Learning Rate: 0.000131\n",
      "Epoch 22723/40000, Loss: 1.3693008440895937e-05, Learning Rate: 0.000131\n",
      "Epoch 22724/40000, Loss: 5.784837412647903e-05, Learning Rate: 0.000131\n",
      "Epoch 22725/40000, Loss: 1.3762394701188896e-05, Learning Rate: 0.000131\n",
      "Epoch 22726/40000, Loss: 2.871828110073693e-05, Learning Rate: 0.000131\n",
      "Epoch 22727/40000, Loss: 5.2024835895281285e-05, Learning Rate: 0.000131\n",
      "Epoch 22728/40000, Loss: 2.8852582545368932e-05, Learning Rate: 0.000131\n",
      "Epoch 22729/40000, Loss: 5.763463195762597e-05, Learning Rate: 0.000131\n",
      "Epoch 22730/40000, Loss: 3.964999268646352e-05, Learning Rate: 0.000131\n",
      "Epoch 22731/40000, Loss: 5.196799975237809e-05, Learning Rate: 0.000131\n",
      "Epoch 22732/40000, Loss: 5.186536509427242e-05, Learning Rate: 0.000131\n",
      "Epoch 22733/40000, Loss: 3.308121449663304e-05, Learning Rate: 0.000131\n",
      "Epoch 22734/40000, Loss: 3.969562385464087e-05, Learning Rate: 0.000131\n",
      "Epoch 22735/40000, Loss: 3.309604653622955e-05, Learning Rate: 0.000131\n",
      "Epoch 22736/40000, Loss: 5.778800186817534e-05, Learning Rate: 0.000131\n",
      "Epoch 22737/40000, Loss: 3.304924757685512e-05, Learning Rate: 0.000131\n",
      "Epoch 22738/40000, Loss: 5.786182373412885e-05, Learning Rate: 0.000131\n",
      "Epoch 22739/40000, Loss: 5.776810212410055e-05, Learning Rate: 0.000131\n",
      "Epoch 22740/40000, Loss: 5.1908078603446484e-05, Learning Rate: 0.000131\n",
      "Epoch 22741/40000, Loss: 5.771654468844645e-05, Learning Rate: 0.000131\n",
      "Epoch 22742/40000, Loss: 5.2071332902414724e-05, Learning Rate: 0.000131\n",
      "Epoch 22743/40000, Loss: 2.8810160074499436e-05, Learning Rate: 0.000131\n",
      "Epoch 22744/40000, Loss: 5.8089979575015604e-05, Learning Rate: 0.000131\n",
      "Epoch 22745/40000, Loss: 3.346317680552602e-05, Learning Rate: 0.000131\n",
      "Epoch 22746/40000, Loss: 5.792362571810372e-05, Learning Rate: 0.000130\n",
      "Epoch 22747/40000, Loss: 4.0104801882989705e-05, Learning Rate: 0.000130\n",
      "Epoch 22748/40000, Loss: 2.8911115805385634e-05, Learning Rate: 0.000130\n",
      "Epoch 22749/40000, Loss: 1.433003035344882e-05, Learning Rate: 0.000130\n",
      "Epoch 22750/40000, Loss: 3.33340467477683e-05, Learning Rate: 0.000130\n",
      "Epoch 22751/40000, Loss: 1.4355041457747575e-05, Learning Rate: 0.000130\n",
      "Epoch 22752/40000, Loss: 1.4004069271322805e-05, Learning Rate: 0.000130\n",
      "Epoch 22753/40000, Loss: 3.361693234182894e-05, Learning Rate: 0.000130\n",
      "Epoch 22754/40000, Loss: 3.322190241306089e-05, Learning Rate: 0.000130\n",
      "Epoch 22755/40000, Loss: 3.3145701308967546e-05, Learning Rate: 0.000130\n",
      "Epoch 22756/40000, Loss: 1.4090455806581303e-05, Learning Rate: 0.000130\n",
      "Epoch 22757/40000, Loss: 1.3874463547836058e-05, Learning Rate: 0.000130\n",
      "Epoch 22758/40000, Loss: 5.2632411097874865e-05, Learning Rate: 0.000130\n",
      "Epoch 22759/40000, Loss: 1.4331332749861758e-05, Learning Rate: 0.000130\n",
      "Epoch 22760/40000, Loss: 2.902598680520896e-05, Learning Rate: 0.000130\n",
      "Epoch 22761/40000, Loss: 2.8858274163212627e-05, Learning Rate: 0.000130\n",
      "Epoch 22762/40000, Loss: 3.312225817353465e-05, Learning Rate: 0.000130\n",
      "Epoch 22763/40000, Loss: 1.3828915143676568e-05, Learning Rate: 0.000130\n",
      "Epoch 22764/40000, Loss: 3.0034065275685862e-05, Learning Rate: 0.000130\n",
      "Epoch 22765/40000, Loss: 1.4118992112344131e-05, Learning Rate: 0.000130\n",
      "Epoch 22766/40000, Loss: 2.9755981813650578e-05, Learning Rate: 0.000130\n",
      "Epoch 22767/40000, Loss: 1.4641816051153e-05, Learning Rate: 0.000130\n",
      "Epoch 22768/40000, Loss: 3.3177726436406374e-05, Learning Rate: 0.000130\n",
      "Epoch 22769/40000, Loss: 5.793732270831242e-05, Learning Rate: 0.000130\n",
      "Epoch 22770/40000, Loss: 3.3791704481700435e-05, Learning Rate: 0.000130\n",
      "Epoch 22771/40000, Loss: 1.392697413393762e-05, Learning Rate: 0.000130\n",
      "Epoch 22772/40000, Loss: 3.979892426286824e-05, Learning Rate: 0.000130\n",
      "Epoch 22773/40000, Loss: 3.978367749368772e-05, Learning Rate: 0.000130\n",
      "Epoch 22774/40000, Loss: 5.229608359513804e-05, Learning Rate: 0.000130\n",
      "Epoch 22775/40000, Loss: 3.9918340917211026e-05, Learning Rate: 0.000130\n",
      "Epoch 22776/40000, Loss: 3.969280078308657e-05, Learning Rate: 0.000130\n",
      "Epoch 22777/40000, Loss: 5.201390740694478e-05, Learning Rate: 0.000130\n",
      "Epoch 22778/40000, Loss: 5.2082516049267724e-05, Learning Rate: 0.000130\n",
      "Epoch 22779/40000, Loss: 3.3036947570508346e-05, Learning Rate: 0.000130\n",
      "Epoch 22780/40000, Loss: 3.3085460017900914e-05, Learning Rate: 0.000130\n",
      "Epoch 22781/40000, Loss: 1.3820655112795066e-05, Learning Rate: 0.000130\n",
      "Epoch 22782/40000, Loss: 5.7702498452272266e-05, Learning Rate: 0.000130\n",
      "Epoch 22783/40000, Loss: 5.201499880058691e-05, Learning Rate: 0.000130\n",
      "Epoch 22784/40000, Loss: 3.309104067739099e-05, Learning Rate: 0.000130\n",
      "Epoch 22785/40000, Loss: 3.307445513200946e-05, Learning Rate: 0.000130\n",
      "Epoch 22786/40000, Loss: 5.210707968217321e-05, Learning Rate: 0.000130\n",
      "Epoch 22787/40000, Loss: 2.876906182791572e-05, Learning Rate: 0.000130\n",
      "Epoch 22788/40000, Loss: 1.3651143490278628e-05, Learning Rate: 0.000130\n",
      "Epoch 22789/40000, Loss: 5.1981765864184126e-05, Learning Rate: 0.000130\n",
      "Epoch 22790/40000, Loss: 5.178987339604646e-05, Learning Rate: 0.000130\n",
      "Epoch 22791/40000, Loss: 5.185670670471154e-05, Learning Rate: 0.000130\n",
      "Epoch 22792/40000, Loss: 2.869391391868703e-05, Learning Rate: 0.000130\n",
      "Epoch 22793/40000, Loss: 1.3913578186475206e-05, Learning Rate: 0.000130\n",
      "Epoch 22794/40000, Loss: 3.9728467527311295e-05, Learning Rate: 0.000130\n",
      "Epoch 22795/40000, Loss: 5.766167669207789e-05, Learning Rate: 0.000130\n",
      "Epoch 22796/40000, Loss: 5.762339787906967e-05, Learning Rate: 0.000130\n",
      "Epoch 22797/40000, Loss: 3.9973729144549e-05, Learning Rate: 0.000130\n",
      "Epoch 22798/40000, Loss: 5.816957855131477e-05, Learning Rate: 0.000130\n",
      "Epoch 22799/40000, Loss: 3.321859549032524e-05, Learning Rate: 0.000130\n",
      "Epoch 22800/40000, Loss: 3.318050585221499e-05, Learning Rate: 0.000130\n",
      "Epoch 22801/40000, Loss: 5.222014806349762e-05, Learning Rate: 0.000130\n",
      "Epoch 22802/40000, Loss: 2.894356475735549e-05, Learning Rate: 0.000130\n",
      "Epoch 22803/40000, Loss: 5.8170498959952965e-05, Learning Rate: 0.000130\n",
      "Epoch 22804/40000, Loss: 2.9087057555443607e-05, Learning Rate: 0.000130\n",
      "Epoch 22805/40000, Loss: 6.190285785123706e-05, Learning Rate: 0.000130\n",
      "Epoch 22806/40000, Loss: 3.3595824788790196e-05, Learning Rate: 0.000130\n",
      "Epoch 22807/40000, Loss: 2.907502494053915e-05, Learning Rate: 0.000130\n",
      "Epoch 22808/40000, Loss: 1.4352505786519032e-05, Learning Rate: 0.000130\n",
      "Epoch 22809/40000, Loss: 3.3412768971174955e-05, Learning Rate: 0.000130\n",
      "Epoch 22810/40000, Loss: 5.824697655043565e-05, Learning Rate: 0.000129\n",
      "Epoch 22811/40000, Loss: 3.4322260034969077e-05, Learning Rate: 0.000129\n",
      "Epoch 22812/40000, Loss: 1.4790386558161117e-05, Learning Rate: 0.000129\n",
      "Epoch 22813/40000, Loss: 3.8609861803706735e-05, Learning Rate: 0.000129\n",
      "Epoch 22814/40000, Loss: 1.6446847439510748e-05, Learning Rate: 0.000129\n",
      "Epoch 22815/40000, Loss: 2.9172504582675174e-05, Learning Rate: 0.000129\n",
      "Epoch 22816/40000, Loss: 5.8935525885317475e-05, Learning Rate: 0.000129\n",
      "Epoch 22817/40000, Loss: 4.06269209634047e-05, Learning Rate: 0.000129\n",
      "Epoch 22818/40000, Loss: 5.2936578867956996e-05, Learning Rate: 0.000129\n",
      "Epoch 22819/40000, Loss: 4.001385605079122e-05, Learning Rate: 0.000129\n",
      "Epoch 22820/40000, Loss: 1.4435569937631954e-05, Learning Rate: 0.000129\n",
      "Epoch 22821/40000, Loss: 2.9460681616910733e-05, Learning Rate: 0.000129\n",
      "Epoch 22822/40000, Loss: 2.9133763746358454e-05, Learning Rate: 0.000129\n",
      "Epoch 22823/40000, Loss: 1.4319041838461999e-05, Learning Rate: 0.000129\n",
      "Epoch 22824/40000, Loss: 5.2544612117344514e-05, Learning Rate: 0.000129\n",
      "Epoch 22825/40000, Loss: 1.4826676306256559e-05, Learning Rate: 0.000129\n",
      "Epoch 22826/40000, Loss: 2.8980553906876594e-05, Learning Rate: 0.000129\n",
      "Epoch 22827/40000, Loss: 5.8256435295334086e-05, Learning Rate: 0.000129\n",
      "Epoch 22828/40000, Loss: 5.235212665866129e-05, Learning Rate: 0.000129\n",
      "Epoch 22829/40000, Loss: 5.1990675274282694e-05, Learning Rate: 0.000129\n",
      "Epoch 22830/40000, Loss: 4.0031496610026807e-05, Learning Rate: 0.000129\n",
      "Epoch 22831/40000, Loss: 2.8977661713724956e-05, Learning Rate: 0.000129\n",
      "Epoch 22832/40000, Loss: 5.8347577578388155e-05, Learning Rate: 0.000129\n",
      "Epoch 22833/40000, Loss: 4.0019851439865306e-05, Learning Rate: 0.000129\n",
      "Epoch 22834/40000, Loss: 2.908303758886177e-05, Learning Rate: 0.000129\n",
      "Epoch 22835/40000, Loss: 5.904584031668492e-05, Learning Rate: 0.000129\n",
      "Epoch 22836/40000, Loss: 5.2763229177799076e-05, Learning Rate: 0.000129\n",
      "Epoch 22837/40000, Loss: 3.351510167703964e-05, Learning Rate: 0.000129\n",
      "Epoch 22838/40000, Loss: 5.342287113307975e-05, Learning Rate: 0.000129\n",
      "Epoch 22839/40000, Loss: 5.2563613280653954e-05, Learning Rate: 0.000129\n",
      "Epoch 22840/40000, Loss: 5.28052551089786e-05, Learning Rate: 0.000129\n",
      "Epoch 22841/40000, Loss: 1.4217630450730212e-05, Learning Rate: 0.000129\n",
      "Epoch 22842/40000, Loss: 2.9252169042592868e-05, Learning Rate: 0.000129\n",
      "Epoch 22843/40000, Loss: 2.901020343415439e-05, Learning Rate: 0.000129\n",
      "Epoch 22844/40000, Loss: 3.9941533032106236e-05, Learning Rate: 0.000129\n",
      "Epoch 22845/40000, Loss: 3.3109350624727085e-05, Learning Rate: 0.000129\n",
      "Epoch 22846/40000, Loss: 2.8904081773362122e-05, Learning Rate: 0.000129\n",
      "Epoch 22847/40000, Loss: 3.339653994771652e-05, Learning Rate: 0.000129\n",
      "Epoch 22848/40000, Loss: 2.9159815312596038e-05, Learning Rate: 0.000129\n",
      "Epoch 22849/40000, Loss: 2.88956580334343e-05, Learning Rate: 0.000129\n",
      "Epoch 22850/40000, Loss: 5.7888209994416684e-05, Learning Rate: 0.000129\n",
      "Epoch 22851/40000, Loss: 1.3744266652793158e-05, Learning Rate: 0.000129\n",
      "Epoch 22852/40000, Loss: 5.7905959693016484e-05, Learning Rate: 0.000129\n",
      "Epoch 22853/40000, Loss: 2.8828797439928167e-05, Learning Rate: 0.000129\n",
      "Epoch 22854/40000, Loss: 1.4225167433323804e-05, Learning Rate: 0.000129\n",
      "Epoch 22855/40000, Loss: 5.806036642752588e-05, Learning Rate: 0.000129\n",
      "Epoch 22856/40000, Loss: 5.305300510372035e-05, Learning Rate: 0.000129\n",
      "Epoch 22857/40000, Loss: 2.940239937743172e-05, Learning Rate: 0.000129\n",
      "Epoch 22858/40000, Loss: 2.8839396691182628e-05, Learning Rate: 0.000129\n",
      "Epoch 22859/40000, Loss: 3.9937818655744195e-05, Learning Rate: 0.000129\n",
      "Epoch 22860/40000, Loss: 2.8942453354829922e-05, Learning Rate: 0.000129\n",
      "Epoch 22861/40000, Loss: 2.885463254642673e-05, Learning Rate: 0.000129\n",
      "Epoch 22862/40000, Loss: 5.774966848548502e-05, Learning Rate: 0.000129\n",
      "Epoch 22863/40000, Loss: 1.3954487258160952e-05, Learning Rate: 0.000129\n",
      "Epoch 22864/40000, Loss: 5.291190973366611e-05, Learning Rate: 0.000129\n",
      "Epoch 22865/40000, Loss: 3.318524613860063e-05, Learning Rate: 0.000129\n",
      "Epoch 22866/40000, Loss: 2.894858153013047e-05, Learning Rate: 0.000129\n",
      "Epoch 22867/40000, Loss: 2.8914409995195456e-05, Learning Rate: 0.000129\n",
      "Epoch 22868/40000, Loss: 2.8964299417566508e-05, Learning Rate: 0.000129\n",
      "Epoch 22869/40000, Loss: 1.4180452126311138e-05, Learning Rate: 0.000129\n",
      "Epoch 22870/40000, Loss: 3.992895653937012e-05, Learning Rate: 0.000129\n",
      "Epoch 22871/40000, Loss: 2.8728794859489426e-05, Learning Rate: 0.000129\n",
      "Epoch 22872/40000, Loss: 1.3837405276717618e-05, Learning Rate: 0.000129\n",
      "Epoch 22873/40000, Loss: 2.8938571631442755e-05, Learning Rate: 0.000129\n",
      "Epoch 22874/40000, Loss: 3.29080612573307e-05, Learning Rate: 0.000129\n",
      "Epoch 22875/40000, Loss: 5.7881527027348056e-05, Learning Rate: 0.000128\n",
      "Epoch 22876/40000, Loss: 3.979010580223985e-05, Learning Rate: 0.000128\n",
      "Epoch 22877/40000, Loss: 3.987059972132556e-05, Learning Rate: 0.000128\n",
      "Epoch 22878/40000, Loss: 5.796070036012679e-05, Learning Rate: 0.000128\n",
      "Epoch 22879/40000, Loss: 3.309339808765799e-05, Learning Rate: 0.000128\n",
      "Epoch 22880/40000, Loss: 5.26815238117706e-05, Learning Rate: 0.000128\n",
      "Epoch 22881/40000, Loss: 1.3806126844428945e-05, Learning Rate: 0.000128\n",
      "Epoch 22882/40000, Loss: 5.786085966974497e-05, Learning Rate: 0.000128\n",
      "Epoch 22883/40000, Loss: 3.2877895137062296e-05, Learning Rate: 0.000128\n",
      "Epoch 22884/40000, Loss: 5.190746742300689e-05, Learning Rate: 0.000128\n",
      "Epoch 22885/40000, Loss: 2.88169649138581e-05, Learning Rate: 0.000128\n",
      "Epoch 22886/40000, Loss: 3.9881000702735037e-05, Learning Rate: 0.000128\n",
      "Epoch 22887/40000, Loss: 5.267718370305374e-05, Learning Rate: 0.000128\n",
      "Epoch 22888/40000, Loss: 1.3931667126598768e-05, Learning Rate: 0.000128\n",
      "Epoch 22889/40000, Loss: 5.200977102504112e-05, Learning Rate: 0.000128\n",
      "Epoch 22890/40000, Loss: 5.321480784914456e-05, Learning Rate: 0.000128\n",
      "Epoch 22891/40000, Loss: 1.392579451930942e-05, Learning Rate: 0.000128\n",
      "Epoch 22892/40000, Loss: 1.4112391909293365e-05, Learning Rate: 0.000128\n",
      "Epoch 22893/40000, Loss: 5.249271634966135e-05, Learning Rate: 0.000128\n",
      "Epoch 22894/40000, Loss: 3.312311309855431e-05, Learning Rate: 0.000128\n",
      "Epoch 22895/40000, Loss: 5.2740026148967445e-05, Learning Rate: 0.000128\n",
      "Epoch 22896/40000, Loss: 5.306217644829303e-05, Learning Rate: 0.000128\n",
      "Epoch 22897/40000, Loss: 1.3781014786218293e-05, Learning Rate: 0.000128\n",
      "Epoch 22898/40000, Loss: 5.845306077389978e-05, Learning Rate: 0.000128\n",
      "Epoch 22899/40000, Loss: 3.405563984415494e-05, Learning Rate: 0.000128\n",
      "Epoch 22900/40000, Loss: 1.4571283827535808e-05, Learning Rate: 0.000128\n",
      "Epoch 22901/40000, Loss: 5.883382254978642e-05, Learning Rate: 0.000128\n",
      "Epoch 22902/40000, Loss: 2.9284336051205173e-05, Learning Rate: 0.000128\n",
      "Epoch 22903/40000, Loss: 3.30555449181702e-05, Learning Rate: 0.000128\n",
      "Epoch 22904/40000, Loss: 3.314729474368505e-05, Learning Rate: 0.000128\n",
      "Epoch 22905/40000, Loss: 4.0111623093253e-05, Learning Rate: 0.000128\n",
      "Epoch 22906/40000, Loss: 4.004142465419136e-05, Learning Rate: 0.000128\n",
      "Epoch 22907/40000, Loss: 5.212882024352439e-05, Learning Rate: 0.000128\n",
      "Epoch 22908/40000, Loss: 3.993849168182351e-05, Learning Rate: 0.000128\n",
      "Epoch 22909/40000, Loss: 2.870857497327961e-05, Learning Rate: 0.000128\n",
      "Epoch 22910/40000, Loss: 2.870617390726693e-05, Learning Rate: 0.000128\n",
      "Epoch 22911/40000, Loss: 2.871865035558585e-05, Learning Rate: 0.000128\n",
      "Epoch 22912/40000, Loss: 5.780538413091563e-05, Learning Rate: 0.000128\n",
      "Epoch 22913/40000, Loss: 5.759820851380937e-05, Learning Rate: 0.000128\n",
      "Epoch 22914/40000, Loss: 2.869977106456645e-05, Learning Rate: 0.000128\n",
      "Epoch 22915/40000, Loss: 5.183549365028739e-05, Learning Rate: 0.000128\n",
      "Epoch 22916/40000, Loss: 5.7560133427614346e-05, Learning Rate: 0.000128\n",
      "Epoch 22917/40000, Loss: 1.3717018191528041e-05, Learning Rate: 0.000128\n",
      "Epoch 22918/40000, Loss: 5.814733958686702e-05, Learning Rate: 0.000128\n",
      "Epoch 22919/40000, Loss: 3.2901574741117656e-05, Learning Rate: 0.000128\n",
      "Epoch 22920/40000, Loss: 5.797997800982557e-05, Learning Rate: 0.000128\n",
      "Epoch 22921/40000, Loss: 5.779732600785792e-05, Learning Rate: 0.000128\n",
      "Epoch 22922/40000, Loss: 2.866097383957822e-05, Learning Rate: 0.000128\n",
      "Epoch 22923/40000, Loss: 1.3763733477389906e-05, Learning Rate: 0.000128\n",
      "Epoch 22924/40000, Loss: 5.2208357374183834e-05, Learning Rate: 0.000128\n",
      "Epoch 22925/40000, Loss: 1.4029474186827429e-05, Learning Rate: 0.000128\n",
      "Epoch 22926/40000, Loss: 5.78382023377344e-05, Learning Rate: 0.000128\n",
      "Epoch 22927/40000, Loss: 4.0138511394616216e-05, Learning Rate: 0.000128\n",
      "Epoch 22928/40000, Loss: 2.8843402105849236e-05, Learning Rate: 0.000128\n",
      "Epoch 22929/40000, Loss: 2.8733189537888393e-05, Learning Rate: 0.000128\n",
      "Epoch 22930/40000, Loss: 5.767707625636831e-05, Learning Rate: 0.000128\n",
      "Epoch 22931/40000, Loss: 2.8926608138135634e-05, Learning Rate: 0.000128\n",
      "Epoch 22932/40000, Loss: 3.301313699921593e-05, Learning Rate: 0.000128\n",
      "Epoch 22933/40000, Loss: 3.292510882602073e-05, Learning Rate: 0.000128\n",
      "Epoch 22934/40000, Loss: 3.293027839390561e-05, Learning Rate: 0.000128\n",
      "Epoch 22935/40000, Loss: 2.8763579393853433e-05, Learning Rate: 0.000128\n",
      "Epoch 22936/40000, Loss: 2.864365160348825e-05, Learning Rate: 0.000128\n",
      "Epoch 22937/40000, Loss: 5.756728205597028e-05, Learning Rate: 0.000128\n",
      "Epoch 22938/40000, Loss: 2.8750302590196952e-05, Learning Rate: 0.000128\n",
      "Epoch 22939/40000, Loss: 5.767019320046529e-05, Learning Rate: 0.000128\n",
      "Epoch 22940/40000, Loss: 2.9200129574746825e-05, Learning Rate: 0.000127\n",
      "Epoch 22941/40000, Loss: 5.7541423302609473e-05, Learning Rate: 0.000127\n",
      "Epoch 22942/40000, Loss: 1.3636143194162287e-05, Learning Rate: 0.000127\n",
      "Epoch 22943/40000, Loss: 5.1701692427741364e-05, Learning Rate: 0.000127\n",
      "Epoch 22944/40000, Loss: 1.374402108922368e-05, Learning Rate: 0.000127\n",
      "Epoch 22945/40000, Loss: 5.751493154093623e-05, Learning Rate: 0.000127\n",
      "Epoch 22946/40000, Loss: 5.754802987212315e-05, Learning Rate: 0.000127\n",
      "Epoch 22947/40000, Loss: 2.8618544092751108e-05, Learning Rate: 0.000127\n",
      "Epoch 22948/40000, Loss: 5.77553037146572e-05, Learning Rate: 0.000127\n",
      "Epoch 22949/40000, Loss: 1.3659659089171328e-05, Learning Rate: 0.000127\n",
      "Epoch 22950/40000, Loss: 1.3810821656079497e-05, Learning Rate: 0.000127\n",
      "Epoch 22951/40000, Loss: 3.957523585995659e-05, Learning Rate: 0.000127\n",
      "Epoch 22952/40000, Loss: 1.3820418644172605e-05, Learning Rate: 0.000127\n",
      "Epoch 22953/40000, Loss: 5.187208080315031e-05, Learning Rate: 0.000127\n",
      "Epoch 22954/40000, Loss: 3.304638448753394e-05, Learning Rate: 0.000127\n",
      "Epoch 22955/40000, Loss: 1.3838905942975543e-05, Learning Rate: 0.000127\n",
      "Epoch 22956/40000, Loss: 3.301026299595833e-05, Learning Rate: 0.000127\n",
      "Epoch 22957/40000, Loss: 5.782351945526898e-05, Learning Rate: 0.000127\n",
      "Epoch 22958/40000, Loss: 5.7495752116665244e-05, Learning Rate: 0.000127\n",
      "Epoch 22959/40000, Loss: 1.4156377801555209e-05, Learning Rate: 0.000127\n",
      "Epoch 22960/40000, Loss: 5.819926445838064e-05, Learning Rate: 0.000127\n",
      "Epoch 22961/40000, Loss: 3.9871421904535964e-05, Learning Rate: 0.000127\n",
      "Epoch 22962/40000, Loss: 1.3795234735880513e-05, Learning Rate: 0.000127\n",
      "Epoch 22963/40000, Loss: 5.201685780775733e-05, Learning Rate: 0.000127\n",
      "Epoch 22964/40000, Loss: 2.884235254896339e-05, Learning Rate: 0.000127\n",
      "Epoch 22965/40000, Loss: 2.8668955565080978e-05, Learning Rate: 0.000127\n",
      "Epoch 22966/40000, Loss: 2.865511123673059e-05, Learning Rate: 0.000127\n",
      "Epoch 22967/40000, Loss: 3.281516183051281e-05, Learning Rate: 0.000127\n",
      "Epoch 22968/40000, Loss: 1.396873176418012e-05, Learning Rate: 0.000127\n",
      "Epoch 22969/40000, Loss: 5.174083707970567e-05, Learning Rate: 0.000127\n",
      "Epoch 22970/40000, Loss: 5.1687882660189644e-05, Learning Rate: 0.000127\n",
      "Epoch 22971/40000, Loss: 1.3724935342906974e-05, Learning Rate: 0.000127\n",
      "Epoch 22972/40000, Loss: 3.9814080082578585e-05, Learning Rate: 0.000127\n",
      "Epoch 22973/40000, Loss: 1.3834161109116394e-05, Learning Rate: 0.000127\n",
      "Epoch 22974/40000, Loss: 1.377585249429103e-05, Learning Rate: 0.000127\n",
      "Epoch 22975/40000, Loss: 3.994012513430789e-05, Learning Rate: 0.000127\n",
      "Epoch 22976/40000, Loss: 1.403591249982128e-05, Learning Rate: 0.000127\n",
      "Epoch 22977/40000, Loss: 5.786187102785334e-05, Learning Rate: 0.000127\n",
      "Epoch 22978/40000, Loss: 1.3713392036152072e-05, Learning Rate: 0.000127\n",
      "Epoch 22979/40000, Loss: 2.861965367628727e-05, Learning Rate: 0.000127\n",
      "Epoch 22980/40000, Loss: 3.286460196250118e-05, Learning Rate: 0.000127\n",
      "Epoch 22981/40000, Loss: 3.281182944192551e-05, Learning Rate: 0.000127\n",
      "Epoch 22982/40000, Loss: 5.764633897342719e-05, Learning Rate: 0.000127\n",
      "Epoch 22983/40000, Loss: 1.3706534446100704e-05, Learning Rate: 0.000127\n",
      "Epoch 22984/40000, Loss: 3.28041787724942e-05, Learning Rate: 0.000127\n",
      "Epoch 22985/40000, Loss: 5.758557017543353e-05, Learning Rate: 0.000127\n",
      "Epoch 22986/40000, Loss: 1.3829112504026853e-05, Learning Rate: 0.000127\n",
      "Epoch 22987/40000, Loss: 3.966137956012972e-05, Learning Rate: 0.000127\n",
      "Epoch 22988/40000, Loss: 1.3730321370530874e-05, Learning Rate: 0.000127\n",
      "Epoch 22989/40000, Loss: 1.3714015949517488e-05, Learning Rate: 0.000127\n",
      "Epoch 22990/40000, Loss: 5.1806466217385605e-05, Learning Rate: 0.000127\n",
      "Epoch 22991/40000, Loss: 5.177859929972328e-05, Learning Rate: 0.000127\n",
      "Epoch 22992/40000, Loss: 2.9149154215701856e-05, Learning Rate: 0.000127\n",
      "Epoch 22993/40000, Loss: 1.389663066220237e-05, Learning Rate: 0.000127\n",
      "Epoch 22994/40000, Loss: 2.8807755370507948e-05, Learning Rate: 0.000127\n",
      "Epoch 22995/40000, Loss: 3.2867850677575916e-05, Learning Rate: 0.000127\n",
      "Epoch 22996/40000, Loss: 3.9910119085107e-05, Learning Rate: 0.000127\n",
      "Epoch 22997/40000, Loss: 3.972460035583936e-05, Learning Rate: 0.000127\n",
      "Epoch 22998/40000, Loss: 5.762522778240964e-05, Learning Rate: 0.000127\n",
      "Epoch 22999/40000, Loss: 3.29690046783071e-05, Learning Rate: 0.000127\n",
      "Epoch 23000/40000, Loss: 5.9025816881330684e-05, Learning Rate: 0.000127\n",
      "Epoch 23001/40000, Loss: 5.2301660616649315e-05, Learning Rate: 0.000127\n",
      "Epoch 23002/40000, Loss: 1.484118456573924e-05, Learning Rate: 0.000127\n",
      "Epoch 23003/40000, Loss: 1.5125082427402958e-05, Learning Rate: 0.000127\n",
      "Epoch 23004/40000, Loss: 1.646085911488626e-05, Learning Rate: 0.000127\n",
      "Epoch 23005/40000, Loss: 1.5522653484367765e-05, Learning Rate: 0.000127\n",
      "Epoch 23006/40000, Loss: 5.336845788406208e-05, Learning Rate: 0.000126\n",
      "Epoch 23007/40000, Loss: 3.4573993616504595e-05, Learning Rate: 0.000126\n",
      "Epoch 23008/40000, Loss: 4.404119317769073e-05, Learning Rate: 0.000126\n",
      "Epoch 23009/40000, Loss: 2.9823670047335327e-05, Learning Rate: 0.000126\n",
      "Epoch 23010/40000, Loss: 2.928006688307505e-05, Learning Rate: 0.000126\n",
      "Epoch 23011/40000, Loss: 5.8426383475307375e-05, Learning Rate: 0.000126\n",
      "Epoch 23012/40000, Loss: 3.0316134143504314e-05, Learning Rate: 0.000126\n",
      "Epoch 23013/40000, Loss: 6.133762508397922e-05, Learning Rate: 0.000126\n",
      "Epoch 23014/40000, Loss: 3.387815741007216e-05, Learning Rate: 0.000126\n",
      "Epoch 23015/40000, Loss: 3.3387950679752976e-05, Learning Rate: 0.000126\n",
      "Epoch 23016/40000, Loss: 2.900895560742356e-05, Learning Rate: 0.000126\n",
      "Epoch 23017/40000, Loss: 2.867765215341933e-05, Learning Rate: 0.000126\n",
      "Epoch 23018/40000, Loss: 4.028728653793223e-05, Learning Rate: 0.000126\n",
      "Epoch 23019/40000, Loss: 5.799801510875113e-05, Learning Rate: 0.000126\n",
      "Epoch 23020/40000, Loss: 5.205363413551822e-05, Learning Rate: 0.000126\n",
      "Epoch 23021/40000, Loss: 2.9006399927311577e-05, Learning Rate: 0.000126\n",
      "Epoch 23022/40000, Loss: 5.8110745158046484e-05, Learning Rate: 0.000126\n",
      "Epoch 23023/40000, Loss: 5.24230272276327e-05, Learning Rate: 0.000126\n",
      "Epoch 23024/40000, Loss: 5.213183248997666e-05, Learning Rate: 0.000126\n",
      "Epoch 23025/40000, Loss: 3.9806975109968334e-05, Learning Rate: 0.000126\n",
      "Epoch 23026/40000, Loss: 1.3698225302505307e-05, Learning Rate: 0.000126\n",
      "Epoch 23027/40000, Loss: 5.776197576778941e-05, Learning Rate: 0.000126\n",
      "Epoch 23028/40000, Loss: 5.1871549658244476e-05, Learning Rate: 0.000126\n",
      "Epoch 23029/40000, Loss: 3.301902688690461e-05, Learning Rate: 0.000126\n",
      "Epoch 23030/40000, Loss: 5.7746783568290994e-05, Learning Rate: 0.000126\n",
      "Epoch 23031/40000, Loss: 5.239954043645412e-05, Learning Rate: 0.000126\n",
      "Epoch 23032/40000, Loss: 1.3853459677193314e-05, Learning Rate: 0.000126\n",
      "Epoch 23033/40000, Loss: 2.8708469471894205e-05, Learning Rate: 0.000126\n",
      "Epoch 23034/40000, Loss: 1.3760460205958225e-05, Learning Rate: 0.000126\n",
      "Epoch 23035/40000, Loss: 2.8614500479307026e-05, Learning Rate: 0.000126\n",
      "Epoch 23036/40000, Loss: 3.2694464607629925e-05, Learning Rate: 0.000126\n",
      "Epoch 23037/40000, Loss: 1.3622346159536391e-05, Learning Rate: 0.000126\n",
      "Epoch 23038/40000, Loss: 5.740260530728847e-05, Learning Rate: 0.000126\n",
      "Epoch 23039/40000, Loss: 5.167024573893286e-05, Learning Rate: 0.000126\n",
      "Epoch 23040/40000, Loss: 1.3678857612831052e-05, Learning Rate: 0.000126\n",
      "Epoch 23041/40000, Loss: 2.8525639208965003e-05, Learning Rate: 0.000126\n",
      "Epoch 23042/40000, Loss: 3.9495625969721004e-05, Learning Rate: 0.000126\n",
      "Epoch 23043/40000, Loss: 3.282802572357468e-05, Learning Rate: 0.000126\n",
      "Epoch 23044/40000, Loss: 1.3762859452981502e-05, Learning Rate: 0.000126\n",
      "Epoch 23045/40000, Loss: 1.3719959497393575e-05, Learning Rate: 0.000126\n",
      "Epoch 23046/40000, Loss: 1.3728074918617494e-05, Learning Rate: 0.000126\n",
      "Epoch 23047/40000, Loss: 5.744958980358206e-05, Learning Rate: 0.000126\n",
      "Epoch 23048/40000, Loss: 5.745499220211059e-05, Learning Rate: 0.000126\n",
      "Epoch 23049/40000, Loss: 3.964230927522294e-05, Learning Rate: 0.000126\n",
      "Epoch 23050/40000, Loss: 5.7879391533788294e-05, Learning Rate: 0.000126\n",
      "Epoch 23051/40000, Loss: 3.276276038377546e-05, Learning Rate: 0.000126\n",
      "Epoch 23052/40000, Loss: 4.002235073130578e-05, Learning Rate: 0.000126\n",
      "Epoch 23053/40000, Loss: 5.204971967032179e-05, Learning Rate: 0.000126\n",
      "Epoch 23054/40000, Loss: 2.8711552658933215e-05, Learning Rate: 0.000126\n",
      "Epoch 23055/40000, Loss: 2.85895039269235e-05, Learning Rate: 0.000126\n",
      "Epoch 23056/40000, Loss: 3.2754018320702016e-05, Learning Rate: 0.000126\n",
      "Epoch 23057/40000, Loss: 1.376122236251831e-05, Learning Rate: 0.000126\n",
      "Epoch 23058/40000, Loss: 3.274883783888072e-05, Learning Rate: 0.000126\n",
      "Epoch 23059/40000, Loss: 5.768554183305241e-05, Learning Rate: 0.000126\n",
      "Epoch 23060/40000, Loss: 1.3710006896872073e-05, Learning Rate: 0.000126\n",
      "Epoch 23061/40000, Loss: 1.3563213542511221e-05, Learning Rate: 0.000126\n",
      "Epoch 23062/40000, Loss: 5.166449409443885e-05, Learning Rate: 0.000126\n",
      "Epoch 23063/40000, Loss: 3.948437370127067e-05, Learning Rate: 0.000126\n",
      "Epoch 23064/40000, Loss: 2.852779653039761e-05, Learning Rate: 0.000126\n",
      "Epoch 23065/40000, Loss: 3.9579597796546295e-05, Learning Rate: 0.000126\n",
      "Epoch 23066/40000, Loss: 3.266995554440655e-05, Learning Rate: 0.000126\n",
      "Epoch 23067/40000, Loss: 3.2656549592502415e-05, Learning Rate: 0.000126\n",
      "Epoch 23068/40000, Loss: 2.8652277251239866e-05, Learning Rate: 0.000126\n",
      "Epoch 23069/40000, Loss: 3.9717022445984185e-05, Learning Rate: 0.000126\n",
      "Epoch 23070/40000, Loss: 1.383843755320413e-05, Learning Rate: 0.000126\n",
      "Epoch 23071/40000, Loss: 5.741951463278383e-05, Learning Rate: 0.000126\n",
      "Epoch 23072/40000, Loss: 3.959812966058962e-05, Learning Rate: 0.000125\n",
      "Epoch 23073/40000, Loss: 5.7750508858589455e-05, Learning Rate: 0.000125\n",
      "Epoch 23074/40000, Loss: 5.174643956706859e-05, Learning Rate: 0.000125\n",
      "Epoch 23075/40000, Loss: 5.166223490959965e-05, Learning Rate: 0.000125\n",
      "Epoch 23076/40000, Loss: 4.076614277437329e-05, Learning Rate: 0.000125\n",
      "Epoch 23077/40000, Loss: 3.293064219178632e-05, Learning Rate: 0.000125\n",
      "Epoch 23078/40000, Loss: 5.76608108531218e-05, Learning Rate: 0.000125\n",
      "Epoch 23079/40000, Loss: 3.988408207078464e-05, Learning Rate: 0.000125\n",
      "Epoch 23080/40000, Loss: 5.165861512068659e-05, Learning Rate: 0.000125\n",
      "Epoch 23081/40000, Loss: 5.81087515456602e-05, Learning Rate: 0.000125\n",
      "Epoch 23082/40000, Loss: 5.180923835723661e-05, Learning Rate: 0.000125\n",
      "Epoch 23083/40000, Loss: 1.3856280929758213e-05, Learning Rate: 0.000125\n",
      "Epoch 23084/40000, Loss: 4.015254307887517e-05, Learning Rate: 0.000125\n",
      "Epoch 23085/40000, Loss: 5.214097836869769e-05, Learning Rate: 0.000125\n",
      "Epoch 23086/40000, Loss: 2.858403058780823e-05, Learning Rate: 0.000125\n",
      "Epoch 23087/40000, Loss: 1.4115596968622413e-05, Learning Rate: 0.000125\n",
      "Epoch 23088/40000, Loss: 1.3827355360263027e-05, Learning Rate: 0.000125\n",
      "Epoch 23089/40000, Loss: 3.2792933780001476e-05, Learning Rate: 0.000125\n",
      "Epoch 23090/40000, Loss: 5.7606892369221896e-05, Learning Rate: 0.000125\n",
      "Epoch 23091/40000, Loss: 3.9659480535192415e-05, Learning Rate: 0.000125\n",
      "Epoch 23092/40000, Loss: 3.953602208639495e-05, Learning Rate: 0.000125\n",
      "Epoch 23093/40000, Loss: 2.8410935556166805e-05, Learning Rate: 0.000125\n",
      "Epoch 23094/40000, Loss: 5.14675812155474e-05, Learning Rate: 0.000125\n",
      "Epoch 23095/40000, Loss: 2.84197558357846e-05, Learning Rate: 0.000125\n",
      "Epoch 23096/40000, Loss: 2.8441190806915984e-05, Learning Rate: 0.000125\n",
      "Epoch 23097/40000, Loss: 3.260128141846508e-05, Learning Rate: 0.000125\n",
      "Epoch 23098/40000, Loss: 5.7345030654687434e-05, Learning Rate: 0.000125\n",
      "Epoch 23099/40000, Loss: 1.3596323697129264e-05, Learning Rate: 0.000125\n",
      "Epoch 23100/40000, Loss: 5.769588824477978e-05, Learning Rate: 0.000125\n",
      "Epoch 23101/40000, Loss: 5.1853454351658e-05, Learning Rate: 0.000125\n",
      "Epoch 23102/40000, Loss: 2.8533966542454436e-05, Learning Rate: 0.000125\n",
      "Epoch 23103/40000, Loss: 3.261763777118176e-05, Learning Rate: 0.000125\n",
      "Epoch 23104/40000, Loss: 2.9258204449433833e-05, Learning Rate: 0.000125\n",
      "Epoch 23105/40000, Loss: 2.863596637325827e-05, Learning Rate: 0.000125\n",
      "Epoch 23106/40000, Loss: 1.3827931979903951e-05, Learning Rate: 0.000125\n",
      "Epoch 23107/40000, Loss: 3.273635229561478e-05, Learning Rate: 0.000125\n",
      "Epoch 23108/40000, Loss: 3.967696102336049e-05, Learning Rate: 0.000125\n",
      "Epoch 23109/40000, Loss: 3.981672489317134e-05, Learning Rate: 0.000125\n",
      "Epoch 23110/40000, Loss: 5.16252766828984e-05, Learning Rate: 0.000125\n",
      "Epoch 23111/40000, Loss: 2.858946936612483e-05, Learning Rate: 0.000125\n",
      "Epoch 23112/40000, Loss: 3.968887904193252e-05, Learning Rate: 0.000125\n",
      "Epoch 23113/40000, Loss: 5.737549508921802e-05, Learning Rate: 0.000125\n",
      "Epoch 23114/40000, Loss: 3.958971137763001e-05, Learning Rate: 0.000125\n",
      "Epoch 23115/40000, Loss: 1.3701435818802565e-05, Learning Rate: 0.000125\n",
      "Epoch 23116/40000, Loss: 1.3730171303905081e-05, Learning Rate: 0.000125\n",
      "Epoch 23117/40000, Loss: 5.145671821082942e-05, Learning Rate: 0.000125\n",
      "Epoch 23118/40000, Loss: 3.259614095441066e-05, Learning Rate: 0.000125\n",
      "Epoch 23119/40000, Loss: 5.758551924373023e-05, Learning Rate: 0.000125\n",
      "Epoch 23120/40000, Loss: 2.8505501177278347e-05, Learning Rate: 0.000125\n",
      "Epoch 23121/40000, Loss: 3.965384166804142e-05, Learning Rate: 0.000125\n",
      "Epoch 23122/40000, Loss: 3.954250132665038e-05, Learning Rate: 0.000125\n",
      "Epoch 23123/40000, Loss: 3.259696313762106e-05, Learning Rate: 0.000125\n",
      "Epoch 23124/40000, Loss: 1.3680865777132567e-05, Learning Rate: 0.000125\n",
      "Epoch 23125/40000, Loss: 5.766865433542989e-05, Learning Rate: 0.000125\n",
      "Epoch 23126/40000, Loss: 5.759164923802018e-05, Learning Rate: 0.000125\n",
      "Epoch 23127/40000, Loss: 2.871757351385895e-05, Learning Rate: 0.000125\n",
      "Epoch 23128/40000, Loss: 5.785331813967787e-05, Learning Rate: 0.000125\n",
      "Epoch 23129/40000, Loss: 5.187163333175704e-05, Learning Rate: 0.000125\n",
      "Epoch 23130/40000, Loss: 3.9630773244425654e-05, Learning Rate: 0.000125\n",
      "Epoch 23131/40000, Loss: 5.289435648592189e-05, Learning Rate: 0.000125\n",
      "Epoch 23132/40000, Loss: 3.279655356891453e-05, Learning Rate: 0.000125\n",
      "Epoch 23133/40000, Loss: 1.3883876817999408e-05, Learning Rate: 0.000125\n",
      "Epoch 23134/40000, Loss: 3.269173976150341e-05, Learning Rate: 0.000125\n",
      "Epoch 23135/40000, Loss: 2.8761862267856486e-05, Learning Rate: 0.000125\n",
      "Epoch 23136/40000, Loss: 3.957694207201712e-05, Learning Rate: 0.000125\n",
      "Epoch 23137/40000, Loss: 3.27556008414831e-05, Learning Rate: 0.000125\n",
      "Epoch 23138/40000, Loss: 5.167343988432549e-05, Learning Rate: 0.000125\n",
      "Epoch 23139/40000, Loss: 5.75454396312125e-05, Learning Rate: 0.000124\n",
      "Epoch 23140/40000, Loss: 3.970265242969617e-05, Learning Rate: 0.000124\n",
      "Epoch 23141/40000, Loss: 1.38734594656853e-05, Learning Rate: 0.000124\n",
      "Epoch 23142/40000, Loss: 5.758061888627708e-05, Learning Rate: 0.000124\n",
      "Epoch 23143/40000, Loss: 1.3919135199103039e-05, Learning Rate: 0.000124\n",
      "Epoch 23144/40000, Loss: 2.864811358449515e-05, Learning Rate: 0.000124\n",
      "Epoch 23145/40000, Loss: 5.219794547883794e-05, Learning Rate: 0.000124\n",
      "Epoch 23146/40000, Loss: 1.3906775166105945e-05, Learning Rate: 0.000124\n",
      "Epoch 23147/40000, Loss: 1.4180250218487345e-05, Learning Rate: 0.000124\n",
      "Epoch 23148/40000, Loss: 5.251506809145212e-05, Learning Rate: 0.000124\n",
      "Epoch 23149/40000, Loss: 5.879276432096958e-05, Learning Rate: 0.000124\n",
      "Epoch 23150/40000, Loss: 1.4783100596105214e-05, Learning Rate: 0.000124\n",
      "Epoch 23151/40000, Loss: 5.802054511150345e-05, Learning Rate: 0.000124\n",
      "Epoch 23152/40000, Loss: 5.329201667336747e-05, Learning Rate: 0.000124\n",
      "Epoch 23153/40000, Loss: 1.4446827663050499e-05, Learning Rate: 0.000124\n",
      "Epoch 23154/40000, Loss: 4.231224374962039e-05, Learning Rate: 0.000124\n",
      "Epoch 23155/40000, Loss: 3.554432623786852e-05, Learning Rate: 0.000124\n",
      "Epoch 23156/40000, Loss: 1.6133522876771167e-05, Learning Rate: 0.000124\n",
      "Epoch 23157/40000, Loss: 3.442136585363187e-05, Learning Rate: 0.000124\n",
      "Epoch 23158/40000, Loss: 5.892911212868057e-05, Learning Rate: 0.000124\n",
      "Epoch 23159/40000, Loss: 4.1544924897607416e-05, Learning Rate: 0.000124\n",
      "Epoch 23160/40000, Loss: 4.279815766494721e-05, Learning Rate: 0.000124\n",
      "Epoch 23161/40000, Loss: 2.924127875303384e-05, Learning Rate: 0.000124\n",
      "Epoch 23162/40000, Loss: 4.165432619629428e-05, Learning Rate: 0.000124\n",
      "Epoch 23163/40000, Loss: 3.168885086779483e-05, Learning Rate: 0.000124\n",
      "Epoch 23164/40000, Loss: 3.574650327209383e-05, Learning Rate: 0.000124\n",
      "Epoch 23165/40000, Loss: 4.527408964349888e-05, Learning Rate: 0.000124\n",
      "Epoch 23166/40000, Loss: 3.945769276469946e-05, Learning Rate: 0.000124\n",
      "Epoch 23167/40000, Loss: 3.200048377038911e-05, Learning Rate: 0.000124\n",
      "Epoch 23168/40000, Loss: 2.9605798772536218e-05, Learning Rate: 0.000124\n",
      "Epoch 23169/40000, Loss: 1.5089229236764368e-05, Learning Rate: 0.000124\n",
      "Epoch 23170/40000, Loss: 3.770382681977935e-05, Learning Rate: 0.000124\n",
      "Epoch 23171/40000, Loss: 1.6586662241024897e-05, Learning Rate: 0.000124\n",
      "Epoch 23172/40000, Loss: 3.4535689337644726e-05, Learning Rate: 0.000124\n",
      "Epoch 23173/40000, Loss: 4.2181309254374355e-05, Learning Rate: 0.000124\n",
      "Epoch 23174/40000, Loss: 1.4449340596911497e-05, Learning Rate: 0.000124\n",
      "Epoch 23175/40000, Loss: 5.2467767091002315e-05, Learning Rate: 0.000124\n",
      "Epoch 23176/40000, Loss: 5.787084955954924e-05, Learning Rate: 0.000124\n",
      "Epoch 23177/40000, Loss: 2.883155320887454e-05, Learning Rate: 0.000124\n",
      "Epoch 23178/40000, Loss: 5.187748320167884e-05, Learning Rate: 0.000124\n",
      "Epoch 23179/40000, Loss: 2.8768978154403158e-05, Learning Rate: 0.000124\n",
      "Epoch 23180/40000, Loss: 2.85441055893898e-05, Learning Rate: 0.000124\n",
      "Epoch 23181/40000, Loss: 3.9781119994586334e-05, Learning Rate: 0.000124\n",
      "Epoch 23182/40000, Loss: 3.961582842748612e-05, Learning Rate: 0.000124\n",
      "Epoch 23183/40000, Loss: 1.371907455904875e-05, Learning Rate: 0.000124\n",
      "Epoch 23184/40000, Loss: 5.162452725926414e-05, Learning Rate: 0.000124\n",
      "Epoch 23185/40000, Loss: 1.3854095413989853e-05, Learning Rate: 0.000124\n",
      "Epoch 23186/40000, Loss: 1.3698889233637601e-05, Learning Rate: 0.000124\n",
      "Epoch 23187/40000, Loss: 5.78455365030095e-05, Learning Rate: 0.000124\n",
      "Epoch 23188/40000, Loss: 5.1495891966624185e-05, Learning Rate: 0.000124\n",
      "Epoch 23189/40000, Loss: 5.780034916824661e-05, Learning Rate: 0.000124\n",
      "Epoch 23190/40000, Loss: 5.162091110832989e-05, Learning Rate: 0.000124\n",
      "Epoch 23191/40000, Loss: 5.779984712717123e-05, Learning Rate: 0.000124\n",
      "Epoch 23192/40000, Loss: 2.8345648388494737e-05, Learning Rate: 0.000124\n",
      "Epoch 23193/40000, Loss: 2.8431109967641532e-05, Learning Rate: 0.000124\n",
      "Epoch 23194/40000, Loss: 5.7468994782539085e-05, Learning Rate: 0.000124\n",
      "Epoch 23195/40000, Loss: 5.140565917827189e-05, Learning Rate: 0.000124\n",
      "Epoch 23196/40000, Loss: 5.144710303284228e-05, Learning Rate: 0.000124\n",
      "Epoch 23197/40000, Loss: 1.3648690583067946e-05, Learning Rate: 0.000124\n",
      "Epoch 23198/40000, Loss: 1.361238810204668e-05, Learning Rate: 0.000124\n",
      "Epoch 23199/40000, Loss: 3.93940499634482e-05, Learning Rate: 0.000124\n",
      "Epoch 23200/40000, Loss: 2.8326881874818355e-05, Learning Rate: 0.000124\n",
      "Epoch 23201/40000, Loss: 3.9437421946786344e-05, Learning Rate: 0.000124\n",
      "Epoch 23202/40000, Loss: 3.9425423892680556e-05, Learning Rate: 0.000124\n",
      "Epoch 23203/40000, Loss: 5.137210973771289e-05, Learning Rate: 0.000124\n",
      "Epoch 23204/40000, Loss: 1.3637688425660599e-05, Learning Rate: 0.000124\n",
      "Epoch 23205/40000, Loss: 2.828819015121553e-05, Learning Rate: 0.000124\n",
      "Epoch 23206/40000, Loss: 3.2437004847452044e-05, Learning Rate: 0.000123\n",
      "Epoch 23207/40000, Loss: 5.1379473006818444e-05, Learning Rate: 0.000123\n",
      "Epoch 23208/40000, Loss: 3.2544594432692975e-05, Learning Rate: 0.000123\n",
      "Epoch 23209/40000, Loss: 5.133673766977154e-05, Learning Rate: 0.000123\n",
      "Epoch 23210/40000, Loss: 2.8366896003717557e-05, Learning Rate: 0.000123\n",
      "Epoch 23211/40000, Loss: 5.1418322982499376e-05, Learning Rate: 0.000123\n",
      "Epoch 23212/40000, Loss: 3.256678974139504e-05, Learning Rate: 0.000123\n",
      "Epoch 23213/40000, Loss: 2.84005709545454e-05, Learning Rate: 0.000123\n",
      "Epoch 23214/40000, Loss: 2.835832128766924e-05, Learning Rate: 0.000123\n",
      "Epoch 23215/40000, Loss: 5.140773282619193e-05, Learning Rate: 0.000123\n",
      "Epoch 23216/40000, Loss: 3.948463563574478e-05, Learning Rate: 0.000123\n",
      "Epoch 23217/40000, Loss: 5.725056689698249e-05, Learning Rate: 0.000123\n",
      "Epoch 23218/40000, Loss: 3.933838161174208e-05, Learning Rate: 0.000123\n",
      "Epoch 23219/40000, Loss: 2.8284170184633695e-05, Learning Rate: 0.000123\n",
      "Epoch 23220/40000, Loss: 5.740463893744163e-05, Learning Rate: 0.000123\n",
      "Epoch 23221/40000, Loss: 5.1407740102149546e-05, Learning Rate: 0.000123\n",
      "Epoch 23222/40000, Loss: 5.143078669789247e-05, Learning Rate: 0.000123\n",
      "Epoch 23223/40000, Loss: 3.250236841267906e-05, Learning Rate: 0.000123\n",
      "Epoch 23224/40000, Loss: 5.755735037382692e-05, Learning Rate: 0.000123\n",
      "Epoch 23225/40000, Loss: 3.2494270271854475e-05, Learning Rate: 0.000123\n",
      "Epoch 23226/40000, Loss: 5.768819755758159e-05, Learning Rate: 0.000123\n",
      "Epoch 23227/40000, Loss: 5.75354861211963e-05, Learning Rate: 0.000123\n",
      "Epoch 23228/40000, Loss: 1.3645926628669258e-05, Learning Rate: 0.000123\n",
      "Epoch 23229/40000, Loss: 5.8057768910657614e-05, Learning Rate: 0.000123\n",
      "Epoch 23230/40000, Loss: 1.3635638424602803e-05, Learning Rate: 0.000123\n",
      "Epoch 23231/40000, Loss: 2.8400281735230237e-05, Learning Rate: 0.000123\n",
      "Epoch 23232/40000, Loss: 3.262336758780293e-05, Learning Rate: 0.000123\n",
      "Epoch 23233/40000, Loss: 3.94954695366323e-05, Learning Rate: 0.000123\n",
      "Epoch 23234/40000, Loss: 2.84215511783259e-05, Learning Rate: 0.000123\n",
      "Epoch 23235/40000, Loss: 3.948951052734628e-05, Learning Rate: 0.000123\n",
      "Epoch 23236/40000, Loss: 2.832219070114661e-05, Learning Rate: 0.000123\n",
      "Epoch 23237/40000, Loss: 3.253957038396038e-05, Learning Rate: 0.000123\n",
      "Epoch 23238/40000, Loss: 5.135349783813581e-05, Learning Rate: 0.000123\n",
      "Epoch 23239/40000, Loss: 2.8350577849778347e-05, Learning Rate: 0.000123\n",
      "Epoch 23240/40000, Loss: 2.835680243151728e-05, Learning Rate: 0.000123\n",
      "Epoch 23241/40000, Loss: 2.8389051294652745e-05, Learning Rate: 0.000123\n",
      "Epoch 23242/40000, Loss: 3.2513598853256553e-05, Learning Rate: 0.000123\n",
      "Epoch 23243/40000, Loss: 2.8381373340380378e-05, Learning Rate: 0.000123\n",
      "Epoch 23244/40000, Loss: 3.946445576730184e-05, Learning Rate: 0.000123\n",
      "Epoch 23245/40000, Loss: 5.1415448979241773e-05, Learning Rate: 0.000123\n",
      "Epoch 23246/40000, Loss: 3.941769682569429e-05, Learning Rate: 0.000123\n",
      "Epoch 23247/40000, Loss: 3.946755168726668e-05, Learning Rate: 0.000123\n",
      "Epoch 23248/40000, Loss: 5.72700664633885e-05, Learning Rate: 0.000123\n",
      "Epoch 23249/40000, Loss: 5.735428931075148e-05, Learning Rate: 0.000123\n",
      "Epoch 23250/40000, Loss: 1.3728796147916e-05, Learning Rate: 0.000123\n",
      "Epoch 23251/40000, Loss: 3.249663132010028e-05, Learning Rate: 0.000123\n",
      "Epoch 23252/40000, Loss: 5.731596320401877e-05, Learning Rate: 0.000123\n",
      "Epoch 23253/40000, Loss: 3.953169652959332e-05, Learning Rate: 0.000123\n",
      "Epoch 23254/40000, Loss: 3.2469175494043157e-05, Learning Rate: 0.000123\n",
      "Epoch 23255/40000, Loss: 5.737523315474391e-05, Learning Rate: 0.000123\n",
      "Epoch 23256/40000, Loss: 5.138648339197971e-05, Learning Rate: 0.000123\n",
      "Epoch 23257/40000, Loss: 3.260553421569057e-05, Learning Rate: 0.000123\n",
      "Epoch 23258/40000, Loss: 2.843938273144886e-05, Learning Rate: 0.000123\n",
      "Epoch 23259/40000, Loss: 5.155890539754182e-05, Learning Rate: 0.000123\n",
      "Epoch 23260/40000, Loss: 3.955715146730654e-05, Learning Rate: 0.000123\n",
      "Epoch 23261/40000, Loss: 3.9519742131233215e-05, Learning Rate: 0.000123\n",
      "Epoch 23262/40000, Loss: 3.952374754589982e-05, Learning Rate: 0.000123\n",
      "Epoch 23263/40000, Loss: 5.155351755092852e-05, Learning Rate: 0.000123\n",
      "Epoch 23264/40000, Loss: 3.261608071625233e-05, Learning Rate: 0.000123\n",
      "Epoch 23265/40000, Loss: 5.15700776304584e-05, Learning Rate: 0.000123\n",
      "Epoch 23266/40000, Loss: 3.24431894114241e-05, Learning Rate: 0.000123\n",
      "Epoch 23267/40000, Loss: 3.243397441110574e-05, Learning Rate: 0.000123\n",
      "Epoch 23268/40000, Loss: 3.969202225562185e-05, Learning Rate: 0.000123\n",
      "Epoch 23269/40000, Loss: 1.379363311571069e-05, Learning Rate: 0.000123\n",
      "Epoch 23270/40000, Loss: 3.968815872212872e-05, Learning Rate: 0.000123\n",
      "Epoch 23271/40000, Loss: 5.761008651461452e-05, Learning Rate: 0.000123\n",
      "Epoch 23272/40000, Loss: 1.361531758448109e-05, Learning Rate: 0.000123\n",
      "Epoch 23273/40000, Loss: 3.95631323044654e-05, Learning Rate: 0.000123\n",
      "Epoch 23274/40000, Loss: 3.9446851587854326e-05, Learning Rate: 0.000122\n",
      "Epoch 23275/40000, Loss: 3.9566511986777186e-05, Learning Rate: 0.000122\n",
      "Epoch 23276/40000, Loss: 2.8343640224193223e-05, Learning Rate: 0.000122\n",
      "Epoch 23277/40000, Loss: 3.9506081520812586e-05, Learning Rate: 0.000122\n",
      "Epoch 23278/40000, Loss: 3.936975554097444e-05, Learning Rate: 0.000122\n",
      "Epoch 23279/40000, Loss: 5.135717583470978e-05, Learning Rate: 0.000122\n",
      "Epoch 23280/40000, Loss: 2.829893674061168e-05, Learning Rate: 0.000122\n",
      "Epoch 23281/40000, Loss: 2.826279160217382e-05, Learning Rate: 0.000122\n",
      "Epoch 23282/40000, Loss: 1.3665584447153378e-05, Learning Rate: 0.000122\n",
      "Epoch 23283/40000, Loss: 2.8416880013537593e-05, Learning Rate: 0.000122\n",
      "Epoch 23284/40000, Loss: 1.3672942259290721e-05, Learning Rate: 0.000122\n",
      "Epoch 23285/40000, Loss: 3.9488906622864306e-05, Learning Rate: 0.000122\n",
      "Epoch 23286/40000, Loss: 3.2414111046819016e-05, Learning Rate: 0.000122\n",
      "Epoch 23287/40000, Loss: 3.9557056879857555e-05, Learning Rate: 0.000122\n",
      "Epoch 23288/40000, Loss: 2.8263559215702116e-05, Learning Rate: 0.000122\n",
      "Epoch 23289/40000, Loss: 5.7203564210794866e-05, Learning Rate: 0.000122\n",
      "Epoch 23290/40000, Loss: 3.243242099415511e-05, Learning Rate: 0.000122\n",
      "Epoch 23291/40000, Loss: 2.827287244144827e-05, Learning Rate: 0.000122\n",
      "Epoch 23292/40000, Loss: 5.1311071729287505e-05, Learning Rate: 0.000122\n",
      "Epoch 23293/40000, Loss: 3.958082743338309e-05, Learning Rate: 0.000122\n",
      "Epoch 23294/40000, Loss: 5.148287527845241e-05, Learning Rate: 0.000122\n",
      "Epoch 23295/40000, Loss: 2.8363829187583178e-05, Learning Rate: 0.000122\n",
      "Epoch 23296/40000, Loss: 5.154787504579872e-05, Learning Rate: 0.000122\n",
      "Epoch 23297/40000, Loss: 2.825825140462257e-05, Learning Rate: 0.000122\n",
      "Epoch 23298/40000, Loss: 3.945455318898894e-05, Learning Rate: 0.000122\n",
      "Epoch 23299/40000, Loss: 1.3576192941400222e-05, Learning Rate: 0.000122\n",
      "Epoch 23300/40000, Loss: 3.943850242649205e-05, Learning Rate: 0.000122\n",
      "Epoch 23301/40000, Loss: 5.148224954609759e-05, Learning Rate: 0.000122\n",
      "Epoch 23302/40000, Loss: 1.3689621482626535e-05, Learning Rate: 0.000122\n",
      "Epoch 23303/40000, Loss: 5.141142901265994e-05, Learning Rate: 0.000122\n",
      "Epoch 23304/40000, Loss: 3.9585826016264036e-05, Learning Rate: 0.000122\n",
      "Epoch 23305/40000, Loss: 5.772673830506392e-05, Learning Rate: 0.000122\n",
      "Epoch 23306/40000, Loss: 5.768133996753022e-05, Learning Rate: 0.000122\n",
      "Epoch 23307/40000, Loss: 1.3741958355240058e-05, Learning Rate: 0.000122\n",
      "Epoch 23308/40000, Loss: 5.762472210335545e-05, Learning Rate: 0.000122\n",
      "Epoch 23309/40000, Loss: 5.1458941015880555e-05, Learning Rate: 0.000122\n",
      "Epoch 23310/40000, Loss: 5.150058859726414e-05, Learning Rate: 0.000122\n",
      "Epoch 23311/40000, Loss: 2.836997009580955e-05, Learning Rate: 0.000122\n",
      "Epoch 23312/40000, Loss: 1.3905733794672415e-05, Learning Rate: 0.000122\n",
      "Epoch 23313/40000, Loss: 3.270011075073853e-05, Learning Rate: 0.000122\n",
      "Epoch 23314/40000, Loss: 5.1580467697931454e-05, Learning Rate: 0.000122\n",
      "Epoch 23315/40000, Loss: 2.8541147912619635e-05, Learning Rate: 0.000122\n",
      "Epoch 23316/40000, Loss: 5.17952794325538e-05, Learning Rate: 0.000122\n",
      "Epoch 23317/40000, Loss: 3.278276926721446e-05, Learning Rate: 0.000122\n",
      "Epoch 23318/40000, Loss: 1.3910847883380484e-05, Learning Rate: 0.000122\n",
      "Epoch 23319/40000, Loss: 4.0185408579418436e-05, Learning Rate: 0.000122\n",
      "Epoch 23320/40000, Loss: 2.8588860004674643e-05, Learning Rate: 0.000122\n",
      "Epoch 23321/40000, Loss: 3.249596920795739e-05, Learning Rate: 0.000122\n",
      "Epoch 23322/40000, Loss: 1.3862988453183789e-05, Learning Rate: 0.000122\n",
      "Epoch 23323/40000, Loss: 5.7281584304291755e-05, Learning Rate: 0.000122\n",
      "Epoch 23324/40000, Loss: 2.832627433235757e-05, Learning Rate: 0.000122\n",
      "Epoch 23325/40000, Loss: 3.9885730075184256e-05, Learning Rate: 0.000122\n",
      "Epoch 23326/40000, Loss: 1.399912616761867e-05, Learning Rate: 0.000122\n",
      "Epoch 23327/40000, Loss: 5.1497230742825195e-05, Learning Rate: 0.000122\n",
      "Epoch 23328/40000, Loss: 5.733473153668456e-05, Learning Rate: 0.000122\n",
      "Epoch 23329/40000, Loss: 3.2445161195937544e-05, Learning Rate: 0.000122\n",
      "Epoch 23330/40000, Loss: 1.4599941096093971e-05, Learning Rate: 0.000122\n",
      "Epoch 23331/40000, Loss: 1.3883255633118097e-05, Learning Rate: 0.000122\n",
      "Epoch 23332/40000, Loss: 1.3893687537347432e-05, Learning Rate: 0.000122\n",
      "Epoch 23333/40000, Loss: 1.3820294043398462e-05, Learning Rate: 0.000122\n",
      "Epoch 23334/40000, Loss: 1.3806207789457403e-05, Learning Rate: 0.000122\n",
      "Epoch 23335/40000, Loss: 5.1832033932441846e-05, Learning Rate: 0.000122\n",
      "Epoch 23336/40000, Loss: 5.755762322223745e-05, Learning Rate: 0.000122\n",
      "Epoch 23337/40000, Loss: 2.849661177606322e-05, Learning Rate: 0.000122\n",
      "Epoch 23338/40000, Loss: 5.2099188906140625e-05, Learning Rate: 0.000122\n",
      "Epoch 23339/40000, Loss: 3.9848015148891136e-05, Learning Rate: 0.000122\n",
      "Epoch 23340/40000, Loss: 3.2494546758243814e-05, Learning Rate: 0.000122\n",
      "Epoch 23341/40000, Loss: 1.3961659533379134e-05, Learning Rate: 0.000122\n",
      "Epoch 23342/40000, Loss: 3.973161801695824e-05, Learning Rate: 0.000121\n",
      "Epoch 23343/40000, Loss: 1.3745167962042615e-05, Learning Rate: 0.000121\n",
      "Epoch 23344/40000, Loss: 3.261084930272773e-05, Learning Rate: 0.000121\n",
      "Epoch 23345/40000, Loss: 3.246009146096185e-05, Learning Rate: 0.000121\n",
      "Epoch 23346/40000, Loss: 3.2393123547080904e-05, Learning Rate: 0.000121\n",
      "Epoch 23347/40000, Loss: 2.849342672561761e-05, Learning Rate: 0.000121\n",
      "Epoch 23348/40000, Loss: 5.132070873514749e-05, Learning Rate: 0.000121\n",
      "Epoch 23349/40000, Loss: 3.945931894122623e-05, Learning Rate: 0.000121\n",
      "Epoch 23350/40000, Loss: 3.2385880331275985e-05, Learning Rate: 0.000121\n",
      "Epoch 23351/40000, Loss: 1.3754144674749114e-05, Learning Rate: 0.000121\n",
      "Epoch 23352/40000, Loss: 5.73016659473069e-05, Learning Rate: 0.000121\n",
      "Epoch 23353/40000, Loss: 3.261552774347365e-05, Learning Rate: 0.000121\n",
      "Epoch 23354/40000, Loss: 5.767141192336567e-05, Learning Rate: 0.000121\n",
      "Epoch 23355/40000, Loss: 1.3788177966489457e-05, Learning Rate: 0.000121\n",
      "Epoch 23356/40000, Loss: 3.9851620385888964e-05, Learning Rate: 0.000121\n",
      "Epoch 23357/40000, Loss: 5.7449218729743734e-05, Learning Rate: 0.000121\n",
      "Epoch 23358/40000, Loss: 3.977052983827889e-05, Learning Rate: 0.000121\n",
      "Epoch 23359/40000, Loss: 5.19603090651799e-05, Learning Rate: 0.000121\n",
      "Epoch 23360/40000, Loss: 3.280981763964519e-05, Learning Rate: 0.000121\n",
      "Epoch 23361/40000, Loss: 1.4501235455099959e-05, Learning Rate: 0.000121\n",
      "Epoch 23362/40000, Loss: 5.766254980699159e-05, Learning Rate: 0.000121\n",
      "Epoch 23363/40000, Loss: 5.794945900561288e-05, Learning Rate: 0.000121\n",
      "Epoch 23364/40000, Loss: 5.209126175031997e-05, Learning Rate: 0.000121\n",
      "Epoch 23365/40000, Loss: 3.367684985278174e-05, Learning Rate: 0.000121\n",
      "Epoch 23366/40000, Loss: 5.793364834971726e-05, Learning Rate: 0.000121\n",
      "Epoch 23367/40000, Loss: 1.388459622830851e-05, Learning Rate: 0.000121\n",
      "Epoch 23368/40000, Loss: 3.2611205824650824e-05, Learning Rate: 0.000121\n",
      "Epoch 23369/40000, Loss: 3.247592394473031e-05, Learning Rate: 0.000121\n",
      "Epoch 23370/40000, Loss: 5.16082945978269e-05, Learning Rate: 0.000121\n",
      "Epoch 23371/40000, Loss: 1.3892286006012e-05, Learning Rate: 0.000121\n",
      "Epoch 23372/40000, Loss: 5.78131148358807e-05, Learning Rate: 0.000121\n",
      "Epoch 23373/40000, Loss: 3.965661016991362e-05, Learning Rate: 0.000121\n",
      "Epoch 23374/40000, Loss: 5.872840483789332e-05, Learning Rate: 0.000121\n",
      "Epoch 23375/40000, Loss: 5.7681972975842655e-05, Learning Rate: 0.000121\n",
      "Epoch 23376/40000, Loss: 2.839530134224333e-05, Learning Rate: 0.000121\n",
      "Epoch 23377/40000, Loss: 2.8419441150617786e-05, Learning Rate: 0.000121\n",
      "Epoch 23378/40000, Loss: 3.255072078900412e-05, Learning Rate: 0.000121\n",
      "Epoch 23379/40000, Loss: 5.1655431889230385e-05, Learning Rate: 0.000121\n",
      "Epoch 23380/40000, Loss: 3.964651114074513e-05, Learning Rate: 0.000121\n",
      "Epoch 23381/40000, Loss: 5.166605478734709e-05, Learning Rate: 0.000121\n",
      "Epoch 23382/40000, Loss: 5.750956916017458e-05, Learning Rate: 0.000121\n",
      "Epoch 23383/40000, Loss: 3.2411197025794536e-05, Learning Rate: 0.000121\n",
      "Epoch 23384/40000, Loss: 3.9628634112887084e-05, Learning Rate: 0.000121\n",
      "Epoch 23385/40000, Loss: 1.3783193935523741e-05, Learning Rate: 0.000121\n",
      "Epoch 23386/40000, Loss: 5.1502611313480884e-05, Learning Rate: 0.000121\n",
      "Epoch 23387/40000, Loss: 1.3962034245196264e-05, Learning Rate: 0.000121\n",
      "Epoch 23388/40000, Loss: 3.241923695895821e-05, Learning Rate: 0.000121\n",
      "Epoch 23389/40000, Loss: 5.156928818905726e-05, Learning Rate: 0.000121\n",
      "Epoch 23390/40000, Loss: 1.3745879186899401e-05, Learning Rate: 0.000121\n",
      "Epoch 23391/40000, Loss: 3.953896521124989e-05, Learning Rate: 0.000121\n",
      "Epoch 23392/40000, Loss: 3.958014349336736e-05, Learning Rate: 0.000121\n",
      "Epoch 23393/40000, Loss: 1.3727973964705598e-05, Learning Rate: 0.000121\n",
      "Epoch 23394/40000, Loss: 5.897853770875372e-05, Learning Rate: 0.000121\n",
      "Epoch 23395/40000, Loss: 3.277695577708073e-05, Learning Rate: 0.000121\n",
      "Epoch 23396/40000, Loss: 3.9857710362412035e-05, Learning Rate: 0.000121\n",
      "Epoch 23397/40000, Loss: 5.1451912440825254e-05, Learning Rate: 0.000121\n",
      "Epoch 23398/40000, Loss: 4.0193754102801904e-05, Learning Rate: 0.000121\n",
      "Epoch 23399/40000, Loss: 3.979725806857459e-05, Learning Rate: 0.000121\n",
      "Epoch 23400/40000, Loss: 5.7850382290780544e-05, Learning Rate: 0.000121\n",
      "Epoch 23401/40000, Loss: 2.8611259040189907e-05, Learning Rate: 0.000121\n",
      "Epoch 23402/40000, Loss: 2.8271844712435268e-05, Learning Rate: 0.000121\n",
      "Epoch 23403/40000, Loss: 1.3769042197964154e-05, Learning Rate: 0.000121\n",
      "Epoch 23404/40000, Loss: 4.0437491406919435e-05, Learning Rate: 0.000121\n",
      "Epoch 23405/40000, Loss: 5.1482562412275e-05, Learning Rate: 0.000121\n",
      "Epoch 23406/40000, Loss: 5.134989260113798e-05, Learning Rate: 0.000121\n",
      "Epoch 23407/40000, Loss: 3.2589665352134034e-05, Learning Rate: 0.000121\n",
      "Epoch 23408/40000, Loss: 5.1481030823197216e-05, Learning Rate: 0.000121\n",
      "Epoch 23409/40000, Loss: 1.3707951438846067e-05, Learning Rate: 0.000121\n",
      "Epoch 23410/40000, Loss: 5.71941927773878e-05, Learning Rate: 0.000121\n",
      "Epoch 23411/40000, Loss: 5.721028719563037e-05, Learning Rate: 0.000120\n",
      "Epoch 23412/40000, Loss: 5.145190880284645e-05, Learning Rate: 0.000120\n",
      "Epoch 23413/40000, Loss: 5.1387800340307876e-05, Learning Rate: 0.000120\n",
      "Epoch 23414/40000, Loss: 3.25067994708661e-05, Learning Rate: 0.000120\n",
      "Epoch 23415/40000, Loss: 1.3712019608647097e-05, Learning Rate: 0.000120\n",
      "Epoch 23416/40000, Loss: 3.984397335443646e-05, Learning Rate: 0.000120\n",
      "Epoch 23417/40000, Loss: 5.163097375771031e-05, Learning Rate: 0.000120\n",
      "Epoch 23418/40000, Loss: 5.808019341202453e-05, Learning Rate: 0.000120\n",
      "Epoch 23419/40000, Loss: 3.26564404531382e-05, Learning Rate: 0.000120\n",
      "Epoch 23420/40000, Loss: 2.8586435291799717e-05, Learning Rate: 0.000120\n",
      "Epoch 23421/40000, Loss: 3.968112650909461e-05, Learning Rate: 0.000120\n",
      "Epoch 23422/40000, Loss: 1.3719594790018164e-05, Learning Rate: 0.000120\n",
      "Epoch 23423/40000, Loss: 2.862938890757505e-05, Learning Rate: 0.000120\n",
      "Epoch 23424/40000, Loss: 5.161174340173602e-05, Learning Rate: 0.000120\n",
      "Epoch 23425/40000, Loss: 2.841723289748188e-05, Learning Rate: 0.000120\n",
      "Epoch 23426/40000, Loss: 5.181073720450513e-05, Learning Rate: 0.000120\n",
      "Epoch 23427/40000, Loss: 5.1695813453989103e-05, Learning Rate: 0.000120\n",
      "Epoch 23428/40000, Loss: 3.999114051111974e-05, Learning Rate: 0.000120\n",
      "Epoch 23429/40000, Loss: 1.3864068932889495e-05, Learning Rate: 0.000120\n",
      "Epoch 23430/40000, Loss: 5.7826127886073664e-05, Learning Rate: 0.000120\n",
      "Epoch 23431/40000, Loss: 3.9897979149827734e-05, Learning Rate: 0.000120\n",
      "Epoch 23432/40000, Loss: 3.2712516258470714e-05, Learning Rate: 0.000120\n",
      "Epoch 23433/40000, Loss: 4.076287950738333e-05, Learning Rate: 0.000120\n",
      "Epoch 23434/40000, Loss: 5.8263314713258296e-05, Learning Rate: 0.000120\n",
      "Epoch 23435/40000, Loss: 5.427289943327196e-05, Learning Rate: 0.000120\n",
      "Epoch 23436/40000, Loss: 6.050309821148403e-05, Learning Rate: 0.000120\n",
      "Epoch 23437/40000, Loss: 5.8281904784962535e-05, Learning Rate: 0.000120\n",
      "Epoch 23438/40000, Loss: 1.4620067304349504e-05, Learning Rate: 0.000120\n",
      "Epoch 23439/40000, Loss: 2.8946456950507127e-05, Learning Rate: 0.000120\n",
      "Epoch 23440/40000, Loss: 5.7881552493199706e-05, Learning Rate: 0.000120\n",
      "Epoch 23441/40000, Loss: 5.2887393394485116e-05, Learning Rate: 0.000120\n",
      "Epoch 23442/40000, Loss: 1.4321190064947587e-05, Learning Rate: 0.000120\n",
      "Epoch 23443/40000, Loss: 1.4089398973737843e-05, Learning Rate: 0.000120\n",
      "Epoch 23444/40000, Loss: 2.860825406969525e-05, Learning Rate: 0.000120\n",
      "Epoch 23445/40000, Loss: 3.999970795121044e-05, Learning Rate: 0.000120\n",
      "Epoch 23446/40000, Loss: 3.242746970499866e-05, Learning Rate: 0.000120\n",
      "Epoch 23447/40000, Loss: 2.861792927433271e-05, Learning Rate: 0.000120\n",
      "Epoch 23448/40000, Loss: 5.74555633647833e-05, Learning Rate: 0.000120\n",
      "Epoch 23449/40000, Loss: 5.738772961194627e-05, Learning Rate: 0.000120\n",
      "Epoch 23450/40000, Loss: 5.1516857638489455e-05, Learning Rate: 0.000120\n",
      "Epoch 23451/40000, Loss: 1.3868700079910923e-05, Learning Rate: 0.000120\n",
      "Epoch 23452/40000, Loss: 5.805609907838516e-05, Learning Rate: 0.000120\n",
      "Epoch 23453/40000, Loss: 5.159692227607593e-05, Learning Rate: 0.000120\n",
      "Epoch 23454/40000, Loss: 2.8695543733192608e-05, Learning Rate: 0.000120\n",
      "Epoch 23455/40000, Loss: 4.026432361570187e-05, Learning Rate: 0.000120\n",
      "Epoch 23456/40000, Loss: 3.2353818824049085e-05, Learning Rate: 0.000120\n",
      "Epoch 23457/40000, Loss: 5.133491140441038e-05, Learning Rate: 0.000120\n",
      "Epoch 23458/40000, Loss: 3.276473580626771e-05, Learning Rate: 0.000120\n",
      "Epoch 23459/40000, Loss: 5.759515624959022e-05, Learning Rate: 0.000120\n",
      "Epoch 23460/40000, Loss: 5.744418012909591e-05, Learning Rate: 0.000120\n",
      "Epoch 23461/40000, Loss: 5.7296168961329386e-05, Learning Rate: 0.000120\n",
      "Epoch 23462/40000, Loss: 5.7239518355345353e-05, Learning Rate: 0.000120\n",
      "Epoch 23463/40000, Loss: 5.727185634896159e-05, Learning Rate: 0.000120\n",
      "Epoch 23464/40000, Loss: 5.7170465879607946e-05, Learning Rate: 0.000120\n",
      "Epoch 23465/40000, Loss: 1.3551325537264347e-05, Learning Rate: 0.000120\n",
      "Epoch 23466/40000, Loss: 1.3626371583086438e-05, Learning Rate: 0.000120\n",
      "Epoch 23467/40000, Loss: 5.7083216233877465e-05, Learning Rate: 0.000120\n",
      "Epoch 23468/40000, Loss: 2.813903302012477e-05, Learning Rate: 0.000120\n",
      "Epoch 23469/40000, Loss: 1.3586668501375243e-05, Learning Rate: 0.000120\n",
      "Epoch 23470/40000, Loss: 5.726497693103738e-05, Learning Rate: 0.000120\n",
      "Epoch 23471/40000, Loss: 2.827580101438798e-05, Learning Rate: 0.000120\n",
      "Epoch 23472/40000, Loss: 3.940108581446111e-05, Learning Rate: 0.000120\n",
      "Epoch 23473/40000, Loss: 1.353971765638562e-05, Learning Rate: 0.000120\n",
      "Epoch 23474/40000, Loss: 3.933884727302939e-05, Learning Rate: 0.000120\n",
      "Epoch 23475/40000, Loss: 3.2250696676783264e-05, Learning Rate: 0.000120\n",
      "Epoch 23476/40000, Loss: 5.112243889016099e-05, Learning Rate: 0.000120\n",
      "Epoch 23477/40000, Loss: 5.113946463097818e-05, Learning Rate: 0.000120\n",
      "Epoch 23478/40000, Loss: 5.1160903240088373e-05, Learning Rate: 0.000120\n",
      "Epoch 23479/40000, Loss: 1.3636159565066919e-05, Learning Rate: 0.000120\n",
      "Epoch 23480/40000, Loss: 1.3488768672687002e-05, Learning Rate: 0.000119\n",
      "Epoch 23481/40000, Loss: 5.707559466827661e-05, Learning Rate: 0.000119\n",
      "Epoch 23482/40000, Loss: 3.936861685360782e-05, Learning Rate: 0.000119\n",
      "Epoch 23483/40000, Loss: 1.3593311450676993e-05, Learning Rate: 0.000119\n",
      "Epoch 23484/40000, Loss: 2.8112304789829068e-05, Learning Rate: 0.000119\n",
      "Epoch 23485/40000, Loss: 5.7085391745204106e-05, Learning Rate: 0.000119\n",
      "Epoch 23486/40000, Loss: 3.9746344555169344e-05, Learning Rate: 0.000119\n",
      "Epoch 23487/40000, Loss: 1.3604835658043157e-05, Learning Rate: 0.000119\n",
      "Epoch 23488/40000, Loss: 3.222693703719415e-05, Learning Rate: 0.000119\n",
      "Epoch 23489/40000, Loss: 2.815344851114787e-05, Learning Rate: 0.000119\n",
      "Epoch 23490/40000, Loss: 5.12051883561071e-05, Learning Rate: 0.000119\n",
      "Epoch 23491/40000, Loss: 2.8154610845376737e-05, Learning Rate: 0.000119\n",
      "Epoch 23492/40000, Loss: 3.2330040994565934e-05, Learning Rate: 0.000119\n",
      "Epoch 23493/40000, Loss: 5.723739741370082e-05, Learning Rate: 0.000119\n",
      "Epoch 23494/40000, Loss: 1.3610837413580157e-05, Learning Rate: 0.000119\n",
      "Epoch 23495/40000, Loss: 3.9619088056497276e-05, Learning Rate: 0.000119\n",
      "Epoch 23496/40000, Loss: 1.367723507428309e-05, Learning Rate: 0.000119\n",
      "Epoch 23497/40000, Loss: 5.740968481404707e-05, Learning Rate: 0.000119\n",
      "Epoch 23498/40000, Loss: 3.955322245019488e-05, Learning Rate: 0.000119\n",
      "Epoch 23499/40000, Loss: 2.8311424102867022e-05, Learning Rate: 0.000119\n",
      "Epoch 23500/40000, Loss: 5.136702748131938e-05, Learning Rate: 0.000119\n",
      "Epoch 23501/40000, Loss: 2.8096594178350642e-05, Learning Rate: 0.000119\n",
      "Epoch 23502/40000, Loss: 5.743210203945637e-05, Learning Rate: 0.000119\n",
      "Epoch 23503/40000, Loss: 2.8245358407730237e-05, Learning Rate: 0.000119\n",
      "Epoch 23504/40000, Loss: 5.853040420333855e-05, Learning Rate: 0.000119\n",
      "Epoch 23505/40000, Loss: 5.141894143889658e-05, Learning Rate: 0.000119\n",
      "Epoch 23506/40000, Loss: 2.822383066813927e-05, Learning Rate: 0.000119\n",
      "Epoch 23507/40000, Loss: 3.964304778492078e-05, Learning Rate: 0.000119\n",
      "Epoch 23508/40000, Loss: 5.192684693611227e-05, Learning Rate: 0.000119\n",
      "Epoch 23509/40000, Loss: 5.143735324963927e-05, Learning Rate: 0.000119\n",
      "Epoch 23510/40000, Loss: 1.3671779015567154e-05, Learning Rate: 0.000119\n",
      "Epoch 23511/40000, Loss: 5.199579391046427e-05, Learning Rate: 0.000119\n",
      "Epoch 23512/40000, Loss: 3.228627974749543e-05, Learning Rate: 0.000119\n",
      "Epoch 23513/40000, Loss: 1.3693504115508404e-05, Learning Rate: 0.000119\n",
      "Epoch 23514/40000, Loss: 5.733149737352505e-05, Learning Rate: 0.000119\n",
      "Epoch 23515/40000, Loss: 2.817162749124691e-05, Learning Rate: 0.000119\n",
      "Epoch 23516/40000, Loss: 5.147654883330688e-05, Learning Rate: 0.000119\n",
      "Epoch 23517/40000, Loss: 5.202516331337392e-05, Learning Rate: 0.000119\n",
      "Epoch 23518/40000, Loss: 1.3674548426934052e-05, Learning Rate: 0.000119\n",
      "Epoch 23519/40000, Loss: 3.244254912715405e-05, Learning Rate: 0.000119\n",
      "Epoch 23520/40000, Loss: 3.972114063799381e-05, Learning Rate: 0.000119\n",
      "Epoch 23521/40000, Loss: 5.1639308367157355e-05, Learning Rate: 0.000119\n",
      "Epoch 23522/40000, Loss: 4.0546106902183965e-05, Learning Rate: 0.000119\n",
      "Epoch 23523/40000, Loss: 2.8612048481591046e-05, Learning Rate: 0.000119\n",
      "Epoch 23524/40000, Loss: 2.8395781555445865e-05, Learning Rate: 0.000119\n",
      "Epoch 23525/40000, Loss: 5.1316488679731265e-05, Learning Rate: 0.000119\n",
      "Epoch 23526/40000, Loss: 2.8347072657197714e-05, Learning Rate: 0.000119\n",
      "Epoch 23527/40000, Loss: 3.229793583159335e-05, Learning Rate: 0.000119\n",
      "Epoch 23528/40000, Loss: 3.959500099881552e-05, Learning Rate: 0.000119\n",
      "Epoch 23529/40000, Loss: 3.950109385186806e-05, Learning Rate: 0.000119\n",
      "Epoch 23530/40000, Loss: 3.222130180802196e-05, Learning Rate: 0.000119\n",
      "Epoch 23531/40000, Loss: 3.2267907954519615e-05, Learning Rate: 0.000119\n",
      "Epoch 23532/40000, Loss: 1.3794754522677977e-05, Learning Rate: 0.000119\n",
      "Epoch 23533/40000, Loss: 3.961356924264692e-05, Learning Rate: 0.000119\n",
      "Epoch 23534/40000, Loss: 3.23502863466274e-05, Learning Rate: 0.000119\n",
      "Epoch 23535/40000, Loss: 5.7235862186644226e-05, Learning Rate: 0.000119\n",
      "Epoch 23536/40000, Loss: 1.3644825230585411e-05, Learning Rate: 0.000119\n",
      "Epoch 23537/40000, Loss: 5.765632158727385e-05, Learning Rate: 0.000119\n",
      "Epoch 23538/40000, Loss: 1.373263330606278e-05, Learning Rate: 0.000119\n",
      "Epoch 23539/40000, Loss: 3.986149749835022e-05, Learning Rate: 0.000119\n",
      "Epoch 23540/40000, Loss: 3.9680810004938394e-05, Learning Rate: 0.000119\n",
      "Epoch 23541/40000, Loss: 3.954221756430343e-05, Learning Rate: 0.000119\n",
      "Epoch 23542/40000, Loss: 5.136874096933752e-05, Learning Rate: 0.000119\n",
      "Epoch 23543/40000, Loss: 5.725767186959274e-05, Learning Rate: 0.000119\n",
      "Epoch 23544/40000, Loss: 4.011455530417152e-05, Learning Rate: 0.000119\n",
      "Epoch 23545/40000, Loss: 2.844464688678272e-05, Learning Rate: 0.000119\n",
      "Epoch 23546/40000, Loss: 1.3734978892898653e-05, Learning Rate: 0.000119\n",
      "Epoch 23547/40000, Loss: 1.3906461390433833e-05, Learning Rate: 0.000119\n",
      "Epoch 23548/40000, Loss: 5.7773489970713854e-05, Learning Rate: 0.000119\n",
      "Epoch 23549/40000, Loss: 2.8401200324879028e-05, Learning Rate: 0.000119\n",
      "Epoch 23550/40000, Loss: 2.8374373869155534e-05, Learning Rate: 0.000118\n",
      "Epoch 23551/40000, Loss: 5.298648102325387e-05, Learning Rate: 0.000118\n",
      "Epoch 23552/40000, Loss: 3.262024256400764e-05, Learning Rate: 0.000118\n",
      "Epoch 23553/40000, Loss: 1.40242273118929e-05, Learning Rate: 0.000118\n",
      "Epoch 23554/40000, Loss: 5.781438812846318e-05, Learning Rate: 0.000118\n",
      "Epoch 23555/40000, Loss: 4.023917790618725e-05, Learning Rate: 0.000118\n",
      "Epoch 23556/40000, Loss: 5.16441032232251e-05, Learning Rate: 0.000118\n",
      "Epoch 23557/40000, Loss: 2.848304939107038e-05, Learning Rate: 0.000118\n",
      "Epoch 23558/40000, Loss: 5.1951807108707726e-05, Learning Rate: 0.000118\n",
      "Epoch 23559/40000, Loss: 5.8755034842761233e-05, Learning Rate: 0.000118\n",
      "Epoch 23560/40000, Loss: 3.297446164651774e-05, Learning Rate: 0.000118\n",
      "Epoch 23561/40000, Loss: 3.995369843323715e-05, Learning Rate: 0.000118\n",
      "Epoch 23562/40000, Loss: 1.4164790627546608e-05, Learning Rate: 0.000118\n",
      "Epoch 23563/40000, Loss: 5.81511267228052e-05, Learning Rate: 0.000118\n",
      "Epoch 23564/40000, Loss: 1.391028672514949e-05, Learning Rate: 0.000118\n",
      "Epoch 23565/40000, Loss: 2.8658858354901895e-05, Learning Rate: 0.000118\n",
      "Epoch 23566/40000, Loss: 1.3989498256705701e-05, Learning Rate: 0.000118\n",
      "Epoch 23567/40000, Loss: 1.3683351426152512e-05, Learning Rate: 0.000118\n",
      "Epoch 23568/40000, Loss: 5.15994724992197e-05, Learning Rate: 0.000118\n",
      "Epoch 23569/40000, Loss: 3.978447784902528e-05, Learning Rate: 0.000118\n",
      "Epoch 23570/40000, Loss: 5.1601080485852435e-05, Learning Rate: 0.000118\n",
      "Epoch 23571/40000, Loss: 1.3892115021008067e-05, Learning Rate: 0.000118\n",
      "Epoch 23572/40000, Loss: 2.8446280339267105e-05, Learning Rate: 0.000118\n",
      "Epoch 23573/40000, Loss: 3.255071715102531e-05, Learning Rate: 0.000118\n",
      "Epoch 23574/40000, Loss: 3.243203536840156e-05, Learning Rate: 0.000118\n",
      "Epoch 23575/40000, Loss: 2.869126183213666e-05, Learning Rate: 0.000118\n",
      "Epoch 23576/40000, Loss: 5.160380533197895e-05, Learning Rate: 0.000118\n",
      "Epoch 23577/40000, Loss: 3.251671660109423e-05, Learning Rate: 0.000118\n",
      "Epoch 23578/40000, Loss: 5.242276529315859e-05, Learning Rate: 0.000118\n",
      "Epoch 23579/40000, Loss: 1.4007248864800204e-05, Learning Rate: 0.000118\n",
      "Epoch 23580/40000, Loss: 5.2508632506942376e-05, Learning Rate: 0.000118\n",
      "Epoch 23581/40000, Loss: 2.860042150132358e-05, Learning Rate: 0.000118\n",
      "Epoch 23582/40000, Loss: 2.816691812768113e-05, Learning Rate: 0.000118\n",
      "Epoch 23583/40000, Loss: 5.136026084073819e-05, Learning Rate: 0.000118\n",
      "Epoch 23584/40000, Loss: 5.134498132974841e-05, Learning Rate: 0.000118\n",
      "Epoch 23585/40000, Loss: 2.8204040063428693e-05, Learning Rate: 0.000118\n",
      "Epoch 23586/40000, Loss: 3.234512041672133e-05, Learning Rate: 0.000118\n",
      "Epoch 23587/40000, Loss: 3.226135959266685e-05, Learning Rate: 0.000118\n",
      "Epoch 23588/40000, Loss: 5.127465192344971e-05, Learning Rate: 0.000118\n",
      "Epoch 23589/40000, Loss: 5.7184741308446974e-05, Learning Rate: 0.000118\n",
      "Epoch 23590/40000, Loss: 3.216845288989134e-05, Learning Rate: 0.000118\n",
      "Epoch 23591/40000, Loss: 1.3626648069475777e-05, Learning Rate: 0.000118\n",
      "Epoch 23592/40000, Loss: 5.741819040849805e-05, Learning Rate: 0.000118\n",
      "Epoch 23593/40000, Loss: 1.3658217540069018e-05, Learning Rate: 0.000118\n",
      "Epoch 23594/40000, Loss: 5.754216181230731e-05, Learning Rate: 0.000118\n",
      "Epoch 23595/40000, Loss: 3.958808156312443e-05, Learning Rate: 0.000118\n",
      "Epoch 23596/40000, Loss: 3.2422005460830405e-05, Learning Rate: 0.000118\n",
      "Epoch 23597/40000, Loss: 2.819197288772557e-05, Learning Rate: 0.000118\n",
      "Epoch 23598/40000, Loss: 2.8125210519647226e-05, Learning Rate: 0.000118\n",
      "Epoch 23599/40000, Loss: 3.239307625335641e-05, Learning Rate: 0.000118\n",
      "Epoch 23600/40000, Loss: 5.725643131881952e-05, Learning Rate: 0.000118\n",
      "Epoch 23601/40000, Loss: 3.949431993532926e-05, Learning Rate: 0.000118\n",
      "Epoch 23602/40000, Loss: 5.122948277858086e-05, Learning Rate: 0.000118\n",
      "Epoch 23603/40000, Loss: 5.118151239003055e-05, Learning Rate: 0.000118\n",
      "Epoch 23604/40000, Loss: 3.217201083316468e-05, Learning Rate: 0.000118\n",
      "Epoch 23605/40000, Loss: 1.3682952157978434e-05, Learning Rate: 0.000118\n",
      "Epoch 23606/40000, Loss: 5.7345416280440986e-05, Learning Rate: 0.000118\n",
      "Epoch 23607/40000, Loss: 3.2219559216173366e-05, Learning Rate: 0.000118\n",
      "Epoch 23608/40000, Loss: 3.22684645652771e-05, Learning Rate: 0.000118\n",
      "Epoch 23609/40000, Loss: 2.8150614525657147e-05, Learning Rate: 0.000118\n",
      "Epoch 23610/40000, Loss: 1.3570991541200783e-05, Learning Rate: 0.000118\n",
      "Epoch 23611/40000, Loss: 5.12165752297733e-05, Learning Rate: 0.000118\n",
      "Epoch 23612/40000, Loss: 5.115916792419739e-05, Learning Rate: 0.000118\n",
      "Epoch 23613/40000, Loss: 3.95654424210079e-05, Learning Rate: 0.000118\n",
      "Epoch 23614/40000, Loss: 1.3641362784255762e-05, Learning Rate: 0.000118\n",
      "Epoch 23615/40000, Loss: 5.143772068549879e-05, Learning Rate: 0.000118\n",
      "Epoch 23616/40000, Loss: 3.972423655795865e-05, Learning Rate: 0.000118\n",
      "Epoch 23617/40000, Loss: 3.972448757849634e-05, Learning Rate: 0.000118\n",
      "Epoch 23618/40000, Loss: 5.771009819000028e-05, Learning Rate: 0.000118\n",
      "Epoch 23619/40000, Loss: 2.8256967198103666e-05, Learning Rate: 0.000118\n",
      "Epoch 23620/40000, Loss: 2.83424487861339e-05, Learning Rate: 0.000118\n",
      "Epoch 23621/40000, Loss: 3.253592149121687e-05, Learning Rate: 0.000117\n",
      "Epoch 23622/40000, Loss: 1.3837398000760004e-05, Learning Rate: 0.000117\n",
      "Epoch 23623/40000, Loss: 2.835632039932534e-05, Learning Rate: 0.000117\n",
      "Epoch 23624/40000, Loss: 2.838649925251957e-05, Learning Rate: 0.000117\n",
      "Epoch 23625/40000, Loss: 5.1708921091631055e-05, Learning Rate: 0.000117\n",
      "Epoch 23626/40000, Loss: 3.287113941041753e-05, Learning Rate: 0.000117\n",
      "Epoch 23627/40000, Loss: 2.863718691514805e-05, Learning Rate: 0.000117\n",
      "Epoch 23628/40000, Loss: 1.4181246115185786e-05, Learning Rate: 0.000117\n",
      "Epoch 23629/40000, Loss: 1.4413978533411864e-05, Learning Rate: 0.000117\n",
      "Epoch 23630/40000, Loss: 4.028914190712385e-05, Learning Rate: 0.000117\n",
      "Epoch 23631/40000, Loss: 1.6540472643100657e-05, Learning Rate: 0.000117\n",
      "Epoch 23632/40000, Loss: 1.4869820915919263e-05, Learning Rate: 0.000117\n",
      "Epoch 23633/40000, Loss: 3.287431172793731e-05, Learning Rate: 0.000117\n",
      "Epoch 23634/40000, Loss: 2.8503889552666806e-05, Learning Rate: 0.000117\n",
      "Epoch 23635/40000, Loss: 1.4046147953195032e-05, Learning Rate: 0.000117\n",
      "Epoch 23636/40000, Loss: 2.87119037238881e-05, Learning Rate: 0.000117\n",
      "Epoch 23637/40000, Loss: 2.8303324143053032e-05, Learning Rate: 0.000117\n",
      "Epoch 23638/40000, Loss: 5.745629096054472e-05, Learning Rate: 0.000117\n",
      "Epoch 23639/40000, Loss: 1.383140443067532e-05, Learning Rate: 0.000117\n",
      "Epoch 23640/40000, Loss: 3.9805046981200576e-05, Learning Rate: 0.000117\n",
      "Epoch 23641/40000, Loss: 2.814230538206175e-05, Learning Rate: 0.000117\n",
      "Epoch 23642/40000, Loss: 5.137729021953419e-05, Learning Rate: 0.000117\n",
      "Epoch 23643/40000, Loss: 3.232797826058231e-05, Learning Rate: 0.000117\n",
      "Epoch 23644/40000, Loss: 3.975643267040141e-05, Learning Rate: 0.000117\n",
      "Epoch 23645/40000, Loss: 3.234414180042222e-05, Learning Rate: 0.000117\n",
      "Epoch 23646/40000, Loss: 3.227755951229483e-05, Learning Rate: 0.000117\n",
      "Epoch 23647/40000, Loss: 2.8149859645054676e-05, Learning Rate: 0.000117\n",
      "Epoch 23648/40000, Loss: 3.236904012737796e-05, Learning Rate: 0.000117\n",
      "Epoch 23649/40000, Loss: 2.815805419231765e-05, Learning Rate: 0.000117\n",
      "Epoch 23650/40000, Loss: 5.112186408950947e-05, Learning Rate: 0.000117\n",
      "Epoch 23651/40000, Loss: 2.8082247808924876e-05, Learning Rate: 0.000117\n",
      "Epoch 23652/40000, Loss: 1.3618062439491041e-05, Learning Rate: 0.000117\n",
      "Epoch 23653/40000, Loss: 1.3552078598877415e-05, Learning Rate: 0.000117\n",
      "Epoch 23654/40000, Loss: 2.809547913784627e-05, Learning Rate: 0.000117\n",
      "Epoch 23655/40000, Loss: 3.943008778151125e-05, Learning Rate: 0.000117\n",
      "Epoch 23656/40000, Loss: 2.8074449801351875e-05, Learning Rate: 0.000117\n",
      "Epoch 23657/40000, Loss: 1.3520851098292042e-05, Learning Rate: 0.000117\n",
      "Epoch 23658/40000, Loss: 5.113452425575815e-05, Learning Rate: 0.000117\n",
      "Epoch 23659/40000, Loss: 3.934749111067504e-05, Learning Rate: 0.000117\n",
      "Epoch 23660/40000, Loss: 5.716147643397562e-05, Learning Rate: 0.000117\n",
      "Epoch 23661/40000, Loss: 3.935096538043581e-05, Learning Rate: 0.000117\n",
      "Epoch 23662/40000, Loss: 5.110311394673772e-05, Learning Rate: 0.000117\n",
      "Epoch 23663/40000, Loss: 1.3629156455863267e-05, Learning Rate: 0.000117\n",
      "Epoch 23664/40000, Loss: 3.9502680010627955e-05, Learning Rate: 0.000117\n",
      "Epoch 23665/40000, Loss: 3.943212504964322e-05, Learning Rate: 0.000117\n",
      "Epoch 23666/40000, Loss: 2.8046450097463094e-05, Learning Rate: 0.000117\n",
      "Epoch 23667/40000, Loss: 5.1077164243906736e-05, Learning Rate: 0.000117\n",
      "Epoch 23668/40000, Loss: 2.8221276807016693e-05, Learning Rate: 0.000117\n",
      "Epoch 23669/40000, Loss: 2.804013274726458e-05, Learning Rate: 0.000117\n",
      "Epoch 23670/40000, Loss: 1.3629282875626814e-05, Learning Rate: 0.000117\n",
      "Epoch 23671/40000, Loss: 3.929765080101788e-05, Learning Rate: 0.000117\n",
      "Epoch 23672/40000, Loss: 2.7989073714707047e-05, Learning Rate: 0.000117\n",
      "Epoch 23673/40000, Loss: 1.3480625966622028e-05, Learning Rate: 0.000117\n",
      "Epoch 23674/40000, Loss: 1.3482307622325607e-05, Learning Rate: 0.000117\n",
      "Epoch 23675/40000, Loss: 5.70970332773868e-05, Learning Rate: 0.000117\n",
      "Epoch 23676/40000, Loss: 3.209226633771323e-05, Learning Rate: 0.000117\n",
      "Epoch 23677/40000, Loss: 5.708428216166794e-05, Learning Rate: 0.000117\n",
      "Epoch 23678/40000, Loss: 3.206418841728009e-05, Learning Rate: 0.000117\n",
      "Epoch 23679/40000, Loss: 5.780862193205394e-05, Learning Rate: 0.000117\n",
      "Epoch 23680/40000, Loss: 1.353213065158343e-05, Learning Rate: 0.000117\n",
      "Epoch 23681/40000, Loss: 3.9380553062073886e-05, Learning Rate: 0.000117\n",
      "Epoch 23682/40000, Loss: 3.2121690310304984e-05, Learning Rate: 0.000117\n",
      "Epoch 23683/40000, Loss: 1.3595943528343923e-05, Learning Rate: 0.000117\n",
      "Epoch 23684/40000, Loss: 3.9497059333371e-05, Learning Rate: 0.000117\n",
      "Epoch 23685/40000, Loss: 5.1224520575487986e-05, Learning Rate: 0.000117\n",
      "Epoch 23686/40000, Loss: 2.8291404305491596e-05, Learning Rate: 0.000117\n",
      "Epoch 23687/40000, Loss: 2.8143696908955462e-05, Learning Rate: 0.000117\n",
      "Epoch 23688/40000, Loss: 3.963979543186724e-05, Learning Rate: 0.000117\n",
      "Epoch 23689/40000, Loss: 5.742803477915004e-05, Learning Rate: 0.000117\n",
      "Epoch 23690/40000, Loss: 3.222536906832829e-05, Learning Rate: 0.000117\n",
      "Epoch 23691/40000, Loss: 3.222404484404251e-05, Learning Rate: 0.000117\n",
      "Epoch 23692/40000, Loss: 5.1428218284854665e-05, Learning Rate: 0.000116\n",
      "Epoch 23693/40000, Loss: 5.128169505042024e-05, Learning Rate: 0.000116\n",
      "Epoch 23694/40000, Loss: 2.8193955586175434e-05, Learning Rate: 0.000116\n",
      "Epoch 23695/40000, Loss: 5.2373387006809935e-05, Learning Rate: 0.000116\n",
      "Epoch 23696/40000, Loss: 5.762689033872448e-05, Learning Rate: 0.000116\n",
      "Epoch 23697/40000, Loss: 2.8432039471226744e-05, Learning Rate: 0.000116\n",
      "Epoch 23698/40000, Loss: 3.232362723792903e-05, Learning Rate: 0.000116\n",
      "Epoch 23699/40000, Loss: 3.2187486795010045e-05, Learning Rate: 0.000116\n",
      "Epoch 23700/40000, Loss: 5.137330663274042e-05, Learning Rate: 0.000116\n",
      "Epoch 23701/40000, Loss: 3.94407834392041e-05, Learning Rate: 0.000116\n",
      "Epoch 23702/40000, Loss: 1.3642943486047443e-05, Learning Rate: 0.000116\n",
      "Epoch 23703/40000, Loss: 1.3623346603708342e-05, Learning Rate: 0.000116\n",
      "Epoch 23704/40000, Loss: 5.710729965358041e-05, Learning Rate: 0.000116\n",
      "Epoch 23705/40000, Loss: 5.70037191209849e-05, Learning Rate: 0.000116\n",
      "Epoch 23706/40000, Loss: 5.697137021343224e-05, Learning Rate: 0.000116\n",
      "Epoch 23707/40000, Loss: 5.109005360282026e-05, Learning Rate: 0.000116\n",
      "Epoch 23708/40000, Loss: 5.714180588256568e-05, Learning Rate: 0.000116\n",
      "Epoch 23709/40000, Loss: 1.3518501873477362e-05, Learning Rate: 0.000116\n",
      "Epoch 23710/40000, Loss: 5.100879934616387e-05, Learning Rate: 0.000116\n",
      "Epoch 23711/40000, Loss: 1.3543522982217837e-05, Learning Rate: 0.000116\n",
      "Epoch 23712/40000, Loss: 3.93806112697348e-05, Learning Rate: 0.000116\n",
      "Epoch 23713/40000, Loss: 2.799656067509204e-05, Learning Rate: 0.000116\n",
      "Epoch 23714/40000, Loss: 5.718977263313718e-05, Learning Rate: 0.000116\n",
      "Epoch 23715/40000, Loss: 5.707837044610642e-05, Learning Rate: 0.000116\n",
      "Epoch 23716/40000, Loss: 3.2091535103973e-05, Learning Rate: 0.000116\n",
      "Epoch 23717/40000, Loss: 1.3670447515323758e-05, Learning Rate: 0.000116\n",
      "Epoch 23718/40000, Loss: 1.3793687685392797e-05, Learning Rate: 0.000116\n",
      "Epoch 23719/40000, Loss: 2.858720108633861e-05, Learning Rate: 0.000116\n",
      "Epoch 23720/40000, Loss: 3.96043251384981e-05, Learning Rate: 0.000116\n",
      "Epoch 23721/40000, Loss: 5.154189057066105e-05, Learning Rate: 0.000116\n",
      "Epoch 23722/40000, Loss: 2.830984340107534e-05, Learning Rate: 0.000116\n",
      "Epoch 23723/40000, Loss: 4.043720764457248e-05, Learning Rate: 0.000116\n",
      "Epoch 23724/40000, Loss: 3.267616921220906e-05, Learning Rate: 0.000116\n",
      "Epoch 23725/40000, Loss: 4.086651460966095e-05, Learning Rate: 0.000116\n",
      "Epoch 23726/40000, Loss: 2.9627466574311256e-05, Learning Rate: 0.000116\n",
      "Epoch 23727/40000, Loss: 5.774652163381688e-05, Learning Rate: 0.000116\n",
      "Epoch 23728/40000, Loss: 5.743392830481753e-05, Learning Rate: 0.000116\n",
      "Epoch 23729/40000, Loss: 1.418625015503494e-05, Learning Rate: 0.000116\n",
      "Epoch 23730/40000, Loss: 3.258793367422186e-05, Learning Rate: 0.000116\n",
      "Epoch 23731/40000, Loss: 3.2734456908656284e-05, Learning Rate: 0.000116\n",
      "Epoch 23732/40000, Loss: 5.194785990170203e-05, Learning Rate: 0.000116\n",
      "Epoch 23733/40000, Loss: 5.1461120165186e-05, Learning Rate: 0.000116\n",
      "Epoch 23734/40000, Loss: 5.7916327932616696e-05, Learning Rate: 0.000116\n",
      "Epoch 23735/40000, Loss: 3.976506559411064e-05, Learning Rate: 0.000116\n",
      "Epoch 23736/40000, Loss: 5.162064917385578e-05, Learning Rate: 0.000116\n",
      "Epoch 23737/40000, Loss: 1.4284705684985965e-05, Learning Rate: 0.000116\n",
      "Epoch 23738/40000, Loss: 5.771216456196271e-05, Learning Rate: 0.000116\n",
      "Epoch 23739/40000, Loss: 1.4054166967980564e-05, Learning Rate: 0.000116\n",
      "Epoch 23740/40000, Loss: 4.011800410808064e-05, Learning Rate: 0.000116\n",
      "Epoch 23741/40000, Loss: 2.8303154977038503e-05, Learning Rate: 0.000116\n",
      "Epoch 23742/40000, Loss: 2.8338605261524208e-05, Learning Rate: 0.000116\n",
      "Epoch 23743/40000, Loss: 3.9823935367166996e-05, Learning Rate: 0.000116\n",
      "Epoch 23744/40000, Loss: 3.961204129154794e-05, Learning Rate: 0.000116\n",
      "Epoch 23745/40000, Loss: 3.996914529125206e-05, Learning Rate: 0.000116\n",
      "Epoch 23746/40000, Loss: 3.267864303779788e-05, Learning Rate: 0.000116\n",
      "Epoch 23747/40000, Loss: 5.1780585636151955e-05, Learning Rate: 0.000116\n",
      "Epoch 23748/40000, Loss: 5.1619557780213654e-05, Learning Rate: 0.000116\n",
      "Epoch 23749/40000, Loss: 1.4363280570250936e-05, Learning Rate: 0.000116\n",
      "Epoch 23750/40000, Loss: 1.4030327292857692e-05, Learning Rate: 0.000116\n",
      "Epoch 23751/40000, Loss: 3.238680073991418e-05, Learning Rate: 0.000116\n",
      "Epoch 23752/40000, Loss: 5.735622835345566e-05, Learning Rate: 0.000116\n",
      "Epoch 23753/40000, Loss: 5.1382921810727566e-05, Learning Rate: 0.000116\n",
      "Epoch 23754/40000, Loss: 2.8093985747545958e-05, Learning Rate: 0.000116\n",
      "Epoch 23755/40000, Loss: 5.213497206568718e-05, Learning Rate: 0.000116\n",
      "Epoch 23756/40000, Loss: 3.2265317713608965e-05, Learning Rate: 0.000116\n",
      "Epoch 23757/40000, Loss: 1.35875188789214e-05, Learning Rate: 0.000116\n",
      "Epoch 23758/40000, Loss: 5.707346281269565e-05, Learning Rate: 0.000116\n",
      "Epoch 23759/40000, Loss: 3.207963891327381e-05, Learning Rate: 0.000116\n",
      "Epoch 23760/40000, Loss: 5.1016409997828305e-05, Learning Rate: 0.000116\n",
      "Epoch 23761/40000, Loss: 1.3539490282710176e-05, Learning Rate: 0.000116\n",
      "Epoch 23762/40000, Loss: 5.707771924789995e-05, Learning Rate: 0.000116\n",
      "Epoch 23763/40000, Loss: 3.9377704524667934e-05, Learning Rate: 0.000116\n",
      "Epoch 23764/40000, Loss: 5.111196878715418e-05, Learning Rate: 0.000115\n",
      "Epoch 23765/40000, Loss: 1.368872653983999e-05, Learning Rate: 0.000115\n",
      "Epoch 23766/40000, Loss: 3.952363840653561e-05, Learning Rate: 0.000115\n",
      "Epoch 23767/40000, Loss: 5.708813478122465e-05, Learning Rate: 0.000115\n",
      "Epoch 23768/40000, Loss: 3.953972191084176e-05, Learning Rate: 0.000115\n",
      "Epoch 23769/40000, Loss: 2.8008129447698593e-05, Learning Rate: 0.000115\n",
      "Epoch 23770/40000, Loss: 2.80041313089896e-05, Learning Rate: 0.000115\n",
      "Epoch 23771/40000, Loss: 2.7942880478804e-05, Learning Rate: 0.000115\n",
      "Epoch 23772/40000, Loss: 3.942095645470545e-05, Learning Rate: 0.000115\n",
      "Epoch 23773/40000, Loss: 5.111003338242881e-05, Learning Rate: 0.000115\n",
      "Epoch 23774/40000, Loss: 3.947876029997133e-05, Learning Rate: 0.000115\n",
      "Epoch 23775/40000, Loss: 1.3620569916383829e-05, Learning Rate: 0.000115\n",
      "Epoch 23776/40000, Loss: 3.2035692129284143e-05, Learning Rate: 0.000115\n",
      "Epoch 23777/40000, Loss: 5.739414336858317e-05, Learning Rate: 0.000115\n",
      "Epoch 23778/40000, Loss: 5.116967804497108e-05, Learning Rate: 0.000115\n",
      "Epoch 23779/40000, Loss: 3.9512386138085276e-05, Learning Rate: 0.000115\n",
      "Epoch 23780/40000, Loss: 1.3583607142209075e-05, Learning Rate: 0.000115\n",
      "Epoch 23781/40000, Loss: 5.7252189435530454e-05, Learning Rate: 0.000115\n",
      "Epoch 23782/40000, Loss: 1.3739092537434772e-05, Learning Rate: 0.000115\n",
      "Epoch 23783/40000, Loss: 1.35636646518833e-05, Learning Rate: 0.000115\n",
      "Epoch 23784/40000, Loss: 5.0971026212209836e-05, Learning Rate: 0.000115\n",
      "Epoch 23785/40000, Loss: 3.1980649509932846e-05, Learning Rate: 0.000115\n",
      "Epoch 23786/40000, Loss: 3.9365804695989937e-05, Learning Rate: 0.000115\n",
      "Epoch 23787/40000, Loss: 5.1109440391883254e-05, Learning Rate: 0.000115\n",
      "Epoch 23788/40000, Loss: 5.718684769817628e-05, Learning Rate: 0.000115\n",
      "Epoch 23789/40000, Loss: 1.3612115253636148e-05, Learning Rate: 0.000115\n",
      "Epoch 23790/40000, Loss: 1.360609348921571e-05, Learning Rate: 0.000115\n",
      "Epoch 23791/40000, Loss: 1.365177558909636e-05, Learning Rate: 0.000115\n",
      "Epoch 23792/40000, Loss: 3.9433893107343465e-05, Learning Rate: 0.000115\n",
      "Epoch 23793/40000, Loss: 3.9389800804201514e-05, Learning Rate: 0.000115\n",
      "Epoch 23794/40000, Loss: 1.3522416338673793e-05, Learning Rate: 0.000115\n",
      "Epoch 23795/40000, Loss: 1.3526778275263496e-05, Learning Rate: 0.000115\n",
      "Epoch 23796/40000, Loss: 2.7871859856531955e-05, Learning Rate: 0.000115\n",
      "Epoch 23797/40000, Loss: 1.34854335556156e-05, Learning Rate: 0.000115\n",
      "Epoch 23798/40000, Loss: 3.932063918909989e-05, Learning Rate: 0.000115\n",
      "Epoch 23799/40000, Loss: 3.1981577194528654e-05, Learning Rate: 0.000115\n",
      "Epoch 23800/40000, Loss: 2.789856262097601e-05, Learning Rate: 0.000115\n",
      "Epoch 23801/40000, Loss: 3.1953306461218745e-05, Learning Rate: 0.000115\n",
      "Epoch 23802/40000, Loss: 5.7093999203061685e-05, Learning Rate: 0.000115\n",
      "Epoch 23803/40000, Loss: 2.789659811242018e-05, Learning Rate: 0.000115\n",
      "Epoch 23804/40000, Loss: 5.709489414584823e-05, Learning Rate: 0.000115\n",
      "Epoch 23805/40000, Loss: 3.929433296434581e-05, Learning Rate: 0.000115\n",
      "Epoch 23806/40000, Loss: 2.7921194487134926e-05, Learning Rate: 0.000115\n",
      "Epoch 23807/40000, Loss: 2.7950243747909553e-05, Learning Rate: 0.000115\n",
      "Epoch 23808/40000, Loss: 3.932707113563083e-05, Learning Rate: 0.000115\n",
      "Epoch 23809/40000, Loss: 3.940382521250285e-05, Learning Rate: 0.000115\n",
      "Epoch 23810/40000, Loss: 2.799426692945417e-05, Learning Rate: 0.000115\n",
      "Epoch 23811/40000, Loss: 5.72979733988177e-05, Learning Rate: 0.000115\n",
      "Epoch 23812/40000, Loss: 2.7996838980470784e-05, Learning Rate: 0.000115\n",
      "Epoch 23813/40000, Loss: 3.941727482015267e-05, Learning Rate: 0.000115\n",
      "Epoch 23814/40000, Loss: 5.1064358558505774e-05, Learning Rate: 0.000115\n",
      "Epoch 23815/40000, Loss: 2.806171505653765e-05, Learning Rate: 0.000115\n",
      "Epoch 23816/40000, Loss: 5.709725519409403e-05, Learning Rate: 0.000115\n",
      "Epoch 23817/40000, Loss: 1.3668065548699815e-05, Learning Rate: 0.000115\n",
      "Epoch 23818/40000, Loss: 3.19742139254231e-05, Learning Rate: 0.000115\n",
      "Epoch 23819/40000, Loss: 1.3918869626650121e-05, Learning Rate: 0.000115\n",
      "Epoch 23820/40000, Loss: 5.73733304918278e-05, Learning Rate: 0.000115\n",
      "Epoch 23821/40000, Loss: 5.144976239535026e-05, Learning Rate: 0.000115\n",
      "Epoch 23822/40000, Loss: 3.978122549597174e-05, Learning Rate: 0.000115\n",
      "Epoch 23823/40000, Loss: 5.1379047363298014e-05, Learning Rate: 0.000115\n",
      "Epoch 23824/40000, Loss: 5.763424269389361e-05, Learning Rate: 0.000115\n",
      "Epoch 23825/40000, Loss: 5.741027416661382e-05, Learning Rate: 0.000115\n",
      "Epoch 23826/40000, Loss: 1.3805268281430472e-05, Learning Rate: 0.000115\n",
      "Epoch 23827/40000, Loss: 5.1771603466477245e-05, Learning Rate: 0.000115\n",
      "Epoch 23828/40000, Loss: 5.136948311701417e-05, Learning Rate: 0.000115\n",
      "Epoch 23829/40000, Loss: 1.3854296412318945e-05, Learning Rate: 0.000115\n",
      "Epoch 23830/40000, Loss: 3.266854037065059e-05, Learning Rate: 0.000115\n",
      "Epoch 23831/40000, Loss: 5.5088981753215194e-05, Learning Rate: 0.000115\n",
      "Epoch 23832/40000, Loss: 2.8482179914135486e-05, Learning Rate: 0.000115\n",
      "Epoch 23833/40000, Loss: 3.0196983061614446e-05, Learning Rate: 0.000115\n",
      "Epoch 23834/40000, Loss: 3.388121331227012e-05, Learning Rate: 0.000115\n",
      "Epoch 23835/40000, Loss: 3.438036947045475e-05, Learning Rate: 0.000115\n",
      "Epoch 23836/40000, Loss: 5.402622991823591e-05, Learning Rate: 0.000114\n",
      "Epoch 23837/40000, Loss: 1.6746047549531795e-05, Learning Rate: 0.000114\n",
      "Epoch 23838/40000, Loss: 1.6311274521285668e-05, Learning Rate: 0.000114\n",
      "Epoch 23839/40000, Loss: 2.9074251870042644e-05, Learning Rate: 0.000114\n",
      "Epoch 23840/40000, Loss: 2.906265581259504e-05, Learning Rate: 0.000114\n",
      "Epoch 23841/40000, Loss: 4.1958399378927425e-05, Learning Rate: 0.000114\n",
      "Epoch 23842/40000, Loss: 3.137962630717084e-05, Learning Rate: 0.000114\n",
      "Epoch 23843/40000, Loss: 5.948790567344986e-05, Learning Rate: 0.000114\n",
      "Epoch 23844/40000, Loss: 5.8570545661496e-05, Learning Rate: 0.000114\n",
      "Epoch 23845/40000, Loss: 5.76283382542897e-05, Learning Rate: 0.000114\n",
      "Epoch 23846/40000, Loss: 5.748192052124068e-05, Learning Rate: 0.000114\n",
      "Epoch 23847/40000, Loss: 5.166011760593392e-05, Learning Rate: 0.000114\n",
      "Epoch 23848/40000, Loss: 1.3987382772029378e-05, Learning Rate: 0.000114\n",
      "Epoch 23849/40000, Loss: 5.1388440624577925e-05, Learning Rate: 0.000114\n",
      "Epoch 23850/40000, Loss: 3.9508871850557625e-05, Learning Rate: 0.000114\n",
      "Epoch 23851/40000, Loss: 5.132711885380559e-05, Learning Rate: 0.000114\n",
      "Epoch 23852/40000, Loss: 3.2040305086411536e-05, Learning Rate: 0.000114\n",
      "Epoch 23853/40000, Loss: 1.3581205166701693e-05, Learning Rate: 0.000114\n",
      "Epoch 23854/40000, Loss: 3.9356775232590735e-05, Learning Rate: 0.000114\n",
      "Epoch 23855/40000, Loss: 2.7974832846666686e-05, Learning Rate: 0.000114\n",
      "Epoch 23856/40000, Loss: 3.9441434637410566e-05, Learning Rate: 0.000114\n",
      "Epoch 23857/40000, Loss: 2.7913991289096884e-05, Learning Rate: 0.000114\n",
      "Epoch 23858/40000, Loss: 3.9242542698048055e-05, Learning Rate: 0.000114\n",
      "Epoch 23859/40000, Loss: 5.7050103350775316e-05, Learning Rate: 0.000114\n",
      "Epoch 23860/40000, Loss: 1.3528915587812662e-05, Learning Rate: 0.000114\n",
      "Epoch 23861/40000, Loss: 5.0933285820065066e-05, Learning Rate: 0.000114\n",
      "Epoch 23862/40000, Loss: 5.6966458942042664e-05, Learning Rate: 0.000114\n",
      "Epoch 23863/40000, Loss: 1.3559515537053812e-05, Learning Rate: 0.000114\n",
      "Epoch 23864/40000, Loss: 5.090706326882355e-05, Learning Rate: 0.000114\n",
      "Epoch 23865/40000, Loss: 2.7943995519308373e-05, Learning Rate: 0.000114\n",
      "Epoch 23866/40000, Loss: 3.2039366487879306e-05, Learning Rate: 0.000114\n",
      "Epoch 23867/40000, Loss: 3.943129922845401e-05, Learning Rate: 0.000114\n",
      "Epoch 23868/40000, Loss: 5.7237564760725945e-05, Learning Rate: 0.000114\n",
      "Epoch 23869/40000, Loss: 2.791270708257798e-05, Learning Rate: 0.000114\n",
      "Epoch 23870/40000, Loss: 5.730136763304472e-05, Learning Rate: 0.000114\n",
      "Epoch 23871/40000, Loss: 1.3572536772699095e-05, Learning Rate: 0.000114\n",
      "Epoch 23872/40000, Loss: 6.0348556871758774e-05, Learning Rate: 0.000114\n",
      "Epoch 23873/40000, Loss: 3.219319478375837e-05, Learning Rate: 0.000114\n",
      "Epoch 23874/40000, Loss: 3.194501914549619e-05, Learning Rate: 0.000114\n",
      "Epoch 23875/40000, Loss: 5.148467971594073e-05, Learning Rate: 0.000114\n",
      "Epoch 23876/40000, Loss: 6.352210039040074e-05, Learning Rate: 0.000114\n",
      "Epoch 23877/40000, Loss: 3.2937576179392636e-05, Learning Rate: 0.000114\n",
      "Epoch 23878/40000, Loss: 8.264838834293187e-05, Learning Rate: 0.000114\n",
      "Epoch 23879/40000, Loss: 3.5081644455203786e-05, Learning Rate: 0.000114\n",
      "Epoch 23880/40000, Loss: 3.247364656999707e-05, Learning Rate: 0.000114\n",
      "Epoch 23881/40000, Loss: 3.2591851777397096e-05, Learning Rate: 0.000114\n",
      "Epoch 23882/40000, Loss: 5.863817932549864e-05, Learning Rate: 0.000114\n",
      "Epoch 23883/40000, Loss: 5.721687921322882e-05, Learning Rate: 0.000114\n",
      "Epoch 23884/40000, Loss: 5.7196724810637534e-05, Learning Rate: 0.000114\n",
      "Epoch 23885/40000, Loss: 5.112539656693116e-05, Learning Rate: 0.000114\n",
      "Epoch 23886/40000, Loss: 2.8223863409948535e-05, Learning Rate: 0.000114\n",
      "Epoch 23887/40000, Loss: 1.3525709618988913e-05, Learning Rate: 0.000114\n",
      "Epoch 23888/40000, Loss: 1.3521181244868785e-05, Learning Rate: 0.000114\n",
      "Epoch 23889/40000, Loss: 1.3516488252207637e-05, Learning Rate: 0.000114\n",
      "Epoch 23890/40000, Loss: 5.0949547585332766e-05, Learning Rate: 0.000114\n",
      "Epoch 23891/40000, Loss: 5.7289726100862026e-05, Learning Rate: 0.000114\n",
      "Epoch 23892/40000, Loss: 3.1890347599983215e-05, Learning Rate: 0.000114\n",
      "Epoch 23893/40000, Loss: 3.191940413671546e-05, Learning Rate: 0.000114\n",
      "Epoch 23894/40000, Loss: 1.3523340385290794e-05, Learning Rate: 0.000114\n",
      "Epoch 23895/40000, Loss: 5.087426688987762e-05, Learning Rate: 0.000114\n",
      "Epoch 23896/40000, Loss: 5.084267468191683e-05, Learning Rate: 0.000114\n",
      "Epoch 23897/40000, Loss: 3.920735252904706e-05, Learning Rate: 0.000114\n",
      "Epoch 23898/40000, Loss: 3.9241316699190065e-05, Learning Rate: 0.000114\n",
      "Epoch 23899/40000, Loss: 5.0810860557248816e-05, Learning Rate: 0.000114\n",
      "Epoch 23900/40000, Loss: 3.924226984963752e-05, Learning Rate: 0.000114\n",
      "Epoch 23901/40000, Loss: 5.0940641813213006e-05, Learning Rate: 0.000114\n",
      "Epoch 23902/40000, Loss: 3.185984678566456e-05, Learning Rate: 0.000114\n",
      "Epoch 23903/40000, Loss: 5.092873107059859e-05, Learning Rate: 0.000114\n",
      "Epoch 23904/40000, Loss: 5.0797978474292904e-05, Learning Rate: 0.000114\n",
      "Epoch 23905/40000, Loss: 5.7024601119337603e-05, Learning Rate: 0.000114\n",
      "Epoch 23906/40000, Loss: 5.0883314543170854e-05, Learning Rate: 0.000114\n",
      "Epoch 23907/40000, Loss: 3.9182083128253e-05, Learning Rate: 0.000114\n",
      "Epoch 23908/40000, Loss: 3.1875199056230485e-05, Learning Rate: 0.000114\n",
      "Epoch 23909/40000, Loss: 1.350504498986993e-05, Learning Rate: 0.000113\n",
      "Epoch 23910/40000, Loss: 2.7839372705784626e-05, Learning Rate: 0.000113\n",
      "Epoch 23911/40000, Loss: 2.778768612188287e-05, Learning Rate: 0.000113\n",
      "Epoch 23912/40000, Loss: 5.0821719923987985e-05, Learning Rate: 0.000113\n",
      "Epoch 23913/40000, Loss: 1.3453229257720523e-05, Learning Rate: 0.000113\n",
      "Epoch 23914/40000, Loss: 1.3492311154550407e-05, Learning Rate: 0.000113\n",
      "Epoch 23915/40000, Loss: 3.1906820368021727e-05, Learning Rate: 0.000113\n",
      "Epoch 23916/40000, Loss: 3.187027323292568e-05, Learning Rate: 0.000113\n",
      "Epoch 23917/40000, Loss: 3.9256741729332134e-05, Learning Rate: 0.000113\n",
      "Epoch 23918/40000, Loss: 1.3476802450895775e-05, Learning Rate: 0.000113\n",
      "Epoch 23919/40000, Loss: 2.7842623239848763e-05, Learning Rate: 0.000113\n",
      "Epoch 23920/40000, Loss: 5.080949267721735e-05, Learning Rate: 0.000113\n",
      "Epoch 23921/40000, Loss: 5.69519615964964e-05, Learning Rate: 0.000113\n",
      "Epoch 23922/40000, Loss: 1.3524911992135458e-05, Learning Rate: 0.000113\n",
      "Epoch 23923/40000, Loss: 2.7801286705653183e-05, Learning Rate: 0.000113\n",
      "Epoch 23924/40000, Loss: 5.689332101610489e-05, Learning Rate: 0.000113\n",
      "Epoch 23925/40000, Loss: 1.3485485396813601e-05, Learning Rate: 0.000113\n",
      "Epoch 23926/40000, Loss: 5.081067138235085e-05, Learning Rate: 0.000113\n",
      "Epoch 23927/40000, Loss: 5.6825017964001745e-05, Learning Rate: 0.000113\n",
      "Epoch 23928/40000, Loss: 3.1834482797421515e-05, Learning Rate: 0.000113\n",
      "Epoch 23929/40000, Loss: 3.1853847758611664e-05, Learning Rate: 0.000113\n",
      "Epoch 23930/40000, Loss: 3.1819923606235534e-05, Learning Rate: 0.000113\n",
      "Epoch 23931/40000, Loss: 5.084609074401669e-05, Learning Rate: 0.000113\n",
      "Epoch 23932/40000, Loss: 3.179044506396167e-05, Learning Rate: 0.000113\n",
      "Epoch 23933/40000, Loss: 2.781562761811074e-05, Learning Rate: 0.000113\n",
      "Epoch 23934/40000, Loss: 2.772844527498819e-05, Learning Rate: 0.000113\n",
      "Epoch 23935/40000, Loss: 5.078199319541454e-05, Learning Rate: 0.000113\n",
      "Epoch 23936/40000, Loss: 1.3452233361022081e-05, Learning Rate: 0.000113\n",
      "Epoch 23937/40000, Loss: 5.078693720861338e-05, Learning Rate: 0.000113\n",
      "Epoch 23938/40000, Loss: 3.1820509320823476e-05, Learning Rate: 0.000113\n",
      "Epoch 23939/40000, Loss: 5.687448719982058e-05, Learning Rate: 0.000113\n",
      "Epoch 23940/40000, Loss: 3.186025423929095e-05, Learning Rate: 0.000113\n",
      "Epoch 23941/40000, Loss: 5.079773472971283e-05, Learning Rate: 0.000113\n",
      "Epoch 23942/40000, Loss: 3.925671262550168e-05, Learning Rate: 0.000113\n",
      "Epoch 23943/40000, Loss: 5.6783286709105596e-05, Learning Rate: 0.000113\n",
      "Epoch 23944/40000, Loss: 3.181591455359012e-05, Learning Rate: 0.000113\n",
      "Epoch 23945/40000, Loss: 3.919191658496857e-05, Learning Rate: 0.000113\n",
      "Epoch 23946/40000, Loss: 3.923259282601066e-05, Learning Rate: 0.000113\n",
      "Epoch 23947/40000, Loss: 3.1851755920797586e-05, Learning Rate: 0.000113\n",
      "Epoch 23948/40000, Loss: 5.687671364285052e-05, Learning Rate: 0.000113\n",
      "Epoch 23949/40000, Loss: 2.7791280444944277e-05, Learning Rate: 0.000113\n",
      "Epoch 23950/40000, Loss: 3.927129728253931e-05, Learning Rate: 0.000113\n",
      "Epoch 23951/40000, Loss: 5.091030470794067e-05, Learning Rate: 0.000113\n",
      "Epoch 23952/40000, Loss: 5.0916911277454346e-05, Learning Rate: 0.000113\n",
      "Epoch 23953/40000, Loss: 5.087869431008585e-05, Learning Rate: 0.000113\n",
      "Epoch 23954/40000, Loss: 3.9243819628609344e-05, Learning Rate: 0.000113\n",
      "Epoch 23955/40000, Loss: 5.684803909389302e-05, Learning Rate: 0.000113\n",
      "Epoch 23956/40000, Loss: 3.1865441997069865e-05, Learning Rate: 0.000113\n",
      "Epoch 23957/40000, Loss: 3.1915289582684636e-05, Learning Rate: 0.000113\n",
      "Epoch 23958/40000, Loss: 5.6896482419688255e-05, Learning Rate: 0.000113\n",
      "Epoch 23959/40000, Loss: 5.081791096017696e-05, Learning Rate: 0.000113\n",
      "Epoch 23960/40000, Loss: 5.078520916868001e-05, Learning Rate: 0.000113\n",
      "Epoch 23961/40000, Loss: 1.3555060832004528e-05, Learning Rate: 0.000113\n",
      "Epoch 23962/40000, Loss: 1.356926622975152e-05, Learning Rate: 0.000113\n",
      "Epoch 23963/40000, Loss: 5.089958358439617e-05, Learning Rate: 0.000113\n",
      "Epoch 23964/40000, Loss: 3.940013630199246e-05, Learning Rate: 0.000113\n",
      "Epoch 23965/40000, Loss: 5.6923789088614285e-05, Learning Rate: 0.000113\n",
      "Epoch 23966/40000, Loss: 2.7792906621471047e-05, Learning Rate: 0.000113\n",
      "Epoch 23967/40000, Loss: 1.345345845038537e-05, Learning Rate: 0.000113\n",
      "Epoch 23968/40000, Loss: 3.182966975145973e-05, Learning Rate: 0.000113\n",
      "Epoch 23969/40000, Loss: 5.6871449487516657e-05, Learning Rate: 0.000113\n",
      "Epoch 23970/40000, Loss: 2.793803287204355e-05, Learning Rate: 0.000113\n",
      "Epoch 23971/40000, Loss: 5.0879010814242065e-05, Learning Rate: 0.000113\n",
      "Epoch 23972/40000, Loss: 2.7792930268333293e-05, Learning Rate: 0.000113\n",
      "Epoch 23973/40000, Loss: 2.7797910661320202e-05, Learning Rate: 0.000113\n",
      "Epoch 23974/40000, Loss: 5.085717930342071e-05, Learning Rate: 0.000113\n",
      "Epoch 23975/40000, Loss: 5.0787806685548276e-05, Learning Rate: 0.000113\n",
      "Epoch 23976/40000, Loss: 3.9270511479116976e-05, Learning Rate: 0.000113\n",
      "Epoch 23977/40000, Loss: 2.7756548661272973e-05, Learning Rate: 0.000113\n",
      "Epoch 23978/40000, Loss: 2.7755435439758003e-05, Learning Rate: 0.000113\n",
      "Epoch 23979/40000, Loss: 3.9167600334621966e-05, Learning Rate: 0.000113\n",
      "Epoch 23980/40000, Loss: 5.0859871407737955e-05, Learning Rate: 0.000113\n",
      "Epoch 23981/40000, Loss: 2.7795240384875797e-05, Learning Rate: 0.000113\n",
      "Epoch 23982/40000, Loss: 3.9337213820545e-05, Learning Rate: 0.000113\n",
      "Epoch 23983/40000, Loss: 3.177724647684954e-05, Learning Rate: 0.000112\n",
      "Epoch 23984/40000, Loss: 3.180001294822432e-05, Learning Rate: 0.000112\n",
      "Epoch 23985/40000, Loss: 5.085541488369927e-05, Learning Rate: 0.000112\n",
      "Epoch 23986/40000, Loss: 5.079742186353542e-05, Learning Rate: 0.000112\n",
      "Epoch 23987/40000, Loss: 3.181928695994429e-05, Learning Rate: 0.000112\n",
      "Epoch 23988/40000, Loss: 1.346938734059222e-05, Learning Rate: 0.000112\n",
      "Epoch 23989/40000, Loss: 3.185267996741459e-05, Learning Rate: 0.000112\n",
      "Epoch 23990/40000, Loss: 3.181633655913174e-05, Learning Rate: 0.000112\n",
      "Epoch 23991/40000, Loss: 5.700640758732334e-05, Learning Rate: 0.000112\n",
      "Epoch 23992/40000, Loss: 1.3692269021703396e-05, Learning Rate: 0.000112\n",
      "Epoch 23993/40000, Loss: 5.699464600184001e-05, Learning Rate: 0.000112\n",
      "Epoch 23994/40000, Loss: 3.956992077291943e-05, Learning Rate: 0.000112\n",
      "Epoch 23995/40000, Loss: 1.3775229490420315e-05, Learning Rate: 0.000112\n",
      "Epoch 23996/40000, Loss: 5.7128047046717256e-05, Learning Rate: 0.000112\n",
      "Epoch 23997/40000, Loss: 1.4018584806763101e-05, Learning Rate: 0.000112\n",
      "Epoch 23998/40000, Loss: 3.2171927159652114e-05, Learning Rate: 0.000112\n",
      "Epoch 23999/40000, Loss: 3.222405939595774e-05, Learning Rate: 0.000112\n",
      "Epoch 24000/40000, Loss: 2.8261574698262848e-05, Learning Rate: 0.000112\n",
      "Epoch 24001/40000, Loss: 1.3786364434054121e-05, Learning Rate: 0.000112\n",
      "Epoch 24002/40000, Loss: 1.3842804946762044e-05, Learning Rate: 0.000112\n",
      "Epoch 24003/40000, Loss: 1.399267330270959e-05, Learning Rate: 0.000112\n",
      "Epoch 24004/40000, Loss: 4.0186048863688484e-05, Learning Rate: 0.000112\n",
      "Epoch 24005/40000, Loss: 5.869215965503827e-05, Learning Rate: 0.000112\n",
      "Epoch 24006/40000, Loss: 1.4382475455931854e-05, Learning Rate: 0.000112\n",
      "Epoch 24007/40000, Loss: 5.2093997510382906e-05, Learning Rate: 0.000112\n",
      "Epoch 24008/40000, Loss: 2.845716335286852e-05, Learning Rate: 0.000112\n",
      "Epoch 24009/40000, Loss: 3.224087777198292e-05, Learning Rate: 0.000112\n",
      "Epoch 24010/40000, Loss: 1.4205862498783972e-05, Learning Rate: 0.000112\n",
      "Epoch 24011/40000, Loss: 1.420138778485125e-05, Learning Rate: 0.000112\n",
      "Epoch 24012/40000, Loss: 5.7191846281057224e-05, Learning Rate: 0.000112\n",
      "Epoch 24013/40000, Loss: 5.76157181058079e-05, Learning Rate: 0.000112\n",
      "Epoch 24014/40000, Loss: 5.801290171802975e-05, Learning Rate: 0.000112\n",
      "Epoch 24015/40000, Loss: 4.0070226532407105e-05, Learning Rate: 0.000112\n",
      "Epoch 24016/40000, Loss: 6.058158032828942e-05, Learning Rate: 0.000112\n",
      "Epoch 24017/40000, Loss: 6.28727357252501e-05, Learning Rate: 0.000112\n",
      "Epoch 24018/40000, Loss: 2.842701542249415e-05, Learning Rate: 0.000112\n",
      "Epoch 24019/40000, Loss: 4.310298027121462e-05, Learning Rate: 0.000112\n",
      "Epoch 24020/40000, Loss: 4.2942519939970225e-05, Learning Rate: 0.000112\n",
      "Epoch 24021/40000, Loss: 5.885526479687542e-05, Learning Rate: 0.000112\n",
      "Epoch 24022/40000, Loss: 5.3151623433222994e-05, Learning Rate: 0.000112\n",
      "Epoch 24023/40000, Loss: 1.554887421661988e-05, Learning Rate: 0.000112\n",
      "Epoch 24024/40000, Loss: 3.072019535466097e-05, Learning Rate: 0.000112\n",
      "Epoch 24025/40000, Loss: 5.432355828816071e-05, Learning Rate: 0.000112\n",
      "Epoch 24026/40000, Loss: 4.134039772907272e-05, Learning Rate: 0.000112\n",
      "Epoch 24027/40000, Loss: 5.923697608523071e-05, Learning Rate: 0.000112\n",
      "Epoch 24028/40000, Loss: 2.923596912296489e-05, Learning Rate: 0.000112\n",
      "Epoch 24029/40000, Loss: 3.3149102819152176e-05, Learning Rate: 0.000112\n",
      "Epoch 24030/40000, Loss: 5.840104495291598e-05, Learning Rate: 0.000112\n",
      "Epoch 24031/40000, Loss: 5.15429928782396e-05, Learning Rate: 0.000112\n",
      "Epoch 24032/40000, Loss: 1.4013170584803447e-05, Learning Rate: 0.000112\n",
      "Epoch 24033/40000, Loss: 1.3745245269092266e-05, Learning Rate: 0.000112\n",
      "Epoch 24034/40000, Loss: 3.208638372598216e-05, Learning Rate: 0.000112\n",
      "Epoch 24035/40000, Loss: 1.3940289136371575e-05, Learning Rate: 0.000112\n",
      "Epoch 24036/40000, Loss: 3.245474727009423e-05, Learning Rate: 0.000112\n",
      "Epoch 24037/40000, Loss: 2.8093865694245324e-05, Learning Rate: 0.000112\n",
      "Epoch 24038/40000, Loss: 3.973907223553397e-05, Learning Rate: 0.000112\n",
      "Epoch 24039/40000, Loss: 3.967176962760277e-05, Learning Rate: 0.000112\n",
      "Epoch 24040/40000, Loss: 2.776996734610293e-05, Learning Rate: 0.000112\n",
      "Epoch 24041/40000, Loss: 3.173251752741635e-05, Learning Rate: 0.000112\n",
      "Epoch 24042/40000, Loss: 2.7783225959865376e-05, Learning Rate: 0.000112\n",
      "Epoch 24043/40000, Loss: 1.3511225915863179e-05, Learning Rate: 0.000112\n",
      "Epoch 24044/40000, Loss: 5.6766875786706805e-05, Learning Rate: 0.000112\n",
      "Epoch 24045/40000, Loss: 5.0828748499043286e-05, Learning Rate: 0.000112\n",
      "Epoch 24046/40000, Loss: 2.7777952709584497e-05, Learning Rate: 0.000112\n",
      "Epoch 24047/40000, Loss: 2.7674808734445833e-05, Learning Rate: 0.000112\n",
      "Epoch 24048/40000, Loss: 1.3470576050167438e-05, Learning Rate: 0.000112\n",
      "Epoch 24049/40000, Loss: 5.079584298073314e-05, Learning Rate: 0.000112\n",
      "Epoch 24050/40000, Loss: 3.171077696606517e-05, Learning Rate: 0.000112\n",
      "Epoch 24051/40000, Loss: 1.3476003005052917e-05, Learning Rate: 0.000112\n",
      "Epoch 24052/40000, Loss: 5.067088204668835e-05, Learning Rate: 0.000112\n",
      "Epoch 24053/40000, Loss: 2.772643529169727e-05, Learning Rate: 0.000112\n",
      "Epoch 24054/40000, Loss: 3.918469155905768e-05, Learning Rate: 0.000112\n",
      "Epoch 24055/40000, Loss: 5.6810120440786704e-05, Learning Rate: 0.000112\n",
      "Epoch 24056/40000, Loss: 5.677806257153861e-05, Learning Rate: 0.000112\n",
      "Epoch 24057/40000, Loss: 1.3493977348844055e-05, Learning Rate: 0.000112\n",
      "Epoch 24058/40000, Loss: 1.3458631656249054e-05, Learning Rate: 0.000111\n",
      "Epoch 24059/40000, Loss: 5.6717901316005737e-05, Learning Rate: 0.000111\n",
      "Epoch 24060/40000, Loss: 5.6718126870691776e-05, Learning Rate: 0.000111\n",
      "Epoch 24061/40000, Loss: 1.3410463907348458e-05, Learning Rate: 0.000111\n",
      "Epoch 24062/40000, Loss: 5.740192500525154e-05, Learning Rate: 0.000111\n",
      "Epoch 24063/40000, Loss: 5.7149274653056636e-05, Learning Rate: 0.000111\n",
      "Epoch 24064/40000, Loss: 5.717957901651971e-05, Learning Rate: 0.000111\n",
      "Epoch 24065/40000, Loss: 5.705269722966477e-05, Learning Rate: 0.000111\n",
      "Epoch 24066/40000, Loss: 1.348691603197949e-05, Learning Rate: 0.000111\n",
      "Epoch 24067/40000, Loss: 3.173377990606241e-05, Learning Rate: 0.000111\n",
      "Epoch 24068/40000, Loss: 3.921816824004054e-05, Learning Rate: 0.000111\n",
      "Epoch 24069/40000, Loss: 3.9188314985949546e-05, Learning Rate: 0.000111\n",
      "Epoch 24070/40000, Loss: 5.6991899327840656e-05, Learning Rate: 0.000111\n",
      "Epoch 24071/40000, Loss: 5.696104926755652e-05, Learning Rate: 0.000111\n",
      "Epoch 24072/40000, Loss: 5.079292168375105e-05, Learning Rate: 0.000111\n",
      "Epoch 24073/40000, Loss: 3.921815732610412e-05, Learning Rate: 0.000111\n",
      "Epoch 24074/40000, Loss: 5.6917073379736394e-05, Learning Rate: 0.000111\n",
      "Epoch 24075/40000, Loss: 1.3427728845272213e-05, Learning Rate: 0.000111\n",
      "Epoch 24076/40000, Loss: 3.17216181429103e-05, Learning Rate: 0.000111\n",
      "Epoch 24077/40000, Loss: 3.9228943933267146e-05, Learning Rate: 0.000111\n",
      "Epoch 24078/40000, Loss: 1.345838882116368e-05, Learning Rate: 0.000111\n",
      "Epoch 24079/40000, Loss: 3.927803845726885e-05, Learning Rate: 0.000111\n",
      "Epoch 24080/40000, Loss: 5.079168113297783e-05, Learning Rate: 0.000111\n",
      "Epoch 24081/40000, Loss: 3.932542313123122e-05, Learning Rate: 0.000111\n",
      "Epoch 24082/40000, Loss: 2.7722049708245322e-05, Learning Rate: 0.000111\n",
      "Epoch 24083/40000, Loss: 1.348289970337646e-05, Learning Rate: 0.000111\n",
      "Epoch 24084/40000, Loss: 3.920896051567979e-05, Learning Rate: 0.000111\n",
      "Epoch 24085/40000, Loss: 2.772349034785293e-05, Learning Rate: 0.000111\n",
      "Epoch 24086/40000, Loss: 2.774258427962195e-05, Learning Rate: 0.000111\n",
      "Epoch 24087/40000, Loss: 3.1736501114210114e-05, Learning Rate: 0.000111\n",
      "Epoch 24088/40000, Loss: 3.921071402146481e-05, Learning Rate: 0.000111\n",
      "Epoch 24089/40000, Loss: 3.174901576130651e-05, Learning Rate: 0.000111\n",
      "Epoch 24090/40000, Loss: 1.3522951121558435e-05, Learning Rate: 0.000111\n",
      "Epoch 24091/40000, Loss: 3.17193589580711e-05, Learning Rate: 0.000111\n",
      "Epoch 24092/40000, Loss: 3.9297920011449605e-05, Learning Rate: 0.000111\n",
      "Epoch 24093/40000, Loss: 3.9266673411475495e-05, Learning Rate: 0.000111\n",
      "Epoch 24094/40000, Loss: 3.166075111948885e-05, Learning Rate: 0.000111\n",
      "Epoch 24095/40000, Loss: 3.173403820255771e-05, Learning Rate: 0.000111\n",
      "Epoch 24096/40000, Loss: 3.9243968785740435e-05, Learning Rate: 0.000111\n",
      "Epoch 24097/40000, Loss: 5.0643891881918535e-05, Learning Rate: 0.000111\n",
      "Epoch 24098/40000, Loss: 3.17916928906925e-05, Learning Rate: 0.000111\n",
      "Epoch 24099/40000, Loss: 2.766207944659982e-05, Learning Rate: 0.000111\n",
      "Epoch 24100/40000, Loss: 3.9306200051214546e-05, Learning Rate: 0.000111\n",
      "Epoch 24101/40000, Loss: 1.349842750641983e-05, Learning Rate: 0.000111\n",
      "Epoch 24102/40000, Loss: 3.957227090722881e-05, Learning Rate: 0.000111\n",
      "Epoch 24103/40000, Loss: 3.99799391743727e-05, Learning Rate: 0.000111\n",
      "Epoch 24104/40000, Loss: 3.207752524758689e-05, Learning Rate: 0.000111\n",
      "Epoch 24105/40000, Loss: 5.166552364244126e-05, Learning Rate: 0.000111\n",
      "Epoch 24106/40000, Loss: 2.8024403945892118e-05, Learning Rate: 0.000111\n",
      "Epoch 24107/40000, Loss: 5.120547939441167e-05, Learning Rate: 0.000111\n",
      "Epoch 24108/40000, Loss: 5.75055782974232e-05, Learning Rate: 0.000111\n",
      "Epoch 24109/40000, Loss: 5.6841243349481374e-05, Learning Rate: 0.000111\n",
      "Epoch 24110/40000, Loss: 2.7690261049428955e-05, Learning Rate: 0.000111\n",
      "Epoch 24111/40000, Loss: 3.171893331455067e-05, Learning Rate: 0.000111\n",
      "Epoch 24112/40000, Loss: 3.1664534617448226e-05, Learning Rate: 0.000111\n",
      "Epoch 24113/40000, Loss: 5.6771943491185084e-05, Learning Rate: 0.000111\n",
      "Epoch 24114/40000, Loss: 3.9213948184624314e-05, Learning Rate: 0.000111\n",
      "Epoch 24115/40000, Loss: 5.0726826884783804e-05, Learning Rate: 0.000111\n",
      "Epoch 24116/40000, Loss: 5.690100442734547e-05, Learning Rate: 0.000111\n",
      "Epoch 24117/40000, Loss: 3.935998392989859e-05, Learning Rate: 0.000111\n",
      "Epoch 24118/40000, Loss: 5.7528861361788586e-05, Learning Rate: 0.000111\n",
      "Epoch 24119/40000, Loss: 3.933327388949692e-05, Learning Rate: 0.000111\n",
      "Epoch 24120/40000, Loss: 3.181165811838582e-05, Learning Rate: 0.000111\n",
      "Epoch 24121/40000, Loss: 5.084279473521747e-05, Learning Rate: 0.000111\n",
      "Epoch 24122/40000, Loss: 1.3612325346912257e-05, Learning Rate: 0.000111\n",
      "Epoch 24123/40000, Loss: 1.3480743291438557e-05, Learning Rate: 0.000111\n",
      "Epoch 24124/40000, Loss: 2.767914156720508e-05, Learning Rate: 0.000111\n",
      "Epoch 24125/40000, Loss: 5.6835011491784826e-05, Learning Rate: 0.000111\n",
      "Epoch 24126/40000, Loss: 1.3488664080796298e-05, Learning Rate: 0.000111\n",
      "Epoch 24127/40000, Loss: 5.0744059990393e-05, Learning Rate: 0.000111\n",
      "Epoch 24128/40000, Loss: 5.073835200164467e-05, Learning Rate: 0.000111\n",
      "Epoch 24129/40000, Loss: 2.7665360903483815e-05, Learning Rate: 0.000111\n",
      "Epoch 24130/40000, Loss: 1.3579322512669023e-05, Learning Rate: 0.000111\n",
      "Epoch 24131/40000, Loss: 1.3502510228136089e-05, Learning Rate: 0.000111\n",
      "Epoch 24132/40000, Loss: 3.1660409149480984e-05, Learning Rate: 0.000111\n",
      "Epoch 24133/40000, Loss: 1.354112373519456e-05, Learning Rate: 0.000110\n",
      "Epoch 24134/40000, Loss: 5.684367715730332e-05, Learning Rate: 0.000110\n",
      "Epoch 24135/40000, Loss: 3.1747957109473646e-05, Learning Rate: 0.000110\n",
      "Epoch 24136/40000, Loss: 3.92747315345332e-05, Learning Rate: 0.000110\n",
      "Epoch 24137/40000, Loss: 2.7648145987768658e-05, Learning Rate: 0.000110\n",
      "Epoch 24138/40000, Loss: 2.775558277789969e-05, Learning Rate: 0.000110\n",
      "Epoch 24139/40000, Loss: 3.930143066099845e-05, Learning Rate: 0.000110\n",
      "Epoch 24140/40000, Loss: 3.172111973981373e-05, Learning Rate: 0.000110\n",
      "Epoch 24141/40000, Loss: 5.6861492339521646e-05, Learning Rate: 0.000110\n",
      "Epoch 24142/40000, Loss: 2.7847858291352168e-05, Learning Rate: 0.000110\n",
      "Epoch 24143/40000, Loss: 5.689567842637189e-05, Learning Rate: 0.000110\n",
      "Epoch 24144/40000, Loss: 5.674371641362086e-05, Learning Rate: 0.000110\n",
      "Epoch 24145/40000, Loss: 3.173613004037179e-05, Learning Rate: 0.000110\n",
      "Epoch 24146/40000, Loss: 3.928607839043252e-05, Learning Rate: 0.000110\n",
      "Epoch 24147/40000, Loss: 5.6838725868146867e-05, Learning Rate: 0.000110\n",
      "Epoch 24148/40000, Loss: 3.1779243727214634e-05, Learning Rate: 0.000110\n",
      "Epoch 24149/40000, Loss: 3.940517490264028e-05, Learning Rate: 0.000110\n",
      "Epoch 24150/40000, Loss: 5.127558688400313e-05, Learning Rate: 0.000110\n",
      "Epoch 24151/40000, Loss: 3.219087375327945e-05, Learning Rate: 0.000110\n",
      "Epoch 24152/40000, Loss: 1.3637238225783221e-05, Learning Rate: 0.000110\n",
      "Epoch 24153/40000, Loss: 5.684437201125547e-05, Learning Rate: 0.000110\n",
      "Epoch 24154/40000, Loss: 1.3695074812858365e-05, Learning Rate: 0.000110\n",
      "Epoch 24155/40000, Loss: 1.3583458894572686e-05, Learning Rate: 0.000110\n",
      "Epoch 24156/40000, Loss: 5.687252269126475e-05, Learning Rate: 0.000110\n",
      "Epoch 24157/40000, Loss: 3.190293500665575e-05, Learning Rate: 0.000110\n",
      "Epoch 24158/40000, Loss: 5.726373638026416e-05, Learning Rate: 0.000110\n",
      "Epoch 24159/40000, Loss: 3.944473064620979e-05, Learning Rate: 0.000110\n",
      "Epoch 24160/40000, Loss: 3.197487603756599e-05, Learning Rate: 0.000110\n",
      "Epoch 24161/40000, Loss: 1.3611936083179899e-05, Learning Rate: 0.000110\n",
      "Epoch 24162/40000, Loss: 2.7883077564183623e-05, Learning Rate: 0.000110\n",
      "Epoch 24163/40000, Loss: 3.195561293978244e-05, Learning Rate: 0.000110\n",
      "Epoch 24164/40000, Loss: 3.181366992066614e-05, Learning Rate: 0.000110\n",
      "Epoch 24165/40000, Loss: 3.943457340938039e-05, Learning Rate: 0.000110\n",
      "Epoch 24166/40000, Loss: 2.7786729333456606e-05, Learning Rate: 0.000110\n",
      "Epoch 24167/40000, Loss: 2.7729469366022386e-05, Learning Rate: 0.000110\n",
      "Epoch 24168/40000, Loss: 2.7718971978174523e-05, Learning Rate: 0.000110\n",
      "Epoch 24169/40000, Loss: 2.7742027668864466e-05, Learning Rate: 0.000110\n",
      "Epoch 24170/40000, Loss: 1.3679259609489236e-05, Learning Rate: 0.000110\n",
      "Epoch 24171/40000, Loss: 3.1946765375323594e-05, Learning Rate: 0.000110\n",
      "Epoch 24172/40000, Loss: 5.716831947211176e-05, Learning Rate: 0.000110\n",
      "Epoch 24173/40000, Loss: 3.235920667066239e-05, Learning Rate: 0.000110\n",
      "Epoch 24174/40000, Loss: 2.77908820862649e-05, Learning Rate: 0.000110\n",
      "Epoch 24175/40000, Loss: 5.753300865762867e-05, Learning Rate: 0.000110\n",
      "Epoch 24176/40000, Loss: 2.8767051844624802e-05, Learning Rate: 0.000110\n",
      "Epoch 24177/40000, Loss: 1.3982316886540502e-05, Learning Rate: 0.000110\n",
      "Epoch 24178/40000, Loss: 5.7492474297760054e-05, Learning Rate: 0.000110\n",
      "Epoch 24179/40000, Loss: 2.898971433751285e-05, Learning Rate: 0.000110\n",
      "Epoch 24180/40000, Loss: 5.696577136404812e-05, Learning Rate: 0.000110\n",
      "Epoch 24181/40000, Loss: 1.3770095392828807e-05, Learning Rate: 0.000110\n",
      "Epoch 24182/40000, Loss: 1.3635196410177741e-05, Learning Rate: 0.000110\n",
      "Epoch 24183/40000, Loss: 5.684057032340206e-05, Learning Rate: 0.000110\n",
      "Epoch 24184/40000, Loss: 3.172839933540672e-05, Learning Rate: 0.000110\n",
      "Epoch 24185/40000, Loss: 2.7681771825882606e-05, Learning Rate: 0.000110\n",
      "Epoch 24186/40000, Loss: 2.763598422461655e-05, Learning Rate: 0.000110\n",
      "Epoch 24187/40000, Loss: 3.9349244616460055e-05, Learning Rate: 0.000110\n",
      "Epoch 24188/40000, Loss: 3.929301965399645e-05, Learning Rate: 0.000110\n",
      "Epoch 24189/40000, Loss: 5.6867796956794336e-05, Learning Rate: 0.000110\n",
      "Epoch 24190/40000, Loss: 1.3503557966032531e-05, Learning Rate: 0.000110\n",
      "Epoch 24191/40000, Loss: 3.930247112293728e-05, Learning Rate: 0.000110\n",
      "Epoch 24192/40000, Loss: 2.7705471438821405e-05, Learning Rate: 0.000110\n",
      "Epoch 24193/40000, Loss: 2.7672707801684737e-05, Learning Rate: 0.000110\n",
      "Epoch 24194/40000, Loss: 3.9512673538411036e-05, Learning Rate: 0.000110\n",
      "Epoch 24195/40000, Loss: 1.3912836948293261e-05, Learning Rate: 0.000110\n",
      "Epoch 24196/40000, Loss: 2.783924719551578e-05, Learning Rate: 0.000110\n",
      "Epoch 24197/40000, Loss: 3.972762715420686e-05, Learning Rate: 0.000110\n",
      "Epoch 24198/40000, Loss: 2.778987618512474e-05, Learning Rate: 0.000110\n",
      "Epoch 24199/40000, Loss: 5.699197936337441e-05, Learning Rate: 0.000110\n",
      "Epoch 24200/40000, Loss: 3.1668023439124227e-05, Learning Rate: 0.000110\n",
      "Epoch 24201/40000, Loss: 2.7670186682371423e-05, Learning Rate: 0.000110\n",
      "Epoch 24202/40000, Loss: 5.087260069558397e-05, Learning Rate: 0.000110\n",
      "Epoch 24203/40000, Loss: 3.1772324291523546e-05, Learning Rate: 0.000110\n",
      "Epoch 24204/40000, Loss: 3.168737021042034e-05, Learning Rate: 0.000110\n",
      "Epoch 24205/40000, Loss: 2.7697025871020742e-05, Learning Rate: 0.000110\n",
      "Epoch 24206/40000, Loss: 2.7625013899523765e-05, Learning Rate: 0.000110\n",
      "Epoch 24207/40000, Loss: 5.080452683614567e-05, Learning Rate: 0.000110\n",
      "Epoch 24208/40000, Loss: 5.6955825129989535e-05, Learning Rate: 0.000109\n",
      "Epoch 24209/40000, Loss: 5.6949982536025345e-05, Learning Rate: 0.000109\n",
      "Epoch 24210/40000, Loss: 3.942641706089489e-05, Learning Rate: 0.000109\n",
      "Epoch 24211/40000, Loss: 5.084192889626138e-05, Learning Rate: 0.000109\n",
      "Epoch 24212/40000, Loss: 1.3605967978946865e-05, Learning Rate: 0.000109\n",
      "Epoch 24213/40000, Loss: 3.923330586985685e-05, Learning Rate: 0.000109\n",
      "Epoch 24214/40000, Loss: 3.9306625694734976e-05, Learning Rate: 0.000109\n",
      "Epoch 24215/40000, Loss: 2.7598282031249255e-05, Learning Rate: 0.000109\n",
      "Epoch 24216/40000, Loss: 5.674999920302071e-05, Learning Rate: 0.000109\n",
      "Epoch 24217/40000, Loss: 3.1658193620387465e-05, Learning Rate: 0.000109\n",
      "Epoch 24218/40000, Loss: 2.761417636065744e-05, Learning Rate: 0.000109\n",
      "Epoch 24219/40000, Loss: 3.91737230529543e-05, Learning Rate: 0.000109\n",
      "Epoch 24220/40000, Loss: 2.7645708541967906e-05, Learning Rate: 0.000109\n",
      "Epoch 24221/40000, Loss: 2.765377030300442e-05, Learning Rate: 0.000109\n",
      "Epoch 24222/40000, Loss: 1.3428742022369988e-05, Learning Rate: 0.000109\n",
      "Epoch 24223/40000, Loss: 5.676943328580819e-05, Learning Rate: 0.000109\n",
      "Epoch 24224/40000, Loss: 2.7599673558142968e-05, Learning Rate: 0.000109\n",
      "Epoch 24225/40000, Loss: 3.922612813767046e-05, Learning Rate: 0.000109\n",
      "Epoch 24226/40000, Loss: 3.1685776775702834e-05, Learning Rate: 0.000109\n",
      "Epoch 24227/40000, Loss: 3.159886546200141e-05, Learning Rate: 0.000109\n",
      "Epoch 24228/40000, Loss: 2.7656664315145463e-05, Learning Rate: 0.000109\n",
      "Epoch 24229/40000, Loss: 3.968625605921261e-05, Learning Rate: 0.000109\n",
      "Epoch 24230/40000, Loss: 3.210198337910697e-05, Learning Rate: 0.000109\n",
      "Epoch 24231/40000, Loss: 3.210694558219984e-05, Learning Rate: 0.000109\n",
      "Epoch 24232/40000, Loss: 3.1863477488514036e-05, Learning Rate: 0.000109\n",
      "Epoch 24233/40000, Loss: 3.193793600075878e-05, Learning Rate: 0.000109\n",
      "Epoch 24234/40000, Loss: 5.1918596000177786e-05, Learning Rate: 0.000109\n",
      "Epoch 24235/40000, Loss: 2.8185964765725657e-05, Learning Rate: 0.000109\n",
      "Epoch 24236/40000, Loss: 5.637861613649875e-05, Learning Rate: 0.000109\n",
      "Epoch 24237/40000, Loss: 3.00035626423778e-05, Learning Rate: 0.000109\n",
      "Epoch 24238/40000, Loss: 4.169571548118256e-05, Learning Rate: 0.000109\n",
      "Epoch 24239/40000, Loss: 2.968599619634915e-05, Learning Rate: 0.000109\n",
      "Epoch 24240/40000, Loss: 5.814167525386438e-05, Learning Rate: 0.000109\n",
      "Epoch 24241/40000, Loss: 1.6944593880907632e-05, Learning Rate: 0.000109\n",
      "Epoch 24242/40000, Loss: 2.887880509661045e-05, Learning Rate: 0.000109\n",
      "Epoch 24243/40000, Loss: 2.8349188141874038e-05, Learning Rate: 0.000109\n",
      "Epoch 24244/40000, Loss: 1.3910378584114369e-05, Learning Rate: 0.000109\n",
      "Epoch 24245/40000, Loss: 1.385366249451181e-05, Learning Rate: 0.000109\n",
      "Epoch 24246/40000, Loss: 3.2358824682887644e-05, Learning Rate: 0.000109\n",
      "Epoch 24247/40000, Loss: 3.1899427995085716e-05, Learning Rate: 0.000109\n",
      "Epoch 24248/40000, Loss: 5.712405618396588e-05, Learning Rate: 0.000109\n",
      "Epoch 24249/40000, Loss: 3.954108615289442e-05, Learning Rate: 0.000109\n",
      "Epoch 24250/40000, Loss: 2.765566750895232e-05, Learning Rate: 0.000109\n",
      "Epoch 24251/40000, Loss: 1.3552093150792643e-05, Learning Rate: 0.000109\n",
      "Epoch 24252/40000, Loss: 1.3556114026869182e-05, Learning Rate: 0.000109\n",
      "Epoch 24253/40000, Loss: 1.3398441296885721e-05, Learning Rate: 0.000109\n",
      "Epoch 24254/40000, Loss: 3.161330096190795e-05, Learning Rate: 0.000109\n",
      "Epoch 24255/40000, Loss: 1.3635276445711497e-05, Learning Rate: 0.000109\n",
      "Epoch 24256/40000, Loss: 2.7658174076350406e-05, Learning Rate: 0.000109\n",
      "Epoch 24257/40000, Loss: 3.166310489177704e-05, Learning Rate: 0.000109\n",
      "Epoch 24258/40000, Loss: 3.159485277137719e-05, Learning Rate: 0.000109\n",
      "Epoch 24259/40000, Loss: 3.161298081977293e-05, Learning Rate: 0.000109\n",
      "Epoch 24260/40000, Loss: 1.3445847798720933e-05, Learning Rate: 0.000109\n",
      "Epoch 24261/40000, Loss: 5.671628605341539e-05, Learning Rate: 0.000109\n",
      "Epoch 24262/40000, Loss: 1.346365661447635e-05, Learning Rate: 0.000109\n",
      "Epoch 24263/40000, Loss: 1.3488663171301596e-05, Learning Rate: 0.000109\n",
      "Epoch 24264/40000, Loss: 2.7604564820649102e-05, Learning Rate: 0.000109\n",
      "Epoch 24265/40000, Loss: 3.91598732676357e-05, Learning Rate: 0.000109\n",
      "Epoch 24266/40000, Loss: 5.66646340303123e-05, Learning Rate: 0.000109\n",
      "Epoch 24267/40000, Loss: 1.3404515811998863e-05, Learning Rate: 0.000109\n",
      "Epoch 24268/40000, Loss: 2.7551963285077363e-05, Learning Rate: 0.000109\n",
      "Epoch 24269/40000, Loss: 5.712754136766307e-05, Learning Rate: 0.000109\n",
      "Epoch 24270/40000, Loss: 3.920100789400749e-05, Learning Rate: 0.000109\n",
      "Epoch 24271/40000, Loss: 3.159188781864941e-05, Learning Rate: 0.000109\n",
      "Epoch 24272/40000, Loss: 3.919135997421108e-05, Learning Rate: 0.000109\n",
      "Epoch 24273/40000, Loss: 5.6791843235259876e-05, Learning Rate: 0.000109\n",
      "Epoch 24274/40000, Loss: 3.159461994073354e-05, Learning Rate: 0.000109\n",
      "Epoch 24275/40000, Loss: 1.354262712993659e-05, Learning Rate: 0.000109\n",
      "Epoch 24276/40000, Loss: 2.7531028536031954e-05, Learning Rate: 0.000109\n",
      "Epoch 24277/40000, Loss: 5.674632120644674e-05, Learning Rate: 0.000109\n",
      "Epoch 24278/40000, Loss: 5.6701115681789815e-05, Learning Rate: 0.000109\n",
      "Epoch 24279/40000, Loss: 3.161181302857585e-05, Learning Rate: 0.000109\n",
      "Epoch 24280/40000, Loss: 1.3526369002647698e-05, Learning Rate: 0.000109\n",
      "Epoch 24281/40000, Loss: 5.0664930313359946e-05, Learning Rate: 0.000109\n",
      "Epoch 24282/40000, Loss: 1.3762895832769573e-05, Learning Rate: 0.000109\n",
      "Epoch 24283/40000, Loss: 3.952931729145348e-05, Learning Rate: 0.000109\n",
      "Epoch 24284/40000, Loss: 3.938211011700332e-05, Learning Rate: 0.000109\n",
      "Epoch 24285/40000, Loss: 1.3473565559252165e-05, Learning Rate: 0.000108\n",
      "Epoch 24286/40000, Loss: 2.7612142730504274e-05, Learning Rate: 0.000108\n",
      "Epoch 24287/40000, Loss: 5.096107270219363e-05, Learning Rate: 0.000108\n",
      "Epoch 24288/40000, Loss: 3.933414336643182e-05, Learning Rate: 0.000108\n",
      "Epoch 24289/40000, Loss: 2.7495812901179306e-05, Learning Rate: 0.000108\n",
      "Epoch 24290/40000, Loss: 1.3498457519744989e-05, Learning Rate: 0.000108\n",
      "Epoch 24291/40000, Loss: 5.084873555460945e-05, Learning Rate: 0.000108\n",
      "Epoch 24292/40000, Loss: 5.707064701709896e-05, Learning Rate: 0.000108\n",
      "Epoch 24293/40000, Loss: 5.125138341099955e-05, Learning Rate: 0.000108\n",
      "Epoch 24294/40000, Loss: 5.081713243271224e-05, Learning Rate: 0.000108\n",
      "Epoch 24295/40000, Loss: 5.715599763789214e-05, Learning Rate: 0.000108\n",
      "Epoch 24296/40000, Loss: 3.9233134884852916e-05, Learning Rate: 0.000108\n",
      "Epoch 24297/40000, Loss: 2.7564097763388418e-05, Learning Rate: 0.000108\n",
      "Epoch 24298/40000, Loss: 2.7500993383000605e-05, Learning Rate: 0.000108\n",
      "Epoch 24299/40000, Loss: 1.3495275197783485e-05, Learning Rate: 0.000108\n",
      "Epoch 24300/40000, Loss: 1.3507336916518398e-05, Learning Rate: 0.000108\n",
      "Epoch 24301/40000, Loss: 1.3384762496571057e-05, Learning Rate: 0.000108\n",
      "Epoch 24302/40000, Loss: 3.90914392482955e-05, Learning Rate: 0.000108\n",
      "Epoch 24303/40000, Loss: 2.7554980988497846e-05, Learning Rate: 0.000108\n",
      "Epoch 24304/40000, Loss: 3.921297684428282e-05, Learning Rate: 0.000108\n",
      "Epoch 24305/40000, Loss: 3.150184056721628e-05, Learning Rate: 0.000108\n",
      "Epoch 24306/40000, Loss: 1.3404376659309492e-05, Learning Rate: 0.000108\n",
      "Epoch 24307/40000, Loss: 1.350843558611814e-05, Learning Rate: 0.000108\n",
      "Epoch 24308/40000, Loss: 2.7494623282109387e-05, Learning Rate: 0.000108\n",
      "Epoch 24309/40000, Loss: 1.3483302609529346e-05, Learning Rate: 0.000108\n",
      "Epoch 24310/40000, Loss: 5.6752254749881104e-05, Learning Rate: 0.000108\n",
      "Epoch 24311/40000, Loss: 1.3462009519571438e-05, Learning Rate: 0.000108\n",
      "Epoch 24312/40000, Loss: 1.3394588677329011e-05, Learning Rate: 0.000108\n",
      "Epoch 24313/40000, Loss: 1.3414565728453454e-05, Learning Rate: 0.000108\n",
      "Epoch 24314/40000, Loss: 3.155806552967988e-05, Learning Rate: 0.000108\n",
      "Epoch 24315/40000, Loss: 3.9184087654575706e-05, Learning Rate: 0.000108\n",
      "Epoch 24316/40000, Loss: 2.7489557396620512e-05, Learning Rate: 0.000108\n",
      "Epoch 24317/40000, Loss: 1.3440627299132757e-05, Learning Rate: 0.000108\n",
      "Epoch 24318/40000, Loss: 1.3457155546348076e-05, Learning Rate: 0.000108\n",
      "Epoch 24319/40000, Loss: 1.3411184227152262e-05, Learning Rate: 0.000108\n",
      "Epoch 24320/40000, Loss: 3.91577668779064e-05, Learning Rate: 0.000108\n",
      "Epoch 24321/40000, Loss: 1.339408026979072e-05, Learning Rate: 0.000108\n",
      "Epoch 24322/40000, Loss: 1.3483281691151205e-05, Learning Rate: 0.000108\n",
      "Epoch 24323/40000, Loss: 5.676368891727179e-05, Learning Rate: 0.000108\n",
      "Epoch 24324/40000, Loss: 1.348160640191054e-05, Learning Rate: 0.000108\n",
      "Epoch 24325/40000, Loss: 3.1520830816589296e-05, Learning Rate: 0.000108\n",
      "Epoch 24326/40000, Loss: 5.676629371009767e-05, Learning Rate: 0.000108\n",
      "Epoch 24327/40000, Loss: 5.666968718287535e-05, Learning Rate: 0.000108\n",
      "Epoch 24328/40000, Loss: 5.6670636695344e-05, Learning Rate: 0.000108\n",
      "Epoch 24329/40000, Loss: 5.6723471061559394e-05, Learning Rate: 0.000108\n",
      "Epoch 24330/40000, Loss: 5.06662399857305e-05, Learning Rate: 0.000108\n",
      "Epoch 24331/40000, Loss: 2.751513966359198e-05, Learning Rate: 0.000108\n",
      "Epoch 24332/40000, Loss: 1.3493884580384474e-05, Learning Rate: 0.000108\n",
      "Epoch 24333/40000, Loss: 5.0770970119629055e-05, Learning Rate: 0.000108\n",
      "Epoch 24334/40000, Loss: 2.7528360078576952e-05, Learning Rate: 0.000108\n",
      "Epoch 24335/40000, Loss: 3.153605211991817e-05, Learning Rate: 0.000108\n",
      "Epoch 24336/40000, Loss: 5.060564217274077e-05, Learning Rate: 0.000108\n",
      "Epoch 24337/40000, Loss: 2.753179614956025e-05, Learning Rate: 0.000108\n",
      "Epoch 24338/40000, Loss: 5.667098957928829e-05, Learning Rate: 0.000108\n",
      "Epoch 24339/40000, Loss: 2.750782914517913e-05, Learning Rate: 0.000108\n",
      "Epoch 24340/40000, Loss: 3.92408728657756e-05, Learning Rate: 0.000108\n",
      "Epoch 24341/40000, Loss: 3.1541334465146065e-05, Learning Rate: 0.000108\n",
      "Epoch 24342/40000, Loss: 3.9284335798583925e-05, Learning Rate: 0.000108\n",
      "Epoch 24343/40000, Loss: 2.7699017664417624e-05, Learning Rate: 0.000108\n",
      "Epoch 24344/40000, Loss: 3.17669146170374e-05, Learning Rate: 0.000108\n",
      "Epoch 24345/40000, Loss: 5.079036418464966e-05, Learning Rate: 0.000108\n",
      "Epoch 24346/40000, Loss: 3.9695132727501914e-05, Learning Rate: 0.000108\n",
      "Epoch 24347/40000, Loss: 1.3672619388671592e-05, Learning Rate: 0.000108\n",
      "Epoch 24348/40000, Loss: 2.7566393328015693e-05, Learning Rate: 0.000108\n",
      "Epoch 24349/40000, Loss: 3.928941805497743e-05, Learning Rate: 0.000108\n",
      "Epoch 24350/40000, Loss: 1.3738369489146862e-05, Learning Rate: 0.000108\n",
      "Epoch 24351/40000, Loss: 2.7781999961007386e-05, Learning Rate: 0.000108\n",
      "Epoch 24352/40000, Loss: 1.3652287634613458e-05, Learning Rate: 0.000108\n",
      "Epoch 24353/40000, Loss: 3.954892963520251e-05, Learning Rate: 0.000108\n",
      "Epoch 24354/40000, Loss: 5.192630487727001e-05, Learning Rate: 0.000108\n",
      "Epoch 24355/40000, Loss: 5.176643389859237e-05, Learning Rate: 0.000108\n",
      "Epoch 24356/40000, Loss: 2.8246187866898254e-05, Learning Rate: 0.000108\n",
      "Epoch 24357/40000, Loss: 3.958058368880302e-05, Learning Rate: 0.000108\n",
      "Epoch 24358/40000, Loss: 1.407278159604175e-05, Learning Rate: 0.000108\n",
      "Epoch 24359/40000, Loss: 5.15461215400137e-05, Learning Rate: 0.000108\n",
      "Epoch 24360/40000, Loss: 5.7952904171543196e-05, Learning Rate: 0.000108\n",
      "Epoch 24361/40000, Loss: 5.142840018379502e-05, Learning Rate: 0.000108\n",
      "Epoch 24362/40000, Loss: 3.2074629416456446e-05, Learning Rate: 0.000107\n",
      "Epoch 24363/40000, Loss: 2.7959613362327218e-05, Learning Rate: 0.000107\n",
      "Epoch 24364/40000, Loss: 1.3883089195587672e-05, Learning Rate: 0.000107\n",
      "Epoch 24365/40000, Loss: 2.843882430170197e-05, Learning Rate: 0.000107\n",
      "Epoch 24366/40000, Loss: 4.0154875023290515e-05, Learning Rate: 0.000107\n",
      "Epoch 24367/40000, Loss: 5.755560414399952e-05, Learning Rate: 0.000107\n",
      "Epoch 24368/40000, Loss: 5.193609104026109e-05, Learning Rate: 0.000107\n",
      "Epoch 24369/40000, Loss: 2.807429882523138e-05, Learning Rate: 0.000107\n",
      "Epoch 24370/40000, Loss: 1.4099106010689866e-05, Learning Rate: 0.000107\n",
      "Epoch 24371/40000, Loss: 1.4122131688054651e-05, Learning Rate: 0.000107\n",
      "Epoch 24372/40000, Loss: 1.378138222207781e-05, Learning Rate: 0.000107\n",
      "Epoch 24373/40000, Loss: 3.994841608800925e-05, Learning Rate: 0.000107\n",
      "Epoch 24374/40000, Loss: 5.716097803087905e-05, Learning Rate: 0.000107\n",
      "Epoch 24375/40000, Loss: 5.7633387768873945e-05, Learning Rate: 0.000107\n",
      "Epoch 24376/40000, Loss: 2.7970138035016134e-05, Learning Rate: 0.000107\n",
      "Epoch 24377/40000, Loss: 1.365164735034341e-05, Learning Rate: 0.000107\n",
      "Epoch 24378/40000, Loss: 5.1189395890105516e-05, Learning Rate: 0.000107\n",
      "Epoch 24379/40000, Loss: 5.10340032633394e-05, Learning Rate: 0.000107\n",
      "Epoch 24380/40000, Loss: 5.717687599826604e-05, Learning Rate: 0.000107\n",
      "Epoch 24381/40000, Loss: 3.9513714000349864e-05, Learning Rate: 0.000107\n",
      "Epoch 24382/40000, Loss: 2.7623209462035447e-05, Learning Rate: 0.000107\n",
      "Epoch 24383/40000, Loss: 5.089555634185672e-05, Learning Rate: 0.000107\n",
      "Epoch 24384/40000, Loss: 3.925930286641233e-05, Learning Rate: 0.000107\n",
      "Epoch 24385/40000, Loss: 3.935307904612273e-05, Learning Rate: 0.000107\n",
      "Epoch 24386/40000, Loss: 3.1589970603818074e-05, Learning Rate: 0.000107\n",
      "Epoch 24387/40000, Loss: 3.158005711156875e-05, Learning Rate: 0.000107\n",
      "Epoch 24388/40000, Loss: 3.1602077797288075e-05, Learning Rate: 0.000107\n",
      "Epoch 24389/40000, Loss: 5.666893775924109e-05, Learning Rate: 0.000107\n",
      "Epoch 24390/40000, Loss: 3.9203529013320804e-05, Learning Rate: 0.000107\n",
      "Epoch 24391/40000, Loss: 1.3601968930743169e-05, Learning Rate: 0.000107\n",
      "Epoch 24392/40000, Loss: 5.715246152249165e-05, Learning Rate: 0.000107\n",
      "Epoch 24393/40000, Loss: 1.3610851056000683e-05, Learning Rate: 0.000107\n",
      "Epoch 24394/40000, Loss: 1.3507838048099075e-05, Learning Rate: 0.000107\n",
      "Epoch 24395/40000, Loss: 3.927194120478816e-05, Learning Rate: 0.000107\n",
      "Epoch 24396/40000, Loss: 3.9335223846137524e-05, Learning Rate: 0.000107\n",
      "Epoch 24397/40000, Loss: 1.3446027878671885e-05, Learning Rate: 0.000107\n",
      "Epoch 24398/40000, Loss: 2.757010042842012e-05, Learning Rate: 0.000107\n",
      "Epoch 24399/40000, Loss: 1.3723523807129823e-05, Learning Rate: 0.000107\n",
      "Epoch 24400/40000, Loss: 5.674217391060665e-05, Learning Rate: 0.000107\n",
      "Epoch 24401/40000, Loss: 2.7607955416897312e-05, Learning Rate: 0.000107\n",
      "Epoch 24402/40000, Loss: 2.753770058916416e-05, Learning Rate: 0.000107\n",
      "Epoch 24403/40000, Loss: 1.3562205822381657e-05, Learning Rate: 0.000107\n",
      "Epoch 24404/40000, Loss: 3.917644789908081e-05, Learning Rate: 0.000107\n",
      "Epoch 24405/40000, Loss: 2.7605223294813186e-05, Learning Rate: 0.000107\n",
      "Epoch 24406/40000, Loss: 5.690346733899787e-05, Learning Rate: 0.000107\n",
      "Epoch 24407/40000, Loss: 3.934016785933636e-05, Learning Rate: 0.000107\n",
      "Epoch 24408/40000, Loss: 5.8288260333938524e-05, Learning Rate: 0.000107\n",
      "Epoch 24409/40000, Loss: 1.3573928299592808e-05, Learning Rate: 0.000107\n",
      "Epoch 24410/40000, Loss: 6.205379031598568e-05, Learning Rate: 0.000107\n",
      "Epoch 24411/40000, Loss: 5.0828748499043286e-05, Learning Rate: 0.000107\n",
      "Epoch 24412/40000, Loss: 3.160929190926254e-05, Learning Rate: 0.000107\n",
      "Epoch 24413/40000, Loss: 5.084417716716416e-05, Learning Rate: 0.000107\n",
      "Epoch 24414/40000, Loss: 3.930106322513893e-05, Learning Rate: 0.000107\n",
      "Epoch 24415/40000, Loss: 5.0656861276365817e-05, Learning Rate: 0.000107\n",
      "Epoch 24416/40000, Loss: 3.920385643141344e-05, Learning Rate: 0.000107\n",
      "Epoch 24417/40000, Loss: 1.34914353111526e-05, Learning Rate: 0.000107\n",
      "Epoch 24418/40000, Loss: 5.705287549062632e-05, Learning Rate: 0.000107\n",
      "Epoch 24419/40000, Loss: 5.0619673856999725e-05, Learning Rate: 0.000107\n",
      "Epoch 24420/40000, Loss: 3.148122050333768e-05, Learning Rate: 0.000107\n",
      "Epoch 24421/40000, Loss: 3.149074109387584e-05, Learning Rate: 0.000107\n",
      "Epoch 24422/40000, Loss: 3.9144008042057976e-05, Learning Rate: 0.000107\n",
      "Epoch 24423/40000, Loss: 3.1453007977688685e-05, Learning Rate: 0.000107\n",
      "Epoch 24424/40000, Loss: 2.7380374376662076e-05, Learning Rate: 0.000107\n",
      "Epoch 24425/40000, Loss: 3.152340286760591e-05, Learning Rate: 0.000107\n",
      "Epoch 24426/40000, Loss: 1.3548441529565025e-05, Learning Rate: 0.000107\n",
      "Epoch 24427/40000, Loss: 2.7524160032044165e-05, Learning Rate: 0.000107\n",
      "Epoch 24428/40000, Loss: 1.3565649169322569e-05, Learning Rate: 0.000107\n",
      "Epoch 24429/40000, Loss: 2.7427086024545133e-05, Learning Rate: 0.000107\n",
      "Epoch 24430/40000, Loss: 2.7448017135611735e-05, Learning Rate: 0.000107\n",
      "Epoch 24431/40000, Loss: 1.3508238225767855e-05, Learning Rate: 0.000107\n",
      "Epoch 24432/40000, Loss: 2.7585056159296073e-05, Learning Rate: 0.000107\n",
      "Epoch 24433/40000, Loss: 5.6622473493916914e-05, Learning Rate: 0.000107\n",
      "Epoch 24434/40000, Loss: 3.152491626678966e-05, Learning Rate: 0.000107\n",
      "Epoch 24435/40000, Loss: 5.6946992117445916e-05, Learning Rate: 0.000107\n",
      "Epoch 24436/40000, Loss: 5.672799306921661e-05, Learning Rate: 0.000107\n",
      "Epoch 24437/40000, Loss: 5.736587627325207e-05, Learning Rate: 0.000107\n",
      "Epoch 24438/40000, Loss: 1.3697313079319429e-05, Learning Rate: 0.000107\n",
      "Epoch 24439/40000, Loss: 3.1750758353155106e-05, Learning Rate: 0.000107\n",
      "Epoch 24440/40000, Loss: 2.7665289962897077e-05, Learning Rate: 0.000106\n",
      "Epoch 24441/40000, Loss: 2.750084786384832e-05, Learning Rate: 0.000106\n",
      "Epoch 24442/40000, Loss: 3.231670052628033e-05, Learning Rate: 0.000106\n",
      "Epoch 24443/40000, Loss: 5.7346456742379814e-05, Learning Rate: 0.000106\n",
      "Epoch 24444/40000, Loss: 1.4846512385702226e-05, Learning Rate: 0.000106\n",
      "Epoch 24445/40000, Loss: 4.1423514630878344e-05, Learning Rate: 0.000106\n",
      "Epoch 24446/40000, Loss: 5.737949686590582e-05, Learning Rate: 0.000106\n",
      "Epoch 24447/40000, Loss: 5.714480721508153e-05, Learning Rate: 0.000106\n",
      "Epoch 24448/40000, Loss: 5.7476372603559867e-05, Learning Rate: 0.000106\n",
      "Epoch 24449/40000, Loss: 3.9772763557266444e-05, Learning Rate: 0.000106\n",
      "Epoch 24450/40000, Loss: 3.935302811441943e-05, Learning Rate: 0.000106\n",
      "Epoch 24451/40000, Loss: 4.018994513899088e-05, Learning Rate: 0.000106\n",
      "Epoch 24452/40000, Loss: 5.1507249736459926e-05, Learning Rate: 0.000106\n",
      "Epoch 24453/40000, Loss: 5.7391036534681916e-05, Learning Rate: 0.000106\n",
      "Epoch 24454/40000, Loss: 3.989711694885045e-05, Learning Rate: 0.000106\n",
      "Epoch 24455/40000, Loss: 2.781228431558702e-05, Learning Rate: 0.000106\n",
      "Epoch 24456/40000, Loss: 1.4080590517551173e-05, Learning Rate: 0.000106\n",
      "Epoch 24457/40000, Loss: 5.220518141868524e-05, Learning Rate: 0.000106\n",
      "Epoch 24458/40000, Loss: 5.7140969147440046e-05, Learning Rate: 0.000106\n",
      "Epoch 24459/40000, Loss: 5.1028942834818736e-05, Learning Rate: 0.000106\n",
      "Epoch 24460/40000, Loss: 5.154976679477841e-05, Learning Rate: 0.000106\n",
      "Epoch 24461/40000, Loss: 5.15303217980545e-05, Learning Rate: 0.000106\n",
      "Epoch 24462/40000, Loss: 3.1493651476921514e-05, Learning Rate: 0.000106\n",
      "Epoch 24463/40000, Loss: 3.9397804357577115e-05, Learning Rate: 0.000106\n",
      "Epoch 24464/40000, Loss: 5.0873717555077747e-05, Learning Rate: 0.000106\n",
      "Epoch 24465/40000, Loss: 5.0997045036638156e-05, Learning Rate: 0.000106\n",
      "Epoch 24466/40000, Loss: 2.7720152502297424e-05, Learning Rate: 0.000106\n",
      "Epoch 24467/40000, Loss: 5.688787859980948e-05, Learning Rate: 0.000106\n",
      "Epoch 24468/40000, Loss: 3.949728125007823e-05, Learning Rate: 0.000106\n",
      "Epoch 24469/40000, Loss: 3.16908372042235e-05, Learning Rate: 0.000106\n",
      "Epoch 24470/40000, Loss: 2.7664551453199238e-05, Learning Rate: 0.000106\n",
      "Epoch 24471/40000, Loss: 2.7583642804529518e-05, Learning Rate: 0.000106\n",
      "Epoch 24472/40000, Loss: 5.159959982847795e-05, Learning Rate: 0.000106\n",
      "Epoch 24473/40000, Loss: 3.1684761779615656e-05, Learning Rate: 0.000106\n",
      "Epoch 24474/40000, Loss: 5.687530210707337e-05, Learning Rate: 0.000106\n",
      "Epoch 24475/40000, Loss: 3.95202987419907e-05, Learning Rate: 0.000106\n",
      "Epoch 24476/40000, Loss: 2.745917117863428e-05, Learning Rate: 0.000106\n",
      "Epoch 24477/40000, Loss: 1.3632969967147801e-05, Learning Rate: 0.000106\n",
      "Epoch 24478/40000, Loss: 3.150664997519925e-05, Learning Rate: 0.000106\n",
      "Epoch 24479/40000, Loss: 1.4151763025438413e-05, Learning Rate: 0.000106\n",
      "Epoch 24480/40000, Loss: 5.682863047695719e-05, Learning Rate: 0.000106\n",
      "Epoch 24481/40000, Loss: 3.1482955819228664e-05, Learning Rate: 0.000106\n",
      "Epoch 24482/40000, Loss: 5.0862210628110915e-05, Learning Rate: 0.000106\n",
      "Epoch 24483/40000, Loss: 2.7517153284861706e-05, Learning Rate: 0.000106\n",
      "Epoch 24484/40000, Loss: 3.917378489859402e-05, Learning Rate: 0.000106\n",
      "Epoch 24485/40000, Loss: 5.698169115930796e-05, Learning Rate: 0.000106\n",
      "Epoch 24486/40000, Loss: 5.09792153025046e-05, Learning Rate: 0.000106\n",
      "Epoch 24487/40000, Loss: 2.7732839953387156e-05, Learning Rate: 0.000106\n",
      "Epoch 24488/40000, Loss: 3.159639163641259e-05, Learning Rate: 0.000106\n",
      "Epoch 24489/40000, Loss: 2.807449891406577e-05, Learning Rate: 0.000106\n",
      "Epoch 24490/40000, Loss: 5.0822105549741536e-05, Learning Rate: 0.000106\n",
      "Epoch 24491/40000, Loss: 2.769431557680946e-05, Learning Rate: 0.000106\n",
      "Epoch 24492/40000, Loss: 2.7456795578473248e-05, Learning Rate: 0.000106\n",
      "Epoch 24493/40000, Loss: 1.3639871212944854e-05, Learning Rate: 0.000106\n",
      "Epoch 24494/40000, Loss: 2.7625374059425667e-05, Learning Rate: 0.000106\n",
      "Epoch 24495/40000, Loss: 2.7507501727086492e-05, Learning Rate: 0.000106\n",
      "Epoch 24496/40000, Loss: 3.1493549613514915e-05, Learning Rate: 0.000106\n",
      "Epoch 24497/40000, Loss: 2.7615247745416127e-05, Learning Rate: 0.000106\n",
      "Epoch 24498/40000, Loss: 1.3561750165536068e-05, Learning Rate: 0.000106\n",
      "Epoch 24499/40000, Loss: 3.929651575163007e-05, Learning Rate: 0.000106\n",
      "Epoch 24500/40000, Loss: 5.062938726041466e-05, Learning Rate: 0.000106\n",
      "Epoch 24501/40000, Loss: 5.0695980462478474e-05, Learning Rate: 0.000106\n",
      "Epoch 24502/40000, Loss: 5.063340358901769e-05, Learning Rate: 0.000106\n",
      "Epoch 24503/40000, Loss: 2.7365122150513344e-05, Learning Rate: 0.000106\n",
      "Epoch 24504/40000, Loss: 1.3515713362721726e-05, Learning Rate: 0.000106\n",
      "Epoch 24505/40000, Loss: 5.069393591838889e-05, Learning Rate: 0.000106\n",
      "Epoch 24506/40000, Loss: 3.1439434678759426e-05, Learning Rate: 0.000106\n",
      "Epoch 24507/40000, Loss: 2.7420668629929423e-05, Learning Rate: 0.000106\n",
      "Epoch 24508/40000, Loss: 1.344936481473269e-05, Learning Rate: 0.000106\n",
      "Epoch 24509/40000, Loss: 3.1732670322526246e-05, Learning Rate: 0.000106\n",
      "Epoch 24510/40000, Loss: 1.3516446415451355e-05, Learning Rate: 0.000106\n",
      "Epoch 24511/40000, Loss: 5.060788316768594e-05, Learning Rate: 0.000106\n",
      "Epoch 24512/40000, Loss: 1.3487970136338845e-05, Learning Rate: 0.000106\n",
      "Epoch 24513/40000, Loss: 3.9245984225999564e-05, Learning Rate: 0.000106\n",
      "Epoch 24514/40000, Loss: 1.3424417375063058e-05, Learning Rate: 0.000106\n",
      "Epoch 24515/40000, Loss: 1.3468836186802946e-05, Learning Rate: 0.000106\n",
      "Epoch 24516/40000, Loss: 5.064247307018377e-05, Learning Rate: 0.000106\n",
      "Epoch 24517/40000, Loss: 2.7369764211471193e-05, Learning Rate: 0.000106\n",
      "Epoch 24518/40000, Loss: 5.090494960313663e-05, Learning Rate: 0.000106\n",
      "Epoch 24519/40000, Loss: 2.7444631996331736e-05, Learning Rate: 0.000105\n",
      "Epoch 24520/40000, Loss: 5.687211159965955e-05, Learning Rate: 0.000105\n",
      "Epoch 24521/40000, Loss: 3.147228198940866e-05, Learning Rate: 0.000105\n",
      "Epoch 24522/40000, Loss: 2.7427475288277492e-05, Learning Rate: 0.000105\n",
      "Epoch 24523/40000, Loss: 1.3642435078509152e-05, Learning Rate: 0.000105\n",
      "Epoch 24524/40000, Loss: 3.156601815135218e-05, Learning Rate: 0.000105\n",
      "Epoch 24525/40000, Loss: 2.741672687989194e-05, Learning Rate: 0.000105\n",
      "Epoch 24526/40000, Loss: 3.153241777908988e-05, Learning Rate: 0.000105\n",
      "Epoch 24527/40000, Loss: 2.7421314371167682e-05, Learning Rate: 0.000105\n",
      "Epoch 24528/40000, Loss: 3.925287091988139e-05, Learning Rate: 0.000105\n",
      "Epoch 24529/40000, Loss: 3.1923347705742344e-05, Learning Rate: 0.000105\n",
      "Epoch 24530/40000, Loss: 1.3758410204900429e-05, Learning Rate: 0.000105\n",
      "Epoch 24531/40000, Loss: 1.3666592167282943e-05, Learning Rate: 0.000105\n",
      "Epoch 24532/40000, Loss: 3.183826265740208e-05, Learning Rate: 0.000105\n",
      "Epoch 24533/40000, Loss: 2.7481843062560074e-05, Learning Rate: 0.000105\n",
      "Epoch 24534/40000, Loss: 3.292844121460803e-05, Learning Rate: 0.000105\n",
      "Epoch 24535/40000, Loss: 1.3531653166864999e-05, Learning Rate: 0.000105\n",
      "Epoch 24536/40000, Loss: 2.7418940589996055e-05, Learning Rate: 0.000105\n",
      "Epoch 24537/40000, Loss: 2.7460724595584907e-05, Learning Rate: 0.000105\n",
      "Epoch 24538/40000, Loss: 3.935245695174672e-05, Learning Rate: 0.000105\n",
      "Epoch 24539/40000, Loss: 3.920411108992994e-05, Learning Rate: 0.000105\n",
      "Epoch 24540/40000, Loss: 3.930046659661457e-05, Learning Rate: 0.000105\n",
      "Epoch 24541/40000, Loss: 2.7353195036994293e-05, Learning Rate: 0.000105\n",
      "Epoch 24542/40000, Loss: 2.7360832973499782e-05, Learning Rate: 0.000105\n",
      "Epoch 24543/40000, Loss: 2.7580272217164747e-05, Learning Rate: 0.000105\n",
      "Epoch 24544/40000, Loss: 1.3494942322722636e-05, Learning Rate: 0.000105\n",
      "Epoch 24545/40000, Loss: 3.153148281853646e-05, Learning Rate: 0.000105\n",
      "Epoch 24546/40000, Loss: 3.942703187931329e-05, Learning Rate: 0.000105\n",
      "Epoch 24547/40000, Loss: 5.673986015608534e-05, Learning Rate: 0.000105\n",
      "Epoch 24548/40000, Loss: 1.3451528502628207e-05, Learning Rate: 0.000105\n",
      "Epoch 24549/40000, Loss: 1.3607677828986198e-05, Learning Rate: 0.000105\n",
      "Epoch 24550/40000, Loss: 5.67630268051289e-05, Learning Rate: 0.000105\n",
      "Epoch 24551/40000, Loss: 5.680292451870628e-05, Learning Rate: 0.000105\n",
      "Epoch 24552/40000, Loss: 5.682611663360149e-05, Learning Rate: 0.000105\n",
      "Epoch 24553/40000, Loss: 5.069347389508039e-05, Learning Rate: 0.000105\n",
      "Epoch 24554/40000, Loss: 3.940327587770298e-05, Learning Rate: 0.000105\n",
      "Epoch 24555/40000, Loss: 5.082472853246145e-05, Learning Rate: 0.000105\n",
      "Epoch 24556/40000, Loss: 3.92353322240524e-05, Learning Rate: 0.000105\n",
      "Epoch 24557/40000, Loss: 2.7426291126175784e-05, Learning Rate: 0.000105\n",
      "Epoch 24558/40000, Loss: 1.3758985915046651e-05, Learning Rate: 0.000105\n",
      "Epoch 24559/40000, Loss: 3.166407623211853e-05, Learning Rate: 0.000105\n",
      "Epoch 24560/40000, Loss: 3.1573796150041744e-05, Learning Rate: 0.000105\n",
      "Epoch 24561/40000, Loss: 5.0620135880308226e-05, Learning Rate: 0.000105\n",
      "Epoch 24562/40000, Loss: 5.6746524933259934e-05, Learning Rate: 0.000105\n",
      "Epoch 24563/40000, Loss: 5.067729580332525e-05, Learning Rate: 0.000105\n",
      "Epoch 24564/40000, Loss: 5.6573488109279424e-05, Learning Rate: 0.000105\n",
      "Epoch 24565/40000, Loss: 5.0782004109350964e-05, Learning Rate: 0.000105\n",
      "Epoch 24566/40000, Loss: 5.068620157544501e-05, Learning Rate: 0.000105\n",
      "Epoch 24567/40000, Loss: 2.7355610654922202e-05, Learning Rate: 0.000105\n",
      "Epoch 24568/40000, Loss: 3.137813473585993e-05, Learning Rate: 0.000105\n",
      "Epoch 24569/40000, Loss: 2.763768679869827e-05, Learning Rate: 0.000105\n",
      "Epoch 24570/40000, Loss: 3.936445864383131e-05, Learning Rate: 0.000105\n",
      "Epoch 24571/40000, Loss: 3.925312194041908e-05, Learning Rate: 0.000105\n",
      "Epoch 24572/40000, Loss: 5.08683588122949e-05, Learning Rate: 0.000105\n",
      "Epoch 24573/40000, Loss: 5.695836807717569e-05, Learning Rate: 0.000105\n",
      "Epoch 24574/40000, Loss: 5.680193498847075e-05, Learning Rate: 0.000105\n",
      "Epoch 24575/40000, Loss: 3.938228837796487e-05, Learning Rate: 0.000105\n",
      "Epoch 24576/40000, Loss: 3.134966755169444e-05, Learning Rate: 0.000105\n",
      "Epoch 24577/40000, Loss: 3.924816701328382e-05, Learning Rate: 0.000105\n",
      "Epoch 24578/40000, Loss: 5.658554073306732e-05, Learning Rate: 0.000105\n",
      "Epoch 24579/40000, Loss: 3.130467302980833e-05, Learning Rate: 0.000105\n",
      "Epoch 24580/40000, Loss: 1.350530146737583e-05, Learning Rate: 0.000105\n",
      "Epoch 24581/40000, Loss: 1.3513071280613076e-05, Learning Rate: 0.000105\n",
      "Epoch 24582/40000, Loss: 5.649243757943623e-05, Learning Rate: 0.000105\n",
      "Epoch 24583/40000, Loss: 2.733085239015054e-05, Learning Rate: 0.000105\n",
      "Epoch 24584/40000, Loss: 5.6706383475102484e-05, Learning Rate: 0.000105\n",
      "Epoch 24585/40000, Loss: 5.6624186981935054e-05, Learning Rate: 0.000105\n",
      "Epoch 24586/40000, Loss: 3.1334424420492724e-05, Learning Rate: 0.000105\n",
      "Epoch 24587/40000, Loss: 1.3420169125311077e-05, Learning Rate: 0.000105\n",
      "Epoch 24588/40000, Loss: 2.7394569769967347e-05, Learning Rate: 0.000105\n",
      "Epoch 24589/40000, Loss: 5.656949360854924e-05, Learning Rate: 0.000105\n",
      "Epoch 24590/40000, Loss: 3.9122907764976844e-05, Learning Rate: 0.000105\n",
      "Epoch 24591/40000, Loss: 2.7375988793210126e-05, Learning Rate: 0.000105\n",
      "Epoch 24592/40000, Loss: 3.951090184273198e-05, Learning Rate: 0.000105\n",
      "Epoch 24593/40000, Loss: 1.3562346794060431e-05, Learning Rate: 0.000105\n",
      "Epoch 24594/40000, Loss: 3.1566214602207765e-05, Learning Rate: 0.000105\n",
      "Epoch 24595/40000, Loss: 3.933875996153802e-05, Learning Rate: 0.000105\n",
      "Epoch 24596/40000, Loss: 5.696669177268632e-05, Learning Rate: 0.000105\n",
      "Epoch 24597/40000, Loss: 1.3596611097455025e-05, Learning Rate: 0.000105\n",
      "Epoch 24598/40000, Loss: 1.3556527846958488e-05, Learning Rate: 0.000104\n",
      "Epoch 24599/40000, Loss: 5.0774546252796426e-05, Learning Rate: 0.000104\n",
      "Epoch 24600/40000, Loss: 5.0812435802072287e-05, Learning Rate: 0.000104\n",
      "Epoch 24601/40000, Loss: 3.146200469927862e-05, Learning Rate: 0.000104\n",
      "Epoch 24602/40000, Loss: 2.740521995292511e-05, Learning Rate: 0.000104\n",
      "Epoch 24603/40000, Loss: 3.941469913115725e-05, Learning Rate: 0.000104\n",
      "Epoch 24604/40000, Loss: 5.070365659776144e-05, Learning Rate: 0.000104\n",
      "Epoch 24605/40000, Loss: 3.136627856292762e-05, Learning Rate: 0.000104\n",
      "Epoch 24606/40000, Loss: 5.0609418394742534e-05, Learning Rate: 0.000104\n",
      "Epoch 24607/40000, Loss: 3.9256701711565256e-05, Learning Rate: 0.000104\n",
      "Epoch 24608/40000, Loss: 3.12991178361699e-05, Learning Rate: 0.000104\n",
      "Epoch 24609/40000, Loss: 3.130264667561278e-05, Learning Rate: 0.000104\n",
      "Epoch 24610/40000, Loss: 5.660732858814299e-05, Learning Rate: 0.000104\n",
      "Epoch 24611/40000, Loss: 3.933214247808792e-05, Learning Rate: 0.000104\n",
      "Epoch 24612/40000, Loss: 5.700267502106726e-05, Learning Rate: 0.000104\n",
      "Epoch 24613/40000, Loss: 3.139747786917724e-05, Learning Rate: 0.000104\n",
      "Epoch 24614/40000, Loss: 3.943741103284992e-05, Learning Rate: 0.000104\n",
      "Epoch 24615/40000, Loss: 5.06784999743104e-05, Learning Rate: 0.000104\n",
      "Epoch 24616/40000, Loss: 5.059726390754804e-05, Learning Rate: 0.000104\n",
      "Epoch 24617/40000, Loss: 5.674333078786731e-05, Learning Rate: 0.000104\n",
      "Epoch 24618/40000, Loss: 5.077433888800442e-05, Learning Rate: 0.000104\n",
      "Epoch 24619/40000, Loss: 5.786314432043582e-05, Learning Rate: 0.000104\n",
      "Epoch 24620/40000, Loss: 1.3768611097475514e-05, Learning Rate: 0.000104\n",
      "Epoch 24621/40000, Loss: 3.168192051816732e-05, Learning Rate: 0.000104\n",
      "Epoch 24622/40000, Loss: 3.1487732485402375e-05, Learning Rate: 0.000104\n",
      "Epoch 24623/40000, Loss: 4.0429484215565026e-05, Learning Rate: 0.000104\n",
      "Epoch 24624/40000, Loss: 1.3802387911709957e-05, Learning Rate: 0.000104\n",
      "Epoch 24625/40000, Loss: 3.96386458305642e-05, Learning Rate: 0.000104\n",
      "Epoch 24626/40000, Loss: 3.945951539208181e-05, Learning Rate: 0.000104\n",
      "Epoch 24627/40000, Loss: 2.7522793971002102e-05, Learning Rate: 0.000104\n",
      "Epoch 24628/40000, Loss: 5.677423541783355e-05, Learning Rate: 0.000104\n",
      "Epoch 24629/40000, Loss: 3.148444011458196e-05, Learning Rate: 0.000104\n",
      "Epoch 24630/40000, Loss: 2.7720789148588665e-05, Learning Rate: 0.000104\n",
      "Epoch 24631/40000, Loss: 5.674357817042619e-05, Learning Rate: 0.000104\n",
      "Epoch 24632/40000, Loss: 4.009392068837769e-05, Learning Rate: 0.000104\n",
      "Epoch 24633/40000, Loss: 3.958046363550238e-05, Learning Rate: 0.000104\n",
      "Epoch 24634/40000, Loss: 3.1647148716729134e-05, Learning Rate: 0.000104\n",
      "Epoch 24635/40000, Loss: 5.189955118112266e-05, Learning Rate: 0.000104\n",
      "Epoch 24636/40000, Loss: 5.259504177956842e-05, Learning Rate: 0.000104\n",
      "Epoch 24637/40000, Loss: 3.986469528172165e-05, Learning Rate: 0.000104\n",
      "Epoch 24638/40000, Loss: 2.7668265829561278e-05, Learning Rate: 0.000104\n",
      "Epoch 24639/40000, Loss: 2.9035782063147053e-05, Learning Rate: 0.000104\n",
      "Epoch 24640/40000, Loss: 3.434334939811379e-05, Learning Rate: 0.000104\n",
      "Epoch 24641/40000, Loss: 1.4206580090103671e-05, Learning Rate: 0.000104\n",
      "Epoch 24642/40000, Loss: 3.179006671416573e-05, Learning Rate: 0.000104\n",
      "Epoch 24643/40000, Loss: 4.014984369860031e-05, Learning Rate: 0.000104\n",
      "Epoch 24644/40000, Loss: 3.982766065746546e-05, Learning Rate: 0.000104\n",
      "Epoch 24645/40000, Loss: 3.976589141529985e-05, Learning Rate: 0.000104\n",
      "Epoch 24646/40000, Loss: 2.7820718969451264e-05, Learning Rate: 0.000104\n",
      "Epoch 24647/40000, Loss: 3.9472572098020464e-05, Learning Rate: 0.000104\n",
      "Epoch 24648/40000, Loss: 5.6960241636261344e-05, Learning Rate: 0.000104\n",
      "Epoch 24649/40000, Loss: 3.964943971368484e-05, Learning Rate: 0.000104\n",
      "Epoch 24650/40000, Loss: 5.755996971856803e-05, Learning Rate: 0.000104\n",
      "Epoch 24651/40000, Loss: 4.0270908357342705e-05, Learning Rate: 0.000104\n",
      "Epoch 24652/40000, Loss: 1.3774091712548397e-05, Learning Rate: 0.000104\n",
      "Epoch 24653/40000, Loss: 3.949890015064739e-05, Learning Rate: 0.000104\n",
      "Epoch 24654/40000, Loss: 1.361765134788584e-05, Learning Rate: 0.000104\n",
      "Epoch 24655/40000, Loss: 2.7299945941194892e-05, Learning Rate: 0.000104\n",
      "Epoch 24656/40000, Loss: 3.139065302093513e-05, Learning Rate: 0.000104\n",
      "Epoch 24657/40000, Loss: 5.064169454271905e-05, Learning Rate: 0.000104\n",
      "Epoch 24658/40000, Loss: 3.922951873391867e-05, Learning Rate: 0.000104\n",
      "Epoch 24659/40000, Loss: 5.658263398800045e-05, Learning Rate: 0.000104\n",
      "Epoch 24660/40000, Loss: 5.070233964943327e-05, Learning Rate: 0.000104\n",
      "Epoch 24661/40000, Loss: 3.140225453535095e-05, Learning Rate: 0.000104\n",
      "Epoch 24662/40000, Loss: 2.7301077352603897e-05, Learning Rate: 0.000104\n",
      "Epoch 24663/40000, Loss: 5.053620407124981e-05, Learning Rate: 0.000104\n",
      "Epoch 24664/40000, Loss: 5.666572542395443e-05, Learning Rate: 0.000104\n",
      "Epoch 24665/40000, Loss: 5.0575916247908026e-05, Learning Rate: 0.000104\n",
      "Epoch 24666/40000, Loss: 5.7033204939216375e-05, Learning Rate: 0.000104\n",
      "Epoch 24667/40000, Loss: 2.740355375863146e-05, Learning Rate: 0.000104\n",
      "Epoch 24668/40000, Loss: 2.7298403438180685e-05, Learning Rate: 0.000104\n",
      "Epoch 24669/40000, Loss: 5.0756734708556905e-05, Learning Rate: 0.000104\n",
      "Epoch 24670/40000, Loss: 3.1457861041417345e-05, Learning Rate: 0.000104\n",
      "Epoch 24671/40000, Loss: 2.738806688284967e-05, Learning Rate: 0.000104\n",
      "Epoch 24672/40000, Loss: 3.926565477740951e-05, Learning Rate: 0.000104\n",
      "Epoch 24673/40000, Loss: 5.692521881428547e-05, Learning Rate: 0.000104\n",
      "Epoch 24674/40000, Loss: 5.064394645160064e-05, Learning Rate: 0.000104\n",
      "Epoch 24675/40000, Loss: 5.6536471674917266e-05, Learning Rate: 0.000104\n",
      "Epoch 24676/40000, Loss: 2.7246653189649805e-05, Learning Rate: 0.000104\n",
      "Epoch 24677/40000, Loss: 5.6693534133955836e-05, Learning Rate: 0.000104\n",
      "Epoch 24678/40000, Loss: 3.913691034540534e-05, Learning Rate: 0.000103\n",
      "Epoch 24679/40000, Loss: 3.912301690434106e-05, Learning Rate: 0.000103\n",
      "Epoch 24680/40000, Loss: 3.133194331894629e-05, Learning Rate: 0.000103\n",
      "Epoch 24681/40000, Loss: 5.6573848269181326e-05, Learning Rate: 0.000103\n",
      "Epoch 24682/40000, Loss: 1.3356249837670475e-05, Learning Rate: 0.000103\n",
      "Epoch 24683/40000, Loss: 3.119342363788746e-05, Learning Rate: 0.000103\n",
      "Epoch 24684/40000, Loss: 1.3471503734763246e-05, Learning Rate: 0.000103\n",
      "Epoch 24685/40000, Loss: 1.3438369933282956e-05, Learning Rate: 0.000103\n",
      "Epoch 24686/40000, Loss: 3.9282873331103474e-05, Learning Rate: 0.000103\n",
      "Epoch 24687/40000, Loss: 1.3393399967753794e-05, Learning Rate: 0.000103\n",
      "Epoch 24688/40000, Loss: 3.127208765363321e-05, Learning Rate: 0.000103\n",
      "Epoch 24689/40000, Loss: 2.7178564778296277e-05, Learning Rate: 0.000103\n",
      "Epoch 24690/40000, Loss: 1.337005141976988e-05, Learning Rate: 0.000103\n",
      "Epoch 24691/40000, Loss: 3.908130383933894e-05, Learning Rate: 0.000103\n",
      "Epoch 24692/40000, Loss: 2.7281226721243e-05, Learning Rate: 0.000103\n",
      "Epoch 24693/40000, Loss: 3.91519206459634e-05, Learning Rate: 0.000103\n",
      "Epoch 24694/40000, Loss: 1.349686772300629e-05, Learning Rate: 0.000103\n",
      "Epoch 24695/40000, Loss: 5.673118357663043e-05, Learning Rate: 0.000103\n",
      "Epoch 24696/40000, Loss: 3.9251201087608933e-05, Learning Rate: 0.000103\n",
      "Epoch 24697/40000, Loss: 3.930174352717586e-05, Learning Rate: 0.000103\n",
      "Epoch 24698/40000, Loss: 1.3561068044509739e-05, Learning Rate: 0.000103\n",
      "Epoch 24699/40000, Loss: 3.983365968451835e-05, Learning Rate: 0.000103\n",
      "Epoch 24700/40000, Loss: 5.66635062568821e-05, Learning Rate: 0.000103\n",
      "Epoch 24701/40000, Loss: 2.7312895326758735e-05, Learning Rate: 0.000103\n",
      "Epoch 24702/40000, Loss: 3.943066985812038e-05, Learning Rate: 0.000103\n",
      "Epoch 24703/40000, Loss: 3.922290125046857e-05, Learning Rate: 0.000103\n",
      "Epoch 24704/40000, Loss: 2.7361338652553968e-05, Learning Rate: 0.000103\n",
      "Epoch 24705/40000, Loss: 1.3455220141622704e-05, Learning Rate: 0.000103\n",
      "Epoch 24706/40000, Loss: 5.6734428653726354e-05, Learning Rate: 0.000103\n",
      "Epoch 24707/40000, Loss: 1.3564334949478507e-05, Learning Rate: 0.000103\n",
      "Epoch 24708/40000, Loss: 5.052977940067649e-05, Learning Rate: 0.000103\n",
      "Epoch 24709/40000, Loss: 1.3495850907929707e-05, Learning Rate: 0.000103\n",
      "Epoch 24710/40000, Loss: 1.3341547855816316e-05, Learning Rate: 0.000103\n",
      "Epoch 24711/40000, Loss: 2.7374491764931008e-05, Learning Rate: 0.000103\n",
      "Epoch 24712/40000, Loss: 1.3492975995177403e-05, Learning Rate: 0.000103\n",
      "Epoch 24713/40000, Loss: 2.7310914447298273e-05, Learning Rate: 0.000103\n",
      "Epoch 24714/40000, Loss: 5.048396633355878e-05, Learning Rate: 0.000103\n",
      "Epoch 24715/40000, Loss: 5.051042535342276e-05, Learning Rate: 0.000103\n",
      "Epoch 24716/40000, Loss: 2.729459447436966e-05, Learning Rate: 0.000103\n",
      "Epoch 24717/40000, Loss: 2.7282501832814887e-05, Learning Rate: 0.000103\n",
      "Epoch 24718/40000, Loss: 3.9176371501525864e-05, Learning Rate: 0.000103\n",
      "Epoch 24719/40000, Loss: 3.9096274122130126e-05, Learning Rate: 0.000103\n",
      "Epoch 24720/40000, Loss: 5.654971027979627e-05, Learning Rate: 0.000103\n",
      "Epoch 24721/40000, Loss: 3.122769339825027e-05, Learning Rate: 0.000103\n",
      "Epoch 24722/40000, Loss: 5.677342051058076e-05, Learning Rate: 0.000103\n",
      "Epoch 24723/40000, Loss: 2.7240574127063155e-05, Learning Rate: 0.000103\n",
      "Epoch 24724/40000, Loss: 2.7261570721748285e-05, Learning Rate: 0.000103\n",
      "Epoch 24725/40000, Loss: 2.728449362621177e-05, Learning Rate: 0.000103\n",
      "Epoch 24726/40000, Loss: 2.7203537683817558e-05, Learning Rate: 0.000103\n",
      "Epoch 24727/40000, Loss: 2.727066930674482e-05, Learning Rate: 0.000103\n",
      "Epoch 24728/40000, Loss: 5.674893327523023e-05, Learning Rate: 0.000103\n",
      "Epoch 24729/40000, Loss: 2.747330472629983e-05, Learning Rate: 0.000103\n",
      "Epoch 24730/40000, Loss: 1.3586439308710396e-05, Learning Rate: 0.000103\n",
      "Epoch 24731/40000, Loss: 5.092840365250595e-05, Learning Rate: 0.000103\n",
      "Epoch 24732/40000, Loss: 3.935451968573034e-05, Learning Rate: 0.000103\n",
      "Epoch 24733/40000, Loss: 2.740180934779346e-05, Learning Rate: 0.000103\n",
      "Epoch 24734/40000, Loss: 3.1796742405276746e-05, Learning Rate: 0.000103\n",
      "Epoch 24735/40000, Loss: 3.170724448864348e-05, Learning Rate: 0.000103\n",
      "Epoch 24736/40000, Loss: 2.7459384000394493e-05, Learning Rate: 0.000103\n",
      "Epoch 24737/40000, Loss: 3.240131991333328e-05, Learning Rate: 0.000103\n",
      "Epoch 24738/40000, Loss: 3.17391604767181e-05, Learning Rate: 0.000103\n",
      "Epoch 24739/40000, Loss: 4.0536448068451136e-05, Learning Rate: 0.000103\n",
      "Epoch 24740/40000, Loss: 5.7327953982166946e-05, Learning Rate: 0.000103\n",
      "Epoch 24741/40000, Loss: 5.723944923374802e-05, Learning Rate: 0.000103\n",
      "Epoch 24742/40000, Loss: 3.9659142203163356e-05, Learning Rate: 0.000103\n",
      "Epoch 24743/40000, Loss: 5.070591578260064e-05, Learning Rate: 0.000103\n",
      "Epoch 24744/40000, Loss: 1.4169903806759976e-05, Learning Rate: 0.000103\n",
      "Epoch 24745/40000, Loss: 3.146122980979271e-05, Learning Rate: 0.000103\n",
      "Epoch 24746/40000, Loss: 5.1030106988037005e-05, Learning Rate: 0.000103\n",
      "Epoch 24747/40000, Loss: 5.68391551496461e-05, Learning Rate: 0.000103\n",
      "Epoch 24748/40000, Loss: 5.691132537322119e-05, Learning Rate: 0.000103\n",
      "Epoch 24749/40000, Loss: 2.7473359295981936e-05, Learning Rate: 0.000103\n",
      "Epoch 24750/40000, Loss: 5.085190787212923e-05, Learning Rate: 0.000103\n",
      "Epoch 24751/40000, Loss: 3.967158409068361e-05, Learning Rate: 0.000103\n",
      "Epoch 24752/40000, Loss: 3.200388891855255e-05, Learning Rate: 0.000103\n",
      "Epoch 24753/40000, Loss: 2.7484802558319643e-05, Learning Rate: 0.000103\n",
      "Epoch 24754/40000, Loss: 5.6574863265268505e-05, Learning Rate: 0.000103\n",
      "Epoch 24755/40000, Loss: 1.3612501788884401e-05, Learning Rate: 0.000103\n",
      "Epoch 24756/40000, Loss: 5.063688149675727e-05, Learning Rate: 0.000103\n",
      "Epoch 24757/40000, Loss: 5.0527822168078274e-05, Learning Rate: 0.000103\n",
      "Epoch 24758/40000, Loss: 3.125952571281232e-05, Learning Rate: 0.000103\n",
      "Epoch 24759/40000, Loss: 5.704397335648537e-05, Learning Rate: 0.000102\n",
      "Epoch 24760/40000, Loss: 3.913296677637845e-05, Learning Rate: 0.000102\n",
      "Epoch 24761/40000, Loss: 3.913680120604113e-05, Learning Rate: 0.000102\n",
      "Epoch 24762/40000, Loss: 5.6819088058546185e-05, Learning Rate: 0.000102\n",
      "Epoch 24763/40000, Loss: 1.3446453522192314e-05, Learning Rate: 0.000102\n",
      "Epoch 24764/40000, Loss: 3.911271414835937e-05, Learning Rate: 0.000102\n",
      "Epoch 24765/40000, Loss: 2.7117157515021972e-05, Learning Rate: 0.000102\n",
      "Epoch 24766/40000, Loss: 2.7145475542056374e-05, Learning Rate: 0.000102\n",
      "Epoch 24767/40000, Loss: 5.0428498070687056e-05, Learning Rate: 0.000102\n",
      "Epoch 24768/40000, Loss: 5.663714910042472e-05, Learning Rate: 0.000102\n",
      "Epoch 24769/40000, Loss: 3.122976704617031e-05, Learning Rate: 0.000102\n",
      "Epoch 24770/40000, Loss: 3.117761661997065e-05, Learning Rate: 0.000102\n",
      "Epoch 24771/40000, Loss: 2.712285095185507e-05, Learning Rate: 0.000102\n",
      "Epoch 24772/40000, Loss: 5.6625598517712206e-05, Learning Rate: 0.000102\n",
      "Epoch 24773/40000, Loss: 1.3318684068508446e-05, Learning Rate: 0.000102\n",
      "Epoch 24774/40000, Loss: 5.66227754461579e-05, Learning Rate: 0.000102\n",
      "Epoch 24775/40000, Loss: 2.7131363822263665e-05, Learning Rate: 0.000102\n",
      "Epoch 24776/40000, Loss: 3.118314270977862e-05, Learning Rate: 0.000102\n",
      "Epoch 24777/40000, Loss: 5.657330257236026e-05, Learning Rate: 0.000102\n",
      "Epoch 24778/40000, Loss: 1.3396527720033191e-05, Learning Rate: 0.000102\n",
      "Epoch 24779/40000, Loss: 2.7184796635992825e-05, Learning Rate: 0.000102\n",
      "Epoch 24780/40000, Loss: 1.3502409274224192e-05, Learning Rate: 0.000102\n",
      "Epoch 24781/40000, Loss: 2.719144686125219e-05, Learning Rate: 0.000102\n",
      "Epoch 24782/40000, Loss: 3.921200186596252e-05, Learning Rate: 0.000102\n",
      "Epoch 24783/40000, Loss: 3.1222218240145594e-05, Learning Rate: 0.000102\n",
      "Epoch 24784/40000, Loss: 5.661054092342965e-05, Learning Rate: 0.000102\n",
      "Epoch 24785/40000, Loss: 1.3468304132402409e-05, Learning Rate: 0.000102\n",
      "Epoch 24786/40000, Loss: 1.3400398529483937e-05, Learning Rate: 0.000102\n",
      "Epoch 24787/40000, Loss: 3.907191057805903e-05, Learning Rate: 0.000102\n",
      "Epoch 24788/40000, Loss: 3.906341953552328e-05, Learning Rate: 0.000102\n",
      "Epoch 24789/40000, Loss: 3.115044819423929e-05, Learning Rate: 0.000102\n",
      "Epoch 24790/40000, Loss: 2.7176871299161576e-05, Learning Rate: 0.000102\n",
      "Epoch 24791/40000, Loss: 1.3396502254181542e-05, Learning Rate: 0.000102\n",
      "Epoch 24792/40000, Loss: 5.0698741688393056e-05, Learning Rate: 0.000102\n",
      "Epoch 24793/40000, Loss: 3.119059692835435e-05, Learning Rate: 0.000102\n",
      "Epoch 24794/40000, Loss: 5.657905057887547e-05, Learning Rate: 0.000102\n",
      "Epoch 24795/40000, Loss: 2.730074447754305e-05, Learning Rate: 0.000102\n",
      "Epoch 24796/40000, Loss: 5.081271956441924e-05, Learning Rate: 0.000102\n",
      "Epoch 24797/40000, Loss: 1.3449480320559815e-05, Learning Rate: 0.000102\n",
      "Epoch 24798/40000, Loss: 2.720449811022263e-05, Learning Rate: 0.000102\n",
      "Epoch 24799/40000, Loss: 2.7212176064494997e-05, Learning Rate: 0.000102\n",
      "Epoch 24800/40000, Loss: 5.650251478073187e-05, Learning Rate: 0.000102\n",
      "Epoch 24801/40000, Loss: 5.650759339914657e-05, Learning Rate: 0.000102\n",
      "Epoch 24802/40000, Loss: 3.11091062030755e-05, Learning Rate: 0.000102\n",
      "Epoch 24803/40000, Loss: 5.688694727723487e-05, Learning Rate: 0.000102\n",
      "Epoch 24804/40000, Loss: 3.922144969692454e-05, Learning Rate: 0.000102\n",
      "Epoch 24805/40000, Loss: 3.9137503335950896e-05, Learning Rate: 0.000102\n",
      "Epoch 24806/40000, Loss: 2.7272897568764165e-05, Learning Rate: 0.000102\n",
      "Epoch 24807/40000, Loss: 5.651726314681582e-05, Learning Rate: 0.000102\n",
      "Epoch 24808/40000, Loss: 3.126759111182764e-05, Learning Rate: 0.000102\n",
      "Epoch 24809/40000, Loss: 1.3445674085232895e-05, Learning Rate: 0.000102\n",
      "Epoch 24810/40000, Loss: 1.3466469681588933e-05, Learning Rate: 0.000102\n",
      "Epoch 24811/40000, Loss: 2.731222593865823e-05, Learning Rate: 0.000102\n",
      "Epoch 24812/40000, Loss: 3.1203038815874606e-05, Learning Rate: 0.000102\n",
      "Epoch 24813/40000, Loss: 1.3426208170130849e-05, Learning Rate: 0.000102\n",
      "Epoch 24814/40000, Loss: 5.061506089987233e-05, Learning Rate: 0.000102\n",
      "Epoch 24815/40000, Loss: 3.916716741514392e-05, Learning Rate: 0.000102\n",
      "Epoch 24816/40000, Loss: 2.7088161004940048e-05, Learning Rate: 0.000102\n",
      "Epoch 24817/40000, Loss: 3.1182338716462255e-05, Learning Rate: 0.000102\n",
      "Epoch 24818/40000, Loss: 2.720035809034016e-05, Learning Rate: 0.000102\n",
      "Epoch 24819/40000, Loss: 3.112173726549372e-05, Learning Rate: 0.000102\n",
      "Epoch 24820/40000, Loss: 1.3404649507720023e-05, Learning Rate: 0.000102\n",
      "Epoch 24821/40000, Loss: 3.118269160040654e-05, Learning Rate: 0.000102\n",
      "Epoch 24822/40000, Loss: 5.662609692080878e-05, Learning Rate: 0.000102\n",
      "Epoch 24823/40000, Loss: 5.056627924204804e-05, Learning Rate: 0.000102\n",
      "Epoch 24824/40000, Loss: 3.119169195997529e-05, Learning Rate: 0.000102\n",
      "Epoch 24825/40000, Loss: 5.667456571245566e-05, Learning Rate: 0.000102\n",
      "Epoch 24826/40000, Loss: 5.0495233153924346e-05, Learning Rate: 0.000102\n",
      "Epoch 24827/40000, Loss: 1.3482094800565392e-05, Learning Rate: 0.000102\n",
      "Epoch 24828/40000, Loss: 2.7114963813801296e-05, Learning Rate: 0.000102\n",
      "Epoch 24829/40000, Loss: 3.929845479433425e-05, Learning Rate: 0.000102\n",
      "Epoch 24830/40000, Loss: 3.913302862201817e-05, Learning Rate: 0.000102\n",
      "Epoch 24831/40000, Loss: 5.661788600264117e-05, Learning Rate: 0.000102\n",
      "Epoch 24832/40000, Loss: 3.9130598452175036e-05, Learning Rate: 0.000102\n",
      "Epoch 24833/40000, Loss: 2.710633816604968e-05, Learning Rate: 0.000102\n",
      "Epoch 24834/40000, Loss: 1.3458059584081639e-05, Learning Rate: 0.000102\n",
      "Epoch 24835/40000, Loss: 5.0364014896331355e-05, Learning Rate: 0.000102\n",
      "Epoch 24836/40000, Loss: 5.041230542701669e-05, Learning Rate: 0.000102\n",
      "Epoch 24837/40000, Loss: 5.641191819449887e-05, Learning Rate: 0.000102\n",
      "Epoch 24838/40000, Loss: 3.1072446290636435e-05, Learning Rate: 0.000102\n",
      "Epoch 24839/40000, Loss: 5.043137207394466e-05, Learning Rate: 0.000102\n",
      "Epoch 24840/40000, Loss: 3.901059608324431e-05, Learning Rate: 0.000102\n",
      "Epoch 24841/40000, Loss: 5.054644134361297e-05, Learning Rate: 0.000101\n",
      "Epoch 24842/40000, Loss: 5.048553430242464e-05, Learning Rate: 0.000101\n",
      "Epoch 24843/40000, Loss: 2.7078429411631078e-05, Learning Rate: 0.000101\n",
      "Epoch 24844/40000, Loss: 5.0485039537306875e-05, Learning Rate: 0.000101\n",
      "Epoch 24845/40000, Loss: 5.644023258355446e-05, Learning Rate: 0.000101\n",
      "Epoch 24846/40000, Loss: 5.631776002701372e-05, Learning Rate: 0.000101\n",
      "Epoch 24847/40000, Loss: 3.1094237783690915e-05, Learning Rate: 0.000101\n",
      "Epoch 24848/40000, Loss: 2.707373096200172e-05, Learning Rate: 0.000101\n",
      "Epoch 24849/40000, Loss: 5.646611680276692e-05, Learning Rate: 0.000101\n",
      "Epoch 24850/40000, Loss: 2.7160764147993177e-05, Learning Rate: 0.000101\n",
      "Epoch 24851/40000, Loss: 3.1221479730447754e-05, Learning Rate: 0.000101\n",
      "Epoch 24852/40000, Loss: 1.3478309483616613e-05, Learning Rate: 0.000101\n",
      "Epoch 24853/40000, Loss: 1.3418685739452485e-05, Learning Rate: 0.000101\n",
      "Epoch 24854/40000, Loss: 2.707508065213915e-05, Learning Rate: 0.000101\n",
      "Epoch 24855/40000, Loss: 5.0472543080104515e-05, Learning Rate: 0.000101\n",
      "Epoch 24856/40000, Loss: 3.1145500543061644e-05, Learning Rate: 0.000101\n",
      "Epoch 24857/40000, Loss: 5.654585038428195e-05, Learning Rate: 0.000101\n",
      "Epoch 24858/40000, Loss: 5.059409522800706e-05, Learning Rate: 0.000101\n",
      "Epoch 24859/40000, Loss: 3.113379716523923e-05, Learning Rate: 0.000101\n",
      "Epoch 24860/40000, Loss: 1.337789490207797e-05, Learning Rate: 0.000101\n",
      "Epoch 24861/40000, Loss: 1.3422610209090635e-05, Learning Rate: 0.000101\n",
      "Epoch 24862/40000, Loss: 3.13476994051598e-05, Learning Rate: 0.000101\n",
      "Epoch 24863/40000, Loss: 2.7115493139717728e-05, Learning Rate: 0.000101\n",
      "Epoch 24864/40000, Loss: 5.051428161095828e-05, Learning Rate: 0.000101\n",
      "Epoch 24865/40000, Loss: 3.1244169804267585e-05, Learning Rate: 0.000101\n",
      "Epoch 24866/40000, Loss: 5.049550236435607e-05, Learning Rate: 0.000101\n",
      "Epoch 24867/40000, Loss: 5.694018182111904e-05, Learning Rate: 0.000101\n",
      "Epoch 24868/40000, Loss: 1.353612242382951e-05, Learning Rate: 0.000101\n",
      "Epoch 24869/40000, Loss: 1.3468796169036068e-05, Learning Rate: 0.000101\n",
      "Epoch 24870/40000, Loss: 2.7497875635162927e-05, Learning Rate: 0.000101\n",
      "Epoch 24871/40000, Loss: 3.126747105852701e-05, Learning Rate: 0.000101\n",
      "Epoch 24872/40000, Loss: 3.1458661396754906e-05, Learning Rate: 0.000101\n",
      "Epoch 24873/40000, Loss: 3.1358726118924096e-05, Learning Rate: 0.000101\n",
      "Epoch 24874/40000, Loss: 2.743524601100944e-05, Learning Rate: 0.000101\n",
      "Epoch 24875/40000, Loss: 1.3846964066033252e-05, Learning Rate: 0.000101\n",
      "Epoch 24876/40000, Loss: 2.7361953470972367e-05, Learning Rate: 0.000101\n",
      "Epoch 24877/40000, Loss: 5.6820437748683617e-05, Learning Rate: 0.000101\n",
      "Epoch 24878/40000, Loss: 3.139868931612e-05, Learning Rate: 0.000101\n",
      "Epoch 24879/40000, Loss: 3.1559095077682287e-05, Learning Rate: 0.000101\n",
      "Epoch 24880/40000, Loss: 2.7405878427089192e-05, Learning Rate: 0.000101\n",
      "Epoch 24881/40000, Loss: 2.718860014283564e-05, Learning Rate: 0.000101\n",
      "Epoch 24882/40000, Loss: 2.7884525479748845e-05, Learning Rate: 0.000101\n",
      "Epoch 24883/40000, Loss: 3.145663868053816e-05, Learning Rate: 0.000101\n",
      "Epoch 24884/40000, Loss: 5.536222306545824e-05, Learning Rate: 0.000101\n",
      "Epoch 24885/40000, Loss: 5.416788189904764e-05, Learning Rate: 0.000101\n",
      "Epoch 24886/40000, Loss: 2.7422373023000546e-05, Learning Rate: 0.000101\n",
      "Epoch 24887/40000, Loss: 2.7617037630989216e-05, Learning Rate: 0.000101\n",
      "Epoch 24888/40000, Loss: 3.954768544645049e-05, Learning Rate: 0.000101\n",
      "Epoch 24889/40000, Loss: 2.783522904792335e-05, Learning Rate: 0.000101\n",
      "Epoch 24890/40000, Loss: 5.692300328519195e-05, Learning Rate: 0.000101\n",
      "Epoch 24891/40000, Loss: 5.096749373478815e-05, Learning Rate: 0.000101\n",
      "Epoch 24892/40000, Loss: 2.726056300161872e-05, Learning Rate: 0.000101\n",
      "Epoch 24893/40000, Loss: 3.129000833723694e-05, Learning Rate: 0.000101\n",
      "Epoch 24894/40000, Loss: 5.689074532710947e-05, Learning Rate: 0.000101\n",
      "Epoch 24895/40000, Loss: 1.3725080862059258e-05, Learning Rate: 0.000101\n",
      "Epoch 24896/40000, Loss: 5.085390148451552e-05, Learning Rate: 0.000101\n",
      "Epoch 24897/40000, Loss: 3.925496639567427e-05, Learning Rate: 0.000101\n",
      "Epoch 24898/40000, Loss: 3.1333824153989553e-05, Learning Rate: 0.000101\n",
      "Epoch 24899/40000, Loss: 1.3765566109213978e-05, Learning Rate: 0.000101\n",
      "Epoch 24900/40000, Loss: 5.6741857406450436e-05, Learning Rate: 0.000101\n",
      "Epoch 24901/40000, Loss: 5.101475471747108e-05, Learning Rate: 0.000101\n",
      "Epoch 24902/40000, Loss: 3.1484669307246804e-05, Learning Rate: 0.000101\n",
      "Epoch 24903/40000, Loss: 5.092989522381686e-05, Learning Rate: 0.000101\n",
      "Epoch 24904/40000, Loss: 5.685362339136191e-05, Learning Rate: 0.000101\n",
      "Epoch 24905/40000, Loss: 3.964267307310365e-05, Learning Rate: 0.000101\n",
      "Epoch 24906/40000, Loss: 5.663751289830543e-05, Learning Rate: 0.000101\n",
      "Epoch 24907/40000, Loss: 1.4189606190484483e-05, Learning Rate: 0.000101\n",
      "Epoch 24908/40000, Loss: 5.086417877464555e-05, Learning Rate: 0.000101\n",
      "Epoch 24909/40000, Loss: 3.950621248804964e-05, Learning Rate: 0.000101\n",
      "Epoch 24910/40000, Loss: 5.081391645944677e-05, Learning Rate: 0.000101\n",
      "Epoch 24911/40000, Loss: 3.120503970421851e-05, Learning Rate: 0.000101\n",
      "Epoch 24912/40000, Loss: 1.3708267943002284e-05, Learning Rate: 0.000101\n",
      "Epoch 24913/40000, Loss: 2.7124036932946183e-05, Learning Rate: 0.000101\n",
      "Epoch 24914/40000, Loss: 5.63983921892941e-05, Learning Rate: 0.000101\n",
      "Epoch 24915/40000, Loss: 3.909879160346463e-05, Learning Rate: 0.000101\n",
      "Epoch 24916/40000, Loss: 5.0469701818656176e-05, Learning Rate: 0.000101\n",
      "Epoch 24917/40000, Loss: 3.9296570321312174e-05, Learning Rate: 0.000101\n",
      "Epoch 24918/40000, Loss: 5.071010673418641e-05, Learning Rate: 0.000101\n",
      "Epoch 24919/40000, Loss: 2.7025678718928248e-05, Learning Rate: 0.000101\n",
      "Epoch 24920/40000, Loss: 1.3446883713186253e-05, Learning Rate: 0.000101\n",
      "Epoch 24921/40000, Loss: 3.914456829079427e-05, Learning Rate: 0.000101\n",
      "Epoch 24922/40000, Loss: 2.7047097319155e-05, Learning Rate: 0.000101\n",
      "Epoch 24923/40000, Loss: 5.0518174248281866e-05, Learning Rate: 0.000100\n",
      "Epoch 24924/40000, Loss: 1.3582544852397405e-05, Learning Rate: 0.000100\n",
      "Epoch 24925/40000, Loss: 5.64628244319465e-05, Learning Rate: 0.000100\n",
      "Epoch 24926/40000, Loss: 3.906857455149293e-05, Learning Rate: 0.000100\n",
      "Epoch 24927/40000, Loss: 3.108441524091177e-05, Learning Rate: 0.000100\n",
      "Epoch 24928/40000, Loss: 5.040135874878615e-05, Learning Rate: 0.000100\n",
      "Epoch 24929/40000, Loss: 2.7064956157119013e-05, Learning Rate: 0.000100\n",
      "Epoch 24930/40000, Loss: 3.901473246514797e-05, Learning Rate: 0.000100\n",
      "Epoch 24931/40000, Loss: 5.647803482133895e-05, Learning Rate: 0.000100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     49\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     51\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     52\u001b[0m         labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Assuming you have a CustomDataset class\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (numpy array): A numpy array of shape [N, 100, 200, 19] where N is the number of samples.\n",
    "        \"\"\"\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.target = torch.tensor(target, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input = self.x[idx]\n",
    "        target = self.target[idx]\n",
    "        return input, target\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#scalerx = StandardScaler()\n",
    "#scalery = StandardScaler()\n",
    "\n",
    "#x_scaled = scalerx.fit_transform(x.reshape(1, -1))\n",
    "#y_scaled = scalery.fit_transform(y.reshape(1 -1))\n",
    "\n",
    "criterion3 = SmoothnessLoss(.1, .1)\n",
    "dataset = CustomDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet(n_channels=100, n_classes=9).to(device)\n",
    "criterion1 = DivLoss()\n",
    "criterion2 = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "gamma = 0.99998  # The exponential decay factor (adjust as needed)\n",
    "scheduler = ExponentialLR(optimizer, gamma=gamma)\n",
    "#import gc\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 40000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        labels = labels[0,:,:,:,:].permute(0,2,1,3)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "         # Assuming you want to compare against the input\n",
    "        loss2 = criterion2(outputs, labels)\n",
    "        if epoch < 3000:\n",
    "            loss1 = criterion1(outputs, labels) \n",
    "            loss3 = criterion3(outputs)\n",
    "            loss = (.2*loss1+loss2*2 + .000000002*loss3)/3\n",
    "        else:\n",
    "            loss1 = criterion1(outputs, labels) \n",
    "            loss = loss2+.001*loss1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Learning Rate: {current_lr:.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32ac9af5-3277-49a3-a8ff-30b2cb552c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30000, Loss: 0.004004552029073238, Learning Rate: 0.001499\n",
      "Epoch 2/30000, Loss: 0.0028416288550943136, Learning Rate: 0.001499\n",
      "Epoch 3/30000, Loss: 0.0023865848779678345, Learning Rate: 0.001498\n",
      "Epoch 4/30000, Loss: 0.0015171573031693697, Learning Rate: 0.001498\n",
      "Epoch 5/30000, Loss: 0.0012848193291574717, Learning Rate: 0.001497\n",
      "Epoch 6/30000, Loss: 0.0014545870944857597, Learning Rate: 0.001497\n",
      "Epoch 7/30000, Loss: 0.0013269478222355247, Learning Rate: 0.001496\n",
      "Epoch 8/30000, Loss: 0.0009922620374709368, Learning Rate: 0.001496\n",
      "Epoch 9/30000, Loss: 0.0008938167011365294, Learning Rate: 0.001495\n",
      "Epoch 10/30000, Loss: 0.0011773461010307074, Learning Rate: 0.001495\n",
      "Epoch 11/30000, Loss: 0.0009026917978189886, Learning Rate: 0.001494\n",
      "Epoch 12/30000, Loss: 0.0008649174124002457, Learning Rate: 0.001494\n",
      "Epoch 13/30000, Loss: 0.0009419328416697681, Learning Rate: 0.001493\n",
      "Epoch 14/30000, Loss: 0.0008815083419904113, Learning Rate: 0.001492\n",
      "Epoch 15/30000, Loss: 0.001176426187157631, Learning Rate: 0.001492\n",
      "Epoch 16/30000, Loss: 0.0009354629437439144, Learning Rate: 0.001491\n",
      "Epoch 17/30000, Loss: 0.0012765899300575256, Learning Rate: 0.001491\n",
      "Epoch 18/30000, Loss: 0.0010584581177681684, Learning Rate: 0.001490\n",
      "Epoch 19/30000, Loss: 0.0010564676485955715, Learning Rate: 0.001490\n",
      "Epoch 20/30000, Loss: 0.0012309686280786991, Learning Rate: 0.001489\n",
      "Epoch 21/30000, Loss: 0.0011594765819609165, Learning Rate: 0.001489\n",
      "Epoch 22/30000, Loss: 0.0009756173240020871, Learning Rate: 0.001488\n",
      "Epoch 23/30000, Loss: 0.0010406447108834982, Learning Rate: 0.001488\n",
      "Epoch 24/30000, Loss: 0.0010785949416458607, Learning Rate: 0.001487\n",
      "Epoch 25/30000, Loss: 0.0009373435750603676, Learning Rate: 0.001487\n",
      "Epoch 26/30000, Loss: 0.0013298431877046824, Learning Rate: 0.001486\n",
      "Epoch 27/30000, Loss: 0.0011934705544263124, Learning Rate: 0.001485\n",
      "Epoch 28/30000, Loss: 0.0011478892993181944, Learning Rate: 0.001485\n",
      "Epoch 29/30000, Loss: 0.0008457875228486955, Learning Rate: 0.001484\n",
      "Epoch 30/30000, Loss: 0.0009544812492094934, Learning Rate: 0.001484\n",
      "Epoch 31/30000, Loss: 0.001133171608671546, Learning Rate: 0.001483\n",
      "Epoch 32/30000, Loss: 0.0011245060013607144, Learning Rate: 0.001483\n",
      "Epoch 33/30000, Loss: 0.0010313688544556499, Learning Rate: 0.001482\n",
      "Epoch 34/30000, Loss: 0.001281604403629899, Learning Rate: 0.001482\n",
      "Epoch 35/30000, Loss: 0.0010580739472061396, Learning Rate: 0.001481\n",
      "Epoch 36/30000, Loss: 0.000961236422881484, Learning Rate: 0.001481\n",
      "Epoch 37/30000, Loss: 0.0018342712428420782, Learning Rate: 0.001480\n",
      "Epoch 38/30000, Loss: 0.0010869117686524987, Learning Rate: 0.001480\n",
      "Epoch 39/30000, Loss: 0.001075696898624301, Learning Rate: 0.001479\n",
      "Epoch 40/30000, Loss: 0.0016558903735131025, Learning Rate: 0.001479\n",
      "Epoch 41/30000, Loss: 0.0019017852609977126, Learning Rate: 0.001478\n",
      "Epoch 42/30000, Loss: 0.0016562857199460268, Learning Rate: 0.001477\n",
      "Epoch 43/30000, Loss: 0.0019363986793905497, Learning Rate: 0.001477\n",
      "Epoch 44/30000, Loss: 0.001436819788068533, Learning Rate: 0.001476\n",
      "Epoch 45/30000, Loss: 0.002728054765611887, Learning Rate: 0.001476\n",
      "Epoch 46/30000, Loss: 0.0014207500498741865, Learning Rate: 0.001475\n",
      "Epoch 47/30000, Loss: 0.0010331422090530396, Learning Rate: 0.001475\n",
      "Epoch 48/30000, Loss: 0.0011038321536034346, Learning Rate: 0.001474\n",
      "Epoch 49/30000, Loss: 0.0008894065394997597, Learning Rate: 0.001474\n",
      "Epoch 50/30000, Loss: 0.0008462319383397698, Learning Rate: 0.001473\n",
      "Epoch 51/30000, Loss: 0.0008434168994426727, Learning Rate: 0.001473\n",
      "Epoch 52/30000, Loss: 0.0009124808711931109, Learning Rate: 0.001472\n",
      "Epoch 53/30000, Loss: 0.000984082231298089, Learning Rate: 0.001472\n",
      "Epoch 54/30000, Loss: 0.0007995280320756137, Learning Rate: 0.001471\n",
      "Epoch 55/30000, Loss: 0.00086748949252069, Learning Rate: 0.001471\n",
      "Epoch 56/30000, Loss: 0.0008438433869741857, Learning Rate: 0.001470\n",
      "Epoch 57/30000, Loss: 0.00076781160896644, Learning Rate: 0.001470\n",
      "Epoch 58/30000, Loss: 0.0008548680925741792, Learning Rate: 0.001469\n",
      "Epoch 59/30000, Loss: 0.0007893924484960735, Learning Rate: 0.001468\n",
      "Epoch 60/30000, Loss: 0.0007464151713065803, Learning Rate: 0.001468\n",
      "Epoch 61/30000, Loss: 0.0009118683519773185, Learning Rate: 0.001467\n",
      "Epoch 62/30000, Loss: 0.0007578565273433924, Learning Rate: 0.001467\n",
      "Epoch 63/30000, Loss: 0.0009270197479054332, Learning Rate: 0.001466\n",
      "Epoch 64/30000, Loss: 0.0008098280522972345, Learning Rate: 0.001466\n",
      "Epoch 65/30000, Loss: 0.0009512307588011026, Learning Rate: 0.001465\n",
      "Epoch 66/30000, Loss: 0.0008196059498004615, Learning Rate: 0.001465\n",
      "Epoch 67/30000, Loss: 0.0008722677594050765, Learning Rate: 0.001464\n",
      "Epoch 68/30000, Loss: 0.0008340537897311151, Learning Rate: 0.001464\n",
      "Epoch 69/30000, Loss: 0.0007588697480969131, Learning Rate: 0.001463\n",
      "Epoch 70/30000, Loss: 0.0007714363746345043, Learning Rate: 0.001463\n",
      "Epoch 71/30000, Loss: 0.0008945049485191703, Learning Rate: 0.001462\n",
      "Epoch 72/30000, Loss: 0.0007745663169771433, Learning Rate: 0.001462\n",
      "Epoch 73/30000, Loss: 0.0009623630321584642, Learning Rate: 0.001461\n",
      "Epoch 74/30000, Loss: 0.0008132474031299353, Learning Rate: 0.001461\n",
      "Epoch 75/30000, Loss: 0.0007628020248375833, Learning Rate: 0.001460\n",
      "Epoch 76/30000, Loss: 0.0007177824154496193, Learning Rate: 0.001460\n",
      "Epoch 77/30000, Loss: 0.0007171868346631527, Learning Rate: 0.001459\n",
      "Epoch 78/30000, Loss: 0.0007690342026762664, Learning Rate: 0.001458\n",
      "Epoch 79/30000, Loss: 0.0007339768926613033, Learning Rate: 0.001458\n",
      "Epoch 80/30000, Loss: 0.000740056624636054, Learning Rate: 0.001457\n",
      "Epoch 81/30000, Loss: 0.0008261248003691435, Learning Rate: 0.001457\n",
      "Epoch 82/30000, Loss: 0.0007759280269965529, Learning Rate: 0.001456\n",
      "Epoch 83/30000, Loss: 0.0008143513696268201, Learning Rate: 0.001456\n",
      "Epoch 84/30000, Loss: 0.0009349293541163206, Learning Rate: 0.001455\n",
      "Epoch 85/30000, Loss: 0.0009482984896749258, Learning Rate: 0.001455\n",
      "Epoch 86/30000, Loss: 0.0008131578797474504, Learning Rate: 0.001454\n",
      "Epoch 87/30000, Loss: 0.000851368298754096, Learning Rate: 0.001454\n",
      "Epoch 88/30000, Loss: 0.000890038616489619, Learning Rate: 0.001453\n",
      "Epoch 89/30000, Loss: 0.0007562568644061685, Learning Rate: 0.001453\n",
      "Epoch 90/30000, Loss: 0.0009556924924254417, Learning Rate: 0.001452\n",
      "Epoch 91/30000, Loss: 0.0009454243700020015, Learning Rate: 0.001452\n",
      "Epoch 92/30000, Loss: 0.0008876663050614297, Learning Rate: 0.001451\n",
      "Epoch 93/30000, Loss: 0.0008873408660292625, Learning Rate: 0.001451\n",
      "Epoch 94/30000, Loss: 0.0007352816173806787, Learning Rate: 0.001450\n",
      "Epoch 95/30000, Loss: 0.0007505663670599461, Learning Rate: 0.001450\n",
      "Epoch 96/30000, Loss: 0.0007302564335986972, Learning Rate: 0.001449\n",
      "Epoch 97/30000, Loss: 0.0007265018648467958, Learning Rate: 0.001449\n",
      "Epoch 98/30000, Loss: 0.0008702421328052878, Learning Rate: 0.001448\n",
      "Epoch 99/30000, Loss: 0.0007211926858872175, Learning Rate: 0.001447\n",
      "Epoch 100/30000, Loss: 0.0008733670692890882, Learning Rate: 0.001447\n",
      "Epoch 101/30000, Loss: 0.0007349832449108362, Learning Rate: 0.001446\n",
      "Epoch 102/30000, Loss: 0.000818186323158443, Learning Rate: 0.001446\n",
      "Epoch 103/30000, Loss: 0.0009050477528944612, Learning Rate: 0.001445\n",
      "Epoch 104/30000, Loss: 0.0009058289579115808, Learning Rate: 0.001445\n",
      "Epoch 105/30000, Loss: 0.0007463630754500628, Learning Rate: 0.001444\n",
      "Epoch 106/30000, Loss: 0.000848837080411613, Learning Rate: 0.001444\n",
      "Epoch 107/30000, Loss: 0.0008340796921402216, Learning Rate: 0.001443\n",
      "Epoch 108/30000, Loss: 0.0007286961190402508, Learning Rate: 0.001443\n",
      "Epoch 109/30000, Loss: 0.0008397748460993171, Learning Rate: 0.001442\n",
      "Epoch 110/30000, Loss: 0.0008200397132895887, Learning Rate: 0.001442\n",
      "Epoch 111/30000, Loss: 0.0007160212844610214, Learning Rate: 0.001441\n",
      "Epoch 112/30000, Loss: 0.000878576363902539, Learning Rate: 0.001441\n",
      "Epoch 113/30000, Loss: 0.0008285962394438684, Learning Rate: 0.001440\n",
      "Epoch 114/30000, Loss: 0.0007277540862560272, Learning Rate: 0.001440\n",
      "Epoch 115/30000, Loss: 0.0007467332761734724, Learning Rate: 0.001439\n",
      "Epoch 116/30000, Loss: 0.0009195720194838941, Learning Rate: 0.001439\n",
      "Epoch 117/30000, Loss: 0.0008613583631813526, Learning Rate: 0.001438\n",
      "Epoch 118/30000, Loss: 0.0007225587032735348, Learning Rate: 0.001438\n",
      "Epoch 119/30000, Loss: 0.0009254211909137666, Learning Rate: 0.001437\n",
      "Epoch 120/30000, Loss: 0.0008985439199022949, Learning Rate: 0.001437\n",
      "Epoch 121/30000, Loss: 0.0007289869245141745, Learning Rate: 0.001436\n",
      "Epoch 122/30000, Loss: 0.0007066479884088039, Learning Rate: 0.001436\n",
      "Epoch 123/30000, Loss: 0.0007729299832135439, Learning Rate: 0.001435\n",
      "Epoch 124/30000, Loss: 0.0007697022520005703, Learning Rate: 0.001435\n",
      "Epoch 125/30000, Loss: 0.0007825132925063372, Learning Rate: 0.001434\n",
      "Epoch 126/30000, Loss: 0.0009787019807845354, Learning Rate: 0.001433\n",
      "Epoch 127/30000, Loss: 0.0008814720786176622, Learning Rate: 0.001433\n",
      "Epoch 128/30000, Loss: 0.0008253822452388704, Learning Rate: 0.001432\n",
      "Epoch 129/30000, Loss: 0.0007384641794487834, Learning Rate: 0.001432\n",
      "Epoch 130/30000, Loss: 0.0007827137596905231, Learning Rate: 0.001431\n",
      "Epoch 131/30000, Loss: 0.0008296099258586764, Learning Rate: 0.001431\n",
      "Epoch 132/30000, Loss: 0.0007743316236883402, Learning Rate: 0.001430\n",
      "Epoch 133/30000, Loss: 0.0009730657911859453, Learning Rate: 0.001430\n",
      "Epoch 134/30000, Loss: 0.0008105183951556683, Learning Rate: 0.001429\n",
      "Epoch 135/30000, Loss: 0.001149617601186037, Learning Rate: 0.001429\n",
      "Epoch 136/30000, Loss: 0.0009120546164922416, Learning Rate: 0.001428\n",
      "Epoch 137/30000, Loss: 0.00076295156031847, Learning Rate: 0.001428\n",
      "Epoch 138/30000, Loss: 0.0008839945076033473, Learning Rate: 0.001427\n",
      "Epoch 139/30000, Loss: 0.0007950973231345415, Learning Rate: 0.001427\n",
      "Epoch 140/30000, Loss: 0.0008162674494087696, Learning Rate: 0.001426\n",
      "Epoch 141/30000, Loss: 0.0007867527892813087, Learning Rate: 0.001426\n",
      "Epoch 142/30000, Loss: 0.0009359844261780381, Learning Rate: 0.001425\n",
      "Epoch 143/30000, Loss: 0.0009849386988207698, Learning Rate: 0.001425\n",
      "Epoch 144/30000, Loss: 0.0007604329730384052, Learning Rate: 0.001424\n",
      "Epoch 145/30000, Loss: 0.0007501723011955619, Learning Rate: 0.001424\n",
      "Epoch 146/30000, Loss: 0.0007416118169203401, Learning Rate: 0.001423\n",
      "Epoch 147/30000, Loss: 0.0008539282716810703, Learning Rate: 0.001423\n",
      "Epoch 148/30000, Loss: 0.0008421886595897377, Learning Rate: 0.001422\n",
      "Epoch 149/30000, Loss: 0.0007904774975031614, Learning Rate: 0.001422\n",
      "Epoch 150/30000, Loss: 0.0008155385730788112, Learning Rate: 0.001421\n",
      "Epoch 151/30000, Loss: 0.0007078488706611097, Learning Rate: 0.001421\n",
      "Epoch 152/30000, Loss: 0.0007730070501565933, Learning Rate: 0.001420\n",
      "Epoch 153/30000, Loss: 0.000680799363180995, Learning Rate: 0.001420\n",
      "Epoch 154/30000, Loss: 0.0006885250331833959, Learning Rate: 0.001419\n",
      "Epoch 155/30000, Loss: 0.000689696753397584, Learning Rate: 0.001419\n",
      "Epoch 156/30000, Loss: 0.0007801703177392483, Learning Rate: 0.001418\n",
      "Epoch 157/30000, Loss: 0.0007079185452312231, Learning Rate: 0.001418\n",
      "Epoch 158/30000, Loss: 0.0007973155006766319, Learning Rate: 0.001417\n",
      "Epoch 159/30000, Loss: 0.0007033822475932539, Learning Rate: 0.001417\n",
      "Epoch 160/30000, Loss: 0.0008090123883448541, Learning Rate: 0.001416\n",
      "Epoch 161/30000, Loss: 0.0008027636213228106, Learning Rate: 0.001416\n",
      "Epoch 162/30000, Loss: 0.0008199384901672602, Learning Rate: 0.001415\n",
      "Epoch 163/30000, Loss: 0.0007254101801663637, Learning Rate: 0.001415\n",
      "Epoch 164/30000, Loss: 0.0011049062013626099, Learning Rate: 0.001414\n",
      "Epoch 165/30000, Loss: 0.0008145184256136417, Learning Rate: 0.001413\n",
      "Epoch 166/30000, Loss: 0.0015159721951931715, Learning Rate: 0.001413\n",
      "Epoch 167/30000, Loss: 0.0012130443938076496, Learning Rate: 0.001412\n",
      "Epoch 168/30000, Loss: 0.0020251856185495853, Learning Rate: 0.001412\n",
      "Epoch 169/30000, Loss: 0.0014259349554777145, Learning Rate: 0.001411\n",
      "Epoch 170/30000, Loss: 0.0016820047749206424, Learning Rate: 0.001411\n",
      "Epoch 171/30000, Loss: 0.0012331815669313073, Learning Rate: 0.001410\n",
      "Epoch 172/30000, Loss: 0.001318412832915783, Learning Rate: 0.001410\n",
      "Epoch 173/30000, Loss: 0.001189343398436904, Learning Rate: 0.001409\n",
      "Epoch 174/30000, Loss: 0.0010603752452880144, Learning Rate: 0.001409\n",
      "Epoch 175/30000, Loss: 0.0010159105295315385, Learning Rate: 0.001408\n",
      "Epoch 176/30000, Loss: 0.0007500830106437206, Learning Rate: 0.001408\n",
      "Epoch 177/30000, Loss: 0.0007302503800019622, Learning Rate: 0.001407\n",
      "Epoch 178/30000, Loss: 0.0008932554046623409, Learning Rate: 0.001407\n",
      "Epoch 179/30000, Loss: 0.0006924480549059808, Learning Rate: 0.001406\n",
      "Epoch 180/30000, Loss: 0.0008162499871104956, Learning Rate: 0.001406\n",
      "Epoch 181/30000, Loss: 0.00072858901694417, Learning Rate: 0.001405\n",
      "Epoch 182/30000, Loss: 0.0007998703513294458, Learning Rate: 0.001405\n",
      "Epoch 183/30000, Loss: 0.0006795613444410264, Learning Rate: 0.001404\n",
      "Epoch 184/30000, Loss: 0.0007706514443270862, Learning Rate: 0.001404\n",
      "Epoch 185/30000, Loss: 0.0007377490401268005, Learning Rate: 0.001403\n",
      "Epoch 186/30000, Loss: 0.0007830635877326131, Learning Rate: 0.001403\n",
      "Epoch 187/30000, Loss: 0.0006729503511451185, Learning Rate: 0.001402\n",
      "Epoch 188/30000, Loss: 0.0007504053646698594, Learning Rate: 0.001402\n",
      "Epoch 189/30000, Loss: 0.0006710123270750046, Learning Rate: 0.001401\n",
      "Epoch 190/30000, Loss: 0.0006801376002840698, Learning Rate: 0.001401\n",
      "Epoch 191/30000, Loss: 0.0007693677907809615, Learning Rate: 0.001400\n",
      "Epoch 192/30000, Loss: 0.0007607204606756568, Learning Rate: 0.001400\n",
      "Epoch 193/30000, Loss: 0.0007724504685029387, Learning Rate: 0.001399\n",
      "Epoch 194/30000, Loss: 0.0007473462610505521, Learning Rate: 0.001399\n",
      "Epoch 195/30000, Loss: 0.0007414196152240038, Learning Rate: 0.001398\n",
      "Epoch 196/30000, Loss: 0.000664703780785203, Learning Rate: 0.001398\n",
      "Epoch 197/30000, Loss: 0.000733858672901988, Learning Rate: 0.001397\n",
      "Epoch 198/30000, Loss: 0.0007420738693326712, Learning Rate: 0.001397\n",
      "Epoch 199/30000, Loss: 0.0006422925507649779, Learning Rate: 0.001396\n",
      "Epoch 200/30000, Loss: 0.0006291968747973442, Learning Rate: 0.001396\n",
      "Epoch 201/30000, Loss: 0.0007460636552423239, Learning Rate: 0.001395\n",
      "Epoch 202/30000, Loss: 0.0007271626964211464, Learning Rate: 0.001395\n",
      "Epoch 203/30000, Loss: 0.0006584589718841016, Learning Rate: 0.001394\n",
      "Epoch 204/30000, Loss: 0.0007535904878750443, Learning Rate: 0.001394\n",
      "Epoch 205/30000, Loss: 0.000662576116155833, Learning Rate: 0.001393\n",
      "Epoch 206/30000, Loss: 0.0006596008315682411, Learning Rate: 0.001393\n",
      "Epoch 207/30000, Loss: 0.0007272922666743398, Learning Rate: 0.001392\n",
      "Epoch 208/30000, Loss: 0.0006722921971231699, Learning Rate: 0.001392\n",
      "Epoch 209/30000, Loss: 0.000763619551435113, Learning Rate: 0.001391\n",
      "Epoch 210/30000, Loss: 0.0006683591054752469, Learning Rate: 0.001391\n",
      "Epoch 211/30000, Loss: 0.0006651844596490264, Learning Rate: 0.001390\n",
      "Epoch 212/30000, Loss: 0.0006960243335925043, Learning Rate: 0.001390\n",
      "Epoch 213/30000, Loss: 0.0008226587669923902, Learning Rate: 0.001389\n",
      "Epoch 214/30000, Loss: 0.0006787762977182865, Learning Rate: 0.001389\n",
      "Epoch 215/30000, Loss: 0.0006663351668976247, Learning Rate: 0.001388\n",
      "Epoch 216/30000, Loss: 0.0007695866515859962, Learning Rate: 0.001388\n",
      "Epoch 217/30000, Loss: 0.0006952012772671878, Learning Rate: 0.001387\n",
      "Epoch 218/30000, Loss: 0.000752110849134624, Learning Rate: 0.001387\n",
      "Epoch 219/30000, Loss: 0.0007338891155086458, Learning Rate: 0.001386\n",
      "Epoch 220/30000, Loss: 0.0007932446314953268, Learning Rate: 0.001386\n",
      "Epoch 221/30000, Loss: 0.0008352600852958858, Learning Rate: 0.001385\n",
      "Epoch 222/30000, Loss: 0.0011832598829641938, Learning Rate: 0.001385\n",
      "Epoch 223/30000, Loss: 0.0006879338761791587, Learning Rate: 0.001384\n",
      "Epoch 224/30000, Loss: 0.0007171470206230879, Learning Rate: 0.001384\n",
      "Epoch 225/30000, Loss: 0.0006955561111681163, Learning Rate: 0.001383\n",
      "Epoch 226/30000, Loss: 0.000672945927362889, Learning Rate: 0.001383\n",
      "Epoch 227/30000, Loss: 0.0007431364501826465, Learning Rate: 0.001382\n",
      "Epoch 228/30000, Loss: 0.0006681671366095543, Learning Rate: 0.001382\n",
      "Epoch 229/30000, Loss: 0.0007673959480598569, Learning Rate: 0.001381\n",
      "Epoch 230/30000, Loss: 0.0007374964188784361, Learning Rate: 0.001381\n",
      "Epoch 231/30000, Loss: 0.0006720685632899404, Learning Rate: 0.001380\n",
      "Epoch 232/30000, Loss: 0.0007145588169805706, Learning Rate: 0.001380\n",
      "Epoch 233/30000, Loss: 0.0007310636574402452, Learning Rate: 0.001379\n",
      "Epoch 234/30000, Loss: 0.0006656610639765859, Learning Rate: 0.001379\n",
      "Epoch 235/30000, Loss: 0.000671225308906287, Learning Rate: 0.001378\n",
      "Epoch 236/30000, Loss: 0.0007481389911845326, Learning Rate: 0.001378\n",
      "Epoch 237/30000, Loss: 0.0007372900727204978, Learning Rate: 0.001377\n",
      "Epoch 238/30000, Loss: 0.0006446436746045947, Learning Rate: 0.001377\n",
      "Epoch 239/30000, Loss: 0.0006337264785543084, Learning Rate: 0.001376\n",
      "Epoch 240/30000, Loss: 0.0006367183523252606, Learning Rate: 0.001376\n",
      "Epoch 241/30000, Loss: 0.0006506676436401904, Learning Rate: 0.001375\n",
      "Epoch 242/30000, Loss: 0.0006315655773505569, Learning Rate: 0.001375\n",
      "Epoch 243/30000, Loss: 0.0006226699333637953, Learning Rate: 0.001374\n",
      "Epoch 244/30000, Loss: 0.0006340991239994764, Learning Rate: 0.001374\n",
      "Epoch 245/30000, Loss: 0.000648467626888305, Learning Rate: 0.001373\n",
      "Epoch 246/30000, Loss: 0.0007851627888157964, Learning Rate: 0.001373\n",
      "Epoch 247/30000, Loss: 0.0006758837262168527, Learning Rate: 0.001372\n",
      "Epoch 248/30000, Loss: 0.0007274344097822905, Learning Rate: 0.001372\n",
      "Epoch 249/30000, Loss: 0.0006660720100626349, Learning Rate: 0.001371\n",
      "Epoch 250/30000, Loss: 0.0008229147642850876, Learning Rate: 0.001371\n",
      "Epoch 251/30000, Loss: 0.0007193558267317712, Learning Rate: 0.001370\n",
      "Epoch 252/30000, Loss: 0.0007480423664674163, Learning Rate: 0.001370\n",
      "Epoch 253/30000, Loss: 0.0007278384873643517, Learning Rate: 0.001369\n",
      "Epoch 254/30000, Loss: 0.0006521320901811123, Learning Rate: 0.001369\n",
      "Epoch 255/30000, Loss: 0.0008784678066149354, Learning Rate: 0.001368\n",
      "Epoch 256/30000, Loss: 0.0006755554350093007, Learning Rate: 0.001368\n",
      "Epoch 257/30000, Loss: 0.000668466673232615, Learning Rate: 0.001367\n",
      "Epoch 258/30000, Loss: 0.0008825844852253795, Learning Rate: 0.001367\n",
      "Epoch 259/30000, Loss: 0.0007366848876699805, Learning Rate: 0.001366\n",
      "Epoch 260/30000, Loss: 0.0006491873646155, Learning Rate: 0.001366\n",
      "Epoch 261/30000, Loss: 0.0007335233385674655, Learning Rate: 0.001365\n",
      "Epoch 262/30000, Loss: 0.000652276212349534, Learning Rate: 0.001365\n",
      "Epoch 263/30000, Loss: 0.0006447887280955911, Learning Rate: 0.001364\n",
      "Epoch 264/30000, Loss: 0.0007354135159403086, Learning Rate: 0.001364\n",
      "Epoch 265/30000, Loss: 0.0007033601286821067, Learning Rate: 0.001364\n",
      "Epoch 266/30000, Loss: 0.0007131858728826046, Learning Rate: 0.001363\n",
      "Epoch 267/30000, Loss: 0.0006311734905466437, Learning Rate: 0.001363\n",
      "Epoch 268/30000, Loss: 0.0007967640412971377, Learning Rate: 0.001362\n",
      "Epoch 269/30000, Loss: 0.0008054440259002149, Learning Rate: 0.001362\n",
      "Epoch 270/30000, Loss: 0.0007814014097675681, Learning Rate: 0.001361\n",
      "Epoch 271/30000, Loss: 0.0007280666613951325, Learning Rate: 0.001361\n",
      "Epoch 272/30000, Loss: 0.000684080645442009, Learning Rate: 0.001360\n",
      "Epoch 273/30000, Loss: 0.0006645647226832807, Learning Rate: 0.001360\n",
      "Epoch 274/30000, Loss: 0.0008914539939723909, Learning Rate: 0.001359\n",
      "Epoch 275/30000, Loss: 0.0006525887874886394, Learning Rate: 0.001359\n",
      "Epoch 276/30000, Loss: 0.0007696574321016669, Learning Rate: 0.001358\n",
      "Epoch 277/30000, Loss: 0.0007382116164080799, Learning Rate: 0.001358\n",
      "Epoch 278/30000, Loss: 0.0007103203097358346, Learning Rate: 0.001357\n",
      "Epoch 279/30000, Loss: 0.0006999724428169429, Learning Rate: 0.001357\n",
      "Epoch 280/30000, Loss: 0.000625886837951839, Learning Rate: 0.001356\n",
      "Epoch 281/30000, Loss: 0.0006405593012459576, Learning Rate: 0.001356\n",
      "Epoch 282/30000, Loss: 0.0006447193445637822, Learning Rate: 0.001355\n",
      "Epoch 283/30000, Loss: 0.0006977502489462495, Learning Rate: 0.001355\n",
      "Epoch 284/30000, Loss: 0.0006578657194040716, Learning Rate: 0.001354\n",
      "Epoch 285/30000, Loss: 0.000646260567009449, Learning Rate: 0.001354\n",
      "Epoch 286/30000, Loss: 0.0006376641103997827, Learning Rate: 0.001353\n",
      "Epoch 287/30000, Loss: 0.0006823764997534454, Learning Rate: 0.001353\n",
      "Epoch 288/30000, Loss: 0.0007480333442799747, Learning Rate: 0.001352\n",
      "Epoch 289/30000, Loss: 0.0007163733243942261, Learning Rate: 0.001352\n",
      "Epoch 290/30000, Loss: 0.0007121388334780931, Learning Rate: 0.001351\n",
      "Epoch 291/30000, Loss: 0.000736395304556936, Learning Rate: 0.001351\n",
      "Epoch 292/30000, Loss: 0.0007011941052041948, Learning Rate: 0.001350\n",
      "Epoch 293/30000, Loss: 0.0007377704023383558, Learning Rate: 0.001350\n",
      "Epoch 294/30000, Loss: 0.0006270078010857105, Learning Rate: 0.001349\n",
      "Epoch 295/30000, Loss: 0.0007627609884366393, Learning Rate: 0.001349\n",
      "Epoch 296/30000, Loss: 0.0006419628043659031, Learning Rate: 0.001348\n",
      "Epoch 297/30000, Loss: 0.0007775340927764773, Learning Rate: 0.001348\n",
      "Epoch 298/30000, Loss: 0.0011200824519619346, Learning Rate: 0.001347\n",
      "Epoch 299/30000, Loss: 0.00088393350597471, Learning Rate: 0.001347\n",
      "Epoch 300/30000, Loss: 0.0008562409202568233, Learning Rate: 0.001346\n",
      "Epoch 301/30000, Loss: 0.000959772733040154, Learning Rate: 0.001346\n",
      "Epoch 302/30000, Loss: 0.0007055331370793283, Learning Rate: 0.001345\n",
      "Epoch 303/30000, Loss: 0.0006989770918153226, Learning Rate: 0.001345\n",
      "Epoch 304/30000, Loss: 0.000915833399631083, Learning Rate: 0.001344\n",
      "Epoch 305/30000, Loss: 0.0011524485889822245, Learning Rate: 0.001344\n",
      "Epoch 306/30000, Loss: 0.0008683095220476389, Learning Rate: 0.001344\n",
      "Epoch 307/30000, Loss: 0.0006982666673138738, Learning Rate: 0.001343\n",
      "Epoch 308/30000, Loss: 0.0007463651709258556, Learning Rate: 0.001343\n",
      "Epoch 309/30000, Loss: 0.0007288516499102116, Learning Rate: 0.001342\n",
      "Epoch 310/30000, Loss: 0.0006573593709617853, Learning Rate: 0.001342\n",
      "Epoch 311/30000, Loss: 0.0007877418538555503, Learning Rate: 0.001341\n",
      "Epoch 312/30000, Loss: 0.0006998025346547365, Learning Rate: 0.001341\n",
      "Epoch 313/30000, Loss: 0.0006822133436799049, Learning Rate: 0.001340\n",
      "Epoch 314/30000, Loss: 0.0007682970608584583, Learning Rate: 0.001340\n",
      "Epoch 315/30000, Loss: 0.0007380002061836421, Learning Rate: 0.001339\n",
      "Epoch 316/30000, Loss: 0.0007051596185192466, Learning Rate: 0.001339\n",
      "Epoch 317/30000, Loss: 0.0006242891540750861, Learning Rate: 0.001338\n",
      "Epoch 318/30000, Loss: 0.0005924595170654356, Learning Rate: 0.001338\n",
      "Epoch 319/30000, Loss: 0.0006014679092913866, Learning Rate: 0.001337\n",
      "Epoch 320/30000, Loss: 0.0006566179799847305, Learning Rate: 0.001337\n",
      "Epoch 321/30000, Loss: 0.0006122621707618237, Learning Rate: 0.001336\n",
      "Epoch 322/30000, Loss: 0.0007016269955784082, Learning Rate: 0.001336\n",
      "Epoch 323/30000, Loss: 0.0006770158652216196, Learning Rate: 0.001335\n",
      "Epoch 324/30000, Loss: 0.0007234083022922277, Learning Rate: 0.001335\n",
      "Epoch 325/30000, Loss: 0.0006321598775684834, Learning Rate: 0.001334\n",
      "Epoch 326/30000, Loss: 0.0006710005691275001, Learning Rate: 0.001334\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      9\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     11\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m         labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/timm_new/lib/python3.9/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/timm_new/lib/python3.9/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/timm_new/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/timm_new/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/timm_new/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/timm_new/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/timm_new/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/anaconda3/envs/timm_new/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.0015)\n",
    "gamma = 0.99994  # The exponential decay factor (adjust as needed)\n",
    "scheduler = ExponentialLR(optimizer, gamma=gamma)\n",
    "#import gc\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        labels = labels[0,:,:,:,:].permute(0,2,1,3)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss1 = criterion1(outputs, labels)  # Assuming you want to compare against the input\n",
    "        loss2 = criterion2(outputs, labels)\n",
    "        loss3 = criterion3(outputs)\n",
    "        loss = (loss1*.1+loss2*2 + .000000001*loss3)/3\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Learning Rate: {current_lr:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6e933791-7d46-4389-b4de-e824e4cd2d84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5.1564e-01, 7.5727e+00, 1.9868e+00,  ..., 1.2030e+00,\n",
       "          1.9212e+00, 3.3515e+00],\n",
       "         [2.5637e+00, 4.9511e+00, 1.4656e+00,  ..., 3.0131e+00,\n",
       "          1.0168e+00, 1.2437e-01],\n",
       "         [8.9360e+00, 3.0648e+00, 2.8402e-01,  ..., 1.1690e+00,\n",
       "          5.0334e+00, 1.2699e+00],\n",
       "         ...,\n",
       "         [7.2156e-01, 1.2304e+01, 7.2504e-01,  ..., 3.0731e+00,\n",
       "          6.1458e+00, 1.2223e+00],\n",
       "         [2.5950e+00, 9.8888e+00, 4.5590e-01,  ..., 1.0342e+00,\n",
       "          3.0683e+00, 5.5829e+00],\n",
       "         [8.3886e+00, 3.4954e+00, 6.3353e-01,  ..., 5.3532e+00,\n",
       "          6.7952e-01, 2.7545e+00]],\n",
       "\n",
       "        [[7.8363e-02, 2.1410e-01, 1.9664e-01,  ..., 4.7573e-01,\n",
       "          2.1009e+00, 5.4924e-01],\n",
       "         [2.0668e+00, 1.8194e+00, 5.1937e-02,  ..., 6.8751e-02,\n",
       "          3.3102e+00, 3.2211e-01],\n",
       "         [1.0261e+00, 1.3700e+00, 1.0835e+00,  ..., 1.6150e+00,\n",
       "          2.1300e+00, 2.0785e+00],\n",
       "         ...,\n",
       "         [1.1075e+00, 1.5402e+00, 7.4922e-01,  ..., 7.0519e-01,\n",
       "          1.1942e+00, 8.1964e-01],\n",
       "         [8.5515e-01, 9.6271e-02, 4.7753e-01,  ..., 1.2279e-01,\n",
       "          1.7408e-01, 1.8140e-01],\n",
       "         [1.0442e+00, 6.9332e-01, 7.7852e-01,  ..., 5.3188e-01,\n",
       "          4.3698e-02, 3.9516e-01]],\n",
       "\n",
       "        [[1.2767e+00, 8.6758e-01, 3.4237e-01,  ..., 8.1006e-02,\n",
       "          1.1035e+00, 1.0872e+00],\n",
       "         [1.3392e+00, 9.3163e-02, 4.3261e-01,  ..., 1.5322e-02,\n",
       "          5.8499e-01, 1.1049e+00],\n",
       "         [1.3994e+00, 3.2777e+00, 8.0088e-01,  ..., 9.7223e-01,\n",
       "          1.0995e+00, 1.0127e+00],\n",
       "         ...,\n",
       "         [6.1338e-01, 4.4494e-01, 5.7567e-01,  ..., 4.0801e-01,\n",
       "          1.5370e+00, 2.8964e-01],\n",
       "         [7.2570e-02, 5.5416e-01, 8.3780e-01,  ..., 3.6690e-02,\n",
       "          1.0334e+00, 1.4686e-01],\n",
       "         [6.1921e-01, 4.3072e-01, 2.2235e-02,  ..., 6.4188e-01,\n",
       "          5.4616e-03, 8.4652e-02]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[7.0159e-02, 4.8356e-01, 1.2908e-01,  ..., 3.8572e-01,\n",
       "          1.8199e-01, 1.7785e-01],\n",
       "         [2.7746e-01, 2.7039e-01, 1.6382e-01,  ..., 2.0107e-01,\n",
       "          2.7627e-01, 1.3434e-01],\n",
       "         [1.3790e+00, 1.8390e+00, 3.6390e-01,  ..., 1.1606e+00,\n",
       "          2.1324e-01, 4.7544e-01],\n",
       "         ...,\n",
       "         [5.1063e-01, 4.5522e-02, 1.1573e-01,  ..., 1.0373e-01,\n",
       "          2.2863e-01, 2.3571e-03],\n",
       "         [2.3400e-01, 1.4003e-01, 2.2547e-01,  ..., 1.2975e-01,\n",
       "          2.3224e-02, 5.9775e-02],\n",
       "         [1.4239e-01, 8.7976e-03, 5.2845e-02,  ..., 1.7499e-01,\n",
       "          1.6634e-01, 1.2141e-01]],\n",
       "\n",
       "        [[4.2326e+00, 1.5895e-01, 6.8699e-03,  ..., 2.1975e+00,\n",
       "          1.4064e+00, 4.9477e+00],\n",
       "         [4.3944e+00, 1.3113e+00, 1.3592e+00,  ..., 3.1494e+00,\n",
       "          3.8051e+00, 7.5005e+00],\n",
       "         [3.6549e+00, 3.3347e+00, 1.8739e+00,  ..., 1.1168e+00,\n",
       "          3.4049e+00, 6.0523e+00],\n",
       "         ...,\n",
       "         [1.1183e+00, 2.6307e-02, 1.4201e+00,  ..., 1.9770e-01,\n",
       "          1.2074e+00, 2.7097e-01],\n",
       "         [8.2859e-01, 3.6117e-01, 9.0108e-01,  ..., 7.6574e-01,\n",
       "          6.4880e-02, 3.3885e-01],\n",
       "         [9.3666e-01, 8.0514e-01, 6.4426e-01,  ..., 8.6963e-01,\n",
       "          9.2896e-01, 1.0058e+00]],\n",
       "\n",
       "        [[3.3548e+00, 7.7312e-01, 5.0616e-01,  ..., 1.9400e+00,\n",
       "          1.0382e+00, 3.7769e+00],\n",
       "         [6.4389e+00, 3.0339e+00, 9.1932e-01,  ..., 3.1480e+00,\n",
       "          5.4433e+00, 8.7724e+00],\n",
       "         [4.9489e+00, 7.5151e+00, 1.4808e+00,  ..., 3.2505e+00,\n",
       "          6.2690e+00, 8.3246e+00],\n",
       "         ...,\n",
       "         [6.6335e-01, 8.7948e-01, 2.0929e+00,  ..., 1.2624e-02,\n",
       "          1.9590e+00, 5.7730e-01],\n",
       "         [1.2564e-01, 8.9405e-02, 2.6354e-01,  ..., 8.8011e-01,\n",
       "          1.1997e+00, 6.8789e-02],\n",
       "         [2.3824e-01, 1.8534e+00, 1.9870e-01,  ..., 6.7659e-01,\n",
       "          9.5488e-01, 1.2768e+00]]], device='cuda:0', grad_fn=<AbsBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = DivLoss()#nn.MSELoss()\n",
    "\n",
    "criterion(outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8945577d-13a2-48c0-80dc-e9c90410d62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.7364e+02, -2.1648e+02, -1.4755e+02,  ..., -2.7043e-02,\n",
       "         -8.6548e-03, -2.8033e-03],\n",
       "        [-1.0891e+02, -1.2843e+02, -1.1331e+02,  ..., -2.6677e-02,\n",
       "         -8.5409e-03, -2.7707e-03],\n",
       "        [-4.0432e+01, -7.3445e+01, -7.9750e+01,  ..., -2.5797e-02,\n",
       "         -8.2954e-03, -2.7144e-03],\n",
       "        ...,\n",
       "        [ 7.5987e+00,  1.3356e+01,  1.4123e+01,  ..., -1.0619e-01,\n",
       "         -1.0559e-01, -6.2215e-02],\n",
       "        [ 2.0840e+01,  2.3782e+01,  2.0323e+01,  ..., -1.6030e-01,\n",
       "         -1.9801e-01, -1.7920e-01],\n",
       "        [ 7.2317e+01,  4.0696e+01,  2.6776e+01,  ..., -2.1897e-01,\n",
       "         -3.5306e-01, -6.4454e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[10,:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1732d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30000\n",
    "\n",
    "dataset = CustomDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "# Model, loss, and optimizer\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0012)\n",
    "gamma = 0.99995  # The exponential decay factor (adjust as needed)\n",
    "scheduler = ExponentialLR(optimizer, gamma=gamma)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0].to(device)\n",
    "        labels = batch[1].to(device)\n",
    "        labels = labels[0,:,:,:,:].permute(0,2,1,3)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)  # Assuming you want to compare against the input\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    # Print the current learning rate    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Learning Rate: {current_lr:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "566b588c-fe89-4077-8e13-5698c15170b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 group \"/Coordinates\" (3 members)>\n",
      "<HDF5 group \"/Provenance\" (2 members)>\n",
      "<HDF5 group \"/Time:  0.00000E+00 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.00800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.05600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.10400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.15200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.20000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.24800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.29600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.34400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.39200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.44000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.44000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.48800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.53600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.58400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.63200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.68000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.72800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.77600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.82400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.87200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.92000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.92000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  1.96800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.01600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.06400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.11200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.16000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.20800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.25600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.30400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.35200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.40000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.40000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.44800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.49600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.54400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.59200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.64000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.68800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.73600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.78400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.83200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.88000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.88000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.92800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  2.97600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.02400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.07200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.12000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.16800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.21600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.26400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.31200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.36000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.36000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.40800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.45600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.50400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.55200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.60000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.64800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.69600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.74400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.79200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.84000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.84000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.88800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.93600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  3.98400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.03200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.08000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.12800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.17600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.22400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.27200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.32000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.32000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.36800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.41600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.46400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.51200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.56000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.60800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.65600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.70400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.75200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.80000E+01 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.80000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.80000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.84800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.89600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.94400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  4.99200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  5.04000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  5.08800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  5.13600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  5.18400E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  5.23200E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  5.28000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  5.28000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  5.32800E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  5.37600E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  5.40000E+03 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  5.76000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  6.24000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  6.72000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  7.20000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  7.68000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  8.16000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  8.64000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  9.12000E+02 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  9.60000E+01 h\" (19 members)>\n",
      "<HDF5 group \"/Time:  9.60000E+02 h\" (19 members)>\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(h5_files[0], 'r') as file:\n",
    "    for name in file:\n",
    "        #if isinstance(file[name], h5py.Group):\n",
    "        print(file[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c4b54a8-6de0-40b3-aa53-e37d6906b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_weights4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68e3da05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005832184 0.0033241045\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 14400x14400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=6\n",
    "out = outputs5[100,:,:,n].permute(1,0).cpu().detach().numpy()\n",
    "out[out<=0] = np.min(out[out>0])\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "fig = plt.figure(figsize=(24,24), dpi=600)\n",
    "plt.imshow(out)#, norm=LogNorm(.0001,.5))\n",
    "#plt.imshow(inputs[0,100,:,:,n].cpu().detach().numpy())\n",
    "#fig.savefig('label.png')  # Replace 'output.png' with your desired file path and format\n",
    "\n",
    "field_names[n]\n",
    "#plt.colorbar()\n",
    "\n",
    "orig_error = np.abs(np.mean(inputs[0,:,:,:,n].cpu().detach().numpy()) - np.mean(labels[:,:,:,n].cpu().detach().numpy()))\n",
    "\n",
    "nn_error = np.abs(np.mean(outputs5[:,:,:,n].cpu().detach().numpy()) - np.mean(labels[:,:,:,n].cpu().detach().numpy()))\n",
    "print (orig_error, nn_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "125c8549-3e43-406e-adc7-b74e09bceed1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14821875"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3764a402-55f0-4dd4-b0da-33e94aa128b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of time steps and features to extract\n",
    "#file_path = '/mnt/data/high_losing_42_11_1xscale.h5'\n",
    "import h5py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "base_dir = '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/'\n",
    "f_dir = os.listdir(base_dir)\n",
    "h5_files = []\n",
    "h5_files_train = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in f_dir:\n",
    "    if (filename.endswith(\".h5\")) & (((\"2x\" in filename))):\n",
    "        h5_files_train.append(base_dir+filename)\n",
    "#     if (filename.endswith(\".h5\")) & ((\"8x\" in filename)):\n",
    "#         if ((\"10_5\" not in filename) & (\"20_20\" not in filename)):\n",
    "#             h5_files_train.append(base_dir+filename)\n",
    "    if (filename.endswith(\".h5\")) & (((\"10x\" in filename))):\n",
    "#         if ((\"10_5\" in filename) | (\"20_20\" in filename)):\n",
    "        h5_files.append(base_dir+filename)\n",
    "# Run the processing in parallel\n",
    "h5_files_train =  sorted(\n",
    "    h5_files_train,\n",
    "    key=lambda x: tuple(map(int, x.split('high_losing_')[1].split('_')[:2]))\n",
    ")\n",
    "h5_files =  sorted(\n",
    "    h5_files,\n",
    "    key=lambda x: tuple(map(int, x.split('high_losing_')[1].split('_')[:2]))\n",
    ")\n",
    "\n",
    "h5_files_test = h5_files[20:21]\n",
    "h5_files_train_test = h5_files_train[20:21]\n",
    "hdf5_file = h5py.File(h5_files_test[0], 'r')\n",
    "# Fetching the first five time steps\n",
    "time_steps = sorted(list(hdf5_file.keys()))\n",
    "\n",
    "# Inspecting the features present in each of these time steps\n",
    "features_per_time_step = {time_step: list(hdf5_file[time_step].keys()) for time_step in time_steps}\n",
    "\n",
    "\n",
    "num_time_steps = 115\n",
    "num_features = 9  # As observed from the dataset\n",
    "\n",
    "# Initialize the final array with the desired shape [5, 100, 200, 19]\n",
    "y_test = np.zeros((len(h5_files_test),num_time_steps-1, 100, 2000, num_features))\n",
    "\n",
    "# Extracting and reshaping data from the first five time steps\n",
    "for i in range(len(h5_files_test)):\n",
    "    hdf5_file = h5py.File(h5_files_test[i], 'r')\n",
    "    for t_idx, time_step in enumerate(time_steps[2:]):  # Skipping the first two non-time-step groups\n",
    "        count=0\n",
    "        for f_idx, feature in enumerate(features_per_time_step[time_step]):\n",
    "            if ('O2' not in feature)& ('Perm' not in feature)& ('Material' not in feature)& ('Sat' not in feature)& ('Z' not in feature)& ('biocide' not in feature)& ('ethanol' not in feature)& ('Chubbite' not in feature):\n",
    "                dataset = hdf5_file[time_step][feature]\n",
    "                y_test[i, t_idx, :, :, count] = dataset[:, :, 0]#cv2.resize(dataset[:, :, 0], [500,50])  # Reshape and assign\n",
    "                count = count+1\n",
    "\n",
    "# Checking the shape of the extracted data array\n",
    "\n",
    "\n",
    "x_test = np.zeros((len(h5_files_train_test),num_time_steps-1, 100, 400, num_features))\n",
    "\n",
    "# Extracting and reshaping data from the first five time steps\n",
    "for i in range(len(h5_files_train_test)):\n",
    "    hdf5_file = h5py.File(h5_files_train_test[i], 'r')\n",
    "    for t_idx, time_step in enumerate(time_steps[2:]):  # Skipping the first two non-time-step groups\n",
    "        count=0\n",
    "        for f_idx, feature in enumerate(features_per_time_step[time_step]):\n",
    "            if ('O2' not in feature)& ('Perm' not in feature)& ('Material' not in feature)& ('Sat' not in feature)& ('Z' not in feature)& ('biocide' not in feature)& ('ethanol' not in feature)& ('Chubbite' not in feature):\n",
    "                dataset = hdf5_file[time_step][feature]\n",
    "                x_test[i, t_idx, :, :, count] = dataset[:, :, 0]#cv2.resize(dataset[:, :, 0], [500,50])  # Reshape and assign\n",
    "                count = count+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c5e27b9-4a99-43d6-86d5-d03b7d6466f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 114, 100, 400, 9)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#torch.save(model, 'model')\n",
    "#torch.save(model.state_dict(), 'model_weights2')\n",
    "x_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86fdd74d-fd1c-41e7-8c13-810ed05873b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet(n_channels=100, n_classes=9).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49a93550-7a32-4f83-a4ed-a1c774beef16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (up): DoubleConv_up(\n",
       "    (double_conv): Sequential(\n",
       "      (0): Conv2d(400, 600, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(600, 800, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): Conv2d(800, 1200, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (outc): Conv2d(1200, 2000, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_weights_path = 'model_weights4'\n",
    "\n",
    "# Load the model's state dictionary (weights)\n",
    "model.load_state_dict(torch.load(model_weights_path))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "750360eb-0432-492b-aa43-a3d96dcf1eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-3.8411125549317013, 597.4671440974266],\n",
       " [-615.1042997637863, 357.84820748247256],\n",
       " [0.0, 6822.169871757096],\n",
       " [0.0001, 0.6],\n",
       " [7.788525219135888, 15.024872883573318],\n",
       " [9.998195511764697e-21, 0.0021615209557147423],\n",
       " [9.992418077905442e-21, 0.00019222389277486797],\n",
       " [1e-10, 353.88041919640534],\n",
       " [9.999999999999992e-21, 4.945182939521276]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales=[[-3.8411125549317013, 597.4671440974266],\n",
    " [-615.1042997637863, 357.84820748247256],\n",
    " [0.0, 6822.169871757096],\n",
    " [0.0001, 0.6],\n",
    " [7.788525219135888, 15.024872883573318],\n",
    " [9.998195511764697e-21, 0.0021615209557147423],\n",
    " [9.992418077905442e-21, 0.00019222389277486797],\n",
    " [1e-10, 353.88041919640534],\n",
    " [9.999999999999992e-21, 4.945182939521276]]\n",
    "scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12943496-d729-40a0-87c0-a5634e1e4a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01872465, 0.05613483, 0.08445839, ..., 0.24322912, 0.19144413,\n",
       "       0.12252723])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(x_test.shape[4]):\n",
    "    xmin = scales[i][0]\n",
    "    xmax = scales[i][1]\n",
    "    x_test[:,:,:,:,i] = (x_test[:,:,:,:,i] - xmin)/(xmax-xmin)\n",
    "    y_test[:,:,:,:,i] = (y_test[:,:,:,:,i] - xmin)/(xmax-xmin)\n",
    "    #scales.append([xmin,xmax])\n",
    "y_test[0,100,0,:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c9220c21-7d50-43b0-a34b-3d06aa612ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0132, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_dataset = CustomDataset(x_test, y_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # Adjust batch size as needed\n",
    "\n",
    "# Define a loss function if needed (e.g., for validation)\n",
    "criterion = nn.MSELoss()  # Replace with your loss function if needed\n",
    "\n",
    "# Lists to store predictions and true labels\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "model.eval()\n",
    "# Iterate through the test data\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        labels = labels[0,:,:,:,:].permute(0,2,1,3)\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        # If you have a loss function, compute the loss (optional)\n",
    "        loss = criterion(outputs, labels)\n",
    "        print (loss)\n",
    "\n",
    "\n",
    "# Calculate accuracy (or other evaluation metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dcaca843-fb8e-4007-91dc-d2f2942997d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Liquid X-Velocity [m_per_h]'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 14400x14400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n=0\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "fig = plt.figure(figsize=(24,24), dpi=600)\n",
    "val = outputs[100,:,:,n].permute(1,0).cpu().detach().numpy()\n",
    "plt.imshow(val)\n",
    "#plt.imshow(inputs[0,100,:,:,n].cpu().detach().numpy())\n",
    "#fig.savefig('label_test.png')  # Replace 'output.png' with your desired file path and format\n",
    "\n",
    "field_names[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d35765ab-763a-4594-bfe5-b89674ace470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002691159 0.00015786244\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "\n",
    "orig_error = np.abs(np.mean(inputs[0,:,:,:,n].cpu().detach().numpy()) - np.mean(labels[:,:,:,n].cpu().detach().numpy()))\n",
    "\n",
    "nn_error = np.abs(np.mean(outputs[:,:,:,n].cpu().detach().numpy()) - np.mean(labels[:,:,:,n].cpu().detach().numpy()))\n",
    "print (orig_error, nn_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "adcecaa7-2c8a-48fd-b5ac-5183a4b22f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00018337276 0.00024984404\n"
     ]
    }
   ],
   "source": [
    "print (orig_error, nn_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd11806-c224-4c51-8d07-7f886316ecaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-12.696747371716862, 674.760452763084],\n",
       " [-674.7761220066308, 357.84820748247256],\n",
       " [0.0, 6822.169871757096],\n",
       " [0.0001, 0.6],\n",
       " [5.788525614365926, 15.024872883573318],\n",
       " [9.998195511764697e-21, 0.0021615209557147423],\n",
       " [9.992418077905442e-21, 0.0018179349917678152],\n",
       " [1e-10, 353.88041919640534],\n",
       " [9.999999999999992e-21, 8.902117136738323]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales=[[-3.8411125549317013, 597.4671440974266],\n",
    " [-615.1042997637863, 357.84820748247256],\n",
    " [0.0, 6822.169871757096],\n",
    " [0.0001, 0.6],\n",
    " [7.788525219135888, 15.024872883573318],\n",
    " [9.998195511764697e-21, 0.0021615209557147423],\n",
    " [9.992418077905442e-21, 0.00019222389277486797],\n",
    " [1e-10, 353.88041919640534],\n",
    " [9.999999999999992e-21, 4.945182939521276]]\n",
    "scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2477536d-4a4b-4b9c-8026-13a51e24db25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_divergence(vx, vy, dx, dy):\n",
    "    \"\"\"\n",
    "    Calculate the divergence of a 2D velocity field using a forward-looking finite difference scheme.\n",
    "    \n",
    "    Parameters:\n",
    "    vx: 2D array of velocity in the x-direction\n",
    "    vy: 2D array of velocity in the y-direction\n",
    "    dx: Grid spacing in the x-direction\n",
    "    dy: Grid spacing in the y-direction\n",
    "    \n",
    "    Returns:\n",
    "    divergence: 2D array of divergence values\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the velocity field\n",
    "    rows, cols = vx.shape\n",
    "\n",
    "    # Initialize an array to store divergence\n",
    "    divergence = np.zeros((rows, cols))\n",
    "\n",
    "    # Calculate divergence using forward differences\n",
    "    # For the edges, we use backward differences to avoid index out of range\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            dvx_dx = (vx[i, min(j+1, cols-1)] - vx[i, j]) / dx\n",
    "            dvy_dy = (vy[min(i+1, rows-1), j] - vy[i, j]) / dy\n",
    "            divergence[i, j] = dvx_dx + dvy_dy\n",
    "\n",
    "    return divergence\n",
    "\n",
    "# Example usage with dummy data\n",
    "# Create sample velocity fields\n",
    "#vx = outputs[100,:,:,0].permute(1,0).cpu().detach().numpy()\n",
    "\n",
    "#vy = outputs[100,:,:,1].permute(1,0).cpu().detach().numpy()\n",
    "\n",
    "\n",
    "# Assume uniform grid spacing\n",
    "#dx = 1  # x-spacing\n",
    "#dy = 1  # y-spacing\n",
    "\n",
    "# Calculate divergence\n",
    "#divergence = calculate_divergence(vx, vy, dx, dy)\n",
    "#divergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6cfd5be6-4c7b-4efd-866b-c06e264aeedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse=nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9788c121-a804-4c7a-a5c4-81d0651cc7c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/timm_new/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/timm_new/lib/python3.9/site-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/timm_new/lib/python3.9/site-packages/torch/nn/functional.py:3284\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[1;32m   3281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   3282\u001b[0m         mse_loss, (\u001b[38;5;28minput\u001b[39m, target), \u001b[38;5;28minput\u001b[39m, target, size_average\u001b[38;5;241m=\u001b[39msize_average, reduce\u001b[38;5;241m=\u001b[39mreduce, reduction\u001b[38;5;241m=\u001b[39mreduction\n\u001b[1;32m   3283\u001b[0m     )\n\u001b[0;32m-> 3284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[1;32m   3285\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   3286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3288\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()),\n\u001b[1;32m   3289\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m   3290\u001b[0m     )\n\u001b[1;32m   3291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "mse(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4c6ff-c92f-425b-9475-a19584ebf898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eec4483-8b74-4a81-8318-128b988da8c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fcf92479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales_train=[]\n",
    "for i in h5_files_train:\n",
    "    if 'high_gaining' in i:\n",
    "        scales_train.append(i.split('_')[7][0])\n",
    "    else:\n",
    "        scales_train.append(i.split('_')[6][0])\n",
    "    \n",
    "scales=[]\n",
    "for i in h5_files:\n",
    "    if 'high_gaining' in i:\n",
    "        scales.append(i.split('_')[7][0])\n",
    "    else:\n",
    "        scales.append(i.split('_')[6][0])\n",
    "scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd255d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4,\n",
       " 0.7727272727272727,\n",
       " 1.8,\n",
       " 0.9473684210526315,\n",
       " 1.375,\n",
       " 1.411764705882353,\n",
       " 1.6666666666666667,\n",
       " 1.625,\n",
       " 3.857142857142857,\n",
       " 3.375,\n",
       " 1.9285714285714286,\n",
       " 3.2222222222222223,\n",
       " 2.0,\n",
       " 4.428571428571429,\n",
       " 5.666666666666667,\n",
       " 2.2666666666666666,\n",
       " 1.619047619047619,\n",
       " 5.0,\n",
       " 1.5217391304347827,\n",
       " 3.272727272727273,\n",
       " 2.0,\n",
       " 4.111111111111111,\n",
       " 2.375,\n",
       " 3.9,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 2.3529411764705883,\n",
       " 2.0,\n",
       " 3.4166666666666665,\n",
       " 2.411764705882353,\n",
       " 2.05,\n",
       " 3.8181818181818183,\n",
       " 5.0,\n",
       " 2.3684210526315788,\n",
       " 5.111111111111111,\n",
       " 4.6,\n",
       " 2.7058823529411766,\n",
       " 5.222222222222222,\n",
       " 4.9,\n",
       " 2.5789473684210527,\n",
       " 2.0416666666666665,\n",
       " 5.0,\n",
       " 5.1,\n",
       " 6.5,\n",
       " 5.888888888888889,\n",
       " 7.125]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "import cv2\n",
    "\n",
    "base_dir = '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/'\n",
    "f_dir = os.listdir(base_dir)\n",
    "h5_files = []\n",
    "h5_files_train = []\n",
    "\n",
    "# Iterate over all files in the directory\n",
    "for filename in f_dir:\n",
    "    if (filename.endswith(\".h5\")) & (((\"2x\" in filename))):\n",
    "        h5_files_train.append(base_dir+filename)\n",
    "#     if (filename.endswith(\".h5\")) & ((\"8x\" in filename)):\n",
    "#         if ((\"10_5\" not in filename) & (\"20_20\" not in filename)):\n",
    "#             h5_files_train.append(base_dir+filename)\n",
    "    if (filename.endswith(\".h5\")) & (((\"10x\" in filename))):\n",
    "#         if ((\"10_5\" in filename) | (\"20_20\" in filename)):\n",
    "        h5_files.append(base_dir+filename)\n",
    "# Run the processing in parallel\n",
    "h5_files_train =  sorted(\n",
    "    h5_files_train,\n",
    "    key=lambda x: tuple(map(int, x.split('high_losing_')[1].split('_')[:2]))\n",
    ")\n",
    "h5_files =  sorted(\n",
    "    h5_files,\n",
    "    key=lambda x: tuple(map(int, x.split('high_losing_')[1].split('_')[:2]))\n",
    ")\n",
    "\n",
    "perm_covar=[]\n",
    "for i in h5_files_train:\n",
    "    if 'high_gaining' in i:\n",
    "        perm_covar.append(int(i.split('_')[5])/int(i.split('_')[6]))\n",
    "    else:\n",
    "        perm_covar.append(int(i.split('_')[4])/int(i.split('_')[5]))\n",
    "perm_covar    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3b37bd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14,\n",
       " 22,\n",
       " 18,\n",
       " 19,\n",
       " 22,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 27,\n",
       " 27,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 34,\n",
       " 34,\n",
       " 34,\n",
       " 35,\n",
       " 35,\n",
       " 36,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 39,\n",
       " 40,\n",
       " 40,\n",
       " 40,\n",
       " 41,\n",
       " 41,\n",
       " 41,\n",
       " 42,\n",
       " 45,\n",
       " 45,\n",
       " 46,\n",
       " 46,\n",
       " 46,\n",
       " 47,\n",
       " 49,\n",
       " 49,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 57]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_l=[]\n",
    "for i in h5_files_train:\n",
    "    if 'high_gaining' in i:\n",
    "        max_l.append(np.max([int(i.split('_')[5]),int(i.split('_')[6])]))\n",
    "    else:\n",
    "        max_l.append(np.max([int(i.split('_')[4]),int(i.split('_')[5])]))\n",
    "max_l  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "382bf516",
   "metadata": {},
   "outputs": [],
   "source": [
    "growth_rates=[]\n",
    "rate_c=[]\n",
    "rate_d=[]\n",
    "exp_b = []\n",
    "for i in range(len(h5_files)):\n",
    "    file_path = h5_files[i][:-3]+'.in'\n",
    "    #print (file_path)\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    gr = float(lines[79].split()[1].replace('d', 'e'))\n",
    "    c = float(lines[81].split()[1].replace('d', 'e'))\n",
    "    d = float(lines[94].split()[1].replace('d', 'e'))\n",
    "    b = float(lines[65].split()[1])\n",
    "    growth_rates.append(gr)\n",
    "    rate_c.append(c)\n",
    "    rate_d.append(d)\n",
    "    exp_b.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e312a7-ad24-40ff-879c-10cbf9baf7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tags =[rate_c, rate_d, exp_b, max_l, perm_covar, growth_rates]\n",
    "count=0\n",
    "tag_scales = []\n",
    "for i in tags:\n",
    "    xmax = np.max(i)\n",
    "    xmin = np.min(i)\n",
    "    tags[count] = (i-xmin)/(xmax-xmin)\n",
    "    tag_scales.append([xmin, xmax])\n",
    "    count = count+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292377e7-5311-4cc0-acba-137048d9ddd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_small = np.zeros([6,114,100,400,1)\n",
    "for i in range(len(h5_files)):\n",
    "    tags_small[i,:,0:16, :, :] = torch.full((1,114,16,400,1),tags[0][i])\n",
    "    tags_small[i,:,16:32, :, :] = torch.full((1,114,16,400,1),tags[1][i])\n",
    "    tags_small[i,:,32:48, :, :] = torch.full((1,114,16,400,1),tags[2][i])\n",
    "    tags_small[i,:,48:64, :, :] = torch.full((1,114,16,400,1),tags[3][i])\n",
    "    tags_small[i,:,64:80, :, :] = torch.full((1,114,16,400,1),tags[4][i])\n",
    "    tags_small[i,:,80:, :, :] = torch.full((1,114,16,400,1),tags[5][i])\n",
    "\n",
    "tags_big = torch.copy(tags_small)\n",
    "#torch.concat(x,tags_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7a38a17-e1da-470e-a956-26f119a3d311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 110\n",
      "1 110\n",
      "2 110\n",
      "3 110\n",
      "4 110\n",
      "5 110\n",
      "6 110\n",
      "7 110\n",
      "8 110\n",
      "9 110\n",
      "10 110\n",
      "11 110\n",
      "12 110\n",
      "13 110\n",
      "14 110\n",
      "15 110\n",
      "16 110\n",
      "17 110\n",
      "18 110\n",
      "19 110\n",
      "20 110\n",
      "21 110\n",
      "22 110\n",
      "23 110\n",
      "24 110\n",
      "25 110\n",
      "26 110\n",
      "27 110\n",
      "28 110\n",
      "29 110\n",
      "30 110\n",
      "31 110\n",
      "32 110\n",
      "33 110\n",
      "34 110\n",
      "35 110\n",
      "36 110\n",
      "37 110\n",
      "38 110\n",
      "39 110\n",
      "40 110\n",
      "41 110\n",
      "42 110\n",
      "43 110\n",
      "44 110\n",
      "45 110\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dfs)):\n",
    "    print (i,len(dfs_train[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74451a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_14_10_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_17_22_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_18_10_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_18_19_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_22_16_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_24_17_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_25_15_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_26_16_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_27_7_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_27_8_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_27_14_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_29_9_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_30_15_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_31_7_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_34_6_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_34_15_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_34_21_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_35_7_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_35_23_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_36_11_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_36_18_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_37_9_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_38_16_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_39_10_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_39_13_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_40_8_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_40_17_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_40_20_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_41_12_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_41_17_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_41_20_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_42_11_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_45_9_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_45_19_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_46_9_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_46_10_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_46_17_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_47_9_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_49_10_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_49_19_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_49_24_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_50_10_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_51_10_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_52_8_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_53_9_10xscale.h5',\n",
       " '/home/mbadhan/Desktop/mberghouse/PFLOTRAN/one_deeper/input_files/high_losing_57_8_10xscale.h5']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50ca002d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5060, 20) (5060, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for i in range(len(dfs)):\n",
    "    #rate b_1 (growth rate)\n",
    "    #rate_c\n",
    "    #exp_b\n",
    "    #rate d_1 (carbon reuse)\n",
    "    dfs[i]['sim_id']=i\n",
    "    #dfs[i]['scale']=scales[i]\n",
    "   # if 'gaining' in h5_files[i]:\n",
    "   #     dfs[i]['type_id']=0\n",
    "   # if 'high_gaining' in h5_files[i]:\n",
    "   #     dfs[i]['type_id']=1\n",
    "   # if 'losing' in h5_files[i]:\n",
    "   #     dfs[i]['type_id']=2\n",
    "   # if 'highlosing' in h5_files[i]:\n",
    "   #     dfs[i]['type_id']=3\n",
    "    dfs[i]['perm_covar']=perm_covar[i]\n",
    "    dfs[i]['max_l']=max_l[i]\n",
    "    perm_values = dfs[i][\"Permeability_X [m^2]\"].values\n",
    "    if perm_covar[i]>1:\n",
    "        k_eff =np.mean(perm_values.size / np.sum(1.0 / perm_values))\n",
    "    else:\n",
    "        k_eff = np.mean(perm_values)\n",
    "\n",
    "    #dfs[i]['k_eff']=k_eff\n",
    "    dfs[i]['growth_rates']=growth_rates[i]\n",
    "    dfs[i]['rate_c']=rate_c[i]\n",
    "    dfs[i]['rate_d']=rate_d[i]\n",
    "    dfs[i]['exp_b']=exp_b[i]\n",
    "\n",
    "for i in range(len(dfs_train)):\n",
    "    dfs_train[i]['sim_id']=i\n",
    "    #dfs_train[i]['scale']=scales_train[i]\n",
    "    #if 'gaining' in h5_files[i]:\n",
    "    #    dfs_train[i]['type_id']=0\n",
    "    #if 'high_gaining' in h5_files[i]:\n",
    "        #print (i[1].fname)\n",
    "    #    dfs_train[i]['type_id']=1\n",
    "    #if 'losing' in h5_files[i]:\n",
    "    #    dfs_train[i]['type_id']=2\n",
    "    #if 'highlosing' in h5_files[i]:\n",
    "    #    dfs_train[i]['type_id']=3\n",
    "#     if i >= 33:\n",
    "#         dfs[i]['sim_id']=4\n",
    "#     dfs[i]['sim_id2']=i\n",
    "    \n",
    "    dfs_train[i]['perm_covar']=perm_covar[i]\n",
    "#     dfs[i]['gaining']=gaining[i]\n",
    "#     dfs[i]['losing']=losing[i]\n",
    "    dfs_train[i]['max_l']=max_l[i]\n",
    "    #dfs[i]['NL']=NL[i]\n",
    "    perm_values = dfs_train[i][\"Permeability_X [m^2]\"].values\n",
    "    #if perm_covar[i]>1:\n",
    "    #    k_eff =np.mean(perm_values.size / np.sum(1.0 / perm_values))\n",
    "    #else:\n",
    "    #    k_eff = np.mean(perm_values)\n",
    "\n",
    "    #dfs_train[i]['k_eff']=k_eff\n",
    "    #dfs_train[i]['k_eff']=k_eff\n",
    "    dfs_train[i]['growth_rates']=growth_rates[i]\n",
    "    dfs_train[i]['rate_c']=rate_c[i]\n",
    "    dfs_train[i]['rate_d']=rate_d[i]\n",
    "    dfs_train[i]['exp_b']=exp_b[i]\n",
    "    \n",
    "\n",
    "combined_df = pd.concat(dfs)\n",
    "combined_df_train = pd.concat(dfs_train)\n",
    "combined_df\n",
    "final_data = np.array(combined_df)\n",
    "final_data_train = np.array(combined_df_train)\n",
    "x = np.delete(final_data_train, [1,2,5,7,8,15,16], 1)\n",
    "target = np.delete(final_data, [1,2,5,7,8,15,16], 1)\n",
    " \n",
    "#x_test = np.delete(final_data_train, [1,2,5,7,8,14,15,16,19], 1)\n",
    "# #target = np.delete(final_data, [1,2,5,7,8,14,15,16,19], 1)\n",
    "# #x = np.delete(final_data, [3, 5, 6, 11, 12, 13, 16,17,18,19], 1)\n",
    "# y_train = final_data_train[:, 13]\n",
    "# y_test = final_data[:, 13]\n",
    "print (x.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "93ddd0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Permeability_X [m^2]     2.445674e-10\n",
       "Porosity                 1.584149e-01\n",
       "Temperature [C]          8.961414e+00\n",
       "Total_CO2 [M]            9.948697e-05\n",
       "Total_Cr(VI) [M]         3.894979e-06\n",
       "Total_O2 [M]             4.999970e+02\n",
       "Total_biocide [M]        9.999972e-11\n",
       "Total_ethanol [M]        9.999972e-11\n",
       "Total_molasses [M]       2.270490e-06\n",
       "biomass [mol_m^3]        9.441274e-07\n",
       "molasses_im [mol_m^3]    5.220698e-02\n",
       "sim_id                   0.000000e+00\n",
       "perm_covar               1.400000e+00\n",
       "max_l                    1.400000e+01\n",
       "growth_rates             1.363760e-05\n",
       "rate_c                   3.170410e-09\n",
       "rate_d                   1.000000e-01\n",
       "exp_b                    2.659390e+00\n",
       "Name: 32, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_train[0].iloc[1,9:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8cc6f60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Chubbite_Rate [mol_m^3_sec]</th>\n",
       "      <th>Chubbite_VF [m^3 mnrl_m^3 bulk]</th>\n",
       "      <th>Liquid X-Velocity [m_per_h]</th>\n",
       "      <th>Liquid Y-Velocity [m_per_h]</th>\n",
       "      <th>Liquid Z-Velocity [m_per_h]</th>\n",
       "      <th>Liquid_Pressure [Pa]</th>\n",
       "      <th>Liquid_Saturation</th>\n",
       "      <th>Material_ID</th>\n",
       "      <th>Permeability_X [m^2]</th>\n",
       "      <th>...</th>\n",
       "      <th>Total_molasses [M]</th>\n",
       "      <th>biomass [mol_m^3]</th>\n",
       "      <th>molasses_im [mol_m^3]</th>\n",
       "      <th>sim_id</th>\n",
       "      <th>perm_covar</th>\n",
       "      <th>max_l</th>\n",
       "      <th>growth_rates</th>\n",
       "      <th>rate_c</th>\n",
       "      <th>rate_d</th>\n",
       "      <th>exp_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.841585</td>\n",
       "      <td>0.877140</td>\n",
       "      <td>-0.010643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1552.525408</td>\n",
       "      <td>0.985904</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.445674e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.223517e-06</td>\n",
       "      <td>2.343395e-07</td>\n",
       "      <td>0.051616</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3.170410e-09</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.65939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>240.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.841585</td>\n",
       "      <td>0.709446</td>\n",
       "      <td>-0.010747</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1261.106711</td>\n",
       "      <td>0.985849</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.445674e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.270490e-06</td>\n",
       "      <td>9.441274e-07</td>\n",
       "      <td>0.052207</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3.170410e-09</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.65939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>288.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.841585</td>\n",
       "      <td>0.877356</td>\n",
       "      <td>-0.010624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1556.111624</td>\n",
       "      <td>0.985905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.445674e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.240250e-06</td>\n",
       "      <td>3.781464e-06</td>\n",
       "      <td>0.051820</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3.170410e-09</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.65939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>336.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.841585</td>\n",
       "      <td>1.036851</td>\n",
       "      <td>-0.010508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1836.479850</td>\n",
       "      <td>0.985958</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.445674e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.217294e-06</td>\n",
       "      <td>1.500408e-05</td>\n",
       "      <td>0.051546</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3.170410e-09</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.65939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>384.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.841585</td>\n",
       "      <td>1.154984</td>\n",
       "      <td>-0.010418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2044.741518</td>\n",
       "      <td>0.985997</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.445674e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>2.208773e-06</td>\n",
       "      <td>5.906743e-05</td>\n",
       "      <td>0.051442</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3.170410e-09</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.65939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5232.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.847638</td>\n",
       "      <td>1.190599</td>\n",
       "      <td>-0.013500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2186.936281</td>\n",
       "      <td>0.986024</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.352886e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>5.506438e-07</td>\n",
       "      <td>6.144571e+01</td>\n",
       "      <td>0.014858</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3.170410e-09</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.65939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>5280.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.847714</td>\n",
       "      <td>1.212166</td>\n",
       "      <td>-0.013330</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2229.067466</td>\n",
       "      <td>0.986032</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.351712e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>5.562375e-07</td>\n",
       "      <td>6.223035e+01</td>\n",
       "      <td>0.015085</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3.170410e-09</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.65939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5328.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.847790</td>\n",
       "      <td>1.182790</td>\n",
       "      <td>-0.013011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2178.777312</td>\n",
       "      <td>0.986022</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.350547e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>5.539103e-07</td>\n",
       "      <td>6.299632e+01</td>\n",
       "      <td>0.015025</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3.170410e-09</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.65939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>5376.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.847862</td>\n",
       "      <td>1.174133</td>\n",
       "      <td>-0.012771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2165.943470</td>\n",
       "      <td>0.986020</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.349423e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>5.538787e-07</td>\n",
       "      <td>6.373198e+01</td>\n",
       "      <td>0.015017</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3.170410e-09</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.65939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>5400.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.847899</td>\n",
       "      <td>1.172252</td>\n",
       "      <td>-0.012657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2163.962184</td>\n",
       "      <td>0.986019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.348863e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>5.546674e-07</td>\n",
       "      <td>6.409972e+01</td>\n",
       "      <td>0.015055</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>3.170410e-09</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.65939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Time  Chubbite_Rate [mol_m^3_sec]  Chubbite_VF [m^3 mnrl_m^3 bulk]  \\\n",
       "21    192.0                          0.0                         0.841585   \n",
       "32    240.0                          0.0                         0.841585   \n",
       "43    288.0                          0.0                         0.841585   \n",
       "54    336.0                          0.0                         0.841585   \n",
       "65    384.0                          0.0                         0.841585   \n",
       "..      ...                          ...                              ...   \n",
       "98   5232.0                          0.0                         0.847638   \n",
       "100  5280.0                          0.0                         0.847714   \n",
       "101  5328.0                          0.0                         0.847790   \n",
       "102  5376.0                          0.0                         0.847862   \n",
       "103  5400.0                          0.0                         0.847899   \n",
       "\n",
       "     Liquid X-Velocity [m_per_h]  Liquid Y-Velocity [m_per_h]  \\\n",
       "21                      0.877140                    -0.010643   \n",
       "32                      0.709446                    -0.010747   \n",
       "43                      0.877356                    -0.010624   \n",
       "54                      1.036851                    -0.010508   \n",
       "65                      1.154984                    -0.010418   \n",
       "..                           ...                          ...   \n",
       "98                      1.190599                    -0.013500   \n",
       "100                     1.212166                    -0.013330   \n",
       "101                     1.182790                    -0.013011   \n",
       "102                     1.174133                    -0.012771   \n",
       "103                     1.172252                    -0.012657   \n",
       "\n",
       "     Liquid Z-Velocity [m_per_h]  Liquid_Pressure [Pa]  Liquid_Saturation  \\\n",
       "21                           0.0           1552.525408           0.985904   \n",
       "32                           0.0           1261.106711           0.985849   \n",
       "43                           0.0           1556.111624           0.985905   \n",
       "54                           0.0           1836.479850           0.985958   \n",
       "65                           0.0           2044.741518           0.985997   \n",
       "..                           ...                   ...                ...   \n",
       "98                           0.0           2186.936281           0.986024   \n",
       "100                          0.0           2229.067466           0.986032   \n",
       "101                          0.0           2178.777312           0.986022   \n",
       "102                          0.0           2165.943470           0.986020   \n",
       "103                          0.0           2163.962184           0.986019   \n",
       "\n",
       "     Material_ID  Permeability_X [m^2]  ...  Total_molasses [M]  \\\n",
       "21           1.0          2.445674e-10  ...        2.223517e-06   \n",
       "32           1.0          2.445674e-10  ...        2.270490e-06   \n",
       "43           1.0          2.445674e-10  ...        2.240250e-06   \n",
       "54           1.0          2.445674e-10  ...        2.217294e-06   \n",
       "65           1.0          2.445674e-10  ...        2.208773e-06   \n",
       "..           ...                   ...  ...                 ...   \n",
       "98           1.0          2.352886e-10  ...        5.506438e-07   \n",
       "100          1.0          2.351712e-10  ...        5.562375e-07   \n",
       "101          1.0          2.350547e-10  ...        5.539103e-07   \n",
       "102          1.0          2.349423e-10  ...        5.538787e-07   \n",
       "103          1.0          2.348863e-10  ...        5.546674e-07   \n",
       "\n",
       "     biomass [mol_m^3]  molasses_im [mol_m^3]  sim_id  perm_covar  max_l  \\\n",
       "21        2.343395e-07               0.051616       0         1.4     14   \n",
       "32        9.441274e-07               0.052207       0         1.4     14   \n",
       "43        3.781464e-06               0.051820       0         1.4     14   \n",
       "54        1.500408e-05               0.051546       0         1.4     14   \n",
       "65        5.906743e-05               0.051442       0         1.4     14   \n",
       "..                 ...                    ...     ...         ...    ...   \n",
       "98        6.144571e+01               0.014858       0         1.4     14   \n",
       "100       6.223035e+01               0.015085       0         1.4     14   \n",
       "101       6.299632e+01               0.015025       0         1.4     14   \n",
       "102       6.373198e+01               0.015017       0         1.4     14   \n",
       "103       6.409972e+01               0.015055       0         1.4     14   \n",
       "\n",
       "     growth_rates        rate_c  rate_d    exp_b  \n",
       "21       0.000014  3.170410e-09     0.1  2.65939  \n",
       "32       0.000014  3.170410e-09     0.1  2.65939  \n",
       "43       0.000014  3.170410e-09     0.1  2.65939  \n",
       "54       0.000014  3.170410e-09     0.1  2.65939  \n",
       "65       0.000014  3.170410e-09     0.1  2.65939  \n",
       "..            ...           ...     ...      ...  \n",
       "98       0.000014  3.170410e-09     0.1  2.65939  \n",
       "100      0.000014  3.170410e-09     0.1  2.65939  \n",
       "101      0.000014  3.170410e-09     0.1  2.65939  \n",
       "102      0.000014  3.170410e-09     0.1  2.65939  \n",
       "103      0.000014  3.170410e-09     0.1  2.65939  \n",
       "\n",
       "[110 rows x 27 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2076073-03c8-4497-96c9-227050e25187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
